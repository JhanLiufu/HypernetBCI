{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Baseline Experiment 1\n",
    "\n",
    "Train model from scratch for each subject. \n",
    "\n",
    "Model: BSFShallowNet\n",
    "\n",
    "Dataset: BCI Competitin IV 2a, BCNI2014001 via MOABB library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.pick_types is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.pick_channels_regexp is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.channel_type is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\moabb\\pipelines\\__init__.py:26: ModuleNotFoundError: Tensorflow is not installed. You won't be able to use these MOABB pipelines if you attempt to do so.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from braindecode.datasets import MOABBDataset\n",
    "from numpy import multiply\n",
    "from braindecode.preprocessing import (Preprocessor,\n",
    "                                       exponential_moving_standardize,\n",
    "                                       preprocess)\n",
    "from braindecode.preprocessing import create_windows_from_events\n",
    "import torch\n",
    "from braindecode.models import ShallowFBCSPNet\n",
    "from braindecode.util import set_random_seeds\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "from braindecode import EEGClassifier\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import os\n",
    "import pickle\n",
    "from matplotlib.lines import Line2D\n",
    "# from braindecode.visualization import plot_confusion_matrix\n",
    "\n",
    "from braindecode.datasets import BaseConcatDataset\n",
    "from braindecode.datasets.base import EEGWindowsDataset\n",
    "from braindecode.preprocessing.windowers import _create_windows_from_events\n",
    "import numpy as np\n",
    "import mne\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing the data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = MOABBDataset(dataset_name=\"BNCI2014_001\", subject_ids=list(range(1, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\preprocessing\\preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
      "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<braindecode.datasets.moabb.MOABBDataset at 0x1afd3710c90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_cut_hz = 4.  # low cut frequency for filtering\n",
    "high_cut_hz = 38.  # high cut frequency for filtering\n",
    "# Parameters for exponential moving standardization\n",
    "factor_new = 1e-3\n",
    "init_block_size = 1000\n",
    "# Factor to convert from V to uV\n",
    "factor = 1e6\n",
    "\n",
    "preprocessors = [\n",
    "    Preprocessor('pick_types', eeg=True, meg=False, stim=False),  # Keep EEG sensors\n",
    "    Preprocessor(lambda data: multiply(data, factor)),  # Convert from V to uV\n",
    "    Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz),  # Bandpass filter\n",
    "    Preprocessor(exponential_moving_standardize,  # Exponential moving standardization\n",
    "                 factor_new=factor_new, init_block_size=init_block_size)\n",
    "]\n",
    "\n",
    "# Transform the data\n",
    "preprocess(dataset, preprocessors, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Compute Windows\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n"
     ]
    }
   ],
   "source": [
    "trial_start_offset_seconds = -0.5\n",
    "# Extract sampling frequency, check that they are same in all datasets\n",
    "sfreq = dataset.datasets[0].raw.info['sfreq']\n",
    "assert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])\n",
    "# Calculate the trial start offset in samples.\n",
    "trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
    "\n",
    "# Create windows using braindecode function for this. It needs parameters to define how\n",
    "# trials should be used.\n",
    "windows_dataset = create_windows_from_events(\n",
    "    dataset,\n",
    "    trial_start_offset_samples=trial_start_offset_samples,\n",
    "    trial_stop_offset_samples=0,\n",
    "    preload=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset(window_set, target_trial_num):\n",
    "    new_ds_lst = []\n",
    "    \n",
    "    for ds in window_set.datasets:\n",
    "        cur_run_trial_num = len(ds.metadata)\n",
    "        if target_trial_num > cur_run_trial_num:\n",
    "            new_ds_lst.append(ds)\n",
    "            target_trial_num -= cur_run_trial_num\n",
    "        else:\n",
    "            new_ds_lst.append(EEGWindowsDataset(ds.raw, ds.metadata[:target_trial_num], description=ds.description[:target_trial_num]))\n",
    "            break\n",
    "\n",
    "    return BaseConcatDataset(new_ds_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split dataset into pre-train set and fine-tune set\n",
    "splitted_by_subj = windows_dataset.split('subject')\n",
    "pre_train_subj = [1, 2, 3, 4, 5]\n",
    "fine_tune_set = [6, 7, 8, 9]\n",
    "pre_train_set = BaseConcatDataset([splitted_by_subj.get(f'{i}') for i in pre_train_subj])\n",
    "fine_tune_set = BaseConcatDataset([splitted_by_subj.get(f'{i}') for i in fine_tune_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split pre-train set into pre-train-train set and pre-train-test set\n",
    "pre_train_train_set_lst = []\n",
    "pre_train_test_set_lst = []\n",
    "pre_train_test_set_size = 1 # runs\n",
    "for key, val in pre_train_set.split('subject').items():\n",
    "    subj_splitted_lst_by_run = list(val.split('run').values())\n",
    "    pre_train_train_set_lst.extend(subj_splitted_lst_by_run[:-pre_train_test_set_size])\n",
    "    pre_train_test_set_lst.extend(subj_splitted_lst_by_run[-pre_train_test_set_size:])\n",
    "\n",
    "pre_train_train_set = BaseConcatDataset(pre_train_train_set_lst)\n",
    "pre_train_test_set = BaseConcatDataset(pre_train_test_set_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "if cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed = 20200220\n",
    "set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "n_classes = 4\n",
    "classes = list(range(n_classes))\n",
    "# Extract number of chans and time steps from dataset\n",
    "n_chans = windows_dataset[0][0].shape[0]\n",
    "input_window_samples = windows_dataset[0][0].shape[1]\n",
    "\n",
    "cur_model = ShallowFBCSPNet(\n",
    "    n_chans,\n",
    "    n_classes,\n",
    "    input_window_samples=input_window_samples,\n",
    "    final_conv_length='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.07 * 0.01\n",
    "weight_decay = 0\n",
    "batch_size = 64\n",
    "n_epochs = 30\n",
    "\n",
    "# Re-initialize EEGClassifier\n",
    "cur_clf = EEGClassifier(\n",
    "    cur_model,\n",
    "    criterion=torch.nn.NLLLoss,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    train_split=predefined_split(pre_train_test_set),  # using valid_set for validation\n",
    "    optimizer__lr=lr,\n",
    "    optimizer__weight_decay=weight_decay,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[\n",
    "        \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
    "    ],\n",
    "    device=device,\n",
    "    classes=classes,\n",
    "    warm_start=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3713\u001b[0m        \u001b[32m1.6246\u001b[0m       \u001b[35m0.3521\u001b[0m            \u001b[31m0.3521\u001b[0m        \u001b[94m1.4352\u001b[0m  0.0007  4.2605\n",
      "      2            \u001b[36m0.4529\u001b[0m        \u001b[32m1.4494\u001b[0m       \u001b[35m0.3875\u001b[0m            \u001b[31m0.3875\u001b[0m        \u001b[94m1.3038\u001b[0m  0.0007  5.0424\n",
      "      3            \u001b[36m0.4942\u001b[0m        \u001b[32m1.3483\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        1.3105  0.0007  4.6133\n",
      "      4            \u001b[36m0.5188\u001b[0m        \u001b[32m1.2806\u001b[0m       \u001b[35m0.4104\u001b[0m            \u001b[31m0.4104\u001b[0m        \u001b[94m1.2838\u001b[0m  0.0007  4.1747\n",
      "      5            \u001b[36m0.5679\u001b[0m        \u001b[32m1.2352\u001b[0m       \u001b[35m0.4333\u001b[0m            \u001b[31m0.4333\u001b[0m        \u001b[94m1.2467\u001b[0m  0.0007  4.5572\n",
      "      6            \u001b[36m0.5808\u001b[0m        \u001b[32m1.1860\u001b[0m       0.4000            0.4000        1.2539  0.0006  4.1781\n",
      "      7            \u001b[36m0.6212\u001b[0m        \u001b[32m1.1197\u001b[0m       \u001b[35m0.4458\u001b[0m            \u001b[31m0.4458\u001b[0m        \u001b[94m1.2110\u001b[0m  0.0006  4.3128\n",
      "      8            \u001b[36m0.6292\u001b[0m        \u001b[32m1.1099\u001b[0m       \u001b[35m0.4521\u001b[0m            \u001b[31m0.4521\u001b[0m        \u001b[94m1.2071\u001b[0m  0.0006  4.2630\n",
      "      9            \u001b[36m0.6338\u001b[0m        \u001b[32m1.0574\u001b[0m       \u001b[35m0.4667\u001b[0m            \u001b[31m0.4667\u001b[0m        1.2102  0.0006  4.2663\n",
      "     10            0.6317        \u001b[32m1.0264\u001b[0m       0.4396            0.4396        1.2111  0.0005  4.5788\n",
      "     11            \u001b[36m0.6496\u001b[0m        \u001b[32m1.0045\u001b[0m       0.4292            0.4292        1.2321  0.0005  4.1062\n",
      "     12            \u001b[36m0.6917\u001b[0m        \u001b[32m0.9544\u001b[0m       \u001b[35m0.4708\u001b[0m            \u001b[31m0.4708\u001b[0m        1.2125  0.0005  4.0948\n",
      "     13            \u001b[36m0.7129\u001b[0m        0.9824       \u001b[35m0.4813\u001b[0m            \u001b[31m0.4813\u001b[0m        \u001b[94m1.1746\u001b[0m  0.0004  4.7641\n",
      "     14            \u001b[36m0.7338\u001b[0m        \u001b[32m0.9268\u001b[0m       0.4604            0.4604        1.1758  0.0004  4.0802\n",
      "     15            0.7304        \u001b[32m0.8751\u001b[0m       0.4729            0.4729        1.1759  0.0004  4.1606\n",
      "     16            \u001b[36m0.7550\u001b[0m        \u001b[32m0.8714\u001b[0m       0.4750            0.4750        \u001b[94m1.1515\u001b[0m  0.0003  4.3897\n",
      "     17            0.7521        0.8750       0.4792            0.4792        1.1640  0.0003  4.0928\n",
      "     18            0.7504        \u001b[32m0.8517\u001b[0m       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        1.1713  0.0003  4.4010\n",
      "     19            \u001b[36m0.7800\u001b[0m        \u001b[32m0.8211\u001b[0m       \u001b[35m0.5021\u001b[0m            \u001b[31m0.5021\u001b[0m        1.1528  0.0002  4.0934\n",
      "     20            \u001b[36m0.7829\u001b[0m        \u001b[32m0.7947\u001b[0m       \u001b[35m0.5146\u001b[0m            \u001b[31m0.5146\u001b[0m        1.1516  0.0002  4.1738\n",
      "     21            0.7754        \u001b[32m0.7831\u001b[0m       0.4938            0.4938        1.1777  0.0002  4.8218\n",
      "     22            0.7804        0.8020       0.4917            0.4917        1.1595  0.0001  4.3629\n",
      "     23            \u001b[36m0.8083\u001b[0m        \u001b[32m0.7734\u001b[0m       0.4854            0.4854        \u001b[94m1.1286\u001b[0m  0.0001  4.3212\n",
      "     24            \u001b[36m0.8092\u001b[0m        \u001b[32m0.7619\u001b[0m       0.4813            0.4813        1.1292  0.0001  4.5983\n",
      "     25            \u001b[36m0.8108\u001b[0m        \u001b[32m0.7507\u001b[0m       0.5021            0.5021        1.1305  0.0001  4.2548\n",
      "     26            0.8071        \u001b[32m0.7370\u001b[0m       0.5042            0.5042        1.1299  0.0000  4.5377\n",
      "     27            0.8108        \u001b[32m0.7305\u001b[0m       0.5062            0.5062        1.1319  0.0000  4.2925\n",
      "     28            0.8108        0.7500       0.5062            0.5062        1.1311  0.0000  4.3285\n",
      "     29            0.8108        0.7429       0.5042            0.5042        1.1311  0.0000  4.8489\n",
      "     30            0.8092        0.7413       0.5042            0.5042        1.1312  0.0000  4.2913\n"
     ]
    }
   ],
   "source": [
    "_ = cur_clf.fit(pre_train_train_set, y=None, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'results'))\n",
    "exp_name = f'baseline_2_1_pretrain'\n",
    "cur_clf.save_params(f_params=os.path.join(results_dir, f'{exp_name}_model.pkl'), \n",
    "                    f_optimizer=os.path.join(results_dir, f'{exp_name}_opt.pkl'), \n",
    "                    f_history=os.path.join(results_dir, f'{exp_name}_history.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning\n",
    "Take the pre-trained model and fine tune (continue training) with new data from unseen subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before finetuning for subject 6, the baseline accuracy is 0.2708333333333333\n",
      "Fine tuning model for subject 6 with 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5667        1.6704       0.3438            0.3438        1.4737  0.0004  1.2052\n",
      "     32            0.6813        1.2084       0.3646            0.3646        1.6763  0.0005  1.4153\n",
      "     33            0.8042        0.9000       0.3438            0.3438        1.5461  0.0005  1.0939\n",
      "     34            \u001b[36m0.8271\u001b[0m        0.8191       0.4271            0.4271        1.6537  0.0006  1.0935\n",
      "     35            \u001b[36m0.8812\u001b[0m        \u001b[32m0.7112\u001b[0m       0.3854            0.3854        1.6616  0.0006  1.1568\n",
      "     36            \u001b[36m0.9167\u001b[0m        \u001b[32m0.6718\u001b[0m       0.4271            0.4271        1.6516  0.0007  1.1917\n",
      "     37            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5227\u001b[0m       0.3646            0.3646        1.8200  0.0007  1.3058\n",
      "     38            0.9417        \u001b[32m0.4759\u001b[0m       0.3333            0.3333        2.0015  0.0007  1.1712\n",
      "     39            \u001b[36m0.9688\u001b[0m        \u001b[32m0.4670\u001b[0m       0.3750            0.3750        1.8133  0.0007  1.4363\n",
      "     40            0.9563        \u001b[32m0.3842\u001b[0m       0.3646            0.3646        1.8657  0.0007  1.2278\n",
      "     41            0.9625        \u001b[32m0.3303\u001b[0m       0.3750            0.3750        1.9987  0.0007  1.3444\n",
      "     42            \u001b[36m0.9896\u001b[0m        \u001b[32m0.3246\u001b[0m       0.3854            0.3854        1.9706  0.0007  1.5845\n",
      "     43            \u001b[36m0.9917\u001b[0m        \u001b[32m0.3031\u001b[0m       0.3750            0.3750        1.9523  0.0006  1.1792\n",
      "     44            0.9875        \u001b[32m0.2623\u001b[0m       0.3854            0.3854        1.9440  0.0006  1.1475\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2302\u001b[0m       0.3646            0.3646        2.0170  0.0005  1.1623\n",
      "     46            1.0000        \u001b[32m0.2158\u001b[0m       0.3333            0.3333        2.0645  0.0005  1.2198\n",
      "     47            1.0000        \u001b[32m0.1982\u001b[0m       0.3542            0.3542        2.1554  0.0004  1.2591\n",
      "     48            1.0000        \u001b[32m0.1528\u001b[0m       0.3958            0.3958        2.1316  0.0004  1.1965\n",
      "     49            0.9979        0.1658       0.3646            0.3646        2.1195  0.0003  1.1577\n",
      "     50            1.0000        \u001b[32m0.1479\u001b[0m       0.3333            0.3333        2.1208  0.0003  1.1260\n",
      "Fine tuning model for subject 6 with 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4958        1.5463       0.3438            0.3438        1.6052  0.0004  1.0915\n",
      "     32            0.6562        1.1883       0.3542            0.3542        1.5101  0.0005  1.1720\n",
      "     33            0.7208        0.9818       0.3646            0.3646        1.5308  0.0005  1.0802\n",
      "     34            0.8042        0.8886       0.3958            0.3958        1.4613  0.0006  0.9280\n",
      "     35            \u001b[36m0.8667\u001b[0m        0.7818       0.3750            0.3750        1.5365  0.0006  0.9376\n",
      "     36            \u001b[36m0.9042\u001b[0m        \u001b[32m0.6373\u001b[0m       0.4167            0.4167        1.5868  0.0007  1.0437\n",
      "     37            \u001b[36m0.9313\u001b[0m        \u001b[32m0.5603\u001b[0m       0.4062            0.4062        1.5689  0.0007  0.9981\n",
      "     38            \u001b[36m0.9688\u001b[0m        \u001b[32m0.5385\u001b[0m       0.3646            0.3646        1.6480  0.0007  0.9221\n",
      "     39            \u001b[36m0.9750\u001b[0m        \u001b[32m0.4359\u001b[0m       0.3854            0.3854        1.6547  0.0007  0.9951\n",
      "     40            \u001b[36m0.9833\u001b[0m        \u001b[32m0.3786\u001b[0m       0.3646            0.3646        1.6893  0.0007  0.9436\n",
      "     41            0.9729        0.4137       0.4167            0.4167        1.5966  0.0007  0.9297\n",
      "     42            \u001b[36m0.9896\u001b[0m        \u001b[32m0.3439\u001b[0m       0.4167            0.4167        1.7178  0.0007  0.9063\n",
      "     43            \u001b[36m0.9979\u001b[0m        \u001b[32m0.2648\u001b[0m       0.3958            0.3958        1.7326  0.0006  1.0517\n",
      "     44            0.9979        \u001b[32m0.2608\u001b[0m       0.4062            0.4062        1.7462  0.0006  1.1401\n",
      "     45            0.9979        0.2650       0.4062            0.4062        1.7314  0.0005  1.1221\n",
      "     46            0.9958        \u001b[32m0.2093\u001b[0m       0.4062            0.4062        1.6960  0.0005  0.9271\n",
      "     47            0.9979        \u001b[32m0.1991\u001b[0m       0.3958            0.3958        1.7355  0.0004  0.9374\n",
      "     48            \u001b[36m1.0000\u001b[0m        0.2099       0.4271            0.4271        1.7288  0.0004  0.9685\n",
      "     49            1.0000        \u001b[32m0.1659\u001b[0m       0.4375            0.4375        1.7454  0.0003  1.3630\n",
      "     50            1.0000        0.1660       0.3958            0.3958        1.7893  0.0003  1.8903\n",
      "Fine tuning model for subject 6 with 60 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5042        1.6150       0.3333            0.3333        1.5637  0.0004  1.7271\n",
      "     32            0.6542        1.2568       0.3646            0.3646        1.5097  0.0005  1.6003\n",
      "     33            0.7542        0.9712       0.4375            0.4375        1.4471  0.0005  1.2773\n",
      "     34            0.7937        0.8334       0.3333            0.3333        1.5793  0.0006  1.1043\n",
      "     35            \u001b[36m0.8708\u001b[0m        0.7769       0.4062            0.4062        1.5180  0.0006  0.8901\n",
      "     36            \u001b[36m0.8833\u001b[0m        \u001b[32m0.6839\u001b[0m       0.3750            0.3750        1.5029  0.0007  0.8767\n",
      "     37            \u001b[36m0.9042\u001b[0m        \u001b[32m0.5949\u001b[0m       0.3646            0.3646        1.5383  0.0007  0.8771\n",
      "     38            \u001b[36m0.9104\u001b[0m        \u001b[32m0.5558\u001b[0m       0.3542            0.3542        1.6151  0.0007  0.8888\n",
      "     39            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5241\u001b[0m       0.3542            0.3542        1.5502  0.0007  0.8672\n",
      "     40            \u001b[36m0.9875\u001b[0m        \u001b[32m0.4569\u001b[0m       0.3438            0.3438        1.6166  0.0007  0.8756\n",
      "     41            0.9833        \u001b[32m0.4044\u001b[0m       0.3958            0.3958        1.6039  0.0007  0.8906\n",
      "     42            0.9792        \u001b[32m0.3614\u001b[0m       0.3542            0.3542        1.6717  0.0007  0.8911\n",
      "     43            \u001b[36m0.9917\u001b[0m        \u001b[32m0.3251\u001b[0m       0.3750            0.3750        1.6780  0.0006  0.8743\n",
      "     44            0.9917        \u001b[32m0.3011\u001b[0m       0.3854            0.3854        1.6387  0.0006  0.9637\n",
      "     45            \u001b[36m0.9979\u001b[0m        \u001b[32m0.2661\u001b[0m       0.3542            0.3542        1.6901  0.0005  0.9937\n",
      "     46            0.9958        0.2781       0.3646            0.3646        1.7205  0.0005  1.1867\n",
      "     47            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2507\u001b[0m       0.4062            0.4062        1.6938  0.0004  1.2220\n",
      "     48            1.0000        \u001b[32m0.2184\u001b[0m       0.4167            0.4167        1.6724  0.0004  1.0665\n",
      "     49            1.0000        0.2197       0.3750            0.3750        1.7153  0.0003  1.0864\n",
      "     50            1.0000        \u001b[32m0.2011\u001b[0m       0.3750            0.3750        1.7234  0.0003  1.1072\n",
      "Fine tuning model for subject 6 with 80 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5292        1.6085       0.3750            0.3750        1.5556  0.0004  0.9784\n",
      "     32            0.6104        1.2577       0.3646            0.3646        1.4311  0.0005  1.0201\n",
      "     33            0.7083        1.0420       0.4062            0.4062        1.4002  0.0005  1.0211\n",
      "     34            0.7604        0.9781       0.4167            0.4167        1.4601  0.0006  0.9039\n",
      "     35            \u001b[36m0.8292\u001b[0m        0.8267       0.3958            0.3958        1.4439  0.0006  0.9140\n",
      "     36            \u001b[36m0.8625\u001b[0m        \u001b[32m0.7115\u001b[0m       0.3958            0.3958        1.5555  0.0007  0.9430\n",
      "     37            \u001b[36m0.8833\u001b[0m        \u001b[32m0.6467\u001b[0m       0.3646            0.3646        1.5227  0.0007  1.1311\n",
      "     38            \u001b[36m0.9125\u001b[0m        \u001b[32m0.5876\u001b[0m       0.3438            0.3438        1.5497  0.0007  1.0568\n",
      "     39            \u001b[36m0.9542\u001b[0m        \u001b[32m0.5226\u001b[0m       0.3854            0.3854        1.5813  0.0007  1.1059\n",
      "     40            \u001b[36m0.9625\u001b[0m        \u001b[32m0.4982\u001b[0m       0.3438            0.3438        1.6181  0.0007  0.9261\n",
      "     41            \u001b[36m0.9729\u001b[0m        \u001b[32m0.4882\u001b[0m       0.3750            0.3750        1.5998  0.0007  0.9033\n",
      "     42            0.9688        \u001b[32m0.4204\u001b[0m       0.3542            0.3542        1.6533  0.0007  0.9764\n",
      "     43            \u001b[36m0.9896\u001b[0m        \u001b[32m0.3949\u001b[0m       0.3438            0.3438        1.6368  0.0006  1.0270\n",
      "     44            0.9812        \u001b[32m0.3495\u001b[0m       0.3646            0.3646        1.6395  0.0006  0.9151\n",
      "     45            \u001b[36m0.9917\u001b[0m        \u001b[32m0.3111\u001b[0m       0.3750            0.3750        1.6850  0.0005  0.8834\n",
      "     46            \u001b[36m0.9958\u001b[0m        0.3406       0.3438            0.3438        1.6857  0.0005  0.8884\n",
      "     47            \u001b[36m0.9979\u001b[0m        \u001b[32m0.2765\u001b[0m       0.3438            0.3438        1.7164  0.0004  0.9434\n",
      "     48            0.9979        \u001b[32m0.2647\u001b[0m       0.3542            0.3542        1.6853  0.0004  0.9336\n",
      "     49            0.9979        0.2703       0.4062            0.4062        1.6922  0.0003  1.1666\n",
      "     50            0.9979        \u001b[32m0.2447\u001b[0m       0.3646            0.3646        1.6834  0.0003  1.1492\n",
      "Fine tuning model for subject 6 with 100 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4313        1.5276       0.3125            0.3125        1.6392  0.0004  1.0366\n",
      "     32            0.5458        1.3120       0.3229            0.3229        1.6009  0.0005  0.8350\n",
      "     33            0.6312        1.1139       0.3438            0.3438        1.5527  0.0005  0.8708\n",
      "     34            0.7104        1.0101       0.3542            0.3542        1.4497  0.0006  0.8534\n",
      "     35            0.7562        0.9132       0.4167            0.4167        1.4222  0.0006  0.8439\n",
      "     36            0.7958        0.8350       0.3750            0.3750        1.4712  0.0007  0.8291\n",
      "     37            \u001b[36m0.8542\u001b[0m        \u001b[32m0.7050\u001b[0m       0.4271            0.4271        1.4699  0.0007  0.8445\n",
      "     38            \u001b[36m0.8958\u001b[0m        \u001b[32m0.6437\u001b[0m       0.3750            0.3750        1.4456  0.0007  0.8608\n",
      "     39            \u001b[36m0.9208\u001b[0m        \u001b[32m0.6103\u001b[0m       0.4062            0.4062        1.4902  0.0007  0.8733\n",
      "     40            \u001b[36m0.9250\u001b[0m        \u001b[32m0.5586\u001b[0m       0.4062            0.4062        1.5179  0.0007  0.8613\n",
      "     41            \u001b[36m0.9417\u001b[0m        \u001b[32m0.5566\u001b[0m       0.4271            0.4271        1.5325  0.0007  0.9065\n",
      "     42            \u001b[36m0.9521\u001b[0m        \u001b[32m0.4727\u001b[0m       0.3854            0.3854        1.4924  0.0007  1.0174\n",
      "     43            0.9417        \u001b[32m0.4508\u001b[0m       0.3958            0.3958        1.5413  0.0006  0.9744\n",
      "     44            \u001b[36m0.9750\u001b[0m        \u001b[32m0.4233\u001b[0m       0.3958            0.3958        1.5665  0.0006  0.9684\n",
      "     45            \u001b[36m0.9771\u001b[0m        \u001b[32m0.3974\u001b[0m       0.4062            0.4062        1.5512  0.0005  0.8737\n",
      "     46            \u001b[36m0.9812\u001b[0m        \u001b[32m0.3698\u001b[0m       0.4271            0.4271        1.5934  0.0005  0.8725\n",
      "     47            0.9792        \u001b[32m0.3480\u001b[0m       0.4062            0.4062        1.6163  0.0004  0.8783\n",
      "     48            \u001b[36m0.9854\u001b[0m        \u001b[32m0.3415\u001b[0m       0.3958            0.3958        1.5894  0.0004  0.9213\n",
      "     49            0.9854        \u001b[32m0.3085\u001b[0m       0.4167            0.4167        1.5889  0.0003  0.8596\n",
      "     50            \u001b[36m0.9875\u001b[0m        0.3374       0.4375            0.4375        1.5832  0.0003  0.8595\n",
      "Fine tuning model for subject 6 with 120 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4146        1.5915       0.3542            0.3542        1.6983  0.0004  0.8828\n",
      "     32            0.5396        1.3330       0.3333            0.3333        1.6213  0.0005  0.8912\n",
      "     33            0.6104        1.1453       0.3542            0.3542        1.5606  0.0005  0.9218\n",
      "     34            0.6771        0.9703       0.4062            0.4062        1.4913  0.0006  0.9988\n",
      "     35            0.7604        0.9131       0.3646            0.3646        1.4763  0.0006  1.0458\n",
      "     36            0.8042        0.8096       0.3646            0.3646        1.4504  0.0007  1.0932\n",
      "     37            \u001b[36m0.8333\u001b[0m        \u001b[32m0.6869\u001b[0m       0.3438            0.3438        1.4837  0.0007  0.9377\n",
      "     38            \u001b[36m0.8792\u001b[0m        0.6941       0.4062            0.4062        1.4644  0.0007  0.9526\n",
      "     39            \u001b[36m0.8854\u001b[0m        \u001b[32m0.6348\u001b[0m       0.4271            0.4271        1.4705  0.0007  0.8740\n",
      "     40            \u001b[36m0.9167\u001b[0m        \u001b[32m0.5802\u001b[0m       0.4271            0.4271        1.4377  0.0007  0.8767\n",
      "     41            0.9146        \u001b[32m0.5518\u001b[0m       0.3750            0.3750        1.5115  0.0007  0.9231\n",
      "     42            \u001b[36m0.9479\u001b[0m        \u001b[32m0.4899\u001b[0m       0.3958            0.3958        1.5230  0.0007  0.8767\n",
      "     43            \u001b[36m0.9542\u001b[0m        \u001b[32m0.4826\u001b[0m       0.3958            0.3958        1.5067  0.0006  0.9220\n",
      "     44            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4174\u001b[0m       0.3646            0.3646        1.5556  0.0006  0.9232\n",
      "     45            \u001b[36m0.9750\u001b[0m        0.4255       0.4167            0.4167        1.5705  0.0005  0.8383\n",
      "     46            0.9708        \u001b[32m0.3917\u001b[0m       0.4167            0.4167        1.5452  0.0005  0.8941\n",
      "     47            \u001b[36m0.9875\u001b[0m        \u001b[32m0.3503\u001b[0m       0.3958            0.3958        1.5521  0.0004  0.9485\n",
      "     48            \u001b[36m0.9896\u001b[0m        \u001b[32m0.3466\u001b[0m       0.4062            0.4062        1.5450  0.0004  0.9453\n",
      "     49            0.9896        \u001b[32m0.3436\u001b[0m       0.4167            0.4167        1.5211  0.0003  1.1153\n",
      "     50            \u001b[36m0.9917\u001b[0m        \u001b[32m0.3114\u001b[0m       0.3854            0.3854        1.5324  0.0003  0.8599\n",
      "Fine tuning model for subject 6 with 140 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4354        1.6633       0.3125            0.3125        1.6374  0.0004  0.8062\n",
      "     32            0.5125        1.3944       0.3229            0.3229        1.6282  0.0005  0.7768\n",
      "     33            0.5687        1.2213       0.3750            0.3750        1.5812  0.0005  0.8083\n",
      "     34            0.6792        1.0347       0.3854            0.3854        1.4837  0.0006  0.8042\n",
      "     35            0.7167        0.9215       0.3958            0.3958        1.4466  0.0006  0.7976\n",
      "     36            0.7792        0.8933       0.3750            0.3750        1.4778  0.0007  0.8013\n",
      "     37            0.8104        0.7539       0.4062            0.4062        1.5364  0.0007  0.8024\n",
      "     38            \u001b[36m0.8479\u001b[0m        0.7735       0.3958            0.3958        1.5362  0.0007  0.7812\n",
      "     39            0.8313        \u001b[32m0.6785\u001b[0m       0.4271            0.4271        1.5625  0.0007  0.8046\n",
      "     40            \u001b[36m0.9104\u001b[0m        \u001b[32m0.6327\u001b[0m       0.3750            0.3750        1.5068  0.0007  0.8266\n",
      "     41            \u001b[36m0.9208\u001b[0m        \u001b[32m0.6002\u001b[0m       0.3854            0.3854        1.5477  0.0007  0.8729\n",
      "     42            0.9125        \u001b[32m0.5417\u001b[0m       0.4271            0.4271        1.5855  0.0007  0.9008\n",
      "     43            \u001b[36m0.9354\u001b[0m        \u001b[32m0.4820\u001b[0m       0.4167            0.4167        1.5247  0.0006  0.9711\n",
      "     44            \u001b[36m0.9458\u001b[0m        \u001b[32m0.4660\u001b[0m       0.3854            0.3854        1.5606  0.0006  0.8862\n",
      "     45            \u001b[36m0.9542\u001b[0m        \u001b[32m0.4380\u001b[0m       0.3958            0.3958        1.5638  0.0005  0.8243\n",
      "     46            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4299\u001b[0m       0.3542            0.3542        1.5335  0.0005  0.8850\n",
      "     47            \u001b[36m0.9688\u001b[0m        0.4361       0.3854            0.3854        1.5296  0.0004  0.7908\n",
      "     48            0.9646        \u001b[32m0.4185\u001b[0m       0.4167            0.4167        1.5478  0.0004  0.8225\n",
      "     49            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3639\u001b[0m       0.4062            0.4062        1.5787  0.0003  0.8607\n",
      "     50            0.9750        \u001b[32m0.3386\u001b[0m       0.4375            0.4375        1.5917  0.0003  0.8582\n",
      "Fine tuning model for subject 6 with 160 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4250        1.6577       0.3021            0.3021        1.6256  0.0004  0.8394\n",
      "     32            0.5229        1.4119       0.3229            0.3229        1.5544  0.0005  0.8456\n",
      "     33            0.6042        1.1890       0.3646            0.3646        1.5200  0.0005  0.7950\n",
      "     34            0.6646        1.0188       0.3854            0.3854        1.4748  0.0006  0.8802\n",
      "     35            0.7312        0.9572       0.4167            0.4167        1.4337  0.0006  0.9376\n",
      "     36            0.7708        0.8321       0.4062            0.4062        1.4876  0.0007  0.9712\n",
      "     37            0.7896        0.7943       0.3958            0.3958        1.5183  0.0007  0.8855\n",
      "     38            \u001b[36m0.8354\u001b[0m        0.7521       0.3750            0.3750        1.5516  0.0007  0.8594\n",
      "     39            \u001b[36m0.8500\u001b[0m        \u001b[32m0.6938\u001b[0m       0.3958            0.3958        1.5577  0.0007  0.8281\n",
      "     40            \u001b[36m0.8958\u001b[0m        \u001b[32m0.6252\u001b[0m       0.4062            0.4062        1.4966  0.0007  0.8124\n",
      "     41            \u001b[36m0.9208\u001b[0m        \u001b[32m0.5866\u001b[0m       0.4062            0.4062        1.4939  0.0007  0.7983\n",
      "     42            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5494\u001b[0m       0.4167            0.4167        1.4921  0.0007  0.7878\n",
      "     43            \u001b[36m0.9354\u001b[0m        \u001b[32m0.5457\u001b[0m       0.3958            0.3958        1.5675  0.0006  0.8221\n",
      "     44            0.9333        \u001b[32m0.4929\u001b[0m       0.3958            0.3958        1.5418  0.0006  0.8344\n",
      "     45            \u001b[36m0.9375\u001b[0m        \u001b[32m0.4229\u001b[0m       0.3958            0.3958        1.5249  0.0005  0.7953\n",
      "     46            \u001b[36m0.9625\u001b[0m        0.4448       0.3854            0.3854        1.5232  0.0005  0.8443\n",
      "     47            \u001b[36m0.9688\u001b[0m        \u001b[32m0.4075\u001b[0m       0.4167            0.4167        1.5085  0.0004  0.8597\n",
      "     48            \u001b[36m0.9708\u001b[0m        \u001b[32m0.3950\u001b[0m       0.4167            0.4167        1.5189  0.0004  1.0626\n",
      "     49            0.9667        0.4176       0.4167            0.4167        1.5436  0.0003  0.9691\n",
      "     50            \u001b[36m0.9729\u001b[0m        \u001b[32m0.3509\u001b[0m       0.4271            0.4271        1.5413  0.0003  1.0002\n",
      "Fine tuning model for subject 6 with 180 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4396        1.6415       0.3438            0.3438        1.5994  0.0004  0.8271\n",
      "     32            0.4875        1.3567       0.3333            0.3333        1.6385  0.0005  0.8334\n",
      "     33            0.5958        1.1900       0.3438            0.3438        1.5633  0.0005  0.7859\n",
      "     34            0.6708        1.0619       0.3958            0.3958        1.4666  0.0006  0.8441\n",
      "     35            0.7167        0.9503       0.3646            0.3646        1.4613  0.0006  0.8441\n",
      "     36            0.7646        0.8283       0.3750            0.3750        1.5145  0.0007  0.9075\n",
      "     37            0.8021        0.8056       0.3646            0.3646        1.5189  0.0007  0.8844\n",
      "     38            \u001b[36m0.8521\u001b[0m        0.7749       0.3958            0.3958        1.4708  0.0007  0.8284\n",
      "     39            \u001b[36m0.8854\u001b[0m        \u001b[32m0.6734\u001b[0m       0.3958            0.3958        1.5298  0.0007  0.8124\n",
      "     40            \u001b[36m0.8979\u001b[0m        \u001b[32m0.6150\u001b[0m       0.4062            0.4062        1.5566  0.0007  0.8271\n",
      "     41            \u001b[36m0.9396\u001b[0m        \u001b[32m0.5666\u001b[0m       0.3958            0.3958        1.4986  0.0007  0.9703\n",
      "     42            \u001b[36m0.9417\u001b[0m        \u001b[32m0.5490\u001b[0m       0.3958            0.3958        1.5267  0.0007  1.0330\n",
      "     43            \u001b[36m0.9479\u001b[0m        \u001b[32m0.5072\u001b[0m       0.4167            0.4167        1.5526  0.0006  1.0188\n",
      "     44            \u001b[36m0.9521\u001b[0m        \u001b[32m0.4545\u001b[0m       0.3958            0.3958        1.5798  0.0006  0.8096\n",
      "     45            \u001b[36m0.9563\u001b[0m        0.4639       0.3854            0.3854        1.5648  0.0005  0.8315\n",
      "     46            \u001b[36m0.9708\u001b[0m        \u001b[32m0.4284\u001b[0m       0.4167            0.4167        1.5099  0.0005  0.7920\n",
      "     47            \u001b[36m0.9792\u001b[0m        0.4392       0.4062            0.4062        1.5275  0.0004  0.8063\n",
      "     48            \u001b[36m0.9812\u001b[0m        \u001b[32m0.3757\u001b[0m       0.3958            0.3958        1.5442  0.0004  0.8288\n",
      "     49            0.9812        0.3797       0.3750            0.3750        1.5335  0.0003  0.8468\n",
      "     50            \u001b[36m0.9833\u001b[0m        \u001b[32m0.3585\u001b[0m       0.3854            0.3854        1.5439  0.0003  0.8273\n",
      "Fine tuning model for subject 6 with 200 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4042        1.6675       0.3125            0.3125        1.6238  0.0004  0.8138\n",
      "     32            0.5271        1.3631       0.3438            0.3438        1.5975  0.0005  0.8130\n",
      "     33            0.6083        1.1346       0.4062            0.4062        1.4533  0.0005  0.8472\n",
      "     34            0.6750        1.0682       0.4271            0.4271        1.4581  0.0006  0.9682\n",
      "     35            0.7125        0.9114       0.3958            0.3958        1.4931  0.0006  0.9301\n",
      "     36            0.7604        0.8442       0.3542            0.3542        1.5015  0.0007  1.0220\n",
      "     37            0.7979        0.7489       0.3750            0.3750        1.5073  0.0007  0.9700\n",
      "     38            \u001b[36m0.8313\u001b[0m        \u001b[32m0.7186\u001b[0m       0.4271            0.4271        1.5033  0.0007  0.8833\n",
      "     39            \u001b[36m0.8854\u001b[0m        \u001b[32m0.7073\u001b[0m       0.4062            0.4062        1.4841  0.0007  0.7896\n",
      "     40            \u001b[36m0.9104\u001b[0m        \u001b[32m0.6463\u001b[0m       0.3854            0.3854        1.4658  0.0007  0.7889\n",
      "     41            0.9083        \u001b[32m0.5831\u001b[0m       0.4479            0.4479        1.4810  0.0007  0.8130\n",
      "     42            \u001b[36m0.9313\u001b[0m        \u001b[32m0.5353\u001b[0m       0.4479            0.4479        1.4876  0.0007  0.8713\n",
      "     43            \u001b[36m0.9542\u001b[0m        \u001b[32m0.5117\u001b[0m       0.3542            0.3542        1.4961  0.0006  0.7919\n",
      "     44            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4824\u001b[0m       0.3750            0.3750        1.5119  0.0006  0.8634\n",
      "     45            \u001b[36m0.9771\u001b[0m        \u001b[32m0.4524\u001b[0m       0.3750            0.3750        1.5284  0.0005  0.7993\n",
      "     46            0.9729        0.4741       0.4167            0.4167        1.5445  0.0005  0.7743\n",
      "     47            0.9750        \u001b[32m0.4104\u001b[0m       0.3854            0.3854        1.5389  0.0004  0.9846\n",
      "     48            0.9771        \u001b[32m0.3835\u001b[0m       0.3854            0.3854        1.5194  0.0004  0.9891\n",
      "     49            \u001b[36m0.9792\u001b[0m        0.3847       0.3854            0.3854        1.5205  0.0003  0.9506\n",
      "     50            0.9792        0.3940       0.3958            0.3958        1.5225  0.0003  0.7939\n",
      "Fine tuning model for subject 6 with 220 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4188        1.6585       0.3438            0.3438        1.5836  0.0004  0.8025\n",
      "     32            0.4979        1.4020       0.3333            0.3333        1.5499  0.0005  0.7651\n",
      "     33            0.5958        1.1835       0.3958            0.3958        1.4854  0.0005  0.8089\n",
      "     34            0.6750        1.0533       0.3854            0.3854        1.4231  0.0006  0.7959\n",
      "     35            0.7312        0.8990       0.3854            0.3854        1.4337  0.0006  0.7657\n",
      "     36            0.7604        0.8881       0.3854            0.3854        1.4708  0.0007  0.8539\n",
      "     37            0.8083        0.8014       0.4062            0.4062        1.4402  0.0007  0.7986\n",
      "     38            \u001b[36m0.8417\u001b[0m        \u001b[32m0.7292\u001b[0m       0.3854            0.3854        1.4698  0.0007  0.7649\n",
      "     39            \u001b[36m0.8542\u001b[0m        \u001b[32m0.6913\u001b[0m       0.3646            0.3646        1.4904  0.0007  0.8291\n",
      "     40            \u001b[36m0.8792\u001b[0m        \u001b[32m0.6232\u001b[0m       0.4271            0.4271        1.4845  0.0007  0.7734\n",
      "     41            \u001b[36m0.9146\u001b[0m        \u001b[32m0.6213\u001b[0m       0.4271            0.4271        1.4944  0.0007  0.9040\n",
      "     42            \u001b[36m0.9333\u001b[0m        \u001b[32m0.6015\u001b[0m       0.4062            0.4062        1.4905  0.0007  0.9212\n",
      "     43            0.9271        \u001b[32m0.5525\u001b[0m       0.4167            0.4167        1.5278  0.0006  0.9461\n",
      "     44            \u001b[36m0.9437\u001b[0m        \u001b[32m0.4729\u001b[0m       0.4062            0.4062        1.5231  0.0006  0.8910\n",
      "     45            \u001b[36m0.9625\u001b[0m        \u001b[32m0.4506\u001b[0m       0.3854            0.3854        1.5317  0.0005  0.8761\n",
      "     46            \u001b[36m0.9708\u001b[0m        0.4656       0.3958            0.3958        1.5283  0.0005  0.8117\n",
      "     47            0.9688        \u001b[32m0.4317\u001b[0m       0.4062            0.4062        1.5576  0.0004  0.7981\n",
      "     48            \u001b[36m0.9771\u001b[0m        \u001b[32m0.4049\u001b[0m       0.3958            0.3958        1.5323  0.0004  0.8052\n",
      "     49            \u001b[36m0.9854\u001b[0m        \u001b[32m0.3912\u001b[0m       0.3958            0.3958        1.5157  0.0003  0.7947\n",
      "     50            \u001b[36m0.9875\u001b[0m        \u001b[32m0.3507\u001b[0m       0.4062            0.4062        1.5253  0.0003  0.8017\n",
      "Fine tuning model for subject 6 with 240 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4188        1.5986       0.3229            0.3229        1.6034  0.0004  0.8674\n",
      "     32            0.5167        1.3697       0.3438            0.3438        1.5906  0.0005  0.7953\n",
      "     33            0.5958        1.1386       0.3750            0.3750        1.5187  0.0005  0.7803\n",
      "     34            0.6500        1.0545       0.3854            0.3854        1.4498  0.0006  0.8269\n",
      "     35            0.7188        0.9292       0.4271            0.4271        1.4295  0.0006  0.9789\n",
      "     36            0.7646        0.8658       0.3958            0.3958        1.4506  0.0007  0.9096\n",
      "     37            0.8021        0.7681       0.3646            0.3646        1.4708  0.0007  0.9729\n",
      "     38            \u001b[36m0.8292\u001b[0m        0.7375       0.4167            0.4167        1.4920  0.0007  0.8173\n",
      "     39            \u001b[36m0.8708\u001b[0m        \u001b[32m0.7206\u001b[0m       0.4479            0.4479        1.4822  0.0007  0.7933\n",
      "     40            0.8521        \u001b[32m0.6162\u001b[0m       0.4375            0.4375        1.5180  0.0007  0.8038\n",
      "     41            \u001b[36m0.9208\u001b[0m        \u001b[32m0.6046\u001b[0m       0.4167            0.4167        1.5011  0.0007  0.7883\n",
      "     42            \u001b[36m0.9292\u001b[0m        \u001b[32m0.5521\u001b[0m       0.3750            0.3750        1.5343  0.0007  0.7922\n",
      "     43            0.9167        \u001b[32m0.5205\u001b[0m       0.3854            0.3854        1.5661  0.0006  0.7804\n",
      "     44            \u001b[36m0.9542\u001b[0m        \u001b[32m0.4645\u001b[0m       0.3646            0.3646        1.5180  0.0006  0.7788\n",
      "     45            0.9521        0.4765       0.3333            0.3333        1.5122  0.0005  0.9542\n",
      "     46            \u001b[36m0.9688\u001b[0m        \u001b[32m0.4280\u001b[0m       0.3542            0.3542        1.5151  0.0005  0.7969\n",
      "     47            \u001b[36m0.9771\u001b[0m        \u001b[32m0.4149\u001b[0m       0.4167            0.4167        1.5177  0.0004  0.8589\n",
      "     48            \u001b[36m0.9812\u001b[0m        0.4165       0.4062            0.4062        1.5318  0.0004  1.2377\n",
      "     49            0.9708        \u001b[32m0.3810\u001b[0m       0.4062            0.4062        1.5566  0.0003  1.1548\n",
      "     50            0.9729        \u001b[32m0.3714\u001b[0m       0.4062            0.4062        1.5575  0.0003  1.0932\n",
      "Fine tuning model for subject 6 with 260 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4250        1.6192       0.3125            0.3125        1.6402  0.0004  0.8024\n",
      "     32            0.5396        1.3813       0.3229            0.3229        1.6004  0.0005  0.8078\n",
      "     33            0.6333        1.1959       0.3125            0.3125        1.5359  0.0005  0.8170\n",
      "     34            0.6333        0.9788       0.3438            0.3438        1.5216  0.0006  0.8189\n",
      "     35            0.7125        0.9673       0.3958            0.3958        1.4493  0.0006  0.8256\n",
      "     36            0.7917        0.8597       0.3958            0.3958        1.4425  0.0007  0.8022\n",
      "     37            \u001b[36m0.8313\u001b[0m        0.7603       0.4375            0.4375        1.4537  0.0007  0.8094\n",
      "     38            0.8083        \u001b[32m0.6934\u001b[0m       0.4375            0.4375        1.4986  0.0007  0.8116\n",
      "     39            \u001b[36m0.8917\u001b[0m        \u001b[32m0.6663\u001b[0m       0.4167            0.4167        1.4823  0.0007  0.7974\n",
      "     40            \u001b[36m0.9062\u001b[0m        \u001b[32m0.6488\u001b[0m       0.3542            0.3542        1.5355  0.0007  0.7967\n",
      "     41            \u001b[36m0.9292\u001b[0m        \u001b[32m0.5986\u001b[0m       0.3646            0.3646        1.5710  0.0007  0.9048\n",
      "     42            \u001b[36m0.9313\u001b[0m        \u001b[32m0.5526\u001b[0m       0.3542            0.3542        1.5617  0.0007  0.9219\n",
      "     43            \u001b[36m0.9521\u001b[0m        \u001b[32m0.4961\u001b[0m       0.3958            0.3958        1.5705  0.0006  0.9374\n",
      "     44            \u001b[36m0.9563\u001b[0m        \u001b[32m0.4838\u001b[0m       0.3854            0.3854        1.5759  0.0006  0.9223\n",
      "     45            \u001b[36m0.9583\u001b[0m        \u001b[32m0.4352\u001b[0m       0.3958            0.3958        1.5845  0.0005  0.8444\n",
      "     46            \u001b[36m0.9604\u001b[0m        0.4515       0.4062            0.4062        1.6114  0.0005  0.7932\n",
      "     47            \u001b[36m0.9708\u001b[0m        0.4457       0.4167            0.4167        1.5818  0.0004  0.9213\n",
      "     48            \u001b[36m0.9729\u001b[0m        \u001b[32m0.3792\u001b[0m       0.3958            0.3958        1.5809  0.0004  0.9750\n",
      "     49            \u001b[36m0.9812\u001b[0m        \u001b[32m0.3545\u001b[0m       0.3958            0.3958        1.5928  0.0003  0.8130\n",
      "     50            0.9792        \u001b[32m0.3534\u001b[0m       0.3854            0.3854        1.6040  0.0003  0.8229\n",
      "Fine tuning model for subject 6 with 280 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4125        1.6749       0.3333            0.3333        1.6323  0.0004  0.7976\n",
      "     32            0.5250        1.4048       0.3229            0.3229        1.6145  0.0005  0.8282\n",
      "     33            0.6000        1.2121       0.3646            0.3646        1.5453  0.0005  0.8181\n",
      "     34            0.6896        1.0458       0.3958            0.3958        1.4665  0.0006  0.8515\n",
      "     35            0.7438        0.9274       0.4062            0.4062        1.4425  0.0006  0.9789\n",
      "     36            0.7833        0.8996       0.4479            0.4479        1.4458  0.0007  0.9472\n",
      "     37            \u001b[36m0.8313\u001b[0m        0.7892       0.4167            0.4167        1.4260  0.0007  0.9344\n",
      "     38            \u001b[36m0.8542\u001b[0m        \u001b[32m0.6784\u001b[0m       0.3958            0.3958        1.4409  0.0007  0.7850\n",
      "     39            \u001b[36m0.8792\u001b[0m        0.7133       0.4167            0.4167        1.4823  0.0007  0.8051\n",
      "     40            \u001b[36m0.8917\u001b[0m        \u001b[32m0.6043\u001b[0m       0.3958            0.3958        1.5022  0.0007  0.8271\n",
      "     41            \u001b[36m0.9146\u001b[0m        \u001b[32m0.5850\u001b[0m       0.3958            0.3958        1.4940  0.0007  0.8037\n",
      "     42            \u001b[36m0.9208\u001b[0m        \u001b[32m0.5228\u001b[0m       0.4062            0.4062        1.5440  0.0007  0.8218\n",
      "     43            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5045\u001b[0m       0.4271            0.4271        1.5619  0.0006  0.8164\n",
      "     44            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4715\u001b[0m       0.3438            0.3438        1.5402  0.0006  0.8166\n",
      "     45            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4542\u001b[0m       0.3438            0.3438        1.5100  0.0005  0.7877\n",
      "     46            0.9625        0.4623       0.4062            0.4062        1.5308  0.0005  0.8030\n",
      "     47            \u001b[36m0.9708\u001b[0m        \u001b[32m0.4163\u001b[0m       0.3854            0.3854        1.5268  0.0004  0.7898\n",
      "     48            \u001b[36m0.9729\u001b[0m        \u001b[32m0.3854\u001b[0m       0.4375            0.4375        1.5342  0.0004  0.8937\n",
      "     49            \u001b[36m0.9812\u001b[0m        \u001b[32m0.3692\u001b[0m       0.3854            0.3854        1.5408  0.0003  1.0058\n",
      "     50            0.9792        \u001b[32m0.3682\u001b[0m       0.3854            0.3854        1.5495  0.0003  0.9724\n",
      "Fine tuning model for subject 6 with 300 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3896        1.6306       0.3646            0.3646        1.5918  0.0004  0.7747\n",
      "     32            0.5104        1.4355       0.3646            0.3646        1.5690  0.0005  0.7961\n",
      "     33            0.6083        1.1406       0.3229            0.3229        1.4863  0.0005  0.8039\n",
      "     34            0.6771        1.0318       0.4167            0.4167        1.4447  0.0006  0.8001\n",
      "     35            0.7229        0.9817       0.4167            0.4167        1.4046  0.0006  0.7818\n",
      "     36            0.7438        0.8425       0.4062            0.4062        1.4070  0.0007  0.8234\n",
      "     37            0.7771        0.8071       0.3854            0.3854        1.4663  0.0007  0.7629\n",
      "     38            \u001b[36m0.8396\u001b[0m        0.7374       0.3854            0.3854        1.4612  0.0007  0.7984\n",
      "     39            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6820\u001b[0m       0.3854            0.3854        1.4609  0.0007  0.7920\n",
      "     40            \u001b[36m0.9125\u001b[0m        \u001b[32m0.6587\u001b[0m       0.3958            0.3958        1.4430  0.0007  0.7779\n",
      "     41            0.8896        \u001b[32m0.5757\u001b[0m       0.3542            0.3542        1.4799  0.0007  0.7901\n",
      "     42            \u001b[36m0.9292\u001b[0m        \u001b[32m0.5461\u001b[0m       0.4062            0.4062        1.4657  0.0007  0.9549\n",
      "     43            \u001b[36m0.9479\u001b[0m        \u001b[32m0.5081\u001b[0m       0.3750            0.3750        1.4374  0.0006  0.8782\n",
      "     44            \u001b[36m0.9542\u001b[0m        \u001b[32m0.4680\u001b[0m       0.4167            0.4167        1.4789  0.0006  0.9796\n",
      "     45            \u001b[36m0.9646\u001b[0m        0.5088       0.3854            0.3854        1.5155  0.0005  0.8084\n",
      "     46            \u001b[36m0.9688\u001b[0m        \u001b[32m0.4232\u001b[0m       0.4062            0.4062        1.5546  0.0005  0.7636\n",
      "     47            \u001b[36m0.9729\u001b[0m        0.4285       0.4062            0.4062        1.5440  0.0004  0.7645\n",
      "     48            \u001b[36m0.9812\u001b[0m        \u001b[32m0.4069\u001b[0m       0.4062            0.4062        1.5461  0.0004  0.7940\n",
      "     49            \u001b[36m0.9854\u001b[0m        \u001b[32m0.3677\u001b[0m       0.4167            0.4167        1.5392  0.0003  0.8345\n",
      "     50            0.9812        \u001b[32m0.3439\u001b[0m       0.4062            0.4062        1.5259  0.0003  0.7664\n",
      "Fine tuning model for subject 6 with 320 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4042        1.6377       0.3229            0.3229        1.6240  0.0004  0.7638\n",
      "     32            0.5417        1.3883       0.3229            0.3229        1.5690  0.0005  0.7638\n",
      "     33            0.5938        1.1265       0.3854            0.3854        1.5168  0.0005  0.7764\n",
      "     34            0.6500        1.0599       0.3646            0.3646        1.5132  0.0006  0.7702\n",
      "     35            0.7104        0.9261       0.3958            0.3958        1.5137  0.0006  0.7594\n",
      "     36            0.7708        0.8636       0.3958            0.3958        1.5039  0.0007  0.9225\n",
      "     37            0.7958        0.7876       0.3646            0.3646        1.5620  0.0007  0.9044\n",
      "     38            0.8083        0.7670       0.3958            0.3958        1.6046  0.0007  0.9364\n",
      "     39            0.7688        \u001b[32m0.6879\u001b[0m       0.4479            0.4479        1.5905  0.0007  0.7905\n",
      "     40            \u001b[36m0.8833\u001b[0m        \u001b[32m0.6463\u001b[0m       0.4375            0.4375        1.4768  0.0007  0.7952\n",
      "     41            \u001b[36m0.9250\u001b[0m        \u001b[32m0.5911\u001b[0m       0.4167            0.4167        1.5214  0.0007  0.8159\n",
      "     42            \u001b[36m0.9292\u001b[0m        \u001b[32m0.5527\u001b[0m       0.3854            0.3854        1.5561  0.0007  0.8089\n",
      "     43            \u001b[36m0.9354\u001b[0m        \u001b[32m0.5274\u001b[0m       0.3958            0.3958        1.5705  0.0006  0.8196\n",
      "     44            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5251\u001b[0m       0.3958            0.3958        1.5332  0.0006  0.8564\n",
      "     45            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4813\u001b[0m       0.4167            0.4167        1.5757  0.0005  0.8308\n",
      "     46            0.9646        \u001b[32m0.4653\u001b[0m       0.4167            0.4167        1.5911  0.0005  0.8214\n",
      "     47            0.9625        \u001b[32m0.3945\u001b[0m       0.3854            0.3854        1.5588  0.0004  0.8812\n",
      "     48            \u001b[36m0.9688\u001b[0m        0.4004       0.3958            0.3958        1.5541  0.0004  0.8392\n",
      "     49            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3782\u001b[0m       0.3958            0.3958        1.5489  0.0003  0.8790\n",
      "     50            \u001b[36m0.9833\u001b[0m        \u001b[32m0.3461\u001b[0m       0.4062            0.4062        1.5436  0.0003  0.9838\n",
      "Fine tuning model for subject 6 with 340 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4083        1.6080       0.3438            0.3438        1.5928  0.0004  1.0347\n",
      "     32            0.4938        1.3365       0.2917            0.2917        1.5980  0.0005  0.8135\n",
      "     33            0.6083        1.2052       0.3542            0.3542        1.4948  0.0005  0.8606\n",
      "     34            0.6771        1.0346       0.4167            0.4167        1.4642  0.0006  0.8174\n",
      "     35            0.7354        0.9261       0.3750            0.3750        1.4426  0.0006  0.8427\n",
      "     36            0.8021        0.8756       0.3646            0.3646        1.4412  0.0007  0.8905\n",
      "     37            \u001b[36m0.8167\u001b[0m        0.7866       0.4062            0.4062        1.4577  0.0007  0.8373\n",
      "     38            \u001b[36m0.8396\u001b[0m        \u001b[32m0.7159\u001b[0m       0.4479            0.4479        1.4896  0.0007  0.7892\n",
      "     39            \u001b[36m0.8583\u001b[0m        \u001b[32m0.6943\u001b[0m       0.3750            0.3750        1.5075  0.0007  0.9481\n",
      "     40            \u001b[36m0.8771\u001b[0m        \u001b[32m0.6343\u001b[0m       0.3542            0.3542        1.4861  0.0007  0.8540\n",
      "     41            \u001b[36m0.9083\u001b[0m        \u001b[32m0.5709\u001b[0m       0.3646            0.3646        1.4736  0.0007  0.8844\n",
      "     42            \u001b[36m0.9437\u001b[0m        0.5733       0.3958            0.3958        1.4900  0.0007  0.9078\n",
      "     43            0.9375        \u001b[32m0.5424\u001b[0m       0.4167            0.4167        1.4699  0.0006  0.9473\n",
      "     44            0.9396        \u001b[32m0.4880\u001b[0m       0.3646            0.3646        1.5149  0.0006  0.9125\n",
      "     45            \u001b[36m0.9563\u001b[0m        \u001b[32m0.4368\u001b[0m       0.3750            0.3750        1.5273  0.0005  0.9271\n",
      "     46            \u001b[36m0.9708\u001b[0m        0.4436       0.3750            0.3750        1.5313  0.0005  0.7899\n",
      "     47            \u001b[36m0.9729\u001b[0m        \u001b[32m0.3985\u001b[0m       0.3438            0.3438        1.5334  0.0004  0.8132\n",
      "     48            0.9729        \u001b[32m0.3832\u001b[0m       0.3438            0.3438        1.5411  0.0004  0.7933\n",
      "     49            \u001b[36m0.9792\u001b[0m        \u001b[32m0.3741\u001b[0m       0.3750            0.3750        1.5614  0.0003  0.7680\n",
      "     50            \u001b[36m0.9896\u001b[0m        \u001b[32m0.3584\u001b[0m       0.3646            0.3646        1.5749  0.0003  0.8442\n",
      "Fine tuning model for subject 6 with 360 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4167        1.6554       0.3125            0.3125        1.6678  0.0004  0.8289\n",
      "     32            0.5208        1.3538       0.3333            0.3333        1.6125  0.0005  0.8324\n",
      "     33            0.6042        1.1509       0.3542            0.3542        1.5525  0.0005  0.7841\n",
      "     34            0.6604        1.0030       0.3750            0.3750        1.4636  0.0006  0.7867\n",
      "     35            0.7312        0.9413       0.4062            0.4062        1.4352  0.0006  0.8018\n",
      "     36            0.7708        0.8300       0.4167            0.4167        1.4319  0.0007  0.9609\n",
      "     37            \u001b[36m0.8125\u001b[0m        0.8375       0.4167            0.4167        1.4733  0.0007  0.8862\n",
      "     38            \u001b[36m0.8354\u001b[0m        0.7314       0.3958            0.3958        1.4838  0.0007  0.9636\n",
      "     39            \u001b[36m0.8646\u001b[0m        \u001b[32m0.7153\u001b[0m       0.3854            0.3854        1.5422  0.0007  0.8913\n",
      "     40            0.8542        \u001b[32m0.6538\u001b[0m       0.4167            0.4167        1.6210  0.0007  0.8599\n",
      "     41            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6043\u001b[0m       0.3958            0.3958        1.5886  0.0007  0.9771\n",
      "     42            \u001b[36m0.9271\u001b[0m        \u001b[32m0.5090\u001b[0m       0.3854            0.3854        1.5805  0.0007  0.7886\n",
      "     43            \u001b[36m0.9500\u001b[0m        0.5141       0.3958            0.3958        1.5648  0.0006  0.8429\n",
      "     44            \u001b[36m0.9542\u001b[0m        \u001b[32m0.4917\u001b[0m       0.3750            0.3750        1.5656  0.0006  0.8669\n",
      "     45            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4691\u001b[0m       0.4167            0.4167        1.5762  0.0005  0.7825\n",
      "     46            \u001b[36m0.9750\u001b[0m        \u001b[32m0.4494\u001b[0m       0.3750            0.3750        1.5748  0.0005  0.8683\n",
      "     47            0.9583        \u001b[32m0.4109\u001b[0m       0.3750            0.3750        1.5963  0.0004  0.9829\n",
      "     48            0.9750        \u001b[32m0.3873\u001b[0m       0.3854            0.3854        1.6010  0.0004  0.8281\n",
      "     49            0.9729        \u001b[32m0.3716\u001b[0m       0.3958            0.3958        1.6008  0.0003  0.9062\n",
      "     50            0.9750        \u001b[32m0.3602\u001b[0m       0.3854            0.3854        1.6147  0.0003  1.0000\n",
      "Fine tuning model for subject 6 with 380 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4146        1.6866       0.3646            0.3646        1.6126  0.0004  1.1094\n",
      "     32            0.5083        1.3313       0.3438            0.3438        1.6245  0.0005  0.8125\n",
      "     33            0.6062        1.1287       0.3958            0.3958        1.5138  0.0005  0.8437\n",
      "     34            0.6521        1.0287       0.4167            0.4167        1.5281  0.0006  0.8750\n",
      "     35            0.6958        0.9109       0.4062            0.4062        1.5222  0.0006  0.8750\n",
      "     36            0.7458        0.8599       0.4062            0.4062        1.4935  0.0007  0.8282\n",
      "     37            0.8104        0.7773       0.4167            0.4167        1.4808  0.0007  0.8438\n",
      "     38            \u001b[36m0.8688\u001b[0m        0.7528       0.4375            0.4375        1.4573  0.0007  0.8438\n",
      "     39            0.8688        \u001b[32m0.6911\u001b[0m       0.4167            0.4167        1.5274  0.0007  0.8281\n",
      "     40            \u001b[36m0.8958\u001b[0m        \u001b[32m0.6375\u001b[0m       0.4062            0.4062        1.4878  0.0007  0.8437\n",
      "     41            \u001b[36m0.9167\u001b[0m        \u001b[32m0.6098\u001b[0m       0.3958            0.3958        1.4813  0.0007  0.8906\n",
      "     42            \u001b[36m0.9187\u001b[0m        \u001b[32m0.5716\u001b[0m       0.4271            0.4271        1.4826  0.0007  0.9531\n",
      "     43            \u001b[36m0.9292\u001b[0m        \u001b[32m0.4647\u001b[0m       0.4167            0.4167        1.4852  0.0006  0.9375\n",
      "     44            \u001b[36m0.9458\u001b[0m        \u001b[32m0.4581\u001b[0m       0.3854            0.3854        1.4948  0.0006  1.1094\n",
      "     45            \u001b[36m0.9542\u001b[0m        0.4960       0.4062            0.4062        1.5107  0.0005  0.8906\n",
      "     46            \u001b[36m0.9625\u001b[0m        \u001b[32m0.4261\u001b[0m       0.3958            0.3958        1.5254  0.0005  0.8906\n",
      "     47            \u001b[36m0.9729\u001b[0m        \u001b[32m0.4150\u001b[0m       0.4062            0.4062        1.5212  0.0004  0.9375\n",
      "     48            \u001b[36m0.9771\u001b[0m        \u001b[32m0.3882\u001b[0m       0.4167            0.4167        1.5199  0.0004  0.8750\n",
      "     49            \u001b[36m0.9812\u001b[0m        \u001b[32m0.3529\u001b[0m       0.3958            0.3958        1.5356  0.0003  0.9219\n",
      "     50            \u001b[36m0.9833\u001b[0m        0.3735       0.3854            0.3854        1.5391  0.0003  0.8437\n",
      "Fine tuning model for subject 6 with 400 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4250        1.6754       0.3229            0.3229        1.6275  0.0004  0.8281\n",
      "     32            0.5167        1.3324       0.3542            0.3542        1.5671  0.0005  0.8281\n",
      "     33            0.5958        1.1969       0.3542            0.3542        1.4990  0.0005  0.8281\n",
      "     34            0.6438        1.0386       0.3750            0.3750        1.4434  0.0006  0.9380\n",
      "     35            0.7375        0.9365       0.4375            0.4375        1.3930  0.0006  0.9531\n",
      "     36            0.7875        0.8548       0.4271            0.4271        1.4090  0.0007  0.9844\n",
      "     37            \u001b[36m0.8187\u001b[0m        0.7752       0.4479            0.4479        1.4365  0.0007  1.0469\n",
      "     38            \u001b[36m0.8646\u001b[0m        \u001b[32m0.6763\u001b[0m       0.4062            0.4062        1.4549  0.0007  0.9555\n",
      "     39            \u001b[36m0.8688\u001b[0m        0.7056       0.4062            0.4062        1.4833  0.0007  0.8932\n",
      "     40            \u001b[36m0.8979\u001b[0m        \u001b[32m0.5912\u001b[0m       0.4062            0.4062        1.4924  0.0007  0.9131\n",
      "     41            0.8958        0.6071       0.3958            0.3958        1.5326  0.0007  0.8786\n",
      "     42            \u001b[36m0.9354\u001b[0m        \u001b[32m0.5737\u001b[0m       0.3958            0.3958        1.5289  0.0007  0.9270\n",
      "     43            \u001b[36m0.9542\u001b[0m        \u001b[32m0.5042\u001b[0m       0.4167            0.4167        1.4865  0.0006  0.9329\n",
      "     44            \u001b[36m0.9646\u001b[0m        \u001b[32m0.4668\u001b[0m       0.4271            0.4271        1.4875  0.0006  0.9078\n",
      "     45            0.9604        \u001b[32m0.4372\u001b[0m       0.4062            0.4062        1.4781  0.0005  0.8594\n",
      "     46            \u001b[36m0.9729\u001b[0m        0.4465       0.4062            0.4062        1.5015  0.0005  0.7974\n",
      "     47            \u001b[36m0.9771\u001b[0m        \u001b[32m0.4108\u001b[0m       0.4167            0.4167        1.5363  0.0004  0.9374\n",
      "     48            \u001b[36m0.9833\u001b[0m        0.4174       0.4167            0.4167        1.5568  0.0004  1.0713\n",
      "     49            0.9833        \u001b[32m0.3597\u001b[0m       0.4167            0.4167        1.5690  0.0003  1.1333\n",
      "     50            \u001b[36m0.9896\u001b[0m        \u001b[32m0.3548\u001b[0m       0.4062            0.4062        1.5650  0.0003  0.9456\n",
      "Fine tuning model for subject 6 with 420 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4104        1.6672       0.3438            0.3438        1.5846  0.0004  0.9180\n",
      "     32            0.5208        1.3818       0.3333            0.3333        1.6023  0.0005  0.8249\n",
      "     33            0.6146        1.1568       0.3333            0.3333        1.5036  0.0005  0.8099\n",
      "     34            0.6750        1.0304       0.3750            0.3750        1.4509  0.0006  1.0113\n",
      "     35            0.7271        0.9316       0.4375            0.4375        1.3992  0.0006  0.8026\n",
      "     36            0.7667        0.8523       0.4167            0.4167        1.4075  0.0007  0.8651\n",
      "     37            0.8042        0.7850       0.4062            0.4062        1.4764  0.0007  0.7938\n",
      "     38            \u001b[36m0.8333\u001b[0m        \u001b[32m0.7171\u001b[0m       0.3854            0.3854        1.5282  0.0007  0.8527\n",
      "     39            \u001b[36m0.8833\u001b[0m        \u001b[32m0.6460\u001b[0m       0.3854            0.3854        1.5310  0.0007  0.8809\n",
      "     40            \u001b[36m0.8958\u001b[0m        \u001b[32m0.6352\u001b[0m       0.3854            0.3854        1.5262  0.0007  1.0296\n",
      "     41            \u001b[36m0.9167\u001b[0m        \u001b[32m0.6321\u001b[0m       0.4062            0.4062        1.4958  0.0007  1.1206\n",
      "     42            \u001b[36m0.9229\u001b[0m        \u001b[32m0.5728\u001b[0m       0.3958            0.3958        1.5526  0.0007  1.3696\n",
      "     43            \u001b[36m0.9437\u001b[0m        \u001b[32m0.5312\u001b[0m       0.3854            0.3854        1.5317  0.0006  0.9588\n",
      "     44            \u001b[36m0.9563\u001b[0m        \u001b[32m0.4574\u001b[0m       0.3854            0.3854        1.5189  0.0006  0.8454\n",
      "     45            \u001b[36m0.9708\u001b[0m        0.4777       0.4271            0.4271        1.5287  0.0005  0.8589\n",
      "     46            0.9708        \u001b[32m0.4197\u001b[0m       0.4167            0.4167        1.5619  0.0005  0.7978\n",
      "     47            \u001b[36m0.9750\u001b[0m        \u001b[32m0.4138\u001b[0m       0.4271            0.4271        1.5731  0.0004  0.8447\n",
      "     48            0.9729        \u001b[32m0.4038\u001b[0m       0.4271            0.4271        1.5771  0.0004  0.7772\n",
      "     49            \u001b[36m0.9812\u001b[0m        \u001b[32m0.3899\u001b[0m       0.4271            0.4271        1.5744  0.0003  0.8939\n",
      "     50            \u001b[36m0.9833\u001b[0m        0.3938       0.4167            0.4167        1.5840  0.0003  0.8880\n",
      "Fine tuning model for subject 6 with 440 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.5470       0.3438            0.3438        1.6303  0.0004  0.9012\n",
      "     32            0.4979        1.3956       0.3125            0.3125        1.7105  0.0005  0.9911\n",
      "     33            0.6062        1.1862       0.3333            0.3333        1.5705  0.0005  1.1759\n",
      "     34            0.6813        1.0010       0.3854            0.3854        1.4741  0.0006  0.9602\n",
      "     35            0.7208        0.9036       0.4062            0.4062        1.4622  0.0006  0.9419\n",
      "     36            0.7583        0.8741       0.4062            0.4062        1.4612  0.0007  0.8341\n",
      "     37            \u001b[36m0.8187\u001b[0m        0.7975       0.3854            0.3854        1.5058  0.0007  0.7854\n",
      "     38            \u001b[36m0.8354\u001b[0m        \u001b[32m0.7247\u001b[0m       0.4062            0.4062        1.4884  0.0007  0.7973\n",
      "     39            \u001b[36m0.8792\u001b[0m        \u001b[32m0.6609\u001b[0m       0.4271            0.4271        1.4747  0.0007  0.7973\n",
      "     40            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6473\u001b[0m       0.3854            0.3854        1.5064  0.0007  0.7985\n",
      "     41            \u001b[36m0.9187\u001b[0m        \u001b[32m0.5761\u001b[0m       0.4062            0.4062        1.5537  0.0007  0.7742\n",
      "     42            \u001b[36m0.9229\u001b[0m        \u001b[32m0.5414\u001b[0m       0.3854            0.3854        1.5629  0.0007  0.8731\n",
      "     43            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5379\u001b[0m       0.3854            0.3854        1.5412  0.0006  0.8383\n",
      "     44            0.9417        \u001b[32m0.5053\u001b[0m       0.3750            0.3750        1.5632  0.0006  0.8648\n",
      "     45            \u001b[36m0.9563\u001b[0m        \u001b[32m0.4872\u001b[0m       0.3854            0.3854        1.5983  0.0005  0.9226\n",
      "     46            0.9500        \u001b[32m0.4818\u001b[0m       0.3958            0.3958        1.5893  0.0005  1.1599\n",
      "     47            \u001b[36m0.9750\u001b[0m        \u001b[32m0.4203\u001b[0m       0.4271            0.4271        1.5415  0.0004  1.1699\n",
      "     48            \u001b[36m0.9792\u001b[0m        \u001b[32m0.4109\u001b[0m       0.4062            0.4062        1.5321  0.0004  0.8337\n",
      "     49            0.9792        \u001b[32m0.4008\u001b[0m       0.3646            0.3646        1.5762  0.0003  0.8168\n",
      "     50            0.9771        \u001b[32m0.3964\u001b[0m       0.3854            0.3854        1.6024  0.0003  0.7875\n",
      "Fine tuning model for subject 6 with 460 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4313        1.5974       0.3438            0.3438        1.6186  0.0004  0.8254\n",
      "     32            0.4979        1.3602       0.3438            0.3438        1.6739  0.0005  0.8051\n",
      "     33            0.6104        1.1585       0.3646            0.3646        1.5151  0.0005  0.8748\n",
      "     34            0.6646        1.0845       0.3958            0.3958        1.4183  0.0006  0.8803\n",
      "     35            0.7063        0.9624       0.4062            0.4062        1.4027  0.0006  0.7914\n",
      "     36            0.7854        0.8319       0.3750            0.3750        1.4019  0.0007  0.7969\n",
      "     37            \u001b[36m0.8333\u001b[0m        0.8127       0.3854            0.3854        1.4314  0.0007  0.8073\n",
      "     38            0.8292        \u001b[32m0.6814\u001b[0m       0.4479            0.4479        1.4781  0.0007  1.0236\n",
      "     39            \u001b[36m0.8750\u001b[0m        \u001b[32m0.6760\u001b[0m       0.4167            0.4167        1.4978  0.0007  1.0708\n",
      "     40            \u001b[36m0.8979\u001b[0m        \u001b[32m0.6195\u001b[0m       0.4062            0.4062        1.5008  0.0007  1.0266\n",
      "     41            \u001b[36m0.9083\u001b[0m        \u001b[32m0.5659\u001b[0m       0.3958            0.3958        1.5057  0.0007  0.8471\n",
      "     42            \u001b[36m0.9125\u001b[0m        0.5676       0.4167            0.4167        1.4885  0.0007  0.7873\n",
      "     43            \u001b[36m0.9354\u001b[0m        \u001b[32m0.5195\u001b[0m       0.4167            0.4167        1.4878  0.0006  0.7880\n",
      "     44            \u001b[36m0.9437\u001b[0m        \u001b[32m0.4632\u001b[0m       0.4167            0.4167        1.4945  0.0006  0.7956\n",
      "     45            \u001b[36m0.9500\u001b[0m        0.4666       0.4375            0.4375        1.5215  0.0005  0.8186\n",
      "     46            \u001b[36m0.9625\u001b[0m        0.4675       0.3854            0.3854        1.5454  0.0005  0.7866\n",
      "     47            \u001b[36m0.9729\u001b[0m        \u001b[32m0.4529\u001b[0m       0.3958            0.3958        1.5146  0.0004  0.8704\n",
      "     48            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3927\u001b[0m       0.3854            0.3854        1.5268  0.0004  0.8258\n",
      "     49            \u001b[36m0.9771\u001b[0m        0.4012       0.3958            0.3958        1.5255  0.0003  0.7850\n",
      "     50            \u001b[36m0.9833\u001b[0m        \u001b[32m0.3714\u001b[0m       0.4062            0.4062        1.5383  0.0003  0.8116\n",
      "Before finetuning for subject 7, the baseline accuracy is 0.34375\n",
      "Fine tuning model for subject 7 with 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6875        1.5629       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        1.1667  0.0004  1.6116\n",
      "     32            0.7438        0.9721       0.4688            0.4688        1.3264  0.0005  1.3972\n",
      "     33            \u001b[36m0.8500\u001b[0m        0.8613       0.4792            0.4792        1.2239  0.0005  1.3719\n",
      "     34            \u001b[36m0.9292\u001b[0m        \u001b[32m0.6430\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0486\u001b[0m  0.0006  1.2500\n",
      "     35            0.9125        \u001b[32m0.5316\u001b[0m       0.5417            0.5417        1.1405  0.0006  1.3599\n",
      "     36            \u001b[36m0.9688\u001b[0m        \u001b[32m0.4124\u001b[0m       0.5729            0.5729        1.0519  0.0007  1.3700\n",
      "     37            \u001b[36m0.9854\u001b[0m        \u001b[32m0.3653\u001b[0m       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.9381\u001b[0m  0.0007  1.3843\n",
      "     38            0.9750        \u001b[32m0.3097\u001b[0m       0.6458            0.6458        \u001b[94m0.9270\u001b[0m  0.0007  1.2749\n",
      "     39            0.9396        \u001b[32m0.2731\u001b[0m       0.5625            0.5625        1.2349  0.0007  1.3024\n",
      "     40            \u001b[36m0.9917\u001b[0m        \u001b[32m0.2236\u001b[0m       0.5938            0.5938        1.0553  0.0007  1.6293\n",
      "     41            0.9792        \u001b[32m0.2017\u001b[0m       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        1.0192  0.0007  1.4063\n",
      "     42            \u001b[36m0.9958\u001b[0m        \u001b[32m0.1822\u001b[0m       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        0.9952  0.0007  1.2656\n",
      "     43            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1430\u001b[0m       0.6562            0.6562        1.0065  0.0006  1.2504\n",
      "     44            1.0000        \u001b[32m0.1297\u001b[0m       0.6667            0.6667        0.9664  0.0006  1.1875\n",
      "     45            0.9979        \u001b[32m0.1088\u001b[0m       0.6042            0.6042        0.9620  0.0005  1.2031\n",
      "     46            1.0000        \u001b[32m0.0863\u001b[0m       0.6562            0.6562        0.9495  0.0005  1.2344\n",
      "     47            1.0000        \u001b[32m0.0749\u001b[0m       0.6667            0.6667        0.9931  0.0004  1.2031\n",
      "     48            1.0000        0.0782       \u001b[35m0.6771\u001b[0m            \u001b[31m0.6771\u001b[0m        0.9564  0.0004  1.2344\n",
      "     49            1.0000        \u001b[32m0.0628\u001b[0m       0.6562            0.6562        0.9605  0.0003  2.0858\n",
      "     50            1.0000        0.0631       0.6771            0.6771        0.9689  0.0003  1.8662\n",
      "Fine tuning model for subject 7 with 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6104        1.5120       0.4479            0.4479        1.3373  0.0004  1.2863\n",
      "     32            0.7042        1.0409       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        1.2342  0.0005  1.0862\n",
      "     33            0.7750        0.8223       0.5104            0.5104        1.2585  0.0005  1.2786\n",
      "     34            \u001b[36m0.9167\u001b[0m        \u001b[32m0.6376\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.1589  0.0006  1.2055\n",
      "     35            \u001b[36m0.9229\u001b[0m        \u001b[32m0.5442\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.1516  0.0006  1.1571\n",
      "     36            \u001b[36m0.9646\u001b[0m        \u001b[32m0.4639\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0699\u001b[0m  0.0007  0.9722\n",
      "     37            \u001b[36m0.9729\u001b[0m        \u001b[32m0.4106\u001b[0m       0.5417            0.5417        \u001b[94m1.0577\u001b[0m  0.0007  1.0778\n",
      "     38            \u001b[36m0.9792\u001b[0m        \u001b[32m0.3360\u001b[0m       0.5938            0.5938        1.1043  0.0007  1.4360\n",
      "     39            \u001b[36m0.9938\u001b[0m        \u001b[32m0.3014\u001b[0m       0.5938            0.5938        \u001b[94m0.9256\u001b[0m  0.0007  1.6571\n",
      "     40            \u001b[36m0.9958\u001b[0m        \u001b[32m0.2585\u001b[0m       0.5833            0.5833        1.0858  0.0007  1.0719\n",
      "     41            0.9854        \u001b[32m0.2062\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        0.9408  0.0007  1.3380\n",
      "     42            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1788\u001b[0m       0.6354            0.6354        \u001b[94m0.8722\u001b[0m  0.0007  0.9653\n",
      "     43            0.9979        \u001b[32m0.1778\u001b[0m       0.6042            0.6042        1.0584  0.0006  1.0053\n",
      "     44            1.0000        \u001b[32m0.1339\u001b[0m       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        0.9327  0.0006  0.9412\n",
      "     45            1.0000        \u001b[32m0.1276\u001b[0m       0.6146            0.6146        1.0031  0.0005  1.0114\n",
      "     46            1.0000        \u001b[32m0.1208\u001b[0m       \u001b[35m0.7292\u001b[0m            \u001b[31m0.7292\u001b[0m        0.9169  0.0005  0.9760\n",
      "     47            0.9979        \u001b[32m0.0976\u001b[0m       0.6875            0.6875        0.9025  0.0004  1.1107\n",
      "     48            1.0000        \u001b[32m0.0920\u001b[0m       0.6875            0.6875        0.9135  0.0004  1.4644\n",
      "     49            1.0000        \u001b[32m0.0900\u001b[0m       0.7188            0.7188        0.9130  0.0003  1.5744\n",
      "     50            1.0000        0.0936       0.7292            0.7292        0.9087  0.0003  1.7113\n",
      "Fine tuning model for subject 7 with 60 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5333        1.4868       0.4271            0.4271        1.3497  0.0004  2.1751\n",
      "     32            0.7271        1.0975       0.4688            0.4688        1.2418  0.0005  2.0331\n",
      "     33            \u001b[36m0.8229\u001b[0m        0.8767       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        \u001b[94m1.1250\u001b[0m  0.0005  1.0575\n",
      "     34            \u001b[36m0.8604\u001b[0m        0.7551       0.4792            0.4792        1.2079  0.0006  1.9641\n",
      "     35            \u001b[36m0.8917\u001b[0m        \u001b[32m0.6562\u001b[0m       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        1.1864  0.0006  1.0129\n",
      "     36            0.8771        \u001b[32m0.5452\u001b[0m       0.5000            0.5000        1.2138  0.0007  1.2531\n",
      "     37            \u001b[36m0.9688\u001b[0m        \u001b[32m0.4987\u001b[0m       0.5000            0.5000        \u001b[94m1.0240\u001b[0m  0.0007  1.2794\n",
      "     38            \u001b[36m0.9708\u001b[0m        \u001b[32m0.4109\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9953\u001b[0m  0.0007  1.1292\n",
      "     39            \u001b[36m0.9833\u001b[0m        \u001b[32m0.3368\u001b[0m       0.5729            0.5729        1.0832  0.0007  0.9925\n",
      "     40            \u001b[36m0.9854\u001b[0m        \u001b[32m0.2930\u001b[0m       0.5417            0.5417        1.0929  0.0007  0.9426\n",
      "     41            \u001b[36m0.9917\u001b[0m        \u001b[32m0.2811\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.9953\u001b[0m  0.0007  0.9071\n",
      "     42            \u001b[36m0.9958\u001b[0m        \u001b[32m0.2243\u001b[0m       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.9718\u001b[0m  0.0007  0.9085\n",
      "     43            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1932\u001b[0m       0.6146            0.6146        1.0015  0.0006  0.9157\n",
      "     44            1.0000        0.1956       0.6458            0.6458        \u001b[94m0.9208\u001b[0m  0.0006  0.8946\n",
      "     45            1.0000        \u001b[32m0.1720\u001b[0m       \u001b[35m0.6771\u001b[0m            \u001b[31m0.6771\u001b[0m        \u001b[94m0.8987\u001b[0m  0.0005  0.8940\n",
      "     46            1.0000        \u001b[32m0.1412\u001b[0m       0.6146            0.6146        1.0459  0.0005  0.8968\n",
      "     47            1.0000        \u001b[32m0.1209\u001b[0m       0.6667            0.6667        0.9407  0.0004  0.9092\n",
      "     48            1.0000        0.1360       0.6667            0.6667        \u001b[94m0.8906\u001b[0m  0.0004  1.1141\n",
      "     49            1.0000        \u001b[32m0.1138\u001b[0m       0.6667            0.6667        0.9023  0.0003  1.0656\n",
      "     50            1.0000        \u001b[32m0.0935\u001b[0m       \u001b[35m0.6979\u001b[0m            \u001b[31m0.6979\u001b[0m        0.8948  0.0003  1.2528\n",
      "Fine tuning model for subject 7 with 80 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5646        1.5153       0.4375            0.4375        1.2478  0.0004  0.8784\n",
      "     32            0.6750        1.1410       0.4896            0.4896        1.2042  0.0005  0.8773\n",
      "     33            0.7500        0.9378       0.5104            0.5104        1.1750  0.0005  0.8778\n",
      "     34            \u001b[36m0.8458\u001b[0m        0.8184       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.1352  0.0006  0.8786\n",
      "     35            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6996\u001b[0m       0.5417            0.5417        \u001b[94m1.0850\u001b[0m  0.0006  0.9094\n",
      "     36            \u001b[36m0.9167\u001b[0m        \u001b[32m0.5802\u001b[0m       0.5625            0.5625        \u001b[94m1.0790\u001b[0m  0.0007  0.8926\n",
      "     37            \u001b[36m0.9604\u001b[0m        \u001b[32m0.5126\u001b[0m       0.5729            0.5729        \u001b[94m1.0428\u001b[0m  0.0007  0.8773\n",
      "     38            0.9229        \u001b[32m0.4645\u001b[0m       0.5521            0.5521        1.1339  0.0007  0.8799\n",
      "     39            \u001b[36m0.9792\u001b[0m        \u001b[32m0.3893\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0098\u001b[0m  0.0007  1.0743\n",
      "     40            \u001b[36m0.9875\u001b[0m        \u001b[32m0.3317\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9872\u001b[0m  0.0007  1.1686\n",
      "     41            0.9875        \u001b[32m0.3281\u001b[0m       0.5625            0.5625        \u001b[94m0.9852\u001b[0m  0.0007  1.0875\n",
      "     42            \u001b[36m0.9917\u001b[0m        \u001b[32m0.2586\u001b[0m       0.5417            0.5417        1.0157  0.0007  1.4472\n",
      "     43            \u001b[36m0.9938\u001b[0m        \u001b[32m0.2351\u001b[0m       0.5521            0.5521        1.0017  0.0006  0.9693\n",
      "     44            0.9938        \u001b[32m0.2147\u001b[0m       0.6146            0.6146        \u001b[94m0.9697\u001b[0m  0.0006  1.0230\n",
      "     45            0.9938        \u001b[32m0.1905\u001b[0m       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        0.9799  0.0005  1.2903\n",
      "     46            \u001b[36m0.9979\u001b[0m        0.1936       0.6146            0.6146        1.0112  0.0005  0.9833\n",
      "     47            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1648\u001b[0m       0.6146            0.6146        \u001b[94m0.9531\u001b[0m  0.0004  0.8677\n",
      "     48            1.0000        0.1713       0.5938            0.5938        1.0117  0.0004  1.2736\n",
      "     49            1.0000        \u001b[32m0.1373\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        0.9546  0.0003  0.9634\n",
      "     50            1.0000        \u001b[32m0.1294\u001b[0m       0.6250            0.6250        \u001b[94m0.9280\u001b[0m  0.0003  0.9937\n",
      "Fine tuning model for subject 7 with 100 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4688        1.4496       0.4583            0.4583        1.2287  0.0004  1.0742\n",
      "     32            0.6250        1.2074       0.4792            0.4792        1.2093  0.0005  0.9765\n",
      "     33            0.7292        1.0017       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        1.1410  0.0005  1.2023\n",
      "     34            0.7833        0.8301       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        \u001b[94m1.0624\u001b[0m  0.0006  0.8873\n",
      "     35            \u001b[36m0.8417\u001b[0m        0.7628       0.5312            0.5312        1.1586  0.0006  0.9486\n",
      "     36            \u001b[36m0.8792\u001b[0m        \u001b[32m0.6917\u001b[0m       0.5208            0.5208        1.1142  0.0007  0.8606\n",
      "     37            \u001b[36m0.9146\u001b[0m        \u001b[32m0.6004\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.1106  0.0007  0.8650\n",
      "     38            \u001b[36m0.9313\u001b[0m        \u001b[32m0.5004\u001b[0m       0.5312            0.5312        1.1014  0.0007  0.9139\n",
      "     39            \u001b[36m0.9583\u001b[0m        \u001b[32m0.4848\u001b[0m       0.5521            0.5521        \u001b[94m1.0557\u001b[0m  0.0007  0.8871\n",
      "     40            \u001b[36m0.9625\u001b[0m        \u001b[32m0.4327\u001b[0m       0.5625            0.5625        \u001b[94m1.0535\u001b[0m  0.0007  1.0037\n",
      "     41            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3760\u001b[0m       0.5625            0.5625        \u001b[94m1.0133\u001b[0m  0.0007  1.0692\n",
      "     42            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3397\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m0.9979\u001b[0m  0.0007  1.0065\n",
      "     43            \u001b[36m0.9792\u001b[0m        \u001b[32m0.3046\u001b[0m       0.5729            0.5729        \u001b[94m0.9863\u001b[0m  0.0006  0.9732\n",
      "     44            \u001b[36m0.9875\u001b[0m        \u001b[32m0.2826\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9705\u001b[0m  0.0006  0.9647\n",
      "     45            0.9875        \u001b[32m0.2673\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        1.0017  0.0005  1.0578\n",
      "     46            \u001b[36m0.9917\u001b[0m        \u001b[32m0.2509\u001b[0m       0.6250            0.6250        \u001b[94m0.9239\u001b[0m  0.0005  0.8630\n",
      "     47            0.9917        \u001b[32m0.2151\u001b[0m       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        0.9435  0.0004  0.8173\n",
      "     48            \u001b[36m0.9938\u001b[0m        0.2241       0.6458            0.6458        0.9362  0.0004  0.8306\n",
      "     49            0.9917        0.2172       0.6354            0.6354        0.9325  0.0003  0.8615\n",
      "     50            0.9938        \u001b[32m0.1825\u001b[0m       0.6354            0.6354        0.9308  0.0003  0.8314\n",
      "Fine tuning model for subject 7 with 120 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4604        1.4816       0.4062            0.4062        1.2548  0.0004  0.8484\n",
      "     32            0.6333        1.2616       0.4375            0.4375        1.2030  0.0005  0.8459\n",
      "     33            0.7146        1.0186       0.5104            0.5104        1.2280  0.0005  0.8439\n",
      "     34            0.7750        0.9141       0.4896            0.4896        1.1382  0.0006  0.9388\n",
      "     35            \u001b[36m0.8313\u001b[0m        0.7582       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        \u001b[94m1.0971\u001b[0m  0.0006  0.8912\n",
      "     36            \u001b[36m0.8625\u001b[0m        \u001b[32m0.7104\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        \u001b[94m1.0948\u001b[0m  0.0007  1.0652\n",
      "     37            \u001b[36m0.9104\u001b[0m        \u001b[32m0.5938\u001b[0m       0.5521            0.5521        \u001b[94m1.0682\u001b[0m  0.0007  1.1522\n",
      "     38            \u001b[36m0.9125\u001b[0m        \u001b[32m0.5620\u001b[0m       0.5417            0.5417        1.0871  0.0007  1.0527\n",
      "     39            \u001b[36m0.9521\u001b[0m        \u001b[32m0.5020\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.1143  0.0007  0.9783\n",
      "     40            \u001b[36m0.9604\u001b[0m        \u001b[32m0.4435\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.0792  0.0007  1.0294\n",
      "     41            \u001b[36m0.9708\u001b[0m        \u001b[32m0.4091\u001b[0m       0.5625            0.5625        \u001b[94m1.0483\u001b[0m  0.0007  1.0517\n",
      "     42            \u001b[36m0.9812\u001b[0m        \u001b[32m0.3662\u001b[0m       0.5312            0.5312        1.0536  0.0007  0.8578\n",
      "     43            0.9792        \u001b[32m0.3340\u001b[0m       0.5417            0.5417        \u001b[94m1.0433\u001b[0m  0.0006  1.0319\n",
      "     44            \u001b[36m0.9896\u001b[0m        \u001b[32m0.3332\u001b[0m       0.5521            0.5521        \u001b[94m0.9918\u001b[0m  0.0006  1.0340\n",
      "     45            0.9833        \u001b[32m0.2877\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.0128  0.0005  0.9933\n",
      "     46            0.9875        \u001b[32m0.2762\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.9761\u001b[0m  0.0005  0.9756\n",
      "     47            \u001b[36m0.9958\u001b[0m        \u001b[32m0.2554\u001b[0m       0.5625            0.5625        0.9794  0.0004  0.9487\n",
      "     48            0.9938        \u001b[32m0.2376\u001b[0m       0.6250            0.6250        0.9794  0.0004  1.0468\n",
      "     49            0.9958        \u001b[32m0.2265\u001b[0m       0.6354            0.6354        0.9818  0.0003  1.0000\n",
      "     50            \u001b[36m0.9979\u001b[0m        \u001b[32m0.2179\u001b[0m       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.9696\u001b[0m  0.0003  1.0156\n",
      "Fine tuning model for subject 7 with 140 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4562        1.5531       0.4479            0.4479        1.2616  0.0004  0.8132\n",
      "     32            0.5604        1.2210       0.4271            0.4271        1.1983  0.0005  0.7812\n",
      "     33            0.6813        1.0914       0.4792            0.4792        1.1704  0.0005  0.7974\n",
      "     34            0.7708        0.8757       0.4583            0.4583        1.1530  0.0006  0.7815\n",
      "     35            0.7979        0.8399       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        1.1316  0.0006  0.7969\n",
      "     36            \u001b[36m0.8167\u001b[0m        \u001b[32m0.7283\u001b[0m       0.5104            0.5104        1.1509  0.0007  0.8437\n",
      "     37            \u001b[36m0.8771\u001b[0m        \u001b[32m0.6588\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m1.1089\u001b[0m  0.0007  0.8681\n",
      "     38            \u001b[36m0.9062\u001b[0m        \u001b[32m0.5799\u001b[0m       0.5729            0.5729        \u001b[94m1.0742\u001b[0m  0.0007  0.9008\n",
      "     39            \u001b[36m0.9313\u001b[0m        \u001b[32m0.5235\u001b[0m       0.5625            0.5625        1.1119  0.0007  0.7885\n",
      "     40            0.9187        \u001b[32m0.5127\u001b[0m       0.5208            0.5208        1.1382  0.0007  0.7984\n",
      "     41            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4563\u001b[0m       0.5521            0.5521        1.0994  0.0007  0.9409\n",
      "     42            \u001b[36m0.9729\u001b[0m        \u001b[32m0.4330\u001b[0m       0.5625            0.5625        1.0810  0.0007  0.8906\n",
      "     43            0.9688        \u001b[32m0.3681\u001b[0m       0.5833            0.5833        \u001b[94m1.0395\u001b[0m  0.0006  0.9062\n",
      "     44            \u001b[36m0.9771\u001b[0m        \u001b[32m0.3662\u001b[0m       0.5833            0.5833        1.0567  0.0006  0.8594\n",
      "     45            \u001b[36m0.9875\u001b[0m        \u001b[32m0.3161\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m1.0330\u001b[0m  0.0005  0.7814\n",
      "     46            0.9854        \u001b[32m0.2924\u001b[0m       0.5729            0.5729        \u001b[94m1.0153\u001b[0m  0.0005  0.8695\n",
      "     47            0.9854        \u001b[32m0.2889\u001b[0m       0.5625            0.5625        \u001b[94m1.0112\u001b[0m  0.0004  0.8936\n",
      "     48            \u001b[36m0.9917\u001b[0m        \u001b[32m0.2672\u001b[0m       0.5729            0.5729        1.0180  0.0004  0.8176\n",
      "     49            0.9917        \u001b[32m0.2589\u001b[0m       0.6042            0.6042        1.0243  0.0003  0.7920\n",
      "     50            \u001b[36m0.9958\u001b[0m        \u001b[32m0.2501\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.9998\u001b[0m  0.0003  0.8182\n",
      "Fine tuning model for subject 7 with 160 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4396        1.5003       0.4271            0.4271        1.2819  0.0004  0.9312\n",
      "     32            0.5854        1.3195       0.4375            0.4375        1.2157  0.0005  0.8022\n",
      "     33            0.7083        1.0524       0.4896            0.4896        1.1750  0.0005  0.9011\n",
      "     34            0.7396        0.9075       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        1.1698  0.0006  0.9052\n",
      "     35            0.7688        0.8628       0.5104            0.5104        1.2073  0.0006  1.0690\n",
      "     36            \u001b[36m0.8229\u001b[0m        0.7397       0.5208            0.5208        1.1690  0.0007  1.0684\n",
      "     37            \u001b[36m0.8771\u001b[0m        \u001b[32m0.6370\u001b[0m       0.4896            0.4896        1.1571  0.0007  0.9237\n",
      "     38            \u001b[36m0.9250\u001b[0m        \u001b[32m0.5662\u001b[0m       0.5104            0.5104        \u001b[94m1.0960\u001b[0m  0.0007  0.8281\n",
      "     39            0.9250        \u001b[32m0.5409\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        \u001b[94m1.0300\u001b[0m  0.0007  0.8079\n",
      "     40            \u001b[36m0.9521\u001b[0m        \u001b[32m0.4998\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        \u001b[94m1.0167\u001b[0m  0.0007  0.8395\n",
      "     41            \u001b[36m0.9688\u001b[0m        \u001b[32m0.4209\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.0338  0.0007  0.8002\n",
      "     42            0.9563        \u001b[32m0.3915\u001b[0m       0.5208            0.5208        1.0785  0.0007  0.8456\n",
      "     43            0.9646        \u001b[32m0.3843\u001b[0m       0.5625            0.5625        1.0593  0.0006  0.8475\n",
      "     44            \u001b[36m0.9854\u001b[0m        \u001b[32m0.3306\u001b[0m       0.5625            0.5625        \u001b[94m0.9928\u001b[0m  0.0006  0.8270\n",
      "     45            0.9792        \u001b[32m0.3157\u001b[0m       0.5417            0.5417        1.0181  0.0005  0.8314\n",
      "     46            \u001b[36m0.9896\u001b[0m        \u001b[32m0.2919\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m0.9691\u001b[0m  0.0005  0.8365\n",
      "     47            \u001b[36m0.9938\u001b[0m        0.2931       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9440\u001b[0m  0.0004  0.9853\n",
      "     48            0.9917        \u001b[32m0.2889\u001b[0m       0.5417            0.5417        0.9831  0.0004  0.9764\n",
      "     49            0.9938        \u001b[32m0.2553\u001b[0m       0.6146            0.6146        0.9702  0.0003  0.9790\n",
      "     50            \u001b[36m0.9958\u001b[0m        \u001b[32m0.2304\u001b[0m       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        0.9468  0.0003  1.0441\n",
      "Fine tuning model for subject 7 with 180 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4250        1.5183       0.3958            0.3958        1.2810  0.0004  0.9974\n",
      "     32            0.5583        1.2889       0.4583            0.4583        1.1905  0.0005  0.8663\n",
      "     33            0.6542        1.0644       0.4479            0.4479        1.2410  0.0005  0.9825\n",
      "     34            0.7417        0.9802       0.4792            0.4792        1.1753  0.0006  0.8944\n",
      "     35            0.7812        0.8616       0.4896            0.4896        1.1361  0.0006  0.9063\n",
      "     36            \u001b[36m0.8438\u001b[0m        \u001b[32m0.7162\u001b[0m       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        \u001b[94m1.1212\u001b[0m  0.0007  0.8685\n",
      "     37            \u001b[36m0.8688\u001b[0m        \u001b[32m0.6578\u001b[0m       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        \u001b[94m1.1014\u001b[0m  0.0007  0.9297\n",
      "     38            \u001b[36m0.9083\u001b[0m        \u001b[32m0.6063\u001b[0m       0.5417            0.5417        \u001b[94m1.0648\u001b[0m  0.0007  0.9542\n",
      "     39            \u001b[36m0.9167\u001b[0m        \u001b[32m0.5175\u001b[0m       0.5312            0.5312        1.0773  0.0007  1.0180\n",
      "     40            \u001b[36m0.9542\u001b[0m        0.5368       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        \u001b[94m1.0395\u001b[0m  0.0007  0.9785\n",
      "     41            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4503\u001b[0m       0.5729            0.5729        1.0486  0.0007  1.0666\n",
      "     42            \u001b[36m0.9771\u001b[0m        \u001b[32m0.4403\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.0557  0.0007  0.9405\n",
      "     43            0.9729        \u001b[32m0.3787\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.0568  0.0006  0.8281\n",
      "     44            0.9563        \u001b[32m0.3626\u001b[0m       0.5938            0.5938        1.0509  0.0006  0.8595\n",
      "     45            \u001b[36m0.9854\u001b[0m        \u001b[32m0.3240\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9933\u001b[0m  0.0005  0.8013\n",
      "     46            \u001b[36m0.9896\u001b[0m        \u001b[32m0.2979\u001b[0m       0.5729            0.5729        \u001b[94m0.9855\u001b[0m  0.0005  0.8594\n",
      "     47            \u001b[36m0.9938\u001b[0m        \u001b[32m0.2733\u001b[0m       0.6146            0.6146        \u001b[94m0.9742\u001b[0m  0.0004  0.9706\n",
      "     48            0.9938        \u001b[32m0.2599\u001b[0m       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        0.9760  0.0004  0.8594\n",
      "     49            0.9938        \u001b[32m0.2513\u001b[0m       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        0.9897  0.0003  0.9375\n",
      "     50            0.9938        \u001b[32m0.2388\u001b[0m       0.6458            0.6458        0.9876  0.0003  0.9399\n",
      "Fine tuning model for subject 7 with 200 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4333        1.5057       0.3958            0.3958        1.3011  0.0004  1.0126\n",
      "     32            0.5687        1.3263       0.4271            0.4271        1.2107  0.0005  1.0726\n",
      "     33            0.6937        1.0856       0.4688            0.4688        1.1535  0.0005  1.1696\n",
      "     34            0.7667        0.9452       0.5104            0.5104        \u001b[94m1.1248\u001b[0m  0.0006  0.9469\n",
      "     35            0.8063        0.8273       0.5104            0.5104        \u001b[94m1.1223\u001b[0m  0.0006  0.7818\n",
      "     36            \u001b[36m0.8354\u001b[0m        \u001b[32m0.6822\u001b[0m       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        \u001b[94m1.1022\u001b[0m  0.0007  0.7973\n",
      "     37            \u001b[36m0.8708\u001b[0m        \u001b[32m0.6029\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        \u001b[94m1.0749\u001b[0m  0.0007  0.7974\n",
      "     38            \u001b[36m0.9083\u001b[0m        0.6151       0.5625            0.5625        \u001b[94m1.0365\u001b[0m  0.0007  0.7812\n",
      "     39            \u001b[36m0.9292\u001b[0m        \u001b[32m0.5220\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.0616  0.0007  0.7976\n",
      "     40            \u001b[36m0.9396\u001b[0m        \u001b[32m0.4779\u001b[0m       0.5833            0.5833        1.0755  0.0007  0.7812\n",
      "     41            \u001b[36m0.9583\u001b[0m        \u001b[32m0.4161\u001b[0m       0.6042            0.6042        1.0389  0.0007  0.7822\n",
      "     42            \u001b[36m0.9729\u001b[0m        0.4258       0.5417            0.5417        1.0597  0.0007  0.7817\n",
      "     43            \u001b[36m0.9792\u001b[0m        \u001b[32m0.3502\u001b[0m       0.5729            0.5729        \u001b[94m1.0244\u001b[0m  0.0006  0.7812\n",
      "     44            0.9792        0.3569       0.5938            0.5938        \u001b[94m0.9832\u001b[0m  0.0006  0.8285\n",
      "     45            \u001b[36m0.9833\u001b[0m        \u001b[32m0.3400\u001b[0m       0.5417            0.5417        0.9948  0.0005  0.9237\n",
      "     46            \u001b[36m0.9875\u001b[0m        \u001b[32m0.2940\u001b[0m       0.6042            0.6042        \u001b[94m0.9777\u001b[0m  0.0005  0.9062\n",
      "     47            0.9854        \u001b[32m0.2666\u001b[0m       0.6146            0.6146        \u001b[94m0.9594\u001b[0m  0.0004  1.0625\n",
      "     48            \u001b[36m0.9938\u001b[0m        0.2678       0.5938            0.5938        0.9741  0.0004  0.7817\n",
      "     49            0.9938        \u001b[32m0.2646\u001b[0m       0.5938            0.5938        0.9622  0.0003  0.7821\n",
      "     50            0.9938        \u001b[32m0.2470\u001b[0m       0.5833            0.5833        \u001b[94m0.9567\u001b[0m  0.0003  0.7813\n",
      "Fine tuning model for subject 7 with 220 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4625        1.5179       0.3958            0.3958        1.2783  0.0004  0.7969\n",
      "     32            0.5833        1.2523       0.4688            0.4688        1.2003  0.0005  0.7812\n",
      "     33            0.6979        1.0855       0.4792            0.4792        1.1747  0.0005  0.7968\n",
      "     34            0.7438        0.8801       0.5000            0.5000        1.1476  0.0006  0.7817\n",
      "     35            \u001b[36m0.8313\u001b[0m        0.8150       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        \u001b[94m1.0792\u001b[0m  0.0006  0.7718\n",
      "     36            \u001b[36m0.8521\u001b[0m        0.7532       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.0970  0.0007  0.7818\n",
      "     37            \u001b[36m0.8854\u001b[0m        \u001b[32m0.6101\u001b[0m       0.5312            0.5312        1.0823  0.0007  0.7974\n",
      "     38            \u001b[36m0.9083\u001b[0m        \u001b[32m0.5647\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.0842  0.0007  0.8598\n",
      "     39            \u001b[36m0.9417\u001b[0m        \u001b[32m0.5276\u001b[0m       0.5625            0.5625        1.1089  0.0007  0.9376\n",
      "     40            \u001b[36m0.9521\u001b[0m        \u001b[32m0.4876\u001b[0m       0.5312            0.5312        \u001b[94m1.0643\u001b[0m  0.0007  0.9062\n",
      "     41            \u001b[36m0.9646\u001b[0m        \u001b[32m0.4186\u001b[0m       0.5312            0.5312        \u001b[94m1.0634\u001b[0m  0.0007  1.0473\n",
      "     42            \u001b[36m0.9750\u001b[0m        \u001b[32m0.4037\u001b[0m       0.5312            0.5312        1.0851  0.0007  0.7969\n",
      "     43            0.9604        \u001b[32m0.3664\u001b[0m       0.5521            0.5521        1.0952  0.0006  0.7817\n",
      "     44            \u001b[36m0.9812\u001b[0m        \u001b[32m0.3338\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0359\u001b[0m  0.0006  0.7812\n",
      "     45            \u001b[36m0.9833\u001b[0m        \u001b[32m0.3293\u001b[0m       0.5938            0.5938        \u001b[94m0.9980\u001b[0m  0.0005  0.7972\n",
      "     46            \u001b[36m0.9854\u001b[0m        \u001b[32m0.3142\u001b[0m       0.5521            0.5521        1.0411  0.0005  0.7812\n",
      "     47            \u001b[36m0.9917\u001b[0m        \u001b[32m0.2810\u001b[0m       0.5312            0.5312        1.0105  0.0004  0.7817\n",
      "     48            \u001b[36m0.9938\u001b[0m        \u001b[32m0.2552\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9946\u001b[0m  0.0004  0.7812\n",
      "     49            0.9938        0.2588       0.6042            0.6042        \u001b[94m0.9877\u001b[0m  0.0003  0.7818\n",
      "     50            \u001b[36m0.9958\u001b[0m        \u001b[32m0.2432\u001b[0m       0.5938            0.5938        0.9933  0.0003  0.8281\n",
      "Fine tuning model for subject 7 with 240 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4250        1.5153       0.4375            0.4375        1.3274  0.0004  0.7813\n",
      "     32            0.5979        1.3238       0.4375            0.4375        1.2341  0.0005  0.9535\n",
      "     33            0.6833        1.1141       0.4688            0.4688        1.2281  0.0005  0.9375\n",
      "     34            0.7479        0.9307       0.4792            0.4792        1.1868  0.0006  0.9062\n",
      "     35            0.7958        0.8419       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        1.1663  0.0006  0.9383\n",
      "     36            \u001b[36m0.8521\u001b[0m        \u001b[32m0.7222\u001b[0m       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        1.1369  0.0007  0.8281\n",
      "     37            \u001b[36m0.8958\u001b[0m        \u001b[32m0.6625\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m1.0709\u001b[0m  0.0007  0.7817\n",
      "     38            \u001b[36m0.9146\u001b[0m        \u001b[32m0.5722\u001b[0m       0.5521            0.5521        1.1005  0.0007  0.7813\n",
      "     39            \u001b[36m0.9396\u001b[0m        \u001b[32m0.5371\u001b[0m       0.5729            0.5729        1.0745  0.0007  0.7818\n",
      "     40            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5062\u001b[0m       0.5417            0.5417        \u001b[94m1.0408\u001b[0m  0.0007  0.7969\n",
      "     41            \u001b[36m0.9646\u001b[0m        \u001b[32m0.4306\u001b[0m       0.5521            0.5521        \u001b[94m1.0232\u001b[0m  0.0007  0.7812\n",
      "     42            \u001b[36m0.9688\u001b[0m        \u001b[32m0.3985\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0129\u001b[0m  0.0007  0.8130\n",
      "     43            \u001b[36m0.9792\u001b[0m        \u001b[32m0.3760\u001b[0m       0.5729            0.5729        \u001b[94m0.9982\u001b[0m  0.0006  0.7817\n",
      "     44            \u001b[36m0.9854\u001b[0m        \u001b[32m0.3214\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9619\u001b[0m  0.0006  0.8437\n",
      "     45            0.9812        \u001b[32m0.3096\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.9546\u001b[0m  0.0005  0.8598\n",
      "     46            \u001b[36m0.9875\u001b[0m        \u001b[32m0.3035\u001b[0m       0.6146            0.6146        0.9717  0.0005  0.8750\n",
      "     47            \u001b[36m0.9917\u001b[0m        \u001b[32m0.2784\u001b[0m       0.5625            0.5625        0.9899  0.0004  0.9062\n",
      "     48            \u001b[36m0.9958\u001b[0m        \u001b[32m0.2694\u001b[0m       0.6250            0.6250        0.9913  0.0004  0.8751\n",
      "     49            0.9958        \u001b[32m0.2617\u001b[0m       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        0.9637  0.0003  0.9692\n",
      "     50            0.9958        0.2695       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.9488\u001b[0m  0.0003  0.8290\n",
      "Fine tuning model for subject 7 with 260 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4583        1.5435       0.4062            0.4062        1.2640  0.0004  0.7813\n",
      "     32            0.5875        1.2679       0.4167            0.4167        1.2087  0.0005  0.7980\n",
      "     33            0.6667        1.0659       0.4583            0.4583        1.1660  0.0005  0.7818\n",
      "     34            0.7479        0.9160       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        1.1806  0.0006  0.7812\n",
      "     35            0.8042        0.8039       0.5208            0.5208        1.2091  0.0006  0.7969\n",
      "     36            \u001b[36m0.8375\u001b[0m        \u001b[32m0.7292\u001b[0m       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        1.1603  0.0007  0.7973\n",
      "     37            \u001b[36m0.8750\u001b[0m        \u001b[32m0.6376\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.1287  0.0007  0.7993\n",
      "     38            \u001b[36m0.9021\u001b[0m        \u001b[32m0.6190\u001b[0m       0.5625            0.5625        \u001b[94m1.0890\u001b[0m  0.0007  0.7973\n",
      "     39            \u001b[36m0.9208\u001b[0m        \u001b[32m0.5241\u001b[0m       0.5312            0.5312        \u001b[94m1.0635\u001b[0m  0.0007  0.7817\n",
      "     40            \u001b[36m0.9437\u001b[0m        \u001b[32m0.4896\u001b[0m       0.5521            0.5521        \u001b[94m1.0598\u001b[0m  0.0007  0.8906\n",
      "     41            \u001b[36m0.9479\u001b[0m        \u001b[32m0.4393\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0422\u001b[0m  0.0007  0.8755\n",
      "     42            \u001b[36m0.9646\u001b[0m        \u001b[32m0.4033\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.0549  0.0007  0.9063\n",
      "     43            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3958\u001b[0m       0.5729            0.5729        1.0869  0.0006  0.9375\n",
      "     44            \u001b[36m0.9812\u001b[0m        \u001b[32m0.3456\u001b[0m       0.5625            0.5625        1.0777  0.0006  0.7818\n",
      "     45            \u001b[36m0.9875\u001b[0m        \u001b[32m0.3379\u001b[0m       0.5417            0.5417        \u001b[94m1.0417\u001b[0m  0.0005  0.7818\n",
      "     46            0.9833        \u001b[32m0.3251\u001b[0m       0.5833            0.5833        \u001b[94m1.0288\u001b[0m  0.0005  0.7813\n",
      "     47            \u001b[36m0.9896\u001b[0m        \u001b[32m0.2827\u001b[0m       0.5729            0.5729        \u001b[94m1.0137\u001b[0m  0.0004  0.7979\n",
      "     48            \u001b[36m0.9917\u001b[0m        \u001b[32m0.2644\u001b[0m       0.6042            0.6042        \u001b[94m0.9753\u001b[0m  0.0004  0.7970\n",
      "     49            0.9917        \u001b[32m0.2450\u001b[0m       0.5938            0.5938        \u001b[94m0.9498\u001b[0m  0.0003  0.7969\n",
      "     50            \u001b[36m0.9938\u001b[0m        0.2554       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.9374\u001b[0m  0.0003  0.7969\n",
      "Fine tuning model for subject 7 with 280 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4479        1.5290       0.4062            0.4062        1.2861  0.0004  0.7969\n",
      "     32            0.5813        1.2084       0.4479            0.4479        1.1934  0.0005  0.7969\n",
      "     33            0.7021        1.0385       0.4479            0.4479        1.1524  0.0005  0.7973\n",
      "     34            0.7458        0.9082       0.4896            0.4896        1.1455  0.0006  0.9063\n",
      "     35            0.8083        0.7833       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        \u001b[94m1.1270\u001b[0m  0.0006  0.9219\n",
      "     36            \u001b[36m0.8500\u001b[0m        \u001b[32m0.7121\u001b[0m       0.5312            0.5312        1.1312  0.0007  0.9063\n",
      "     37            0.8479        \u001b[32m0.6358\u001b[0m       0.5312            0.5312        1.1587  0.0007  0.7817\n",
      "     38            \u001b[36m0.9146\u001b[0m        \u001b[32m0.6209\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.1323  0.0007  0.7813\n",
      "     39            \u001b[36m0.9396\u001b[0m        \u001b[32m0.4887\u001b[0m       0.5521            0.5521        \u001b[94m1.0400\u001b[0m  0.0007  0.7818\n",
      "     40            0.9229        \u001b[32m0.4677\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.0644  0.0007  0.7975\n",
      "     41            0.9187        \u001b[32m0.4593\u001b[0m       0.5417            0.5417        1.1554  0.0007  0.7817\n",
      "     42            \u001b[36m0.9708\u001b[0m        \u001b[32m0.4130\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.0801  0.0007  0.7817\n",
      "     43            0.9708        \u001b[32m0.3630\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.0442  0.0006  0.7969\n",
      "     44            \u001b[36m0.9833\u001b[0m        \u001b[32m0.3275\u001b[0m       0.6042            0.6042        \u001b[94m1.0223\u001b[0m  0.0006  0.8750\n",
      "     45            \u001b[36m0.9875\u001b[0m        0.3331       0.6146            0.6146        \u001b[94m1.0006\u001b[0m  0.0005  0.7812\n",
      "     46            \u001b[36m0.9917\u001b[0m        \u001b[32m0.2772\u001b[0m       0.6146            0.6146        \u001b[94m0.9870\u001b[0m  0.0005  0.7813\n",
      "     47            0.9875        \u001b[32m0.2737\u001b[0m       0.6146            0.6146        0.9908  0.0004  0.8129\n",
      "     48            0.9875        0.2805       0.5938            0.5938        0.9918  0.0004  1.0001\n",
      "     49            0.9917        \u001b[32m0.2292\u001b[0m       0.6146            0.6146        \u001b[94m0.9834\u001b[0m  0.0003  0.8750\n",
      "     50            \u001b[36m0.9938\u001b[0m        0.2327       0.6042            0.6042        \u001b[94m0.9701\u001b[0m  0.0003  1.0156\n",
      "Fine tuning model for subject 7 with 300 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4333        1.5223       0.4062            0.4062        1.3009  0.0004  0.7817\n",
      "     32            0.5687        1.3370       0.4062            0.4062        1.2458  0.0005  0.7973\n",
      "     33            0.6687        1.0880       0.4375            0.4375        1.1972  0.0005  0.7817\n",
      "     34            0.7562        0.9658       0.5104            0.5104        1.1849  0.0006  0.7812\n",
      "     35            0.8042        0.8714       0.4688            0.4688        1.1656  0.0006  0.7812\n",
      "     36            \u001b[36m0.8333\u001b[0m        \u001b[32m0.7131\u001b[0m       0.4792            0.4792        1.1423  0.0007  0.8125\n",
      "     37            \u001b[36m0.8771\u001b[0m        \u001b[32m0.6710\u001b[0m       0.4792            0.4792        1.1455  0.0007  0.7813\n",
      "     38            \u001b[36m0.8917\u001b[0m        \u001b[32m0.5644\u001b[0m       0.5104            0.5104        \u001b[94m1.0737\u001b[0m  0.0007  0.7817\n",
      "     39            \u001b[36m0.9208\u001b[0m        \u001b[32m0.5315\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        \u001b[94m1.0191\u001b[0m  0.0007  0.7823\n",
      "     40            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4651\u001b[0m       0.5521            0.5521        1.0844  0.0007  0.7817\n",
      "     41            \u001b[36m0.9396\u001b[0m        \u001b[32m0.4521\u001b[0m       0.5208            0.5208        1.0638  0.0007  0.9068\n",
      "     42            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4449\u001b[0m       0.5625            0.5625        \u001b[94m1.0105\u001b[0m  0.0007  0.9219\n",
      "     43            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3648\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9757\u001b[0m  0.0006  0.9532\n",
      "     44            \u001b[36m0.9854\u001b[0m        \u001b[32m0.3600\u001b[0m       0.5938            0.5938        1.0012  0.0006  0.9688\n",
      "     45            0.9854        \u001b[32m0.3408\u001b[0m       0.5938            0.5938        1.0213  0.0005  0.7817\n",
      "     46            \u001b[36m0.9896\u001b[0m        \u001b[32m0.2956\u001b[0m       0.5729            0.5729        1.0631  0.0005  0.8281\n",
      "     47            0.9875        \u001b[32m0.2709\u001b[0m       0.5938            0.5938        1.0482  0.0004  0.7812\n",
      "     48            0.9875        0.2820       0.5729            0.5729        1.0133  0.0004  0.7973\n",
      "     49            \u001b[36m0.9917\u001b[0m        \u001b[32m0.2564\u001b[0m       0.5729            0.5729        1.0164  0.0003  0.8125\n",
      "     50            0.9917        \u001b[32m0.2423\u001b[0m       0.5729            0.5729        1.0183  0.0003  0.7817\n",
      "Fine tuning model for subject 7 with 320 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4604        1.5097       0.3958            0.3958        1.3030  0.0004  0.7818\n",
      "     32            0.5813        1.2451       0.4271            0.4271        1.2510  0.0005  0.7813\n",
      "     33            0.6646        1.0665       0.4896            0.4896        1.1870  0.0005  0.7817\n",
      "     34            0.7646        0.9611       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        1.1457  0.0006  0.7819\n",
      "     35            0.8063        0.8120       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.1390  0.0006  0.9062\n",
      "     36            \u001b[36m0.8354\u001b[0m        \u001b[32m0.6909\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.2029  0.0007  0.9063\n",
      "     37            \u001b[36m0.8917\u001b[0m        \u001b[32m0.6391\u001b[0m       0.5417            0.5417        1.1543  0.0007  0.8906\n",
      "     38            \u001b[36m0.9042\u001b[0m        \u001b[32m0.5622\u001b[0m       0.5625            0.5625        \u001b[94m1.0801\u001b[0m  0.0007  0.9531\n",
      "     39            \u001b[36m0.9417\u001b[0m        \u001b[32m0.5030\u001b[0m       0.5521            0.5521        1.0943  0.0007  0.7969\n",
      "     40            \u001b[36m0.9521\u001b[0m        \u001b[32m0.4385\u001b[0m       0.5625            0.5625        1.0927  0.0007  0.7812\n",
      "     41            \u001b[36m0.9729\u001b[0m        \u001b[32m0.4196\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0488\u001b[0m  0.0007  0.7817\n",
      "     42            \u001b[36m0.9771\u001b[0m        \u001b[32m0.3746\u001b[0m       0.5729            0.5729        1.0528  0.0007  0.7975\n",
      "     43            0.9750        \u001b[32m0.3590\u001b[0m       0.5625            0.5625        1.0925  0.0006  0.7979\n",
      "     44            0.9771        0.3711       0.5833            0.5833        1.0633  0.0006  0.8125\n",
      "     45            \u001b[36m0.9812\u001b[0m        \u001b[32m0.2981\u001b[0m       0.5938            0.5938        1.0571  0.0005  0.7978\n",
      "     46            \u001b[36m0.9896\u001b[0m        0.3265       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0410\u001b[0m  0.0005  0.7819\n",
      "     47            \u001b[36m0.9938\u001b[0m        \u001b[32m0.2890\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9951\u001b[0m  0.0004  0.7969\n",
      "     48            0.9938        \u001b[32m0.2661\u001b[0m       0.5833            0.5833        \u001b[94m0.9634\u001b[0m  0.0004  0.7817\n",
      "     49            \u001b[36m0.9958\u001b[0m        \u001b[32m0.2434\u001b[0m       0.5729            0.5729        0.9767  0.0003  0.9531\n",
      "     50            0.9896        0.2543       0.6146            0.6146        0.9930  0.0003  0.9375\n",
      "Fine tuning model for subject 7 with 340 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4521        1.5457       0.3854            0.3854        1.2758  0.0004  0.9532\n",
      "     32            0.6167        1.2871       0.4479            0.4479        1.1891  0.0005  0.8129\n",
      "     33            0.7000        1.0581       0.5000            0.5000        1.1789  0.0005  0.7970\n",
      "     34            0.7646        0.9354       0.5104            0.5104        1.1777  0.0006  0.8442\n",
      "     35            \u001b[36m0.8125\u001b[0m        0.7975       0.5104            0.5104        \u001b[94m1.1254\u001b[0m  0.0006  0.8443\n",
      "     36            \u001b[36m0.8438\u001b[0m        \u001b[32m0.7204\u001b[0m       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        \u001b[94m1.1164\u001b[0m  0.0007  0.7974\n",
      "     37            \u001b[36m0.8542\u001b[0m        \u001b[32m0.6609\u001b[0m       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        \u001b[94m1.1114\u001b[0m  0.0007  0.7817\n",
      "     38            \u001b[36m0.9083\u001b[0m        \u001b[32m0.6545\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        \u001b[94m1.0445\u001b[0m  0.0007  0.7813\n",
      "     39            \u001b[36m0.9521\u001b[0m        \u001b[32m0.5313\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0358\u001b[0m  0.0007  0.7975\n",
      "     40            0.9479        \u001b[32m0.4859\u001b[0m       0.5833            0.5833        1.0674  0.0007  0.8437\n",
      "     41            0.9479        \u001b[32m0.4534\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0210\u001b[0m  0.0007  0.7973\n",
      "     42            \u001b[36m0.9646\u001b[0m        \u001b[32m0.3631\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.9918\u001b[0m  0.0007  0.7974\n",
      "     43            \u001b[36m0.9792\u001b[0m        0.3824       0.6250            0.6250        0.9980  0.0006  0.9219\n",
      "     44            \u001b[36m0.9812\u001b[0m        \u001b[32m0.3198\u001b[0m       0.5938            0.5938        1.0324  0.0006  0.9375\n",
      "     45            0.9812        \u001b[32m0.3169\u001b[0m       0.6042            0.6042        1.0168  0.0005  1.0000\n",
      "     46            \u001b[36m0.9833\u001b[0m        \u001b[32m0.2909\u001b[0m       0.5625            0.5625        1.0209  0.0005  0.7969\n",
      "     47            0.9833        \u001b[32m0.2762\u001b[0m       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.9425\u001b[0m  0.0004  0.7974\n",
      "     48            0.9833        0.2785       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.9244\u001b[0m  0.0004  0.7974\n",
      "     49            \u001b[36m0.9917\u001b[0m        \u001b[32m0.2639\u001b[0m       0.6458            0.6458        0.9388  0.0003  0.7969\n",
      "     50            0.9917        \u001b[32m0.2599\u001b[0m       0.6146            0.6146        0.9580  0.0003  0.8599\n",
      "Fine tuning model for subject 7 with 360 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4542        1.5436       0.3854            0.3854        1.2826  0.0004  0.7817\n",
      "     32            0.5875        1.2990       0.4583            0.4583        1.2070  0.0005  0.7980\n",
      "     33            0.6979        1.0705       0.4688            0.4688        1.2154  0.0005  0.7968\n",
      "     34            0.7583        0.9324       0.5000            0.5000        1.1874  0.0006  0.7813\n",
      "     35            0.7792        0.8378       0.5000            0.5000        \u001b[94m1.1143\u001b[0m  0.0006  0.7969\n",
      "     36            \u001b[36m0.8125\u001b[0m        \u001b[32m0.7281\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        \u001b[94m1.0842\u001b[0m  0.0007  0.8598\n",
      "     37            \u001b[36m0.8583\u001b[0m        \u001b[32m0.6520\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.1327  0.0007  0.9688\n",
      "     38            \u001b[36m0.8896\u001b[0m        \u001b[32m0.6146\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.0856  0.0007  0.9844\n",
      "     39            \u001b[36m0.9146\u001b[0m        \u001b[32m0.5230\u001b[0m       0.5417            0.5417        \u001b[94m1.0634\u001b[0m  0.0007  0.8126\n",
      "     40            \u001b[36m0.9396\u001b[0m        0.5284       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0203\u001b[0m  0.0007  0.7841\n",
      "     41            \u001b[36m0.9479\u001b[0m        \u001b[32m0.4639\u001b[0m       0.5521            0.5521        \u001b[94m1.0119\u001b[0m  0.0007  0.7817\n",
      "     42            \u001b[36m0.9688\u001b[0m        \u001b[32m0.4313\u001b[0m       0.5312            0.5312        1.0393  0.0007  0.7837\n",
      "     43            \u001b[36m0.9708\u001b[0m        \u001b[32m0.3770\u001b[0m       0.5833            0.5833        1.0627  0.0006  0.7817\n",
      "     44            \u001b[36m0.9792\u001b[0m        \u001b[32m0.3561\u001b[0m       0.5833            0.5833        1.0197  0.0006  0.7974\n",
      "     45            \u001b[36m0.9896\u001b[0m        \u001b[32m0.3186\u001b[0m       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9694\u001b[0m  0.0005  0.8438\n",
      "     46            \u001b[36m0.9917\u001b[0m        \u001b[32m0.3020\u001b[0m       0.6250            0.6250        0.9803  0.0005  0.7974\n",
      "     47            \u001b[36m0.9938\u001b[0m        \u001b[32m0.2720\u001b[0m       0.6146            0.6146        0.9855  0.0004  0.7818\n",
      "     48            0.9917        0.2789       0.6250            0.6250        \u001b[94m0.9629\u001b[0m  0.0004  0.7974\n",
      "     49            0.9917        0.2873       0.5833            0.5833        \u001b[94m0.9605\u001b[0m  0.0003  0.7978\n",
      "     50            0.9938        \u001b[32m0.2332\u001b[0m       0.6042            0.6042        0.9618  0.0003  0.8594\n",
      "Fine tuning model for subject 7 with 380 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4750        1.5239       0.4167            0.4167        1.2677  0.0004  0.9375\n",
      "     32            0.5854        1.3554       0.5000            0.5000        1.2405  0.0005  0.9688\n",
      "     33            0.6750        1.0421       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        1.1517  0.0005  0.8131\n",
      "     34            0.7792        0.9169       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        \u001b[94m1.1277\u001b[0m  0.0006  0.7974\n",
      "     35            \u001b[36m0.8125\u001b[0m        0.8109       0.5417            0.5417        1.1319  0.0006  0.7818\n",
      "     36            \u001b[36m0.8625\u001b[0m        0.7584       0.5417            0.5417        1.1726  0.0007  0.7969\n",
      "     37            \u001b[36m0.8812\u001b[0m        \u001b[32m0.6282\u001b[0m       0.5312            0.5312        \u001b[94m1.1250\u001b[0m  0.0007  0.7978\n",
      "     38            \u001b[36m0.9042\u001b[0m        \u001b[32m0.5989\u001b[0m       0.5208            0.5208        \u001b[94m1.1098\u001b[0m  0.0007  0.7812\n",
      "     39            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5364\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m1.1074\u001b[0m  0.0007  0.7973\n",
      "     40            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4840\u001b[0m       0.5521            0.5521        \u001b[94m1.0916\u001b[0m  0.0007  0.7818\n",
      "     41            0.9396        \u001b[32m0.4258\u001b[0m       0.5521            0.5521        1.1077  0.0007  0.7974\n",
      "     42            \u001b[36m0.9625\u001b[0m        \u001b[32m0.4087\u001b[0m       0.5625            0.5625        \u001b[94m1.0902\u001b[0m  0.0007  0.7913\n",
      "     43            \u001b[36m0.9792\u001b[0m        \u001b[32m0.3733\u001b[0m       0.5625            0.5625        \u001b[94m1.0316\u001b[0m  0.0006  0.7979\n",
      "     44            0.9750        \u001b[32m0.3411\u001b[0m       0.5729            0.5729        1.0478  0.0006  0.8906\n",
      "     45            \u001b[36m0.9875\u001b[0m        \u001b[32m0.3006\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m1.0242\u001b[0m  0.0005  0.9692\n",
      "     46            \u001b[36m0.9938\u001b[0m        0.3009       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m1.0109\u001b[0m  0.0005  0.9375\n",
      "     47            0.9938        \u001b[32m0.2780\u001b[0m       0.6250            0.6250        \u001b[94m0.9872\u001b[0m  0.0004  0.9380\n",
      "     48            0.9938        \u001b[32m0.2446\u001b[0m       0.6250            0.6250        \u001b[94m0.9783\u001b[0m  0.0004  0.7812\n",
      "     49            \u001b[36m0.9958\u001b[0m        0.2637       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        0.9818  0.0003  0.7817\n",
      "     50            0.9938        0.2572       0.6354            0.6354        0.9788  0.0003  0.7812\n",
      "Fine tuning model for subject 7 with 400 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4583        1.5694       0.4062            0.4062        1.2728  0.0004  0.7969\n",
      "     32            0.5750        1.2516       0.4583            0.4583        1.2164  0.0005  0.7817\n",
      "     33            0.7208        1.0171       0.5000            0.5000        1.1655  0.0005  0.7974\n",
      "     34            0.7479        0.9176       0.4792            0.4792        1.1753  0.0006  0.7969\n",
      "     35            0.7917        0.8550       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        1.1555  0.0006  0.8906\n",
      "     36            \u001b[36m0.8583\u001b[0m        \u001b[32m0.7160\u001b[0m       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        \u001b[94m1.0950\u001b[0m  0.0007  0.7818\n",
      "     37            \u001b[36m0.8750\u001b[0m        \u001b[32m0.6407\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0762\u001b[0m  0.0007  0.7813\n",
      "     38            \u001b[36m0.9062\u001b[0m        \u001b[32m0.5983\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.0838  0.0007  0.8750\n",
      "     39            \u001b[36m0.9208\u001b[0m        \u001b[32m0.5224\u001b[0m       0.5938            0.5938        1.1147  0.0007  0.9375\n",
      "     40            \u001b[36m0.9479\u001b[0m        \u001b[32m0.5057\u001b[0m       0.5833            0.5833        \u001b[94m1.0738\u001b[0m  0.0007  0.9219\n",
      "     41            0.9354        \u001b[32m0.4377\u001b[0m       0.5833            0.5833        \u001b[94m1.0664\u001b[0m  0.0007  1.2812\n",
      "     42            \u001b[36m0.9542\u001b[0m        \u001b[32m0.4211\u001b[0m       0.6042            0.6042        \u001b[94m1.0336\u001b[0m  0.0007  0.8594\n",
      "     43            \u001b[36m0.9688\u001b[0m        \u001b[32m0.3851\u001b[0m       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m1.0146\u001b[0m  0.0006  0.7817\n",
      "     44            \u001b[36m0.9896\u001b[0m        \u001b[32m0.3104\u001b[0m       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        1.0192  0.0006  0.7969\n",
      "     45            \u001b[36m0.9917\u001b[0m        0.3265       0.6042            0.6042        1.0376  0.0005  0.7969\n",
      "     46            0.9917        \u001b[32m0.2727\u001b[0m       0.6042            0.6042        \u001b[94m1.0139\u001b[0m  0.0005  0.7812\n",
      "     47            \u001b[36m0.9938\u001b[0m        0.2745       0.6354            0.6354        \u001b[94m0.9633\u001b[0m  0.0004  0.7817\n",
      "     48            0.9917        \u001b[32m0.2613\u001b[0m       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.9576\u001b[0m  0.0004  0.7812\n",
      "     49            0.9896        \u001b[32m0.2441\u001b[0m       0.6562            0.6562        \u001b[94m0.9559\u001b[0m  0.0003  0.7817\n",
      "     50            0.9938        0.2534       0.6458            0.6458        0.9781  0.0003  0.7818\n",
      "Fine tuning model for subject 7 with 420 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4625        1.5186       0.4167            0.4167        1.2758  0.0004  0.8281\n",
      "     32            0.5875        1.2529       0.4479            0.4479        1.1680  0.0005  0.9869\n",
      "     33            0.6750        1.0820       0.4792            0.4792        1.1635  0.0005  0.9062\n",
      "     34            0.7396        0.9673       0.5104            0.5104        1.1921  0.0006  0.9225\n",
      "     35            0.8063        0.8127       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.1458  0.0006  0.7973\n",
      "     36            \u001b[36m0.8438\u001b[0m        \u001b[32m0.7144\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        \u001b[94m1.1264\u001b[0m  0.0007  0.7817\n",
      "     37            \u001b[36m0.8938\u001b[0m        \u001b[32m0.6313\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m1.0946\u001b[0m  0.0007  0.7969\n",
      "     38            \u001b[36m0.9021\u001b[0m        \u001b[32m0.6005\u001b[0m       0.5521            0.5521        \u001b[94m1.0795\u001b[0m  0.0007  0.7817\n",
      "     39            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5467\u001b[0m       0.5521            0.5521        1.1243  0.0007  0.7969\n",
      "     40            \u001b[36m0.9542\u001b[0m        \u001b[32m0.4947\u001b[0m       0.5208            0.5208        \u001b[94m1.0596\u001b[0m  0.0007  0.7969\n",
      "     41            0.9417        \u001b[32m0.4178\u001b[0m       0.5312            0.5312        \u001b[94m1.0544\u001b[0m  0.0007  0.7819\n",
      "     42            \u001b[36m0.9688\u001b[0m        \u001b[32m0.4048\u001b[0m       0.5729            0.5729        \u001b[94m1.0366\u001b[0m  0.0007  0.7817\n",
      "     43            \u001b[36m0.9792\u001b[0m        0.4193       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0247\u001b[0m  0.0006  0.7818\n",
      "     44            \u001b[36m0.9875\u001b[0m        \u001b[32m0.3421\u001b[0m       0.5938            0.5938        \u001b[94m1.0219\u001b[0m  0.0006  0.7969\n",
      "     45            0.9875        \u001b[32m0.3178\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0187\u001b[0m  0.0005  0.8906\n",
      "     46            0.9875        \u001b[32m0.3138\u001b[0m       0.5833            0.5833        \u001b[94m0.9929\u001b[0m  0.0005  0.9532\n",
      "     47            \u001b[36m0.9896\u001b[0m        \u001b[32m0.2893\u001b[0m       0.5729            0.5729        \u001b[94m0.9739\u001b[0m  0.0004  0.9375\n",
      "     48            \u001b[36m0.9917\u001b[0m        \u001b[32m0.2628\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9577\u001b[0m  0.0004  0.8750\n",
      "     49            0.9917        \u001b[32m0.2574\u001b[0m       0.5938            0.5938        0.9758  0.0003  0.7975\n",
      "     50            0.9917        \u001b[32m0.2566\u001b[0m       0.5938            0.5938        1.0044  0.0003  0.7973\n",
      "Fine tuning model for subject 7 with 440 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4375        1.4770       0.4062            0.4062        1.2755  0.0004  0.9918\n",
      "     32            0.5958        1.2608       0.4479            0.4479        1.2070  0.0005  0.8150\n",
      "     33            0.6875        1.0528       0.4583            0.4583        1.2032  0.0005  0.7919\n",
      "     34            0.7521        0.9421       0.4688            0.4688        1.1909  0.0006  0.9147\n",
      "     35            \u001b[36m0.8146\u001b[0m        0.8354       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        \u001b[94m1.1222\u001b[0m  0.0006  0.8923\n",
      "     36            \u001b[36m0.8354\u001b[0m        0.7425       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        1.1265  0.0007  0.8981\n",
      "     37            \u001b[36m0.8708\u001b[0m        \u001b[32m0.6653\u001b[0m       0.5104            0.5104        1.1320  0.0007  0.8982\n",
      "     38            \u001b[36m0.9104\u001b[0m        \u001b[32m0.5856\u001b[0m       0.5417            0.5417        \u001b[94m1.0592\u001b[0m  0.0007  1.0158\n",
      "     39            \u001b[36m0.9417\u001b[0m        \u001b[32m0.5442\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m1.0521\u001b[0m  0.0007  0.9860\n",
      "     40            \u001b[36m0.9479\u001b[0m        \u001b[32m0.4893\u001b[0m       0.5625            0.5625        1.0684  0.0007  0.9661\n",
      "     41            0.9458        \u001b[32m0.4244\u001b[0m       0.5521            0.5521        1.0957  0.0007  0.8603\n",
      "     42            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3993\u001b[0m       0.5625            0.5625        \u001b[94m1.0450\u001b[0m  0.0007  0.8107\n",
      "     43            \u001b[36m0.9792\u001b[0m        \u001b[32m0.3626\u001b[0m       0.5729            0.5729        \u001b[94m1.0311\u001b[0m  0.0006  0.8805\n",
      "     44            \u001b[36m0.9875\u001b[0m        0.3679       0.5417            0.5417        1.0506  0.0006  0.8408\n",
      "     45            0.9854        \u001b[32m0.3105\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0097\u001b[0m  0.0005  0.8286\n",
      "     46            0.9875        \u001b[32m0.2764\u001b[0m       0.5833            0.5833        1.0117  0.0005  0.8518\n",
      "     47            \u001b[36m0.9896\u001b[0m        \u001b[32m0.2667\u001b[0m       0.5938            0.5938        \u001b[94m1.0060\u001b[0m  0.0004  0.8613\n",
      "     48            \u001b[36m0.9917\u001b[0m        0.2696       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        1.0069  0.0004  0.8397\n",
      "     49            0.9917        0.2684       0.6250            0.6250        \u001b[94m0.9807\u001b[0m  0.0003  0.8595\n",
      "     50            0.9917        \u001b[32m0.2451\u001b[0m       0.6146            0.6146        \u001b[94m0.9775\u001b[0m  0.0003  0.8239\n",
      "Fine tuning model for subject 7 with 460 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4667        1.5463       0.4062            0.4062        1.2503  0.0004  1.0242\n",
      "     32            0.5854        1.2534       0.4688            0.4688        1.2093  0.0005  1.0521\n",
      "     33            0.6417        1.1098       0.4479            0.4479        1.1950  0.0005  1.0416\n",
      "     34            0.7417        0.9163       0.4792            0.4792        1.1728  0.0006  0.8300\n",
      "     35            0.8063        0.7965       0.5000            0.5000        1.1741  0.0006  0.8257\n",
      "     36            \u001b[36m0.8417\u001b[0m        0.7688       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        \u001b[94m1.0953\u001b[0m  0.0007  0.8313\n",
      "     37            \u001b[36m0.8792\u001b[0m        \u001b[32m0.6726\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m1.0948\u001b[0m  0.0007  0.8351\n",
      "     38            \u001b[36m0.9083\u001b[0m        \u001b[32m0.6295\u001b[0m       0.5208            0.5208        1.1132  0.0007  0.8957\n",
      "     39            \u001b[36m0.9167\u001b[0m        \u001b[32m0.5395\u001b[0m       0.5417            0.5417        \u001b[94m1.0864\u001b[0m  0.0007  0.8398\n",
      "     40            \u001b[36m0.9417\u001b[0m        \u001b[32m0.4733\u001b[0m       0.5625            0.5625        \u001b[94m1.0572\u001b[0m  0.0007  0.9175\n",
      "     41            \u001b[36m0.9604\u001b[0m        \u001b[32m0.4340\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0148\u001b[0m  0.0007  0.9109\n",
      "     42            \u001b[36m0.9625\u001b[0m        \u001b[32m0.3576\u001b[0m       0.5833            0.5833        1.0166  0.0007  1.0998\n",
      "     43            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3489\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.0201  0.0006  1.3156\n",
      "     44            \u001b[36m0.9771\u001b[0m        0.3506       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9761\u001b[0m  0.0006  0.9450\n",
      "     45            \u001b[36m0.9875\u001b[0m        \u001b[32m0.3083\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        0.9812  0.0005  1.0156\n",
      "     46            \u001b[36m0.9917\u001b[0m        0.3150       0.6354            0.6354        \u001b[94m0.9752\u001b[0m  0.0005  0.8292\n",
      "     47            \u001b[36m0.9938\u001b[0m        \u001b[32m0.2890\u001b[0m       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.9430\u001b[0m  0.0004  0.8380\n",
      "     48            \u001b[36m0.9958\u001b[0m        \u001b[32m0.2835\u001b[0m       0.6667            0.6667        \u001b[94m0.9333\u001b[0m  0.0004  0.8409\n",
      "     49            0.9958        \u001b[32m0.2445\u001b[0m       0.6458            0.6458        0.9448  0.0003  0.8696\n",
      "     50            0.9938        \u001b[32m0.2345\u001b[0m       0.6667            0.6667        0.9526  0.0003  0.8699\n",
      "Before finetuning for subject 8, the baseline accuracy is 0.4791666666666667\n",
      "Fine tuning model for subject 8 with 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7479        1.1390       0.5104            0.5104        1.2512  0.0004  1.2248\n",
      "     32            0.7604        0.8576       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        \u001b[94m1.0827\u001b[0m  0.0005  1.2046\n",
      "     33            \u001b[36m0.8187\u001b[0m        \u001b[32m0.6719\u001b[0m       0.4479            0.4479        1.2862  0.0005  1.2753\n",
      "     34            \u001b[36m0.9104\u001b[0m        \u001b[32m0.6105\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.1377  0.0006  1.3284\n",
      "     35            \u001b[36m0.9271\u001b[0m        \u001b[32m0.4667\u001b[0m       0.5208            0.5208        1.1279  0.0006  1.6650\n",
      "     36            \u001b[36m0.9479\u001b[0m        \u001b[32m0.4257\u001b[0m       0.5104            0.5104        1.1171  0.0007  1.2025\n",
      "     37            0.9458        \u001b[32m0.4144\u001b[0m       0.5729            0.5729        1.2043  0.0007  1.1975\n",
      "     38            \u001b[36m0.9625\u001b[0m        \u001b[32m0.3477\u001b[0m       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        1.1338  0.0007  1.2351\n",
      "     39            \u001b[36m0.9708\u001b[0m        \u001b[32m0.3331\u001b[0m       0.5417            0.5417        1.2609  0.0007  1.2211\n",
      "     40            \u001b[36m0.9812\u001b[0m        \u001b[32m0.2580\u001b[0m       0.5938            0.5938        1.1433  0.0007  1.3184\n",
      "     41            0.9688        \u001b[32m0.2296\u001b[0m       0.5417            0.5417        1.3390  0.0007  1.1836\n",
      "     42            \u001b[36m0.9854\u001b[0m        \u001b[32m0.1963\u001b[0m       0.5625            0.5625        1.2643  0.0007  1.2563\n",
      "     43            \u001b[36m0.9917\u001b[0m        \u001b[32m0.1671\u001b[0m       0.5104            0.5104        1.2910  0.0006  1.2000\n",
      "     44            \u001b[36m0.9958\u001b[0m        0.1687       0.6250            0.6250        1.1593  0.0006  1.4794\n",
      "     45            0.9958        \u001b[32m0.1513\u001b[0m       0.5938            0.5938        1.2625  0.0005  1.6620\n",
      "     46            \u001b[36m0.9979\u001b[0m        \u001b[32m0.1226\u001b[0m       0.6042            0.6042        1.2056  0.0005  1.5019\n",
      "     47            0.9979        0.1231       0.6250            0.6250        1.1981  0.0004  1.3151\n",
      "     48            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1164\u001b[0m       0.5938            0.5938        1.2550  0.0004  1.2448\n",
      "     49            1.0000        \u001b[32m0.0960\u001b[0m       0.6146            0.6146        1.2268  0.0003  1.2356\n",
      "     50            1.0000        \u001b[32m0.0842\u001b[0m       0.6146            0.6146        1.2030  0.0003  1.2258\n",
      "Fine tuning model for subject 8 with 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6937        1.1607       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.2652  0.0004  1.0033\n",
      "     32            0.8042        0.8355       0.5104            0.5104        1.1699  0.0005  1.1438\n",
      "     33            \u001b[36m0.8292\u001b[0m        \u001b[32m0.6750\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.1357  0.0005  1.1460\n",
      "     34            \u001b[36m0.9021\u001b[0m        \u001b[32m0.5574\u001b[0m       0.5417            0.5417        \u001b[94m1.1111\u001b[0m  0.0006  1.5205\n",
      "     35            \u001b[36m0.9229\u001b[0m        \u001b[32m0.4978\u001b[0m       0.5312            0.5312        1.2239  0.0006  1.4322\n",
      "     36            0.9208        \u001b[32m0.4731\u001b[0m       0.5417            0.5417        1.1679  0.0007  1.8458\n",
      "     37            \u001b[36m0.9458\u001b[0m        \u001b[32m0.4037\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m1.0688\u001b[0m  0.0007  1.5930\n",
      "     38            0.9354        \u001b[32m0.3419\u001b[0m       0.5625            0.5625        1.1598  0.0007  1.8911\n",
      "     39            \u001b[36m0.9771\u001b[0m        \u001b[32m0.3016\u001b[0m       0.5521            0.5521        1.1939  0.0007  1.8643\n",
      "     40            0.9729        \u001b[32m0.2708\u001b[0m       0.5729            0.5729        1.1839  0.0007  1.9769\n",
      "     41            \u001b[36m0.9833\u001b[0m        \u001b[32m0.2355\u001b[0m       0.5521            0.5521        1.2032  0.0007  2.0570\n",
      "     42            0.9812        \u001b[32m0.2221\u001b[0m       0.5521            0.5521        1.3797  0.0007  1.8334\n",
      "     43            0.9833        0.2326       0.5312            0.5312        1.3348  0.0006  1.0574\n",
      "     44            \u001b[36m0.9958\u001b[0m        \u001b[32m0.1993\u001b[0m       0.6146            0.6146        1.1806  0.0006  1.0061\n",
      "     45            0.9833        \u001b[32m0.1507\u001b[0m       0.5417            0.5417        1.2049  0.0005  0.9861\n",
      "     46            \u001b[36m0.9979\u001b[0m        0.1712       0.5521            0.5521        1.2411  0.0005  1.1174\n",
      "     47            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1281\u001b[0m       0.5625            0.5625        1.2363  0.0004  0.9997\n",
      "     48            1.0000        \u001b[32m0.1256\u001b[0m       0.5417            0.5417        1.2512  0.0004  1.0202\n",
      "     49            1.0000        \u001b[32m0.1185\u001b[0m       0.5417            0.5417        1.2152  0.0003  1.0217\n",
      "     50            1.0000        \u001b[32m0.0904\u001b[0m       0.5521            0.5521        1.2316  0.0003  0.9705\n",
      "Fine tuning model for subject 8 with 60 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6896        1.1803       0.5104            0.5104        1.2203  0.0004  1.3000\n",
      "     32            0.7708        0.8191       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.1895  0.0005  1.1422\n",
      "     33            \u001b[36m0.8396\u001b[0m        \u001b[32m0.6672\u001b[0m       0.5417            0.5417        \u001b[94m1.0983\u001b[0m  0.0005  0.9933\n",
      "     34            \u001b[36m0.8875\u001b[0m        \u001b[32m0.5642\u001b[0m       0.5521            0.5521        \u001b[94m1.0421\u001b[0m  0.0006  1.0591\n",
      "     35            \u001b[36m0.9125\u001b[0m        \u001b[32m0.4954\u001b[0m       0.5208            0.5208        1.0950  0.0006  0.9545\n",
      "     36            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4146\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.1206  0.0007  0.9075\n",
      "     37            0.9333        \u001b[32m0.3838\u001b[0m       0.5417            0.5417        1.1775  0.0007  0.9229\n",
      "     38            \u001b[36m0.9583\u001b[0m        \u001b[32m0.3451\u001b[0m       0.5417            0.5417        1.1501  0.0007  0.9068\n",
      "     39            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3449\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.1389  0.0007  0.8911\n",
      "     40            0.9750        \u001b[32m0.2731\u001b[0m       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        1.1238  0.0007  0.9181\n",
      "     41            \u001b[36m0.9958\u001b[0m        \u001b[32m0.2632\u001b[0m       0.5521            0.5521        1.1852  0.0007  0.9066\n",
      "     42            0.9875        \u001b[32m0.2476\u001b[0m       0.5625            0.5625        1.0963  0.0007  0.9223\n",
      "     43            0.9792        \u001b[32m0.2055\u001b[0m       0.5625            0.5625        1.0700  0.0006  1.1930\n",
      "     44            0.9938        0.2096       0.6042            0.6042        1.1211  0.0006  1.2480\n",
      "     45            0.9938        \u001b[32m0.1634\u001b[0m       0.6042            0.6042        1.1121  0.0005  1.7453\n",
      "     46            0.9958        \u001b[32m0.1589\u001b[0m       0.5521            0.5521        1.0935  0.0005  0.9219\n",
      "     47            \u001b[36m0.9979\u001b[0m        \u001b[32m0.1393\u001b[0m       0.5729            0.5729        1.1619  0.0004  0.9219\n",
      "     48            0.9979        \u001b[32m0.1284\u001b[0m       0.6042            0.6042        1.1430  0.0004  1.0047\n",
      "     49            \u001b[36m1.0000\u001b[0m        0.1393       0.6042            0.6042        1.1565  0.0003  1.0007\n",
      "     50            0.9979        \u001b[32m0.1176\u001b[0m       0.6250            0.6250        1.1198  0.0003  1.0766\n",
      "Fine tuning model for subject 8 with 80 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6104        1.1611       0.4688            0.4688        1.3480  0.0004  0.9327\n",
      "     32            0.7438        0.8624       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.1919  0.0005  0.9156\n",
      "     33            0.8042        \u001b[32m0.7048\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.1771  0.0005  0.9505\n",
      "     34            \u001b[36m0.8771\u001b[0m        \u001b[32m0.6227\u001b[0m       0.5625            0.5625        \u001b[94m1.0962\u001b[0m  0.0006  1.2479\n",
      "     35            \u001b[36m0.9062\u001b[0m        \u001b[32m0.5313\u001b[0m       0.5625            0.5625        \u001b[94m1.0799\u001b[0m  0.0006  1.7214\n",
      "     36            \u001b[36m0.9187\u001b[0m        \u001b[32m0.4491\u001b[0m       0.5625            0.5625        1.1525  0.0007  1.1301\n",
      "     37            \u001b[36m0.9313\u001b[0m        \u001b[32m0.4393\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0745\u001b[0m  0.0007  1.0282\n",
      "     38            \u001b[36m0.9417\u001b[0m        \u001b[32m0.3812\u001b[0m       0.5417            0.5417        1.1318  0.0007  0.9986\n",
      "     39            \u001b[36m0.9625\u001b[0m        \u001b[32m0.3360\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.1393  0.0007  1.0847\n",
      "     40            \u001b[36m0.9729\u001b[0m        \u001b[32m0.3025\u001b[0m       0.6042            0.6042        1.1093  0.0007  0.9330\n",
      "     41            0.9688        \u001b[32m0.2754\u001b[0m       0.5625            0.5625        1.0919  0.0007  1.0347\n",
      "     42            \u001b[36m0.9833\u001b[0m        \u001b[32m0.2394\u001b[0m       0.5833            0.5833        1.1245  0.0007  1.0322\n",
      "     43            \u001b[36m0.9917\u001b[0m        0.2561       0.5521            0.5521        1.1149  0.0006  1.1045\n",
      "     44            \u001b[36m0.9938\u001b[0m        \u001b[32m0.2157\u001b[0m       0.6042            0.6042        \u001b[94m1.0699\u001b[0m  0.0006  1.0220\n",
      "     45            0.9938        0.2198       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        1.0897  0.0005  1.1741\n",
      "     46            \u001b[36m0.9979\u001b[0m        \u001b[32m0.1881\u001b[0m       0.5833            0.5833        1.1224  0.0005  1.1617\n",
      "     47            0.9979        \u001b[32m0.1726\u001b[0m       0.5938            0.5938        1.1216  0.0004  0.9799\n",
      "     48            0.9979        \u001b[32m0.1524\u001b[0m       0.5833            0.5833        1.1266  0.0004  1.3486\n",
      "     49            0.9979        \u001b[32m0.1478\u001b[0m       0.5938            0.5938        1.1300  0.0003  1.3023\n",
      "     50            0.9979        \u001b[32m0.1335\u001b[0m       0.5938            0.5938        1.0932  0.0003  0.9875\n",
      "Fine tuning model for subject 8 with 100 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6375        1.1959       0.4792            0.4792        1.1464  0.0004  0.9371\n",
      "     32            0.7146        0.9246       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        1.2404  0.0005  0.9850\n",
      "     33            0.7583        0.7890       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.2532  0.0005  1.1040\n",
      "     34            \u001b[36m0.8333\u001b[0m        \u001b[32m0.6762\u001b[0m       0.5208            0.5208        \u001b[94m1.1262\u001b[0m  0.0006  1.0614\n",
      "     35            \u001b[36m0.8521\u001b[0m        \u001b[32m0.5847\u001b[0m       0.5729            0.5729        \u001b[94m1.1156\u001b[0m  0.0006  1.1872\n",
      "     36            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5526\u001b[0m       0.5833            0.5833        \u001b[94m1.0931\u001b[0m  0.0007  1.3484\n",
      "     37            \u001b[36m0.9062\u001b[0m        \u001b[32m0.4578\u001b[0m       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m1.0656\u001b[0m  0.0007  1.1093\n",
      "     38            \u001b[36m0.9375\u001b[0m        \u001b[32m0.4275\u001b[0m       0.5625            0.5625        1.0950  0.0007  1.0483\n",
      "     39            \u001b[36m0.9458\u001b[0m        \u001b[32m0.3536\u001b[0m       0.5208            0.5208        1.1116  0.0007  0.9331\n",
      "     40            \u001b[36m0.9625\u001b[0m        \u001b[32m0.3276\u001b[0m       0.5312            0.5312        1.1087  0.0007  0.9537\n",
      "     41            0.9604        \u001b[32m0.2968\u001b[0m       0.6042            0.6042        1.1010  0.0007  0.9159\n",
      "     42            \u001b[36m0.9771\u001b[0m        \u001b[32m0.2592\u001b[0m       0.5625            0.5625        1.0955  0.0007  1.0521\n",
      "     43            \u001b[36m0.9854\u001b[0m        0.2759       0.5729            0.5729        1.1009  0.0006  1.1847\n",
      "     44            0.9833        \u001b[32m0.2585\u001b[0m       0.5625            0.5625        1.1386  0.0006  1.1055\n",
      "     45            0.9854        \u001b[32m0.2291\u001b[0m       0.5417            0.5417        1.1467  0.0005  1.1461\n",
      "     46            0.9854        \u001b[32m0.2245\u001b[0m       0.5729            0.5729        1.1512  0.0005  1.0836\n",
      "     47            \u001b[36m0.9958\u001b[0m        \u001b[32m0.1844\u001b[0m       0.5833            0.5833        1.1053  0.0004  1.0457\n",
      "     48            0.9958        0.1974       0.5729            0.5729        1.1187  0.0004  1.1484\n",
      "     49            0.9958        \u001b[32m0.1723\u001b[0m       0.5625            0.5625        1.1240  0.0003  0.8569\n",
      "     50            0.9958        0.2043       0.5521            0.5521        1.1193  0.0003  0.9437\n",
      "Fine tuning model for subject 8 with 120 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6167        1.2024       0.4792            0.4792        1.1631  0.0004  0.8959\n",
      "     32            0.6771        0.9625       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        1.2736  0.0005  0.9661\n",
      "     33            0.7396        0.8265       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.1961  0.0005  0.9612\n",
      "     34            0.8083        \u001b[32m0.6712\u001b[0m       0.5625            0.5625        1.1437  0.0006  0.9518\n",
      "     35            \u001b[36m0.8625\u001b[0m        \u001b[32m0.6009\u001b[0m       0.5521            0.5521        \u001b[94m1.0780\u001b[0m  0.0006  1.0941\n",
      "     36            0.8583        \u001b[32m0.5571\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.0890  0.0007  0.9934\n",
      "     37            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4657\u001b[0m       0.5625            0.5625        \u001b[94m1.0706\u001b[0m  0.0007  1.0000\n",
      "     38            \u001b[36m0.9187\u001b[0m        \u001b[32m0.4374\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.0744  0.0007  1.2310\n",
      "     39            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4062\u001b[0m       0.5729            0.5729        1.1032  0.0007  1.2747\n",
      "     40            \u001b[36m0.9542\u001b[0m        \u001b[32m0.3777\u001b[0m       0.5938            0.5938        1.1059  0.0007  0.9541\n",
      "     41            \u001b[36m0.9688\u001b[0m        \u001b[32m0.3508\u001b[0m       0.5938            0.5938        1.0734  0.0007  0.8841\n",
      "     42            \u001b[36m0.9771\u001b[0m        \u001b[32m0.2867\u001b[0m       0.5833            0.5833        1.0732  0.0007  0.8636\n",
      "     43            \u001b[36m0.9896\u001b[0m        0.3032       0.5938            0.5938        \u001b[94m1.0620\u001b[0m  0.0006  0.8595\n",
      "     44            0.9875        \u001b[32m0.2742\u001b[0m       0.5938            0.5938        1.1100  0.0006  0.9983\n",
      "     45            0.9854        0.2763       0.5833            0.5833        1.1144  0.0005  0.9731\n",
      "     46            \u001b[36m0.9938\u001b[0m        \u001b[32m0.2226\u001b[0m       0.5938            0.5938        1.1062  0.0005  0.8983\n",
      "     47            \u001b[36m0.9958\u001b[0m        0.2267       0.5729            0.5729        1.0956  0.0004  0.9169\n",
      "     48            0.9896        \u001b[32m0.2049\u001b[0m       0.5833            0.5833        1.1133  0.0004  0.9126\n",
      "     49            0.9938        \u001b[32m0.2026\u001b[0m       0.5938            0.5938        1.1050  0.0003  0.9989\n",
      "     50            \u001b[36m0.9979\u001b[0m        \u001b[32m0.1912\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.0865  0.0003  1.0903\n",
      "Fine tuning model for subject 8 with 140 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6396        1.2537       0.4792            0.4792        \u001b[94m1.1069\u001b[0m  0.0004  1.0970\n",
      "     32            0.6917        1.0460       0.4792            0.4792        1.2151  0.0005  0.8863\n",
      "     33            0.7292        0.8158       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        1.2049  0.0005  0.8511\n",
      "     34            0.7958        0.7445       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.1316  0.0006  0.9583\n",
      "     35            \u001b[36m0.8417\u001b[0m        \u001b[32m0.6109\u001b[0m       0.5729            0.5729        \u001b[94m1.0750\u001b[0m  0.0006  0.8132\n",
      "     36            \u001b[36m0.8604\u001b[0m        \u001b[32m0.5375\u001b[0m       0.5625            0.5625        \u001b[94m1.0524\u001b[0m  0.0007  0.8327\n",
      "     37            \u001b[36m0.8625\u001b[0m        \u001b[32m0.5148\u001b[0m       0.5625            0.5625        1.0742  0.0007  0.7962\n",
      "     38            \u001b[36m0.9187\u001b[0m        \u001b[32m0.4908\u001b[0m       0.5729            0.5729        1.0825  0.0007  0.7756\n",
      "     39            \u001b[36m0.9354\u001b[0m        \u001b[32m0.4427\u001b[0m       0.5729            0.5729        \u001b[94m1.0358\u001b[0m  0.0007  0.8051\n",
      "     40            \u001b[36m0.9479\u001b[0m        \u001b[32m0.3994\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0197\u001b[0m  0.0007  0.7883\n",
      "     41            \u001b[36m0.9563\u001b[0m        \u001b[32m0.3291\u001b[0m       0.5938            0.5938        1.0435  0.0007  0.8090\n",
      "     42            \u001b[36m0.9688\u001b[0m        0.3400       0.6042            0.6042        1.0628  0.0007  0.8374\n",
      "     43            \u001b[36m0.9708\u001b[0m        \u001b[32m0.3143\u001b[0m       0.5938            0.5938        1.0783  0.0006  0.9614\n",
      "     44            \u001b[36m0.9792\u001b[0m        \u001b[32m0.2943\u001b[0m       0.5521            0.5521        1.1067  0.0006  0.9674\n",
      "     45            0.9771        \u001b[32m0.2703\u001b[0m       0.5417            0.5417        1.1220  0.0005  0.9979\n",
      "     46            \u001b[36m0.9833\u001b[0m        \u001b[32m0.2498\u001b[0m       0.5521            0.5521        1.1034  0.0005  0.8547\n",
      "     47            \u001b[36m0.9938\u001b[0m        \u001b[32m0.2468\u001b[0m       0.6042            0.6042        1.0816  0.0004  0.8515\n",
      "     48            0.9938        \u001b[32m0.2428\u001b[0m       0.5833            0.5833        1.0808  0.0004  0.8033\n",
      "     49            0.9875        \u001b[32m0.2183\u001b[0m       0.5938            0.5938        1.0985  0.0003  0.7962\n",
      "     50            0.9938        \u001b[32m0.2108\u001b[0m       0.5938            0.5938        1.0937  0.0003  0.8274\n",
      "Fine tuning model for subject 8 with 160 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6229        1.1483       0.4792            0.4792        1.1472  0.0004  0.8345\n",
      "     32            0.7042        0.9721       0.5000            0.5000        1.2387  0.0005  0.8025\n",
      "     33            0.7500        0.8585       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.2500  0.0005  0.8492\n",
      "     34            0.7792        \u001b[32m0.7035\u001b[0m       0.5625            0.5625        1.1715  0.0006  0.8780\n",
      "     35            \u001b[36m0.8313\u001b[0m        \u001b[32m0.6440\u001b[0m       0.5625            0.5625        \u001b[94m1.1062\u001b[0m  0.0006  0.8250\n",
      "     36            0.8292        \u001b[32m0.5776\u001b[0m       0.5521            0.5521        1.1062  0.0007  1.0594\n",
      "     37            \u001b[36m0.9021\u001b[0m        \u001b[32m0.4970\u001b[0m       0.5625            0.5625        \u001b[94m1.0805\u001b[0m  0.0007  0.9357\n",
      "     38            \u001b[36m0.9146\u001b[0m        \u001b[32m0.4683\u001b[0m       0.5521            0.5521        1.0925  0.0007  1.1296\n",
      "     39            \u001b[36m0.9375\u001b[0m        \u001b[32m0.3949\u001b[0m       0.5417            0.5417        \u001b[94m1.0770\u001b[0m  0.0007  0.8608\n",
      "     40            0.9250        \u001b[32m0.3761\u001b[0m       0.5729            0.5729        1.0775  0.0007  0.9169\n",
      "     41            \u001b[36m0.9479\u001b[0m        \u001b[32m0.3702\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m1.0746\u001b[0m  0.0007  0.9504\n",
      "     42            \u001b[36m0.9625\u001b[0m        \u001b[32m0.3464\u001b[0m       0.5729            0.5729        1.0818  0.0007  0.8856\n",
      "     43            \u001b[36m0.9667\u001b[0m        \u001b[32m0.2994\u001b[0m       0.5417            0.5417        1.0909  0.0006  0.9467\n",
      "     44            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2762\u001b[0m       0.5521            0.5521        1.0786  0.0006  0.8198\n",
      "     45            \u001b[36m0.9833\u001b[0m        \u001b[32m0.2646\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.0753  0.0005  0.8382\n",
      "     46            \u001b[36m0.9896\u001b[0m        \u001b[32m0.2557\u001b[0m       0.5938            0.5938        1.0773  0.0005  0.9047\n",
      "     47            0.9875        \u001b[32m0.2220\u001b[0m       0.6042            0.6042        1.1036  0.0004  0.8868\n",
      "     48            \u001b[36m0.9917\u001b[0m        0.2386       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.1062  0.0004  1.0122\n",
      "     49            0.9896        0.2546       0.6146            0.6146        1.1180  0.0003  1.0339\n",
      "     50            0.9917        \u001b[32m0.1929\u001b[0m       0.5938            0.5938        1.1022  0.0003  1.0165\n",
      "Fine tuning model for subject 8 with 180 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6292        1.1813       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        \u001b[94m1.0866\u001b[0m  0.0004  0.8534\n",
      "     32            0.6854        0.9752       0.5000            0.5000        1.1671  0.0005  0.8012\n",
      "     33            0.7208        0.8009       0.5312            0.5312        1.2963  0.0005  0.8366\n",
      "     34            0.7917        \u001b[32m0.6857\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.1912  0.0006  0.8559\n",
      "     35            \u001b[36m0.8354\u001b[0m        \u001b[32m0.6147\u001b[0m       0.5521            0.5521        1.1358  0.0006  1.1354\n",
      "     36            \u001b[36m0.8562\u001b[0m        \u001b[32m0.5572\u001b[0m       0.5625            0.5625        1.1559  0.0007  0.8446\n",
      "     37            \u001b[36m0.8792\u001b[0m        \u001b[32m0.5201\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.1093  0.0007  0.7912\n",
      "     38            \u001b[36m0.8917\u001b[0m        \u001b[32m0.4714\u001b[0m       0.5729            0.5729        1.0946  0.0007  0.8154\n",
      "     39            \u001b[36m0.9208\u001b[0m        \u001b[32m0.4121\u001b[0m       0.5729            0.5729        1.1021  0.0007  0.8997\n",
      "     40            0.9208        \u001b[32m0.3965\u001b[0m       0.5417            0.5417        1.1249  0.0007  0.8415\n",
      "     41            \u001b[36m0.9500\u001b[0m        \u001b[32m0.3608\u001b[0m       0.5417            0.5417        1.1310  0.0007  0.9150\n",
      "     42            \u001b[36m0.9625\u001b[0m        0.3636       0.5417            0.5417        1.1150  0.0007  0.9156\n",
      "     43            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3209\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.1063  0.0006  0.9342\n",
      "     44            \u001b[36m0.9833\u001b[0m        \u001b[32m0.2895\u001b[0m       0.5521            0.5521        1.1121  0.0006  0.9156\n",
      "     45            0.9771        \u001b[32m0.2781\u001b[0m       0.5833            0.5833        1.1373  0.0005  0.8117\n",
      "     46            \u001b[36m0.9917\u001b[0m        \u001b[32m0.2534\u001b[0m       0.5521            0.5521        1.1073  0.0005  0.7994\n",
      "     47            \u001b[36m0.9979\u001b[0m        0.2708       0.5625            0.5625        1.0979  0.0004  0.8351\n",
      "     48            0.9896        \u001b[32m0.2161\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.1198  0.0004  0.7985\n",
      "     49            0.9958        \u001b[32m0.1979\u001b[0m       0.5938            0.5938        1.0961  0.0003  0.8533\n",
      "     50            0.9979        0.2328       0.5833            0.5833        \u001b[94m1.0726\u001b[0m  0.0003  0.9867\n",
      "Fine tuning model for subject 8 with 200 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6333        1.2844       0.5104            0.5104        \u001b[94m1.1068\u001b[0m  0.0004  0.8867\n",
      "     32            0.7042        0.9915       0.5000            0.5000        1.1609  0.0005  0.8018\n",
      "     33            0.7250        0.8748       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.2002  0.0005  0.7976\n",
      "     34            0.7833        \u001b[32m0.7232\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        \u001b[94m1.0995\u001b[0m  0.0006  0.9561\n",
      "     35            \u001b[36m0.8375\u001b[0m        \u001b[32m0.6497\u001b[0m       0.5417            0.5417        \u001b[94m1.0706\u001b[0m  0.0006  1.0172\n",
      "     36            \u001b[36m0.8583\u001b[0m        \u001b[32m0.5364\u001b[0m       0.5417            0.5417        1.0939  0.0007  0.9559\n",
      "     37            \u001b[36m0.8938\u001b[0m        \u001b[32m0.5025\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.1067  0.0007  0.8561\n",
      "     38            \u001b[36m0.9125\u001b[0m        \u001b[32m0.4445\u001b[0m       0.5729            0.5729        1.1095  0.0007  0.8022\n",
      "     39            \u001b[36m0.9250\u001b[0m        0.4762       0.5938            0.5938        \u001b[94m1.0618\u001b[0m  0.0007  0.7988\n",
      "     40            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4118\u001b[0m       0.5729            0.5729        1.1178  0.0007  0.7880\n",
      "     41            \u001b[36m0.9437\u001b[0m        \u001b[32m0.3607\u001b[0m       0.5729            0.5729        \u001b[94m1.0584\u001b[0m  0.0007  0.8338\n",
      "     42            \u001b[36m0.9646\u001b[0m        \u001b[32m0.3449\u001b[0m       0.5833            0.5833        \u001b[94m1.0570\u001b[0m  0.0007  0.8688\n",
      "     43            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3140\u001b[0m       0.5625            0.5625        1.0804  0.0006  0.8672\n",
      "     44            \u001b[36m0.9729\u001b[0m        \u001b[32m0.2967\u001b[0m       0.5833            0.5833        1.0637  0.0006  0.8738\n",
      "     45            0.9729        \u001b[32m0.2743\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.0643  0.0005  0.8612\n",
      "     46            \u001b[36m0.9792\u001b[0m        \u001b[32m0.2396\u001b[0m       0.5833            0.5833        1.0792  0.0005  0.9751\n",
      "     47            \u001b[36m0.9854\u001b[0m        0.2725       0.5938            0.5938        1.0873  0.0004  1.0594\n",
      "     48            \u001b[36m0.9896\u001b[0m        \u001b[32m0.2156\u001b[0m       0.5938            0.5938        1.0682  0.0004  0.9797\n",
      "     49            \u001b[36m0.9938\u001b[0m        0.2362       0.6042            0.6042        1.0764  0.0003  1.0326\n",
      "     50            0.9938        \u001b[32m0.2016\u001b[0m       0.5938            0.5938        1.0830  0.0003  1.0052\n",
      "Fine tuning model for subject 8 with 220 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6292        1.2198       0.4792            0.4792        \u001b[94m1.1155\u001b[0m  0.0004  0.9257\n",
      "     32            0.6979        0.9276       0.5000            0.5000        1.1803  0.0005  0.9397\n",
      "     33            0.7188        0.7754       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.2307  0.0005  0.7993\n",
      "     34            0.7729        \u001b[32m0.7262\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.1172  0.0006  0.7817\n",
      "     35            \u001b[36m0.8292\u001b[0m        \u001b[32m0.6405\u001b[0m       0.5417            0.5417        \u001b[94m1.0665\u001b[0m  0.0006  0.7818\n",
      "     36            \u001b[36m0.8542\u001b[0m        \u001b[32m0.5385\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.1134  0.0007  0.7813\n",
      "     37            \u001b[36m0.8938\u001b[0m        \u001b[32m0.5056\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.1191  0.0007  0.7812\n",
      "     38            \u001b[36m0.9021\u001b[0m        \u001b[32m0.4836\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.1169  0.0007  0.7813\n",
      "     39            \u001b[36m0.9208\u001b[0m        \u001b[32m0.4264\u001b[0m       0.5833            0.5833        \u001b[94m1.0563\u001b[0m  0.0007  0.8130\n",
      "     40            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3801\u001b[0m       0.5938            0.5938        1.0776  0.0007  0.8906\n",
      "     41            \u001b[36m0.9458\u001b[0m        \u001b[32m0.3552\u001b[0m       0.5938            0.5938        1.0834  0.0007  0.9062\n",
      "     42            \u001b[36m0.9604\u001b[0m        \u001b[32m0.3174\u001b[0m       0.5938            0.5938        1.0914  0.0007  0.9765\n",
      "     43            \u001b[36m0.9688\u001b[0m        \u001b[32m0.2979\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.0952  0.0006  0.9062\n",
      "     44            \u001b[36m0.9792\u001b[0m        \u001b[32m0.2900\u001b[0m       0.5938            0.5938        1.0846  0.0006  0.7969\n",
      "     45            0.9771        \u001b[32m0.2708\u001b[0m       0.5833            0.5833        1.1064  0.0005  0.8209\n",
      "     46            0.9792        0.2772       0.5833            0.5833        1.1012  0.0005  0.7817\n",
      "     47            \u001b[36m0.9896\u001b[0m        \u001b[32m0.2355\u001b[0m       0.5625            0.5625        1.0722  0.0004  0.7816\n",
      "     48            0.9896        \u001b[32m0.2317\u001b[0m       0.5729            0.5729        1.0772  0.0004  0.7969\n",
      "     49            0.9896        0.2502       0.5938            0.5938        1.0946  0.0003  0.7816\n",
      "     50            0.9875        \u001b[32m0.2079\u001b[0m       0.6042            0.6042        1.1064  0.0003  0.7817\n",
      "Fine tuning model for subject 8 with 240 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6396        1.2404       0.4792            0.4792        \u001b[94m1.1005\u001b[0m  0.0004  0.7817\n",
      "     32            0.7000        0.9569       0.4896            0.4896        1.1441  0.0005  0.7973\n",
      "     33            0.7417        0.8301       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        1.2360  0.0005  0.7969\n",
      "     34            0.7917        0.7344       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.1863  0.0006  0.9063\n",
      "     35            0.8083        \u001b[32m0.6370\u001b[0m       0.5625            0.5625        1.1258  0.0006  0.8906\n",
      "     36            \u001b[36m0.8604\u001b[0m        \u001b[32m0.5558\u001b[0m       0.5625            0.5625        \u001b[94m1.0915\u001b[0m  0.0007  0.9380\n",
      "     37            \u001b[36m0.8875\u001b[0m        \u001b[32m0.4722\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0810\u001b[0m  0.0007  0.9219\n",
      "     38            \u001b[36m0.9208\u001b[0m        0.4803       0.5625            0.5625        1.1024  0.0007  0.7812\n",
      "     39            \u001b[36m0.9375\u001b[0m        \u001b[32m0.4079\u001b[0m       0.5312            0.5312        1.1006  0.0007  0.7812\n",
      "     40            \u001b[36m0.9500\u001b[0m        \u001b[32m0.3733\u001b[0m       0.5521            0.5521        1.1071  0.0007  0.7969\n",
      "     41            0.9313        \u001b[32m0.3588\u001b[0m       0.5938            0.5938        1.1153  0.0007  0.7812\n",
      "     42            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3376\u001b[0m       0.5729            0.5729        1.0993  0.0007  0.7812\n",
      "     43            \u001b[36m0.9792\u001b[0m        \u001b[32m0.3005\u001b[0m       0.5833            0.5833        1.1270  0.0006  0.8125\n",
      "     44            0.9625        \u001b[32m0.2819\u001b[0m       0.6042            0.6042        1.1343  0.0006  0.7969\n",
      "     45            \u001b[36m0.9917\u001b[0m        \u001b[32m0.2624\u001b[0m       0.5729            0.5729        1.0929  0.0005  0.8130\n",
      "     46            0.9896        \u001b[32m0.2490\u001b[0m       0.5833            0.5833        1.1157  0.0005  0.7969\n",
      "     47            0.9917        0.2604       0.5833            0.5833        1.1062  0.0004  0.8125\n",
      "     48            0.9917        \u001b[32m0.2307\u001b[0m       0.6042            0.6042        1.0926  0.0004  0.8906\n",
      "     49            \u001b[36m0.9979\u001b[0m        \u001b[32m0.2228\u001b[0m       0.5729            0.5729        1.0826  0.0003  0.8906\n",
      "     50            0.9958        \u001b[32m0.2060\u001b[0m       0.6042            0.6042        \u001b[94m1.0784\u001b[0m  0.0003  1.0312\n",
      "Fine tuning model for subject 8 with 260 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6354        1.1983       0.4896            0.4896        \u001b[94m1.1144\u001b[0m  0.0004  0.7817\n",
      "     32            0.6771        0.9974       0.5000            0.5000        1.1960  0.0005  0.7973\n",
      "     33            0.7562        0.7968       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        1.1601  0.0005  0.8135\n",
      "     34            0.8000        \u001b[32m0.7050\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.1301  0.0006  0.7819\n",
      "     35            \u001b[36m0.8167\u001b[0m        \u001b[32m0.6567\u001b[0m       0.5625            0.5625        1.1220  0.0006  0.7812\n",
      "     36            \u001b[36m0.8625\u001b[0m        \u001b[32m0.5297\u001b[0m       0.5417            0.5417        \u001b[94m1.0663\u001b[0m  0.0007  0.7813\n",
      "     37            \u001b[36m0.8917\u001b[0m        \u001b[32m0.5288\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.0728  0.0007  0.7969\n",
      "     38            \u001b[36m0.9125\u001b[0m        \u001b[32m0.4717\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0650\u001b[0m  0.0007  0.7974\n",
      "     39            \u001b[36m0.9167\u001b[0m        \u001b[32m0.4094\u001b[0m       0.5833            0.5833        1.0815  0.0007  0.7973\n",
      "     40            \u001b[36m0.9271\u001b[0m        \u001b[32m0.3864\u001b[0m       0.5833            0.5833        1.0687  0.0007  0.7816\n",
      "     41            \u001b[36m0.9458\u001b[0m        0.4080       0.5833            0.5833        1.0855  0.0007  0.8285\n",
      "     42            \u001b[36m0.9583\u001b[0m        \u001b[32m0.3565\u001b[0m       0.5729            0.5729        1.1133  0.0007  0.9067\n",
      "     43            \u001b[36m0.9729\u001b[0m        \u001b[32m0.3261\u001b[0m       0.5625            0.5625        1.1507  0.0006  0.8906\n",
      "     44            0.9708        \u001b[32m0.2934\u001b[0m       0.5833            0.5833        1.1277  0.0006  0.9375\n",
      "     45            0.9688        \u001b[32m0.2859\u001b[0m       0.5625            0.5625        1.1937  0.0005  0.8144\n",
      "     46            \u001b[36m0.9812\u001b[0m        \u001b[32m0.2839\u001b[0m       0.5833            0.5833        1.1074  0.0005  0.7817\n",
      "     47            0.9771        \u001b[32m0.2635\u001b[0m       0.5521            0.5521        1.1205  0.0004  0.7834\n",
      "     48            \u001b[36m0.9938\u001b[0m        \u001b[32m0.2474\u001b[0m       0.5833            0.5833        1.1068  0.0004  0.7818\n",
      "     49            0.9896        \u001b[32m0.2209\u001b[0m       0.5729            0.5729        1.1176  0.0003  0.7817\n",
      "     50            0.9875        \u001b[32m0.2018\u001b[0m       0.5938            0.5938        1.1319  0.0003  0.8285\n",
      "Fine tuning model for subject 8 with 280 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6333        1.1937       0.4792            0.4792        \u001b[94m1.1198\u001b[0m  0.0004  0.7974\n",
      "     32            0.6917        1.0194       0.5000            0.5000        1.1575  0.0005  0.7813\n",
      "     33            0.7625        0.8077       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        1.1693  0.0005  0.7817\n",
      "     34            \u001b[36m0.8313\u001b[0m        \u001b[32m0.6868\u001b[0m       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        \u001b[94m1.1006\u001b[0m  0.0006  0.7818\n",
      "     35            \u001b[36m0.8521\u001b[0m        \u001b[32m0.5886\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        \u001b[94m1.0815\u001b[0m  0.0006  0.8437\n",
      "     36            \u001b[36m0.8729\u001b[0m        \u001b[32m0.5217\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        \u001b[94m1.0569\u001b[0m  0.0007  0.9063\n",
      "     37            0.8708        \u001b[32m0.5037\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.1080  0.0007  0.9375\n",
      "     38            \u001b[36m0.8938\u001b[0m        \u001b[32m0.4521\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.1204  0.0007  1.0313\n",
      "     39            \u001b[36m0.9208\u001b[0m        0.4602       0.5729            0.5729        1.1332  0.0007  0.7813\n",
      "     40            \u001b[36m0.9354\u001b[0m        \u001b[32m0.3930\u001b[0m       0.5833            0.5833        1.0968  0.0007  0.7973\n",
      "     41            0.9313        \u001b[32m0.3846\u001b[0m       0.5625            0.5625        1.0846  0.0007  0.7969\n",
      "     42            \u001b[36m0.9542\u001b[0m        \u001b[32m0.3674\u001b[0m       0.5833            0.5833        1.0839  0.0007  0.7812\n",
      "     43            0.9542        \u001b[32m0.3264\u001b[0m       0.6042            0.6042        1.1360  0.0006  0.8437\n",
      "     44            \u001b[36m0.9729\u001b[0m        \u001b[32m0.2831\u001b[0m       0.5833            0.5833        1.1098  0.0006  0.8131\n",
      "     45            \u001b[36m0.9771\u001b[0m        0.2853       0.5729            0.5729        1.1125  0.0005  0.7969\n",
      "     46            0.9771        \u001b[32m0.2576\u001b[0m       0.5729            0.5729        1.0605  0.0005  0.7812\n",
      "     47            \u001b[36m0.9854\u001b[0m        \u001b[32m0.2390\u001b[0m       0.5938            0.5938        1.0854  0.0004  0.7812\n",
      "     48            \u001b[36m0.9875\u001b[0m        \u001b[32m0.2220\u001b[0m       0.6042            0.6042        1.0934  0.0004  0.7812\n",
      "     49            \u001b[36m0.9938\u001b[0m        0.2280       0.6042            0.6042        1.0869  0.0003  0.8438\n",
      "     50            0.9917        0.2361       0.6042            0.6042        1.0687  0.0003  0.9219\n",
      "Fine tuning model for subject 8 with 300 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6188        1.2199       0.4583            0.4583        \u001b[94m1.1200\u001b[0m  0.0004  0.9063\n",
      "     32            0.6937        0.9456       0.4688            0.4688        1.1410  0.0005  0.9375\n",
      "     33            0.7312        0.8158       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.1758  0.0005  0.7980\n",
      "     34            0.7688        \u001b[32m0.6679\u001b[0m       0.5729            0.5729        1.1576  0.0006  0.7973\n",
      "     35            \u001b[36m0.8292\u001b[0m        \u001b[32m0.6360\u001b[0m       0.5312            0.5312        \u001b[94m1.0915\u001b[0m  0.0006  0.7817\n",
      "     36            \u001b[36m0.8771\u001b[0m        \u001b[32m0.5326\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m1.0698\u001b[0m  0.0007  0.7818\n",
      "     37            \u001b[36m0.8979\u001b[0m        \u001b[32m0.4631\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0402\u001b[0m  0.0007  0.7812\n",
      "     38            \u001b[36m0.9000\u001b[0m        0.4712       0.6042            0.6042        1.0505  0.0007  0.7813\n",
      "     39            \u001b[36m0.9083\u001b[0m        \u001b[32m0.4001\u001b[0m       0.5938            0.5938        1.0748  0.0007  0.7979\n",
      "     40            \u001b[36m0.9271\u001b[0m        \u001b[32m0.3950\u001b[0m       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        1.0873  0.0007  0.7817\n",
      "     41            \u001b[36m0.9458\u001b[0m        \u001b[32m0.3767\u001b[0m       0.5938            0.5938        1.0969  0.0007  0.7812\n",
      "     42            \u001b[36m0.9625\u001b[0m        \u001b[32m0.3127\u001b[0m       0.5938            0.5938        1.0740  0.0007  0.7813\n",
      "     43            0.9458        \u001b[32m0.3025\u001b[0m       0.5521            0.5521        1.1053  0.0006  0.8750\n",
      "     44            \u001b[36m0.9771\u001b[0m        \u001b[32m0.2960\u001b[0m       0.5729            0.5729        1.0882  0.0006  0.9380\n",
      "     45            \u001b[36m0.9812\u001b[0m        0.2961       0.5938            0.5938        1.1179  0.0005  0.9062\n",
      "     46            0.9812        \u001b[32m0.2642\u001b[0m       0.6042            0.6042        1.1025  0.0005  0.9219\n",
      "     47            \u001b[36m0.9875\u001b[0m        0.2736       0.5938            0.5938        1.0706  0.0004  0.7969\n",
      "     48            \u001b[36m0.9917\u001b[0m        \u001b[32m0.2304\u001b[0m       0.5729            0.5729        1.0705  0.0004  0.7817\n",
      "     49            \u001b[36m0.9938\u001b[0m        0.2372       0.5833            0.5833        1.0614  0.0003  0.7969\n",
      "     50            0.9938        \u001b[32m0.2056\u001b[0m       0.5625            0.5625        1.0574  0.0003  0.7973\n",
      "Fine tuning model for subject 8 with 320 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6396        1.2096       0.5000            0.5000        1.1318  0.0004  0.7812\n",
      "     32            0.6708        0.9843       0.5104            0.5104        1.2248  0.0005  0.7817\n",
      "     33            0.7479        0.8166       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        1.2106  0.0005  0.7969\n",
      "     34            0.7771        \u001b[32m0.6703\u001b[0m       0.5312            0.5312        1.1522  0.0006  0.7817\n",
      "     35            \u001b[36m0.8167\u001b[0m        \u001b[32m0.6423\u001b[0m       0.5312            0.5312        \u001b[94m1.0824\u001b[0m  0.0006  0.7813\n",
      "     36            \u001b[36m0.8583\u001b[0m        \u001b[32m0.5604\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        \u001b[94m1.0672\u001b[0m  0.0007  0.7816\n",
      "     37            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5207\u001b[0m       0.5729            0.5729        \u001b[94m1.0644\u001b[0m  0.0007  0.8926\n",
      "     38            \u001b[36m0.9104\u001b[0m        \u001b[32m0.4677\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.0812  0.0007  0.8594\n",
      "     39            \u001b[36m0.9125\u001b[0m        \u001b[32m0.3797\u001b[0m       0.5833            0.5833        1.0698  0.0007  0.8906\n",
      "     40            \u001b[36m0.9292\u001b[0m        0.4020       0.5833            0.5833        \u001b[94m1.0354\u001b[0m  0.0007  1.0000\n",
      "     41            \u001b[36m0.9583\u001b[0m        0.3956       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.0435  0.0007  0.7817\n",
      "     42            \u001b[36m0.9604\u001b[0m        \u001b[32m0.3277\u001b[0m       0.5938            0.5938        1.0615  0.0007  0.7817\n",
      "     43            \u001b[36m0.9833\u001b[0m        \u001b[32m0.2874\u001b[0m       0.5729            0.5729        1.0730  0.0006  0.7978\n",
      "     44            0.9812        \u001b[32m0.2819\u001b[0m       0.5625            0.5625        1.0650  0.0006  0.7812\n",
      "     45            0.9792        \u001b[32m0.2519\u001b[0m       0.5729            0.5729        1.0635  0.0005  0.7811\n",
      "     46            0.9812        0.2692       0.5729            0.5729        1.0737  0.0005  0.8124\n",
      "     47            \u001b[36m0.9875\u001b[0m        0.2559       0.5729            0.5729        1.0700  0.0004  0.8594\n",
      "     48            0.9812        \u001b[32m0.2237\u001b[0m       0.6042            0.6042        1.1081  0.0004  0.9063\n",
      "     49            \u001b[36m0.9917\u001b[0m        0.2328       0.5938            0.5938        1.0826  0.0003  0.7818\n",
      "     50            0.9896        \u001b[32m0.1912\u001b[0m       0.5938            0.5938        1.0860  0.0003  0.7973\n",
      "Fine tuning model for subject 8 with 340 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6229        1.2422       0.4896            0.4896        \u001b[94m1.1203\u001b[0m  0.0004  0.9844\n",
      "     32            0.7042        1.0240       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        1.1450  0.0005  0.9375\n",
      "     33            0.7562        0.8257       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        1.1769  0.0005  1.0000\n",
      "     34            0.8021        \u001b[32m0.6974\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.1402  0.0006  0.7970\n",
      "     35            \u001b[36m0.8250\u001b[0m        \u001b[32m0.6265\u001b[0m       0.5521            0.5521        \u001b[94m1.1120\u001b[0m  0.0006  0.7817\n",
      "     36            \u001b[36m0.8521\u001b[0m        \u001b[32m0.5561\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        \u001b[94m1.0811\u001b[0m  0.0007  0.9063\n",
      "     37            \u001b[36m0.9021\u001b[0m        \u001b[32m0.5217\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m1.0698\u001b[0m  0.0007  0.8438\n",
      "     38            \u001b[36m0.9083\u001b[0m        \u001b[32m0.4238\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.1176  0.0007  0.8281\n",
      "     39            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4162\u001b[0m       0.5833            0.5833        1.1155  0.0007  0.8125\n",
      "     40            \u001b[36m0.9375\u001b[0m        \u001b[32m0.4120\u001b[0m       0.5938            0.5938        1.0798  0.0007  0.8285\n",
      "     41            \u001b[36m0.9521\u001b[0m        \u001b[32m0.3619\u001b[0m       0.5938            0.5938        1.0922  0.0007  0.7812\n",
      "     42            \u001b[36m0.9688\u001b[0m        \u001b[32m0.2931\u001b[0m       0.5833            0.5833        1.0914  0.0007  0.7812\n",
      "     43            \u001b[36m0.9812\u001b[0m        0.3188       0.5938            0.5938        1.0744  0.0006  0.7812\n",
      "     44            0.9667        \u001b[32m0.2750\u001b[0m       0.5729            0.5729        1.0833  0.0006  0.8286\n",
      "     45            \u001b[36m0.9875\u001b[0m        \u001b[32m0.2735\u001b[0m       0.5833            0.5833        \u001b[94m1.0592\u001b[0m  0.0005  1.0000\n",
      "     46            0.9833        \u001b[32m0.2570\u001b[0m       0.5729            0.5729        1.0988  0.0005  0.8754\n",
      "     47            0.9875        0.2750       0.5625            0.5625        1.1072  0.0004  0.9844\n",
      "     48            \u001b[36m0.9896\u001b[0m        \u001b[32m0.2357\u001b[0m       0.6042            0.6042        1.1193  0.0004  0.7813\n",
      "     49            \u001b[36m0.9938\u001b[0m        \u001b[32m0.2215\u001b[0m       0.6042            0.6042        1.0922  0.0003  0.7969\n",
      "     50            \u001b[36m0.9958\u001b[0m        \u001b[32m0.2136\u001b[0m       0.5938            0.5938        \u001b[94m1.0555\u001b[0m  0.0003  0.7969\n",
      "Fine tuning model for subject 8 with 360 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6312        1.2005       0.4792            0.4792        \u001b[94m1.1232\u001b[0m  0.0004  0.8130\n",
      "     32            0.6958        0.9781       0.4688            0.4688        1.2072  0.0005  0.7814\n",
      "     33            0.7375        0.8472       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        1.2535  0.0005  0.7812\n",
      "     34            0.7625        \u001b[32m0.6859\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.2521  0.0006  0.7812\n",
      "     35            0.8104        \u001b[32m0.6117\u001b[0m       0.5521            0.5521        1.1788  0.0006  0.7813\n",
      "     36            \u001b[36m0.8688\u001b[0m        \u001b[32m0.5741\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.1346  0.0007  0.8131\n",
      "     37            \u001b[36m0.8938\u001b[0m        \u001b[32m0.4809\u001b[0m       0.5729            0.5729        \u001b[94m1.1229\u001b[0m  0.0007  0.8126\n",
      "     38            \u001b[36m0.9146\u001b[0m        \u001b[32m0.4645\u001b[0m       0.5833            0.5833        \u001b[94m1.1045\u001b[0m  0.0007  0.9063\n",
      "     39            \u001b[36m0.9271\u001b[0m        \u001b[32m0.4298\u001b[0m       0.5938            0.5938        \u001b[94m1.0858\u001b[0m  0.0007  0.9531\n",
      "     40            \u001b[36m0.9479\u001b[0m        \u001b[32m0.3901\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.1235  0.0007  0.9688\n",
      "     41            \u001b[36m0.9542\u001b[0m        \u001b[32m0.3765\u001b[0m       0.5729            0.5729        1.1086  0.0007  0.8287\n",
      "     42            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3313\u001b[0m       0.6042            0.6042        1.1532  0.0007  0.8125\n",
      "     43            \u001b[36m0.9708\u001b[0m        \u001b[32m0.3194\u001b[0m       0.5521            0.5521        1.1807  0.0006  0.7817\n",
      "     44            \u001b[36m0.9812\u001b[0m        \u001b[32m0.3039\u001b[0m       0.5938            0.5938        1.1193  0.0006  0.7973\n",
      "     45            \u001b[36m0.9833\u001b[0m        \u001b[32m0.2775\u001b[0m       0.5729            0.5729        1.1127  0.0005  0.7817\n",
      "     46            \u001b[36m0.9854\u001b[0m        \u001b[32m0.2584\u001b[0m       0.5625            0.5625        1.1272  0.0005  0.7812\n",
      "     47            \u001b[36m0.9917\u001b[0m        \u001b[32m0.2232\u001b[0m       0.5625            0.5625        1.1045  0.0004  0.7971\n",
      "     48            0.9896        0.2341       0.5729            0.5729        1.1095  0.0004  0.7969\n",
      "     49            \u001b[36m0.9938\u001b[0m        0.2471       0.5833            0.5833        1.1110  0.0003  0.7989\n",
      "     50            0.9938        \u001b[32m0.2173\u001b[0m       0.5938            0.5938        1.1085  0.0003  0.7969\n",
      "Fine tuning model for subject 8 with 380 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6312        1.2347       0.4688            0.4688        \u001b[94m1.1129\u001b[0m  0.0004  0.7830\n",
      "     32            0.7125        1.0077       0.5000            0.5000        1.1450  0.0005  0.9063\n",
      "     33            0.7562        0.7976       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        1.1964  0.0005  0.8599\n",
      "     34            0.7833        \u001b[32m0.6917\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.1677  0.0006  0.9844\n",
      "     35            \u001b[36m0.8271\u001b[0m        \u001b[32m0.6132\u001b[0m       0.5625            0.5625        \u001b[94m1.0884\u001b[0m  0.0006  0.8750\n",
      "     36            \u001b[36m0.8750\u001b[0m        \u001b[32m0.5841\u001b[0m       0.5417            0.5417        \u001b[94m1.0758\u001b[0m  0.0007  0.8282\n",
      "     37            \u001b[36m0.9083\u001b[0m        \u001b[32m0.5045\u001b[0m       0.5521            0.5521        \u001b[94m1.0564\u001b[0m  0.0007  0.7734\n",
      "     38            \u001b[36m0.9146\u001b[0m        \u001b[32m0.4561\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        1.0730  0.0007  0.7817\n",
      "     39            \u001b[36m0.9229\u001b[0m        \u001b[32m0.4304\u001b[0m       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        1.0617  0.0007  0.7818\n",
      "     40            \u001b[36m0.9375\u001b[0m        \u001b[32m0.4195\u001b[0m       0.5833            0.5833        1.0658  0.0007  0.7812\n",
      "     41            \u001b[36m0.9500\u001b[0m        \u001b[32m0.3366\u001b[0m       0.5833            0.5833        \u001b[94m1.0513\u001b[0m  0.0007  0.7817\n",
      "     42            \u001b[36m0.9604\u001b[0m        \u001b[32m0.3059\u001b[0m       0.5833            0.5833        1.0563  0.0007  0.8281\n",
      "     43            \u001b[36m0.9667\u001b[0m        \u001b[32m0.2810\u001b[0m       0.5938            0.5938        1.0633  0.0006  0.7817\n",
      "     44            \u001b[36m0.9708\u001b[0m        \u001b[32m0.2720\u001b[0m       0.6146            0.6146        1.0709  0.0006  0.7817\n",
      "     45            \u001b[36m0.9792\u001b[0m        0.2787       0.5938            0.5938        1.0517  0.0005  0.7974\n",
      "     46            \u001b[36m0.9812\u001b[0m        \u001b[32m0.2586\u001b[0m       0.5938            0.5938        1.0635  0.0005  0.9223\n",
      "     47            \u001b[36m0.9917\u001b[0m        0.2680       0.5938            0.5938        1.0732  0.0004  0.8750\n",
      "     48            0.9854        \u001b[32m0.2423\u001b[0m       0.5625            0.5625        1.0889  0.0004  0.9850\n",
      "     49            0.9833        \u001b[32m0.2223\u001b[0m       0.5729            0.5729        1.0938  0.0003  0.8907\n",
      "     50            0.9917        0.2278       0.5833            0.5833        1.0789  0.0003  0.7818\n",
      "Fine tuning model for subject 8 with 400 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6354        1.2339       0.5104            0.5104        1.1411  0.0004  0.7969\n",
      "     32            0.6937        0.9274       0.4896            0.4896        1.1861  0.0005  0.8126\n",
      "     33            0.7562        0.8361       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.1758  0.0005  0.7817\n",
      "     34            0.7958        0.7360       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        \u001b[94m1.1165\u001b[0m  0.0006  0.7974\n",
      "     35            \u001b[36m0.8229\u001b[0m        \u001b[32m0.6221\u001b[0m       0.5417            0.5417        \u001b[94m1.0915\u001b[0m  0.0006  0.7974\n",
      "     36            \u001b[36m0.8438\u001b[0m        \u001b[32m0.5431\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0846\u001b[0m  0.0007  0.7969\n",
      "     37            \u001b[36m0.8646\u001b[0m        \u001b[32m0.5215\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0811\u001b[0m  0.0007  0.7813\n",
      "     38            \u001b[36m0.9104\u001b[0m        \u001b[32m0.4688\u001b[0m       0.5521            0.5521        1.0841  0.0007  0.7818\n",
      "     39            \u001b[36m0.9313\u001b[0m        \u001b[32m0.4178\u001b[0m       0.5521            0.5521        1.1016  0.0007  0.7812\n",
      "     40            \u001b[36m0.9458\u001b[0m        \u001b[32m0.3935\u001b[0m       0.5938            0.5938        \u001b[94m1.0473\u001b[0m  0.0007  0.9063\n",
      "     41            0.9250        \u001b[32m0.3914\u001b[0m       0.5625            0.5625        1.0488  0.0007  0.9219\n",
      "     42            0.9437        \u001b[32m0.3548\u001b[0m       0.5625            0.5625        \u001b[94m0.9998\u001b[0m  0.0007  1.0469\n",
      "     43            \u001b[36m0.9583\u001b[0m        \u001b[32m0.2989\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.0400  0.0006  0.7813\n",
      "     44            \u001b[36m0.9625\u001b[0m        0.3035       0.5938            0.5938        1.0694  0.0006  0.7817\n",
      "     45            \u001b[36m0.9833\u001b[0m        \u001b[32m0.2901\u001b[0m       0.6042            0.6042        1.0477  0.0005  0.7812\n",
      "     46            0.9812        \u001b[32m0.2696\u001b[0m       0.6042            0.6042        1.0396  0.0005  0.7817\n",
      "     47            \u001b[36m0.9896\u001b[0m        \u001b[32m0.2188\u001b[0m       0.5833            0.5833        1.0522  0.0004  0.7813\n",
      "     48            \u001b[36m0.9958\u001b[0m        0.2384       0.6146            0.6146        1.0478  0.0004  0.7974\n",
      "     49            0.9958        0.2225       0.6042            0.6042        1.0438  0.0003  0.7973\n",
      "     50            0.9896        \u001b[32m0.2035\u001b[0m       0.5938            0.5938        1.0552  0.0003  0.7969\n",
      "Fine tuning model for subject 8 with 420 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6354        1.2104       0.5104            0.5104        \u001b[94m1.1150\u001b[0m  0.0004  0.7815\n",
      "     32            0.6625        0.9611       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        1.3017  0.0005  0.7812\n",
      "     33            0.7229        0.8111       0.5208            0.5208        1.3434  0.0005  0.8125\n",
      "     34            0.8083        0.7366       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.1512  0.0006  0.9531\n",
      "     35            \u001b[36m0.8333\u001b[0m        \u001b[32m0.6416\u001b[0m       0.5625            0.5625        \u001b[94m1.0979\u001b[0m  0.0006  0.9062\n",
      "     36            \u001b[36m0.8646\u001b[0m        \u001b[32m0.5451\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m1.0854\u001b[0m  0.0007  0.9844\n",
      "     37            \u001b[36m0.8750\u001b[0m        \u001b[32m0.5078\u001b[0m       0.5833            0.5833        1.0863  0.0007  0.7823\n",
      "     38            \u001b[36m0.9125\u001b[0m        \u001b[32m0.4750\u001b[0m       0.5625            0.5625        \u001b[94m1.0791\u001b[0m  0.0007  0.7818\n",
      "     39            \u001b[36m0.9250\u001b[0m        \u001b[32m0.4459\u001b[0m       0.5833            0.5833        1.1085  0.0007  0.7812\n",
      "     40            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4262\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0576\u001b[0m  0.0007  0.7968\n",
      "     41            \u001b[36m0.9500\u001b[0m        \u001b[32m0.3676\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.0671  0.0007  0.7813\n",
      "     42            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3489\u001b[0m       0.5729            0.5729        1.0764  0.0007  0.7818\n",
      "     43            \u001b[36m0.9729\u001b[0m        \u001b[32m0.3409\u001b[0m       0.6146            0.6146        1.0824  0.0006  0.7818\n",
      "     44            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3177\u001b[0m       0.6146            0.6146        1.0766  0.0006  0.7816\n",
      "     45            \u001b[36m0.9833\u001b[0m        \u001b[32m0.2628\u001b[0m       0.6042            0.6042        1.0826  0.0005  0.7969\n",
      "     46            0.9833        0.2898       0.5833            0.5833        1.0992  0.0005  0.7969\n",
      "     47            \u001b[36m0.9875\u001b[0m        \u001b[32m0.2483\u001b[0m       0.5833            0.5833        1.0752  0.0004  0.8281\n",
      "     48            \u001b[36m0.9958\u001b[0m        \u001b[32m0.2480\u001b[0m       0.6042            0.6042        1.0756  0.0004  0.9219\n",
      "     49            0.9938        \u001b[32m0.2156\u001b[0m       0.6042            0.6042        1.0734  0.0003  0.9063\n",
      "     50            0.9938        0.2401       0.5938            0.5938        1.0693  0.0003  0.9693\n",
      "Fine tuning model for subject 8 with 440 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6396        1.2315       0.5000            0.5000        \u001b[94m1.1000\u001b[0m  0.0004  0.7817\n",
      "     32            0.6875        1.0143       0.5000            0.5000        1.1800  0.0005  0.7817\n",
      "     33            0.7000        0.8396       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.3024  0.0005  1.0005\n",
      "     34            0.7729        \u001b[32m0.6748\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.2762  0.0006  0.7969\n",
      "     35            \u001b[36m0.8313\u001b[0m        \u001b[32m0.5975\u001b[0m       0.5521            0.5521        1.1539  0.0006  0.7823\n",
      "     36            \u001b[36m0.8562\u001b[0m        0.6076       0.5417            0.5417        1.1362  0.0007  0.7811\n",
      "     37            \u001b[36m0.8833\u001b[0m        \u001b[32m0.5086\u001b[0m       0.5417            0.5417        1.1031  0.0007  0.7814\n",
      "     38            \u001b[36m0.9062\u001b[0m        \u001b[32m0.4867\u001b[0m       0.5625            0.5625        \u001b[94m1.0790\u001b[0m  0.0007  0.8756\n",
      "     39            \u001b[36m0.9292\u001b[0m        \u001b[32m0.4528\u001b[0m       0.5625            0.5625        \u001b[94m1.0753\u001b[0m  0.0007  0.7812\n",
      "     40            \u001b[36m0.9417\u001b[0m        \u001b[32m0.4223\u001b[0m       0.5521            0.5521        \u001b[94m1.0649\u001b[0m  0.0007  0.7969\n",
      "     41            \u001b[36m0.9583\u001b[0m        \u001b[32m0.3644\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.0652  0.0007  0.9219\n",
      "     42            \u001b[36m0.9646\u001b[0m        \u001b[32m0.3193\u001b[0m       0.5625            0.5625        1.1032  0.0007  0.9219\n",
      "     43            \u001b[36m0.9708\u001b[0m        \u001b[32m0.2738\u001b[0m       0.5729            0.5729        1.1190  0.0006  0.9219\n",
      "     44            \u001b[36m0.9812\u001b[0m        0.2889       0.5625            0.5625        1.1129  0.0006  0.9380\n",
      "     45            0.9750        0.3174       0.5833            0.5833        1.0806  0.0005  0.7817\n",
      "     46            \u001b[36m0.9917\u001b[0m        \u001b[32m0.2723\u001b[0m       0.5833            0.5833        1.0754  0.0005  0.7817\n",
      "     47            0.9896        \u001b[32m0.2379\u001b[0m       0.5729            0.5729        1.0849  0.0004  0.7813\n",
      "     48            0.9875        0.2405       0.5729            0.5729        1.0787  0.0004  0.7969\n",
      "     49            0.9917        \u001b[32m0.2214\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.0671  0.0003  0.7974\n",
      "     50            0.9917        \u001b[32m0.1942\u001b[0m       0.5833            0.5833        1.0729  0.0003  0.7817\n",
      "Fine tuning model for subject 8 with 460 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6104        1.2103       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        \u001b[94m1.1101\u001b[0m  0.0004  0.7973\n",
      "     32            0.7000        0.9964       0.5104            0.5104        1.1574  0.0005  0.8129\n",
      "     33            0.7354        0.8387       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        1.2485  0.0005  0.7813\n",
      "     34            0.7750        \u001b[32m0.7280\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.2214  0.0006  0.7969\n",
      "     35            0.7958        \u001b[32m0.6574\u001b[0m       0.5521            0.5521        1.1522  0.0006  0.9531\n",
      "     36            \u001b[36m0.8688\u001b[0m        \u001b[32m0.5493\u001b[0m       0.5521            0.5521        1.1179  0.0007  0.9062\n",
      "     37            \u001b[36m0.8958\u001b[0m        \u001b[32m0.5222\u001b[0m       0.5625            0.5625        \u001b[94m1.0943\u001b[0m  0.0007  0.9375\n",
      "     38            \u001b[36m0.9146\u001b[0m        \u001b[32m0.4674\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        \u001b[94m1.0840\u001b[0m  0.0007  0.8130\n",
      "     39            \u001b[36m0.9271\u001b[0m        \u001b[32m0.3935\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m1.0522\u001b[0m  0.0007  0.7812\n",
      "     40            \u001b[36m0.9375\u001b[0m        \u001b[32m0.3666\u001b[0m       0.5729            0.5729        \u001b[94m1.0300\u001b[0m  0.0007  0.7969\n",
      "     41            \u001b[36m0.9500\u001b[0m        \u001b[32m0.3502\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.0387  0.0007  0.7813\n",
      "     42            \u001b[36m0.9688\u001b[0m        \u001b[32m0.3144\u001b[0m       0.5938            0.5938        1.0620  0.0007  0.7970\n",
      "     43            \u001b[36m0.9729\u001b[0m        0.3203       0.5833            0.5833        1.0895  0.0006  0.7818\n",
      "     44            0.9708        \u001b[32m0.3047\u001b[0m       0.5417            0.5417        1.1095  0.0006  0.7817\n",
      "     45            \u001b[36m0.9833\u001b[0m        \u001b[32m0.2519\u001b[0m       0.5521            0.5521        1.1245  0.0005  0.7972\n",
      "     46            \u001b[36m0.9875\u001b[0m        \u001b[32m0.2498\u001b[0m       0.5625            0.5625        1.1028  0.0005  0.7975\n",
      "     47            0.9812        \u001b[32m0.2315\u001b[0m       0.5625            0.5625        1.1064  0.0004  0.7818\n",
      "     48            0.9854        0.2331       0.5625            0.5625        1.0951  0.0004  0.7813\n",
      "     49            \u001b[36m0.9917\u001b[0m        \u001b[32m0.2202\u001b[0m       0.5833            0.5833        1.0831  0.0003  0.9062\n",
      "     50            \u001b[36m0.9938\u001b[0m        0.2214       0.5938            0.5938        1.0760  0.0003  0.9063\n",
      "Before finetuning for subject 9, the baseline accuracy is 0.4375\n",
      "Fine tuning model for subject 9 with 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7354        1.2044       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.9006\u001b[0m  0.0004  1.4692\n",
      "     32            \u001b[36m0.8917\u001b[0m        \u001b[32m0.6747\u001b[0m       0.6458            0.6458        0.9018  0.0005  1.1580\n",
      "     33            \u001b[36m0.9062\u001b[0m        \u001b[32m0.5022\u001b[0m       \u001b[35m0.7396\u001b[0m            \u001b[31m0.7396\u001b[0m        \u001b[94m0.8415\u001b[0m  0.0005  1.1589\n",
      "     34            \u001b[36m0.9563\u001b[0m        \u001b[32m0.3800\u001b[0m       0.7292            0.7292        0.8434  0.0006  1.1406\n",
      "     35            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3306\u001b[0m       0.7083            0.7083        \u001b[94m0.8204\u001b[0m  0.0006  1.1568\n",
      "     36            0.9604        \u001b[32m0.2735\u001b[0m       0.6562            0.6562        0.9936  0.0007  1.1723\n",
      "     37            0.9729        \u001b[32m0.2588\u001b[0m       0.7396            0.7396        0.8819  0.0007  1.1563\n",
      "     38            0.9667        \u001b[32m0.1963\u001b[0m       0.6979            0.6979        0.9470  0.0007  1.1563\n",
      "     39            \u001b[36m0.9875\u001b[0m        0.2234       0.7188            0.7188        0.9728  0.0007  1.3125\n",
      "     40            \u001b[36m0.9917\u001b[0m        \u001b[32m0.1543\u001b[0m       0.6875            0.6875        1.0706  0.0007  1.5625\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1429\u001b[0m       0.6979            0.6979        0.8809  0.0007  1.1719\n",
      "     42            0.9958        \u001b[32m0.1050\u001b[0m       0.6979            0.6979        0.9843  0.0007  1.1719\n",
      "     43            0.9979        \u001b[32m0.0826\u001b[0m       0.7396            0.7396        1.0072  0.0006  1.1568\n",
      "     44            1.0000        \u001b[32m0.0767\u001b[0m       0.6875            0.6875        1.0840  0.0006  1.1569\n",
      "     45            1.0000        \u001b[32m0.0746\u001b[0m       0.7396            0.7396        0.9965  0.0005  1.1723\n",
      "     46            1.0000        \u001b[32m0.0423\u001b[0m       0.7083            0.7083        1.0205  0.0005  1.2036\n",
      "     47            1.0000        0.0533       0.7188            0.7188        0.9773  0.0004  1.2031\n",
      "     48            1.0000        0.0500       0.7396            0.7396        0.9718  0.0004  1.1723\n",
      "     49            1.0000        0.0495       \u001b[35m0.7604\u001b[0m            \u001b[31m0.7604\u001b[0m        0.9877  0.0003  1.3750\n",
      "     50            1.0000        \u001b[32m0.0370\u001b[0m       0.7396            0.7396        1.0066  0.0003  1.5938\n",
      "Fine tuning model for subject 9 with 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7604        1.2443       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9819\u001b[0m  0.0004  1.0000\n",
      "     32            \u001b[36m0.8375\u001b[0m        \u001b[32m0.7244\u001b[0m       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8480\u001b[0m  0.0005  0.9541\n",
      "     33            \u001b[36m0.9104\u001b[0m        \u001b[32m0.4354\u001b[0m       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        \u001b[94m0.7974\u001b[0m  0.0005  0.9375\n",
      "     34            \u001b[36m0.9583\u001b[0m        \u001b[32m0.3971\u001b[0m       \u001b[35m0.7292\u001b[0m            \u001b[31m0.7292\u001b[0m        0.8093  0.0006  0.9375\n",
      "     35            \u001b[36m0.9688\u001b[0m        \u001b[32m0.3580\u001b[0m       \u001b[35m0.7812\u001b[0m            \u001b[31m0.7812\u001b[0m        \u001b[94m0.7370\u001b[0m  0.0006  0.9536\n",
      "     36            0.9417        \u001b[32m0.2989\u001b[0m       0.7292            0.7292        0.7950  0.0007  0.9531\n",
      "     37            0.9667        \u001b[32m0.2629\u001b[0m       0.7083            0.7083        0.7913  0.0007  0.9375\n",
      "     38            \u001b[36m0.9938\u001b[0m        \u001b[32m0.1928\u001b[0m       0.7604            0.7604        0.7630  0.0007  0.9375\n",
      "     39            0.9896        \u001b[32m0.1872\u001b[0m       0.7500            0.7500        0.7481  0.0007  0.9375\n",
      "     40            0.9917        \u001b[32m0.1544\u001b[0m       0.7292            0.7292        0.7869  0.0007  0.9380\n",
      "     41            \u001b[36m0.9958\u001b[0m        \u001b[32m0.1381\u001b[0m       0.7396            0.7396        0.8611  0.0007  1.2031\n",
      "     42            \u001b[36m0.9979\u001b[0m        \u001b[32m0.1109\u001b[0m       0.7396            0.7396        0.8361  0.0007  1.2188\n",
      "     43            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0837\u001b[0m       0.7708            0.7708        0.8373  0.0006  0.9531\n",
      "     44            1.0000        \u001b[32m0.0683\u001b[0m       0.7708            0.7708        0.8178  0.0006  0.9531\n",
      "     45            1.0000        \u001b[32m0.0627\u001b[0m       \u001b[35m0.8021\u001b[0m            \u001b[31m0.8021\u001b[0m        0.8043  0.0005  0.9375\n",
      "     46            1.0000        0.0784       0.7917            0.7917        0.7894  0.0005  0.9375\n",
      "     47            1.0000        0.0747       0.7708            0.7708        0.8323  0.0004  0.9531\n",
      "     48            1.0000        \u001b[32m0.0561\u001b[0m       0.8021            0.8021        0.8276  0.0004  0.9536\n",
      "     49            1.0000        \u001b[32m0.0442\u001b[0m       0.7708            0.7708        0.8174  0.0003  0.9375\n",
      "     50            1.0000        \u001b[32m0.0401\u001b[0m       0.7812            0.7812        0.8225  0.0003  0.9375\n",
      "Fine tuning model for subject 9 with 60 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6958        1.3218       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m1.0074\u001b[0m  0.0004  0.8910\n",
      "     32            \u001b[36m0.8333\u001b[0m        0.7451       \u001b[35m0.7083\u001b[0m            \u001b[31m0.7083\u001b[0m        \u001b[94m0.8151\u001b[0m  0.0005  0.8906\n",
      "     33            \u001b[36m0.9042\u001b[0m        \u001b[32m0.5169\u001b[0m       0.7083            0.7083        \u001b[94m0.7973\u001b[0m  0.0005  1.0645\n",
      "     34            0.9042        \u001b[32m0.4524\u001b[0m       \u001b[35m0.7292\u001b[0m            \u001b[31m0.7292\u001b[0m        \u001b[94m0.7850\u001b[0m  0.0006  1.0469\n",
      "     35            \u001b[36m0.9583\u001b[0m        \u001b[32m0.3908\u001b[0m       \u001b[35m0.7500\u001b[0m            \u001b[31m0.7500\u001b[0m        \u001b[94m0.7711\u001b[0m  0.0006  1.2031\n",
      "     36            \u001b[36m0.9625\u001b[0m        \u001b[32m0.2915\u001b[0m       0.7292            0.7292        0.8301  0.0007  0.9063\n",
      "     37            0.9625        0.2964       0.7083            0.7083        0.8212  0.0007  0.8912\n",
      "     38            \u001b[36m0.9812\u001b[0m        \u001b[32m0.2409\u001b[0m       0.6979            0.6979        0.8492  0.0007  0.8907\n",
      "     39            \u001b[36m0.9938\u001b[0m        \u001b[32m0.1814\u001b[0m       0.6979            0.6979        0.8117  0.0007  0.8906\n",
      "     40            \u001b[36m0.9958\u001b[0m        \u001b[32m0.1539\u001b[0m       \u001b[35m0.7604\u001b[0m            \u001b[31m0.7604\u001b[0m        0.8528  0.0007  0.8913\n",
      "     41            \u001b[36m0.9979\u001b[0m        \u001b[32m0.1451\u001b[0m       0.7396            0.7396        0.8270  0.0007  0.8750\n",
      "     42            0.9979        \u001b[32m0.1149\u001b[0m       \u001b[35m0.7708\u001b[0m            \u001b[31m0.7708\u001b[0m        0.8054  0.0007  0.8750\n",
      "     43            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1118\u001b[0m       0.7396            0.7396        0.8786  0.0006  0.8906\n",
      "     44            1.0000        \u001b[32m0.1022\u001b[0m       0.7292            0.7292        0.8832  0.0006  0.8906\n",
      "     45            1.0000        \u001b[32m0.0892\u001b[0m       0.7708            0.7708        0.8469  0.0005  0.8906\n",
      "     46            1.0000        0.0998       0.7604            0.7604        0.8319  0.0005  1.0781\n",
      "     47            1.0000        \u001b[32m0.0825\u001b[0m       \u001b[35m0.7917\u001b[0m            \u001b[31m0.7917\u001b[0m        0.8228  0.0004  1.0625\n",
      "     48            1.0000        \u001b[32m0.0748\u001b[0m       0.7604            0.7604        0.8389  0.0004  1.1250\n",
      "     49            1.0000        \u001b[32m0.0612\u001b[0m       0.7396            0.7396        0.8710  0.0003  0.9063\n",
      "     50            1.0000        0.0791       0.7812            0.7812        0.8733  0.0003  0.8906\n",
      "Fine tuning model for subject 9 with 80 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6562        1.3358       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.1948  0.0004  0.8598\n",
      "     32            0.8000        0.8590       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9587\u001b[0m  0.0005  0.8602\n",
      "     33            \u001b[36m0.8625\u001b[0m        \u001b[32m0.6326\u001b[0m       \u001b[35m0.7083\u001b[0m            \u001b[31m0.7083\u001b[0m        \u001b[94m0.7760\u001b[0m  0.0005  0.8593\n",
      "     34            \u001b[36m0.9042\u001b[0m        \u001b[32m0.4762\u001b[0m       \u001b[35m0.7396\u001b[0m            \u001b[31m0.7396\u001b[0m        \u001b[94m0.7546\u001b[0m  0.0006  0.8594\n",
      "     35            \u001b[36m0.9250\u001b[0m        \u001b[32m0.3444\u001b[0m       0.7292            0.7292        \u001b[94m0.7534\u001b[0m  0.0006  0.8750\n",
      "     36            \u001b[36m0.9521\u001b[0m        0.3575       0.7396            0.7396        0.7765  0.0007  0.8599\n",
      "     37            \u001b[36m0.9667\u001b[0m        \u001b[32m0.2887\u001b[0m       \u001b[35m0.8021\u001b[0m            \u001b[31m0.8021\u001b[0m        \u001b[94m0.6859\u001b[0m  0.0007  0.8594\n",
      "     38            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2489\u001b[0m       0.7917            0.7917        0.7496  0.0007  0.8911\n",
      "     39            \u001b[36m0.9896\u001b[0m        \u001b[32m0.2002\u001b[0m       0.7500            0.7500        0.7706  0.0007  1.0469\n",
      "     40            \u001b[36m0.9979\u001b[0m        \u001b[32m0.1821\u001b[0m       0.7812            0.7812        0.6909  0.0007  1.0156\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1335\u001b[0m       0.7500            0.7500        0.7037  0.0007  1.0000\n",
      "     42            1.0000        0.1406       0.7500            0.7500        0.6994  0.0007  0.8750\n",
      "     43            0.9979        \u001b[32m0.1186\u001b[0m       0.7708            0.7708        0.7189  0.0006  0.8911\n",
      "     44            1.0000        \u001b[32m0.1114\u001b[0m       0.8021            0.8021        \u001b[94m0.6857\u001b[0m  0.0006  0.9223\n",
      "     45            1.0000        \u001b[32m0.1024\u001b[0m       0.7917            0.7917        0.7170  0.0005  0.8599\n",
      "     46            1.0000        \u001b[32m0.1020\u001b[0m       0.7917            0.7917        0.7008  0.0005  0.8750\n",
      "     47            1.0000        \u001b[32m0.0902\u001b[0m       0.7708            0.7708        0.7014  0.0004  0.8755\n",
      "     48            1.0000        \u001b[32m0.0755\u001b[0m       0.7917            0.7917        \u001b[94m0.6811\u001b[0m  0.0004  0.8598\n",
      "     49            1.0000        0.0816       0.7708            0.7708        0.6832  0.0003  0.8906\n",
      "     50            1.0000        0.0834       0.7812            0.7812        0.6989  0.0003  0.8906\n",
      "Fine tuning model for subject 9 with 100 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6375        1.3570       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        1.2446  0.0004  0.9063\n",
      "     32            0.7542        0.9851       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m0.9048\u001b[0m  0.0005  0.9688\n",
      "     33            \u001b[36m0.8125\u001b[0m        \u001b[32m0.7259\u001b[0m       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8936\u001b[0m  0.0005  0.9845\n",
      "     34            \u001b[36m0.8854\u001b[0m        \u001b[32m0.5485\u001b[0m       0.6458            0.6458        \u001b[94m0.8375\u001b[0m  0.0006  0.9531\n",
      "     35            0.8792        \u001b[32m0.4512\u001b[0m       0.6562            0.6562        0.8670  0.0006  0.8287\n",
      "     36            \u001b[36m0.9437\u001b[0m        \u001b[32m0.4199\u001b[0m       \u001b[35m0.7188\u001b[0m            \u001b[31m0.7188\u001b[0m        \u001b[94m0.7777\u001b[0m  0.0007  0.8287\n",
      "     37            \u001b[36m0.9583\u001b[0m        \u001b[32m0.3392\u001b[0m       \u001b[35m0.7396\u001b[0m            \u001b[31m0.7396\u001b[0m        \u001b[94m0.7520\u001b[0m  0.0007  0.8129\n",
      "     38            \u001b[36m0.9708\u001b[0m        \u001b[32m0.2793\u001b[0m       \u001b[35m0.7500\u001b[0m            \u001b[31m0.7500\u001b[0m        \u001b[94m0.7087\u001b[0m  0.0007  0.8125\n",
      "     39            \u001b[36m0.9833\u001b[0m        \u001b[32m0.2551\u001b[0m       0.7500            0.7500        0.7461  0.0007  0.8281\n",
      "     40            0.9771        \u001b[32m0.2155\u001b[0m       0.7292            0.7292        0.7769  0.0007  0.8285\n",
      "     41            0.9833        0.2172       0.7083            0.7083        0.7477  0.0007  0.8285\n",
      "     42            \u001b[36m0.9896\u001b[0m        \u001b[32m0.1903\u001b[0m       0.7500            0.7500        0.7597  0.0007  0.8270\n",
      "     43            \u001b[36m0.9979\u001b[0m        \u001b[32m0.1759\u001b[0m       \u001b[35m0.7812\u001b[0m            \u001b[31m0.7812\u001b[0m        0.7419  0.0006  0.8125\n",
      "     44            0.9979        \u001b[32m0.1504\u001b[0m       \u001b[35m0.7917\u001b[0m            \u001b[31m0.7917\u001b[0m        \u001b[94m0.7056\u001b[0m  0.0006  0.8144\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1216\u001b[0m       0.7604            0.7604        \u001b[94m0.6963\u001b[0m  0.0005  0.9063\n",
      "     46            1.0000        0.1333       \u001b[35m0.8021\u001b[0m            \u001b[31m0.8021\u001b[0m        0.7159  0.0005  1.0000\n",
      "     47            1.0000        \u001b[32m0.1134\u001b[0m       0.7708            0.7708        0.7556  0.0004  0.9844\n",
      "     48            1.0000        \u001b[32m0.1015\u001b[0m       0.7604            0.7604        0.7367  0.0004  0.8443\n",
      "     49            1.0000        \u001b[32m0.0933\u001b[0m       0.7604            0.7604        0.7451  0.0003  0.8282\n",
      "     50            0.9979        0.0952       0.7500            0.7500        0.7469  0.0003  0.8126\n",
      "Fine tuning model for subject 9 with 120 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6458        1.3238       0.4896            0.4896        1.2324  0.0004  0.8443\n",
      "     32            0.7521        0.9880       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        \u001b[94m0.9983\u001b[0m  0.0005  0.8449\n",
      "     33            \u001b[36m0.8271\u001b[0m        \u001b[32m0.7107\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.8797\u001b[0m  0.0005  0.8442\n",
      "     34            \u001b[36m0.8729\u001b[0m        \u001b[32m0.5656\u001b[0m       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8258\u001b[0m  0.0006  0.8592\n",
      "     35            \u001b[36m0.9083\u001b[0m        \u001b[32m0.4647\u001b[0m       \u001b[35m0.7083\u001b[0m            \u001b[31m0.7083\u001b[0m        \u001b[94m0.8080\u001b[0m  0.0006  0.8447\n",
      "     36            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3708\u001b[0m       \u001b[35m0.7188\u001b[0m            \u001b[31m0.7188\u001b[0m        \u001b[94m0.7684\u001b[0m  0.0007  0.8597\n",
      "     37            \u001b[36m0.9521\u001b[0m        \u001b[32m0.3379\u001b[0m       0.6875            0.6875        \u001b[94m0.7616\u001b[0m  0.0007  0.8437\n",
      "     38            \u001b[36m0.9729\u001b[0m        \u001b[32m0.3079\u001b[0m       \u001b[35m0.7708\u001b[0m            \u001b[31m0.7708\u001b[0m        \u001b[94m0.7262\u001b[0m  0.0007  0.9687\n",
      "     39            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2634\u001b[0m       0.7292            0.7292        0.7306  0.0007  1.0469\n",
      "     40            \u001b[36m0.9875\u001b[0m        \u001b[32m0.2161\u001b[0m       0.7604            0.7604        \u001b[94m0.7156\u001b[0m  0.0007  1.2344\n",
      "     41            \u001b[36m0.9917\u001b[0m        \u001b[32m0.1956\u001b[0m       \u001b[35m0.7812\u001b[0m            \u001b[31m0.7812\u001b[0m        0.7368  0.0007  1.2656\n",
      "     42            0.9917        \u001b[32m0.1655\u001b[0m       0.7708            0.7708        0.7536  0.0007  1.3595\n",
      "     43            \u001b[36m1.0000\u001b[0m        0.1761       0.7708            0.7708        0.7441  0.0006  1.2187\n",
      "     44            0.9979        0.1754       0.7500            0.7500        0.7345  0.0006  1.1879\n",
      "     45            0.9979        \u001b[32m0.1379\u001b[0m       0.7500            0.7500        0.7668  0.0005  0.8438\n",
      "     46            0.9979        \u001b[32m0.1258\u001b[0m       \u001b[35m0.7917\u001b[0m            \u001b[31m0.7917\u001b[0m        0.7455  0.0005  0.8594\n",
      "     47            0.9979        \u001b[32m0.1191\u001b[0m       0.7604            0.7604        0.7427  0.0004  0.8594\n",
      "     48            0.9979        \u001b[32m0.1127\u001b[0m       0.7708            0.7708        0.7375  0.0004  0.8437\n",
      "     49            1.0000        \u001b[32m0.1067\u001b[0m       0.7604            0.7604        0.7397  0.0003  0.8912\n",
      "     50            1.0000        \u001b[32m0.0934\u001b[0m       0.7708            0.7708        0.7527  0.0003  1.0938\n",
      "Fine tuning model for subject 9 with 140 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6354        1.3642       0.5000            0.5000        1.2754  0.0004  0.9219\n",
      "     32            0.7063        1.0056       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        \u001b[94m1.0434\u001b[0m  0.0005  0.9219\n",
      "     33            \u001b[36m0.8167\u001b[0m        \u001b[32m0.7215\u001b[0m       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8947\u001b[0m  0.0005  0.7813\n",
      "     34            \u001b[36m0.8313\u001b[0m        \u001b[32m0.6178\u001b[0m       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8850\u001b[0m  0.0006  0.7819\n",
      "     35            \u001b[36m0.8771\u001b[0m        \u001b[32m0.4947\u001b[0m       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        \u001b[94m0.8314\u001b[0m  0.0006  0.7812\n",
      "     36            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4437\u001b[0m       0.6875            0.6875        \u001b[94m0.8075\u001b[0m  0.0007  0.7813\n",
      "     37            \u001b[36m0.9167\u001b[0m        \u001b[32m0.3838\u001b[0m       \u001b[35m0.6979\u001b[0m            \u001b[31m0.6979\u001b[0m        \u001b[94m0.7908\u001b[0m  0.0007  0.7969\n",
      "     38            \u001b[36m0.9417\u001b[0m        \u001b[32m0.3078\u001b[0m       \u001b[35m0.7604\u001b[0m            \u001b[31m0.7604\u001b[0m        \u001b[94m0.7581\u001b[0m  0.0007  0.7968\n",
      "     39            \u001b[36m0.9521\u001b[0m        0.3115       0.7500            0.7500        \u001b[94m0.7566\u001b[0m  0.0007  0.7818\n",
      "     40            \u001b[36m0.9729\u001b[0m        \u001b[32m0.2780\u001b[0m       0.7188            0.7188        0.7816  0.0007  0.7974\n",
      "     41            \u001b[36m0.9854\u001b[0m        \u001b[32m0.2367\u001b[0m       0.7188            0.7188        0.7725  0.0007  0.7969\n",
      "     42            \u001b[36m0.9875\u001b[0m        \u001b[32m0.1931\u001b[0m       0.6979            0.6979        0.7757  0.0007  0.7817\n",
      "     43            \u001b[36m0.9896\u001b[0m        \u001b[32m0.1810\u001b[0m       0.7292            0.7292        \u001b[94m0.7413\u001b[0m  0.0006  0.8750\n",
      "     44            \u001b[36m0.9917\u001b[0m        \u001b[32m0.1561\u001b[0m       0.7500            0.7500        \u001b[94m0.7325\u001b[0m  0.0006  0.9536\n",
      "     45            \u001b[36m0.9958\u001b[0m        0.1711       0.7396            0.7396        0.7677  0.0005  0.9219\n",
      "     46            0.9938        \u001b[32m0.1507\u001b[0m       0.7500            0.7500        0.7669  0.0005  1.0476\n",
      "     47            0.9958        \u001b[32m0.1386\u001b[0m       0.7500            0.7500        0.7466  0.0004  0.7969\n",
      "     48            \u001b[36m0.9979\u001b[0m        \u001b[32m0.1227\u001b[0m       0.7396            0.7396        0.7466  0.0004  0.7812\n",
      "     49            \u001b[36m1.0000\u001b[0m        0.1278       0.7500            0.7500        0.7487  0.0003  0.7817\n",
      "     50            1.0000        \u001b[32m0.1170\u001b[0m       0.7604            0.7604        0.7508  0.0003  0.7990\n",
      "Fine tuning model for subject 9 with 160 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6188        1.2851       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        1.2131  0.0004  0.7813\n",
      "     32            0.7333        1.0212       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        \u001b[94m1.0267\u001b[0m  0.0005  0.7812\n",
      "     33            0.7854        0.7777       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m0.9318\u001b[0m  0.0005  0.7813\n",
      "     34            \u001b[36m0.8208\u001b[0m        \u001b[32m0.5696\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.8729\u001b[0m  0.0006  0.7973\n",
      "     35            \u001b[36m0.8792\u001b[0m        \u001b[32m0.4982\u001b[0m       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8448\u001b[0m  0.0006  0.7817\n",
      "     36            \u001b[36m0.9042\u001b[0m        \u001b[32m0.4472\u001b[0m       0.6667            0.6667        \u001b[94m0.8409\u001b[0m  0.0007  0.7969\n",
      "     37            \u001b[36m0.9229\u001b[0m        \u001b[32m0.4038\u001b[0m       \u001b[35m0.7083\u001b[0m            \u001b[31m0.7083\u001b[0m        \u001b[94m0.7897\u001b[0m  0.0007  0.9537\n",
      "     38            \u001b[36m0.9500\u001b[0m        \u001b[32m0.3188\u001b[0m       0.6979            0.6979        \u001b[94m0.7438\u001b[0m  0.0007  0.9219\n",
      "     39            \u001b[36m0.9688\u001b[0m        \u001b[32m0.2761\u001b[0m       \u001b[35m0.7500\u001b[0m            \u001b[31m0.7500\u001b[0m        \u001b[94m0.7153\u001b[0m  0.0007  0.9446\n",
      "     40            \u001b[36m0.9875\u001b[0m        \u001b[32m0.2756\u001b[0m       0.7500            0.7500        0.7160  0.0007  0.9067\n",
      "     41            0.9854        \u001b[32m0.2374\u001b[0m       0.7396            0.7396        0.7608  0.0007  0.7969\n",
      "     42            0.9854        \u001b[32m0.2285\u001b[0m       0.7500            0.7500        \u001b[94m0.7079\u001b[0m  0.0007  0.8531\n",
      "     43            \u001b[36m0.9958\u001b[0m        \u001b[32m0.1752\u001b[0m       \u001b[35m0.7604\u001b[0m            \u001b[31m0.7604\u001b[0m        0.7399  0.0006  0.8652\n",
      "     44            0.9958        \u001b[32m0.1735\u001b[0m       0.7188            0.7188        0.7639  0.0006  0.8255\n",
      "     45            \u001b[36m0.9979\u001b[0m        \u001b[32m0.1448\u001b[0m       0.7396            0.7396        0.7299  0.0005  0.8041\n",
      "     46            0.9979        0.1462       \u001b[35m0.7708\u001b[0m            \u001b[31m0.7708\u001b[0m        0.7220  0.0005  0.8715\n",
      "     47            0.9979        \u001b[32m0.1426\u001b[0m       0.7604            0.7604        0.7405  0.0004  0.8752\n",
      "     48            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1263\u001b[0m       0.7708            0.7708        0.7261  0.0004  0.8244\n",
      "     49            1.0000        \u001b[32m0.1255\u001b[0m       \u001b[35m0.7917\u001b[0m            \u001b[31m0.7917\u001b[0m        \u001b[94m0.7050\u001b[0m  0.0003  0.8994\n",
      "     50            1.0000        \u001b[32m0.1113\u001b[0m       0.7500            0.7500        \u001b[94m0.6991\u001b[0m  0.0003  0.9224\n",
      "Fine tuning model for subject 9 with 180 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6312        1.3198       0.5104            0.5104        1.2708  0.0004  1.0134\n",
      "     32            0.7167        0.9793       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        \u001b[94m1.0981\u001b[0m  0.0005  1.1909\n",
      "     33            0.7896        0.7656       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9529\u001b[0m  0.0005  0.8897\n",
      "     34            \u001b[36m0.8354\u001b[0m        \u001b[32m0.6068\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.9081\u001b[0m  0.0006  0.8324\n",
      "     35            \u001b[36m0.9042\u001b[0m        \u001b[32m0.4759\u001b[0m       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        \u001b[94m0.8445\u001b[0m  0.0006  0.7902\n",
      "     36            \u001b[36m0.9146\u001b[0m        \u001b[32m0.4184\u001b[0m       0.6771            0.6771        \u001b[94m0.8158\u001b[0m  0.0007  0.8042\n",
      "     37            \u001b[36m0.9354\u001b[0m        \u001b[32m0.3860\u001b[0m       \u001b[35m0.7292\u001b[0m            \u001b[31m0.7292\u001b[0m        \u001b[94m0.7901\u001b[0m  0.0007  0.7808\n",
      "     38            \u001b[36m0.9729\u001b[0m        \u001b[32m0.3246\u001b[0m       0.7292            0.7292        0.8141  0.0007  0.8046\n",
      "     39            0.9708        \u001b[32m0.2753\u001b[0m       \u001b[35m0.7396\u001b[0m            \u001b[31m0.7396\u001b[0m        0.8144  0.0007  0.8497\n",
      "     40            \u001b[36m0.9750\u001b[0m        0.2810       \u001b[35m0.7500\u001b[0m            \u001b[31m0.7500\u001b[0m        0.8124  0.0007  0.8001\n",
      "     41            0.9729        \u001b[32m0.2207\u001b[0m       0.7083            0.7083        0.8603  0.0007  0.8228\n",
      "     42            \u001b[36m0.9917\u001b[0m        \u001b[32m0.2103\u001b[0m       0.7396            0.7396        0.7904  0.0007  0.8177\n",
      "     43            \u001b[36m0.9958\u001b[0m        \u001b[32m0.1667\u001b[0m       \u001b[35m0.7604\u001b[0m            \u001b[31m0.7604\u001b[0m        \u001b[94m0.7691\u001b[0m  0.0006  0.8072\n",
      "     44            0.9938        \u001b[32m0.1630\u001b[0m       0.7604            0.7604        0.7917  0.0006  0.9704\n",
      "     45            0.9938        \u001b[32m0.1629\u001b[0m       0.7396            0.7396        0.8026  0.0005  0.9228\n",
      "     46            0.9958        0.1669       0.7396            0.7396        0.8120  0.0005  0.9536\n",
      "     47            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1340\u001b[0m       0.7604            0.7604        0.7775  0.0004  0.7934\n",
      "     48            0.9979        \u001b[32m0.1186\u001b[0m       0.7604            0.7604        0.7869  0.0004  0.8271\n",
      "     49            1.0000        0.1337       0.7604            0.7604        0.7880  0.0003  0.7829\n",
      "     50            0.9979        0.1214       0.7604            0.7604        0.7928  0.0003  0.7947\n",
      "Fine tuning model for subject 9 with 200 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6458        1.4152       0.5000            0.5000        1.2476  0.0004  0.8187\n",
      "     32            0.6771        0.9960       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        \u001b[94m1.0540\u001b[0m  0.0005  0.8206\n",
      "     33            0.8000        0.7491       \u001b[35m0.6771\u001b[0m            \u001b[31m0.6771\u001b[0m        \u001b[94m0.8914\u001b[0m  0.0005  0.8054\n",
      "     34            \u001b[36m0.8438\u001b[0m        \u001b[32m0.6289\u001b[0m       0.6771            0.6771        \u001b[94m0.8523\u001b[0m  0.0006  0.7887\n",
      "     35            \u001b[36m0.8854\u001b[0m        \u001b[32m0.5143\u001b[0m       \u001b[35m0.7083\u001b[0m            \u001b[31m0.7083\u001b[0m        \u001b[94m0.8118\u001b[0m  0.0006  0.8107\n",
      "     36            \u001b[36m0.9271\u001b[0m        \u001b[32m0.4309\u001b[0m       \u001b[35m0.7292\u001b[0m            \u001b[31m0.7292\u001b[0m        \u001b[94m0.7668\u001b[0m  0.0007  0.7962\n",
      "     37            \u001b[36m0.9313\u001b[0m        \u001b[32m0.3416\u001b[0m       \u001b[35m0.7500\u001b[0m            \u001b[31m0.7500\u001b[0m        \u001b[94m0.7283\u001b[0m  0.0007  0.8911\n",
      "     38            \u001b[36m0.9563\u001b[0m        \u001b[32m0.3109\u001b[0m       \u001b[35m0.7917\u001b[0m            \u001b[31m0.7917\u001b[0m        \u001b[94m0.7032\u001b[0m  0.0007  0.9538\n",
      "     39            \u001b[36m0.9771\u001b[0m        \u001b[32m0.2873\u001b[0m       0.7396            0.7396        0.7125  0.0007  0.9985\n",
      "     40            0.9729        \u001b[32m0.2768\u001b[0m       0.7604            0.7604        0.7141  0.0007  0.8599\n",
      "     41            \u001b[36m0.9792\u001b[0m        \u001b[32m0.2286\u001b[0m       0.7604            0.7604        \u001b[94m0.6755\u001b[0m  0.0007  0.7940\n",
      "     42            \u001b[36m0.9833\u001b[0m        \u001b[32m0.2164\u001b[0m       0.7604            0.7604        0.6995  0.0007  0.7964\n",
      "     43            \u001b[36m0.9917\u001b[0m        \u001b[32m0.1720\u001b[0m       0.7604            0.7604        0.7363  0.0006  0.7830\n",
      "     44            \u001b[36m0.9979\u001b[0m        \u001b[32m0.1590\u001b[0m       0.7500            0.7500        0.7049  0.0006  0.7950\n",
      "     45            0.9917        0.1613       0.7917            0.7917        0.6933  0.0005  0.7995\n",
      "     46            0.9979        0.1613       0.7604            0.7604        0.6889  0.0005  0.7905\n",
      "     47            0.9979        \u001b[32m0.1366\u001b[0m       0.7708            0.7708        \u001b[94m0.6750\u001b[0m  0.0004  0.7928\n",
      "     48            0.9979        \u001b[32m0.1336\u001b[0m       0.7917            0.7917        \u001b[94m0.6738\u001b[0m  0.0004  0.7969\n",
      "     49            0.9979        \u001b[32m0.1220\u001b[0m       0.7812            0.7812        \u001b[94m0.6709\u001b[0m  0.0003  0.7972\n",
      "     50            0.9979        \u001b[32m0.1066\u001b[0m       0.7396            0.7396        0.6814  0.0003  0.7814\n",
      "Fine tuning model for subject 9 with 220 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6292        1.4410       0.5000            0.5000        1.2621  0.0004  0.9308\n",
      "     32            0.7042        1.0108       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        \u001b[94m1.0609\u001b[0m  0.0005  0.8909\n",
      "     33            0.8083        0.7452       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9339\u001b[0m  0.0005  0.9813\n",
      "     34            \u001b[36m0.8417\u001b[0m        \u001b[32m0.6044\u001b[0m       0.5938            0.5938        0.9379  0.0006  0.8495\n",
      "     35            \u001b[36m0.9021\u001b[0m        \u001b[32m0.5156\u001b[0m       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        \u001b[94m0.8266\u001b[0m  0.0006  0.7903\n",
      "     36            \u001b[36m0.9250\u001b[0m        \u001b[32m0.4255\u001b[0m       \u001b[35m0.7083\u001b[0m            \u001b[31m0.7083\u001b[0m        \u001b[94m0.7748\u001b[0m  0.0007  0.7900\n",
      "     37            \u001b[36m0.9354\u001b[0m        \u001b[32m0.3570\u001b[0m       \u001b[35m0.7396\u001b[0m            \u001b[31m0.7396\u001b[0m        0.7853  0.0007  0.7877\n",
      "     38            \u001b[36m0.9646\u001b[0m        \u001b[32m0.3176\u001b[0m       0.7083            0.7083        \u001b[94m0.7574\u001b[0m  0.0007  0.7937\n",
      "     39            \u001b[36m0.9688\u001b[0m        \u001b[32m0.2986\u001b[0m       0.7083            0.7083        \u001b[94m0.7454\u001b[0m  0.0007  0.8047\n",
      "     40            \u001b[36m0.9729\u001b[0m        \u001b[32m0.2324\u001b[0m       0.7188            0.7188        0.7507  0.0007  0.8007\n",
      "     41            \u001b[36m0.9812\u001b[0m        \u001b[32m0.2159\u001b[0m       \u001b[35m0.7604\u001b[0m            \u001b[31m0.7604\u001b[0m        \u001b[94m0.7452\u001b[0m  0.0007  0.7816\n",
      "     42            \u001b[36m0.9854\u001b[0m        \u001b[32m0.2013\u001b[0m       0.6979            0.6979        0.7698  0.0007  0.7786\n",
      "     43            \u001b[36m0.9896\u001b[0m        \u001b[32m0.1947\u001b[0m       0.7083            0.7083        0.7950  0.0006  0.8292\n",
      "     44            \u001b[36m0.9938\u001b[0m        \u001b[32m0.1533\u001b[0m       0.7396            0.7396        0.7586  0.0006  0.7958\n",
      "     45            \u001b[36m0.9958\u001b[0m        0.1651       0.7500            0.7500        0.7581  0.0005  0.9576\n",
      "     46            0.9938        \u001b[32m0.1434\u001b[0m       0.7500            0.7500        0.7775  0.0005  0.9226\n",
      "     47            \u001b[36m0.9979\u001b[0m        \u001b[32m0.1340\u001b[0m       0.7500            0.7500        0.7717  0.0004  0.9791\n",
      "     48            0.9979        0.1466       \u001b[35m0.7708\u001b[0m            \u001b[31m0.7708\u001b[0m        0.7691  0.0004  0.8028\n",
      "     49            0.9979        \u001b[32m0.1190\u001b[0m       0.7604            0.7604        0.7765  0.0003  0.7917\n",
      "     50            0.9979        0.1348       0.7604            0.7604        0.7754  0.0003  0.7898\n",
      "Fine tuning model for subject 9 with 240 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6312        1.4275       0.5000            0.5000        1.1778  0.0004  0.8302\n",
      "     32            0.7271        1.0277       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        \u001b[94m1.0347\u001b[0m  0.0005  0.8455\n",
      "     33            0.8000        0.7479       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8665\u001b[0m  0.0005  0.7811\n",
      "     34            \u001b[36m0.8500\u001b[0m        \u001b[32m0.6122\u001b[0m       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        \u001b[94m0.8212\u001b[0m  0.0006  0.7835\n",
      "     35            \u001b[36m0.8917\u001b[0m        \u001b[32m0.4544\u001b[0m       \u001b[35m0.6979\u001b[0m            \u001b[31m0.6979\u001b[0m        \u001b[94m0.7875\u001b[0m  0.0006  0.7791\n",
      "     36            \u001b[36m0.9104\u001b[0m        \u001b[32m0.4066\u001b[0m       0.6771            0.6771        0.8059  0.0007  0.7789\n",
      "     37            \u001b[36m0.9167\u001b[0m        0.4135       0.6979            0.6979        0.7951  0.0007  0.7872\n",
      "     38            \u001b[36m0.9396\u001b[0m        \u001b[32m0.3434\u001b[0m       \u001b[35m0.7083\u001b[0m            \u001b[31m0.7083\u001b[0m        \u001b[94m0.7707\u001b[0m  0.0007  0.8502\n",
      "     39            \u001b[36m0.9646\u001b[0m        \u001b[32m0.2977\u001b[0m       \u001b[35m0.7188\u001b[0m            \u001b[31m0.7188\u001b[0m        \u001b[94m0.7494\u001b[0m  0.0007  0.9521\n",
      "     40            \u001b[36m0.9812\u001b[0m        \u001b[32m0.2750\u001b[0m       0.6979            0.6979        \u001b[94m0.7327\u001b[0m  0.0007  0.9528\n",
      "     41            \u001b[36m0.9833\u001b[0m        \u001b[32m0.2350\u001b[0m       0.7188            0.7188        \u001b[94m0.7141\u001b[0m  0.0007  0.9323\n",
      "     42            \u001b[36m0.9917\u001b[0m        \u001b[32m0.1947\u001b[0m       \u001b[35m0.7604\u001b[0m            \u001b[31m0.7604\u001b[0m        0.7526  0.0007  0.7891\n",
      "     43            \u001b[36m0.9979\u001b[0m        0.2056       \u001b[35m0.7708\u001b[0m            \u001b[31m0.7708\u001b[0m        0.7277  0.0006  0.7964\n",
      "     44            0.9938        \u001b[32m0.1863\u001b[0m       0.7708            0.7708        0.7327  0.0006  0.7979\n",
      "     45            0.9979        \u001b[32m0.1790\u001b[0m       0.7604            0.7604        0.7414  0.0005  0.7970\n",
      "     46            0.9979        \u001b[32m0.1567\u001b[0m       0.7604            0.7604        0.7451  0.0005  0.7969\n",
      "     47            0.9979        \u001b[32m0.1397\u001b[0m       0.7396            0.7396        0.7338  0.0004  0.7975\n",
      "     48            0.9979        \u001b[32m0.1321\u001b[0m       0.7604            0.7604        \u001b[94m0.7141\u001b[0m  0.0004  0.7807\n",
      "     49            0.9979        \u001b[32m0.1222\u001b[0m       0.7500            0.7500        \u001b[94m0.7139\u001b[0m  0.0003  0.8137\n",
      "     50            0.9979        \u001b[32m0.1076\u001b[0m       0.7500            0.7500        \u001b[94m0.7127\u001b[0m  0.0003  0.7976\n",
      "Fine tuning model for subject 9 with 260 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6312        1.4122       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        1.3013  0.0004  0.7904\n",
      "     32            0.6833        0.9019       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        \u001b[94m1.0939\u001b[0m  0.0005  0.8297\n",
      "     33            \u001b[36m0.8229\u001b[0m        0.7668       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.9175\u001b[0m  0.0005  0.9346\n",
      "     34            \u001b[36m0.8417\u001b[0m        \u001b[32m0.6281\u001b[0m       0.6458            0.6458        \u001b[94m0.8662\u001b[0m  0.0006  0.8745\n",
      "     35            \u001b[36m0.8771\u001b[0m        \u001b[32m0.4748\u001b[0m       \u001b[35m0.7083\u001b[0m            \u001b[31m0.7083\u001b[0m        \u001b[94m0.8144\u001b[0m  0.0006  0.9845\n",
      "     36            \u001b[36m0.9125\u001b[0m        \u001b[32m0.4561\u001b[0m       \u001b[35m0.7500\u001b[0m            \u001b[31m0.7500\u001b[0m        \u001b[94m0.7892\u001b[0m  0.0007  0.7966\n",
      "     37            \u001b[36m0.9292\u001b[0m        \u001b[32m0.3720\u001b[0m       0.6979            0.6979        0.8017  0.0007  0.8144\n",
      "     38            \u001b[36m0.9521\u001b[0m        \u001b[32m0.3014\u001b[0m       0.7083            0.7083        \u001b[94m0.7798\u001b[0m  0.0007  0.8121\n",
      "     39            \u001b[36m0.9729\u001b[0m        \u001b[32m0.2791\u001b[0m       0.7396            0.7396        \u001b[94m0.7543\u001b[0m  0.0007  0.7969\n",
      "     40            \u001b[36m0.9833\u001b[0m        \u001b[32m0.2490\u001b[0m       \u001b[35m0.7604\u001b[0m            \u001b[31m0.7604\u001b[0m        \u001b[94m0.7404\u001b[0m  0.0007  0.7969\n",
      "     41            0.9792        \u001b[32m0.2202\u001b[0m       \u001b[35m0.7708\u001b[0m            \u001b[31m0.7708\u001b[0m        0.7614  0.0007  0.7810\n",
      "     42            \u001b[36m0.9854\u001b[0m        \u001b[32m0.1911\u001b[0m       \u001b[35m0.8021\u001b[0m            \u001b[31m0.8021\u001b[0m        0.7692  0.0007  0.7830\n",
      "     43            \u001b[36m0.9896\u001b[0m        \u001b[32m0.1667\u001b[0m       0.7917            0.7917        0.7588  0.0006  0.7898\n",
      "     44            \u001b[36m0.9917\u001b[0m        \u001b[32m0.1492\u001b[0m       0.7500            0.7500        0.7595  0.0006  0.7777\n",
      "     45            \u001b[36m0.9958\u001b[0m        \u001b[32m0.1466\u001b[0m       0.7500            0.7500        0.7405  0.0005  0.7956\n",
      "     46            \u001b[36m0.9979\u001b[0m        \u001b[32m0.1459\u001b[0m       0.7917            0.7917        \u001b[94m0.7301\u001b[0m  0.0005  0.8707\n",
      "     47            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1405\u001b[0m       0.7708            0.7708        \u001b[94m0.7175\u001b[0m  0.0004  0.9437\n",
      "     48            0.9979        \u001b[32m0.1233\u001b[0m       0.7604            0.7604        0.7415  0.0004  0.8986\n",
      "     49            0.9979        0.1330       0.7500            0.7500        0.7581  0.0003  1.0055\n",
      "     50            0.9979        0.1239       0.7500            0.7500        0.7606  0.0003  0.7931\n",
      "Fine tuning model for subject 9 with 280 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6271        1.3575       0.5104            0.5104        1.3356  0.0004  0.8477\n",
      "     32            0.6896        0.9509       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m1.1010\u001b[0m  0.0005  0.7965\n",
      "     33            0.8063        0.8128       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8804\u001b[0m  0.0005  0.7801\n",
      "     34            \u001b[36m0.8500\u001b[0m        \u001b[32m0.6691\u001b[0m       \u001b[35m0.6771\u001b[0m            \u001b[31m0.6771\u001b[0m        \u001b[94m0.8578\u001b[0m  0.0006  0.8052\n",
      "     35            \u001b[36m0.8667\u001b[0m        \u001b[32m0.5007\u001b[0m       0.6354            0.6354        0.8863  0.0006  0.7995\n",
      "     36            \u001b[36m0.9187\u001b[0m        \u001b[32m0.4787\u001b[0m       \u001b[35m0.7500\u001b[0m            \u001b[31m0.7500\u001b[0m        \u001b[94m0.7820\u001b[0m  0.0007  0.7925\n",
      "     37            \u001b[36m0.9375\u001b[0m        \u001b[32m0.4373\u001b[0m       \u001b[35m0.7604\u001b[0m            \u001b[31m0.7604\u001b[0m        \u001b[94m0.7340\u001b[0m  0.0007  0.7818\n",
      "     38            \u001b[36m0.9604\u001b[0m        \u001b[32m0.3158\u001b[0m       0.7604            0.7604        0.7478  0.0007  0.7892\n",
      "     39            \u001b[36m0.9667\u001b[0m        \u001b[32m0.2809\u001b[0m       0.7396            0.7396        0.7501  0.0007  0.7901\n",
      "     40            \u001b[36m0.9896\u001b[0m        \u001b[32m0.2503\u001b[0m       0.7500            0.7500        0.7545  0.0007  0.8989\n",
      "     41            \u001b[36m0.9938\u001b[0m        \u001b[32m0.2142\u001b[0m       0.7396            0.7396        0.7428  0.0007  0.9433\n",
      "     42            0.9896        \u001b[32m0.1807\u001b[0m       \u001b[35m0.7708\u001b[0m            \u001b[31m0.7708\u001b[0m        0.7410  0.0007  0.9073\n",
      "     43            0.9938        0.1856       0.7604            0.7604        \u001b[94m0.7236\u001b[0m  0.0006  0.9588\n",
      "     44            0.9917        \u001b[32m0.1750\u001b[0m       \u001b[35m0.7812\u001b[0m            \u001b[31m0.7812\u001b[0m        0.7673  0.0006  0.8082\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1609\u001b[0m       0.7812            0.7812        \u001b[94m0.7147\u001b[0m  0.0005  0.8039\n",
      "     46            1.0000        \u001b[32m0.1470\u001b[0m       0.7708            0.7708        0.7154  0.0005  0.7813\n",
      "     47            1.0000        0.1504       0.7708            0.7708        0.7277  0.0004  0.8126\n",
      "     48            0.9979        \u001b[32m0.1408\u001b[0m       0.7708            0.7708        0.7319  0.0004  0.8118\n",
      "     49            1.0000        \u001b[32m0.0992\u001b[0m       0.7604            0.7604        0.7313  0.0003  0.7966\n",
      "     50            1.0000        0.1109       0.7604            0.7604        0.7327  0.0003  0.7814\n",
      "Fine tuning model for subject 9 with 300 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6188        1.4192       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        1.2893  0.0004  0.8292\n",
      "     32            0.6792        1.0076       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        1.1591  0.0005  0.8270\n",
      "     33            0.8042        \u001b[32m0.7265\u001b[0m       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        \u001b[94m0.9134\u001b[0m  0.0005  0.8156\n",
      "     34            \u001b[36m0.8396\u001b[0m        \u001b[32m0.6356\u001b[0m       0.6771            0.6771        \u001b[94m0.8637\u001b[0m  0.0006  0.9698\n",
      "     35            \u001b[36m0.8792\u001b[0m        \u001b[32m0.5173\u001b[0m       \u001b[35m0.7083\u001b[0m            \u001b[31m0.7083\u001b[0m        \u001b[94m0.8258\u001b[0m  0.0006  0.8894\n",
      "     36            \u001b[36m0.9125\u001b[0m        \u001b[32m0.4318\u001b[0m       \u001b[35m0.7188\u001b[0m            \u001b[31m0.7188\u001b[0m        \u001b[94m0.7951\u001b[0m  0.0007  1.0252\n",
      "     37            \u001b[36m0.9375\u001b[0m        \u001b[32m0.4070\u001b[0m       \u001b[35m0.7396\u001b[0m            \u001b[31m0.7396\u001b[0m        \u001b[94m0.7523\u001b[0m  0.0007  0.7815\n",
      "     38            \u001b[36m0.9437\u001b[0m        \u001b[32m0.2973\u001b[0m       0.6979            0.6979        0.7581  0.0007  0.7947\n",
      "     39            \u001b[36m0.9521\u001b[0m        \u001b[32m0.2822\u001b[0m       0.7396            0.7396        0.7664  0.0007  0.7828\n",
      "     40            \u001b[36m0.9708\u001b[0m        0.2937       \u001b[35m0.7604\u001b[0m            \u001b[31m0.7604\u001b[0m        \u001b[94m0.7388\u001b[0m  0.0007  0.7957\n",
      "     41            \u001b[36m0.9771\u001b[0m        \u001b[32m0.2107\u001b[0m       0.7500            0.7500        \u001b[94m0.7360\u001b[0m  0.0007  0.7952\n",
      "     42            \u001b[36m0.9896\u001b[0m        0.2111       0.7292            0.7292        0.7392  0.0007  0.7983\n",
      "     43            \u001b[36m0.9917\u001b[0m        \u001b[32m0.1992\u001b[0m       0.7396            0.7396        0.7434  0.0006  0.8113\n",
      "     44            \u001b[36m0.9979\u001b[0m        \u001b[32m0.1859\u001b[0m       0.7500            0.7500        \u001b[94m0.7236\u001b[0m  0.0006  0.7955\n",
      "     45            0.9979        \u001b[32m0.1675\u001b[0m       \u001b[35m0.7812\u001b[0m            \u001b[31m0.7812\u001b[0m        \u001b[94m0.7096\u001b[0m  0.0005  0.8238\n",
      "     46            0.9979        \u001b[32m0.1566\u001b[0m       0.7604            0.7604        0.7186  0.0005  0.8299\n",
      "     47            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1449\u001b[0m       0.7604            0.7604        0.7185  0.0004  0.8581\n",
      "     48            1.0000        0.1552       0.7604            0.7604        0.7256  0.0004  0.9370\n",
      "     49            1.0000        \u001b[32m0.1177\u001b[0m       0.7604            0.7604        0.7404  0.0003  1.0214\n",
      "     50            1.0000        \u001b[32m0.1153\u001b[0m       0.7604            0.7604        0.7419  0.0003  0.9741\n",
      "Fine tuning model for subject 9 with 320 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6354        1.3589       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        1.2449  0.0004  0.8339\n",
      "     32            0.7146        1.0163       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        \u001b[94m1.0776\u001b[0m  0.0005  0.8979\n",
      "     33            0.8021        \u001b[32m0.7227\u001b[0m       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9151\u001b[0m  0.0005  0.8345\n",
      "     34            \u001b[36m0.8542\u001b[0m        \u001b[32m0.6409\u001b[0m       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8876\u001b[0m  0.0006  0.8567\n",
      "     35            \u001b[36m0.8917\u001b[0m        \u001b[32m0.5181\u001b[0m       \u001b[35m0.7083\u001b[0m            \u001b[31m0.7083\u001b[0m        \u001b[94m0.8415\u001b[0m  0.0006  0.8377\n",
      "     36            \u001b[36m0.9062\u001b[0m        \u001b[32m0.4234\u001b[0m       \u001b[35m0.7396\u001b[0m            \u001b[31m0.7396\u001b[0m        \u001b[94m0.8193\u001b[0m  0.0007  0.8201\n",
      "     37            \u001b[36m0.9313\u001b[0m        \u001b[32m0.3447\u001b[0m       0.6875            0.6875        \u001b[94m0.8093\u001b[0m  0.0007  0.7802\n",
      "     38            \u001b[36m0.9563\u001b[0m        \u001b[32m0.3099\u001b[0m       0.6562            0.6562        0.8246  0.0007  0.8204\n",
      "     39            \u001b[36m0.9708\u001b[0m        \u001b[32m0.2723\u001b[0m       0.6771            0.6771        0.8147  0.0007  0.9356\n",
      "     40            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2539\u001b[0m       0.7083            0.7083        \u001b[94m0.7733\u001b[0m  0.0007  1.0600\n",
      "     41            0.9708        \u001b[32m0.2092\u001b[0m       0.6667            0.6667        0.7933  0.0007  0.9216\n",
      "     42            \u001b[36m0.9958\u001b[0m        0.2146       0.6979            0.6979        0.7929  0.0007  0.9554\n",
      "     43            0.9854        \u001b[32m0.1829\u001b[0m       0.7188            0.7188        0.7889  0.0006  0.9595\n",
      "     44            \u001b[36m0.9979\u001b[0m        \u001b[32m0.1760\u001b[0m       0.7292            0.7292        0.8024  0.0006  0.8405\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1617\u001b[0m       0.7292            0.7292        0.7988  0.0005  0.8691\n",
      "     46            1.0000        \u001b[32m0.1527\u001b[0m       0.7396            0.7396        0.8027  0.0005  0.8291\n",
      "     47            1.0000        \u001b[32m0.1342\u001b[0m       0.7396            0.7396        0.7798  0.0004  0.8249\n",
      "     48            1.0000        \u001b[32m0.1217\u001b[0m       0.7396            0.7396        \u001b[94m0.7609\u001b[0m  0.0004  0.7834\n",
      "     49            1.0000        \u001b[32m0.1210\u001b[0m       \u001b[35m0.7500\u001b[0m            \u001b[31m0.7500\u001b[0m        0.7788  0.0003  0.8357\n",
      "     50            1.0000        0.1290       \u001b[35m0.7604\u001b[0m            \u001b[31m0.7604\u001b[0m        0.7818  0.0003  0.7968\n",
      "Fine tuning model for subject 9 with 340 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6417        1.3799       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        1.2374  0.0004  0.8411\n",
      "     32            0.6958        1.0035       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        \u001b[94m1.0918\u001b[0m  0.0005  0.8502\n",
      "     33            0.8083        0.7703       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.9083\u001b[0m  0.0005  0.8654\n",
      "     34            \u001b[36m0.8542\u001b[0m        \u001b[32m0.6282\u001b[0m       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8722\u001b[0m  0.0006  1.0844\n",
      "     35            \u001b[36m0.8854\u001b[0m        \u001b[32m0.4954\u001b[0m       \u001b[35m0.7188\u001b[0m            \u001b[31m0.7188\u001b[0m        \u001b[94m0.8367\u001b[0m  0.0006  1.0815\n",
      "     36            \u001b[36m0.8979\u001b[0m        \u001b[32m0.4671\u001b[0m       0.6771            0.6771        \u001b[94m0.8269\u001b[0m  0.0007  1.0201\n",
      "     37            \u001b[36m0.9208\u001b[0m        \u001b[32m0.3821\u001b[0m       0.6979            0.6979        \u001b[94m0.8165\u001b[0m  0.0007  1.3665\n",
      "     38            \u001b[36m0.9354\u001b[0m        \u001b[32m0.3151\u001b[0m       0.6979            0.6979        \u001b[94m0.7795\u001b[0m  0.0007  1.3376\n",
      "     39            \u001b[36m0.9563\u001b[0m        \u001b[32m0.3051\u001b[0m       0.6875            0.6875        \u001b[94m0.7699\u001b[0m  0.0007  1.0504\n",
      "     40            \u001b[36m0.9708\u001b[0m        \u001b[32m0.2520\u001b[0m       \u001b[35m0.7292\u001b[0m            \u001b[31m0.7292\u001b[0m        \u001b[94m0.7648\u001b[0m  0.0007  1.0793\n",
      "     41            \u001b[36m0.9771\u001b[0m        0.2594       0.7083            0.7083        0.7805  0.0007  1.3371\n",
      "     42            0.9750        \u001b[32m0.2342\u001b[0m       0.6875            0.6875        0.7718  0.0007  1.2240\n",
      "     43            \u001b[36m0.9854\u001b[0m        \u001b[32m0.1950\u001b[0m       \u001b[35m0.7604\u001b[0m            \u001b[31m0.7604\u001b[0m        \u001b[94m0.7556\u001b[0m  0.0006  1.3618\n",
      "     44            \u001b[36m0.9896\u001b[0m        \u001b[32m0.1532\u001b[0m       0.7500            0.7500        \u001b[94m0.7442\u001b[0m  0.0006  1.4229\n",
      "     45            \u001b[36m0.9958\u001b[0m        0.1537       0.7396            0.7396        \u001b[94m0.7312\u001b[0m  0.0005  1.4950\n",
      "     46            0.9958        0.1627       0.7500            0.7500        \u001b[94m0.7255\u001b[0m  0.0005  1.1027\n",
      "     47            0.9938        0.1540       0.7500            0.7500        0.7332  0.0004  0.8572\n",
      "     48            0.9938        \u001b[32m0.1334\u001b[0m       0.7396            0.7396        0.7472  0.0004  0.8225\n",
      "     49            \u001b[36m0.9979\u001b[0m        \u001b[32m0.1165\u001b[0m       0.7500            0.7500        0.7485  0.0003  0.8704\n",
      "     50            \u001b[36m1.0000\u001b[0m        0.1191       0.7500            0.7500        0.7310  0.0003  0.8715\n",
      "Fine tuning model for subject 9 with 360 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6229        1.3638       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        1.2707  0.0004  0.8122\n",
      "     32            0.6875        1.0000       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        \u001b[94m1.0615\u001b[0m  0.0005  0.8681\n",
      "     33            0.7958        0.7773       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8987\u001b[0m  0.0005  0.9675\n",
      "     34            \u001b[36m0.8458\u001b[0m        \u001b[32m0.6359\u001b[0m       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        \u001b[94m0.8341\u001b[0m  0.0006  0.8589\n",
      "     35            \u001b[36m0.8917\u001b[0m        \u001b[32m0.4923\u001b[0m       0.6875            0.6875        0.8356  0.0006  1.0318\n",
      "     36            \u001b[36m0.9104\u001b[0m        \u001b[32m0.3979\u001b[0m       \u001b[35m0.7083\u001b[0m            \u001b[31m0.7083\u001b[0m        0.8351  0.0007  1.0195\n",
      "     37            \u001b[36m0.9250\u001b[0m        0.4100       \u001b[35m0.7292\u001b[0m            \u001b[31m0.7292\u001b[0m        \u001b[94m0.8137\u001b[0m  0.0007  0.9935\n",
      "     38            \u001b[36m0.9604\u001b[0m        \u001b[32m0.3229\u001b[0m       0.7188            0.7188        \u001b[94m0.7550\u001b[0m  0.0007  0.8229\n",
      "     39            0.9542        0.3271       0.7083            0.7083        0.7566  0.0007  0.9173\n",
      "     40            \u001b[36m0.9708\u001b[0m        \u001b[32m0.2834\u001b[0m       0.7188            0.7188        \u001b[94m0.7332\u001b[0m  0.0007  0.7985\n",
      "     41            \u001b[36m0.9771\u001b[0m        \u001b[32m0.2340\u001b[0m       \u001b[35m0.7500\u001b[0m            \u001b[31m0.7500\u001b[0m        0.7407  0.0007  0.8604\n",
      "     42            \u001b[36m0.9833\u001b[0m        \u001b[32m0.2150\u001b[0m       0.7500            0.7500        0.7545  0.0007  0.8011\n",
      "     43            \u001b[36m0.9917\u001b[0m        \u001b[32m0.1839\u001b[0m       \u001b[35m0.7604\u001b[0m            \u001b[31m0.7604\u001b[0m        \u001b[94m0.7225\u001b[0m  0.0006  0.9400\n",
      "     44            0.9875        \u001b[32m0.1774\u001b[0m       0.7500            0.7500        0.7337  0.0006  0.8951\n",
      "     45            0.9896        \u001b[32m0.1567\u001b[0m       0.7604            0.7604        0.7692  0.0005  0.8231\n",
      "     46            0.9917        \u001b[32m0.1359\u001b[0m       \u001b[35m0.7708\u001b[0m            \u001b[31m0.7708\u001b[0m        0.7643  0.0005  0.8443\n",
      "     47            \u001b[36m0.9979\u001b[0m        \u001b[32m0.1331\u001b[0m       \u001b[35m0.8125\u001b[0m            \u001b[31m0.8125\u001b[0m        0.7478  0.0004  0.7924\n",
      "     48            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1323\u001b[0m       0.8125            0.8125        0.7389  0.0004  0.9239\n",
      "     49            1.0000        0.1390       0.8125            0.8125        0.7331  0.0003  0.8784\n",
      "     50            1.0000        \u001b[32m0.1199\u001b[0m       0.8021            0.8021        0.7256  0.0003  0.9572\n",
      "Fine tuning model for subject 9 with 380 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6021        1.2685       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        1.3267  0.0004  0.9174\n",
      "     32            0.6958        1.0289       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        \u001b[94m1.0584\u001b[0m  0.0005  0.9084\n",
      "     33            0.7812        0.7544       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.9031\u001b[0m  0.0005  0.8072\n",
      "     34            \u001b[36m0.8521\u001b[0m        \u001b[32m0.6223\u001b[0m       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8660\u001b[0m  0.0006  0.8045\n",
      "     35            \u001b[36m0.8625\u001b[0m        \u001b[32m0.4871\u001b[0m       \u001b[35m0.6979\u001b[0m            \u001b[31m0.6979\u001b[0m        \u001b[94m0.8411\u001b[0m  0.0006  0.9064\n",
      "     36            \u001b[36m0.9042\u001b[0m        \u001b[32m0.4785\u001b[0m       0.6979            0.6979        \u001b[94m0.7997\u001b[0m  0.0007  0.8554\n",
      "     37            \u001b[36m0.9229\u001b[0m        \u001b[32m0.3624\u001b[0m       \u001b[35m0.7188\u001b[0m            \u001b[31m0.7188\u001b[0m        \u001b[94m0.7316\u001b[0m  0.0007  0.8125\n",
      "     38            \u001b[36m0.9479\u001b[0m        \u001b[32m0.3573\u001b[0m       \u001b[35m0.7292\u001b[0m            \u001b[31m0.7292\u001b[0m        0.7378  0.0007  0.8803\n",
      "     39            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3048\u001b[0m       0.7083            0.7083        0.7541  0.0007  0.9321\n",
      "     40            0.9625        \u001b[32m0.2365\u001b[0m       0.6979            0.6979        0.7488  0.0007  0.9608\n",
      "     41            \u001b[36m0.9854\u001b[0m        0.2403       \u001b[35m0.7396\u001b[0m            \u001b[31m0.7396\u001b[0m        \u001b[94m0.7257\u001b[0m  0.0007  0.9819\n",
      "     42            \u001b[36m0.9917\u001b[0m        \u001b[32m0.2038\u001b[0m       \u001b[35m0.7500\u001b[0m            \u001b[31m0.7500\u001b[0m        \u001b[94m0.7246\u001b[0m  0.0007  0.9091\n",
      "     43            0.9896        \u001b[32m0.1700\u001b[0m       0.7396            0.7396        \u001b[94m0.6804\u001b[0m  0.0006  0.9698\n",
      "     44            0.9896        0.1769       \u001b[35m0.7708\u001b[0m            \u001b[31m0.7708\u001b[0m        0.6810  0.0006  0.8290\n",
      "     45            \u001b[36m0.9958\u001b[0m        \u001b[32m0.1669\u001b[0m       \u001b[35m0.7812\u001b[0m            \u001b[31m0.7812\u001b[0m        \u001b[94m0.6737\u001b[0m  0.0005  0.8909\n",
      "     46            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1612\u001b[0m       0.7292            0.7292        0.7271  0.0005  0.9830\n",
      "     47            1.0000        \u001b[32m0.1468\u001b[0m       0.7396            0.7396        0.7309  0.0004  0.8098\n",
      "     48            0.9979        \u001b[32m0.1311\u001b[0m       0.7708            0.7708        0.7454  0.0004  0.7839\n",
      "     49            1.0000        \u001b[32m0.1132\u001b[0m       0.7708            0.7708        0.7239  0.0003  0.9263\n",
      "     50            1.0000        0.1328       0.7708            0.7708        0.7086  0.0003  0.8379\n",
      "Fine tuning model for subject 9 with 400 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6167        1.3274       0.4792            0.4792        1.2975  0.0004  0.8628\n",
      "     32            0.7125        0.9794       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        \u001b[94m1.1104\u001b[0m  0.0005  0.8631\n",
      "     33            0.7958        \u001b[32m0.7249\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9206\u001b[0m  0.0005  0.8953\n",
      "     34            \u001b[36m0.8625\u001b[0m        \u001b[32m0.6268\u001b[0m       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        \u001b[94m0.8598\u001b[0m  0.0006  0.9564\n",
      "     35            \u001b[36m0.8938\u001b[0m        \u001b[32m0.5148\u001b[0m       \u001b[35m0.7083\u001b[0m            \u001b[31m0.7083\u001b[0m        \u001b[94m0.8242\u001b[0m  0.0006  1.0090\n",
      "     36            \u001b[36m0.9062\u001b[0m        \u001b[32m0.4278\u001b[0m       0.7083            0.7083        0.8370  0.0007  1.1346\n",
      "     37            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3849\u001b[0m       0.7083            0.7083        \u001b[94m0.8173\u001b[0m  0.0007  0.8319\n",
      "     38            \u001b[36m0.9479\u001b[0m        \u001b[32m0.3286\u001b[0m       \u001b[35m0.7292\u001b[0m            \u001b[31m0.7292\u001b[0m        \u001b[94m0.7809\u001b[0m  0.0007  0.7989\n",
      "     39            0.9479        \u001b[32m0.2973\u001b[0m       0.7188            0.7188        0.7866  0.0007  1.0456\n",
      "     40            \u001b[36m0.9875\u001b[0m        \u001b[32m0.2539\u001b[0m       0.7188            0.7188        \u001b[94m0.7729\u001b[0m  0.0007  0.9131\n",
      "     41            0.9771        0.2639       0.7292            0.7292        \u001b[94m0.7586\u001b[0m  0.0007  0.8116\n",
      "     42            0.9875        \u001b[32m0.1965\u001b[0m       \u001b[35m0.7604\u001b[0m            \u001b[31m0.7604\u001b[0m        \u001b[94m0.7497\u001b[0m  0.0007  0.8163\n",
      "     43            \u001b[36m0.9917\u001b[0m        \u001b[32m0.1810\u001b[0m       0.7604            0.7604        0.7509  0.0006  0.8069\n",
      "     44            \u001b[36m0.9958\u001b[0m        \u001b[32m0.1755\u001b[0m       0.7292            0.7292        0.7576  0.0006  0.8148\n",
      "     45            \u001b[36m0.9979\u001b[0m        \u001b[32m0.1350\u001b[0m       0.7292            0.7292        0.7742  0.0005  0.8234\n",
      "     46            0.9979        0.1428       0.7604            0.7604        \u001b[94m0.7437\u001b[0m  0.0005  0.9638\n",
      "     47            0.9979        0.1440       \u001b[35m0.7708\u001b[0m            \u001b[31m0.7708\u001b[0m        \u001b[94m0.7348\u001b[0m  0.0004  0.9803\n",
      "     48            0.9979        \u001b[32m0.1320\u001b[0m       0.7604            0.7604        \u001b[94m0.7155\u001b[0m  0.0004  0.9488\n",
      "     49            0.9979        0.1343       0.7604            0.7604        0.7197  0.0003  1.0481\n",
      "     50            0.9979        0.1465       0.7708            0.7708        \u001b[94m0.7037\u001b[0m  0.0003  0.8705\n",
      "Fine tuning model for subject 9 with 420 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6375        1.4174       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        1.2751  0.0004  0.7984\n",
      "     32            0.6792        1.0090       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.1610  0.0005  0.7927\n",
      "     33            0.8000        0.8214       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8940\u001b[0m  0.0005  0.7990\n",
      "     34            \u001b[36m0.8479\u001b[0m        \u001b[32m0.5910\u001b[0m       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8912\u001b[0m  0.0006  0.8030\n",
      "     35            \u001b[36m0.8854\u001b[0m        \u001b[32m0.5621\u001b[0m       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        \u001b[94m0.8223\u001b[0m  0.0006  0.8067\n",
      "     36            \u001b[36m0.9042\u001b[0m        \u001b[32m0.4168\u001b[0m       \u001b[35m0.6979\u001b[0m            \u001b[31m0.6979\u001b[0m        \u001b[94m0.7891\u001b[0m  0.0007  0.7990\n",
      "     37            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3735\u001b[0m       0.6979            0.6979        \u001b[94m0.7446\u001b[0m  0.0007  0.9087\n",
      "     38            \u001b[36m0.9542\u001b[0m        \u001b[32m0.3072\u001b[0m       \u001b[35m0.7396\u001b[0m            \u001b[31m0.7396\u001b[0m        \u001b[94m0.7368\u001b[0m  0.0007  0.8770\n",
      "     39            \u001b[36m0.9688\u001b[0m        0.3153       0.7292            0.7292        0.7629  0.0007  0.9632\n",
      "     40            \u001b[36m0.9792\u001b[0m        \u001b[32m0.3036\u001b[0m       0.7188            0.7188        0.7614  0.0007  1.0747\n",
      "     41            0.9771        \u001b[32m0.1953\u001b[0m       \u001b[35m0.7604\u001b[0m            \u001b[31m0.7604\u001b[0m        0.7396  0.0007  0.9181\n",
      "     42            \u001b[36m0.9833\u001b[0m        0.2011       0.7500            0.7500        \u001b[94m0.7305\u001b[0m  0.0007  1.0473\n",
      "     43            \u001b[36m0.9896\u001b[0m        \u001b[32m0.1865\u001b[0m       \u001b[35m0.7708\u001b[0m            \u001b[31m0.7708\u001b[0m        0.7698  0.0006  0.7817\n",
      "     44            0.9896        \u001b[32m0.1695\u001b[0m       0.7604            0.7604        0.7715  0.0006  0.8125\n",
      "     45            \u001b[36m0.9938\u001b[0m        \u001b[32m0.1492\u001b[0m       0.7292            0.7292        0.7592  0.0005  0.8219\n",
      "     46            0.9896        \u001b[32m0.1392\u001b[0m       0.7604            0.7604        0.7655  0.0005  0.9349\n",
      "     47            0.9917        0.1627       0.7500            0.7500        0.7699  0.0004  0.8491\n",
      "     48            \u001b[36m0.9979\u001b[0m        \u001b[32m0.1296\u001b[0m       0.7604            0.7604        0.7691  0.0004  0.8865\n",
      "     49            0.9979        \u001b[32m0.1261\u001b[0m       0.7708            0.7708        0.7714  0.0003  0.9058\n",
      "     50            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1197\u001b[0m       0.7708            0.7708        0.7754  0.0003  0.8091\n",
      "Fine tuning model for subject 9 with 440 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6438        1.4555       0.4792            0.4792        1.2930  0.0004  0.7820\n",
      "     32            0.6792        1.0243       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        \u001b[94m1.0761\u001b[0m  0.0005  0.8281\n",
      "     33            \u001b[36m0.8146\u001b[0m        \u001b[32m0.7169\u001b[0m       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8273\u001b[0m  0.0005  0.9286\n",
      "     34            \u001b[36m0.8438\u001b[0m        \u001b[32m0.6075\u001b[0m       \u001b[35m0.7083\u001b[0m            \u001b[31m0.7083\u001b[0m        \u001b[94m0.7788\u001b[0m  0.0006  0.9480\n",
      "     35            \u001b[36m0.8938\u001b[0m        \u001b[32m0.5201\u001b[0m       \u001b[35m0.7396\u001b[0m            \u001b[31m0.7396\u001b[0m        0.7821  0.0006  1.0113\n",
      "     36            \u001b[36m0.9125\u001b[0m        \u001b[32m0.4404\u001b[0m       0.6875            0.6875        0.8618  0.0007  0.7932\n",
      "     37            \u001b[36m0.9396\u001b[0m        \u001b[32m0.3510\u001b[0m       0.7188            0.7188        0.8130  0.0007  0.7813\n",
      "     38            \u001b[36m0.9458\u001b[0m        \u001b[32m0.3156\u001b[0m       0.7396            0.7396        \u001b[94m0.7734\u001b[0m  0.0007  0.7975\n",
      "     39            \u001b[36m0.9688\u001b[0m        \u001b[32m0.2853\u001b[0m       0.7396            0.7396        \u001b[94m0.7611\u001b[0m  0.0007  0.7812\n",
      "     40            0.9667        \u001b[32m0.2540\u001b[0m       0.7292            0.7292        0.7771  0.0007  0.7816\n",
      "     41            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2143\u001b[0m       0.7292            0.7292        \u001b[94m0.7431\u001b[0m  0.0007  0.8440\n",
      "     42            \u001b[36m0.9854\u001b[0m        0.2249       \u001b[35m0.7708\u001b[0m            \u001b[31m0.7708\u001b[0m        \u001b[94m0.7066\u001b[0m  0.0007  0.8914\n",
      "     43            \u001b[36m0.9917\u001b[0m        \u001b[32m0.1992\u001b[0m       0.7604            0.7604        0.7093  0.0006  0.8253\n",
      "     44            \u001b[36m0.9958\u001b[0m        \u001b[32m0.1721\u001b[0m       \u001b[35m0.7812\u001b[0m            \u001b[31m0.7812\u001b[0m        \u001b[94m0.6941\u001b[0m  0.0006  0.8641\n",
      "     45            0.9958        0.1809       0.7500            0.7500        0.7083  0.0005  0.9196\n",
      "     46            0.9958        \u001b[32m0.1515\u001b[0m       0.7812            0.7812        0.7103  0.0005  0.9794\n",
      "     47            0.9958        0.1517       0.7708            0.7708        \u001b[94m0.6917\u001b[0m  0.0004  0.9813\n",
      "     48            \u001b[36m0.9979\u001b[0m        \u001b[32m0.1300\u001b[0m       \u001b[35m0.7917\u001b[0m            \u001b[31m0.7917\u001b[0m        \u001b[94m0.6766\u001b[0m  0.0004  0.9771\n",
      "     49            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1198\u001b[0m       0.7812            0.7812        0.6826  0.0003  0.8761\n",
      "     50            1.0000        0.1206       \u001b[35m0.8021\u001b[0m            \u001b[31m0.8021\u001b[0m        0.6894  0.0003  0.9130\n",
      "Fine tuning model for subject 9 with 460 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6479        1.3761       0.5104            0.5104        1.2506  0.0004  0.9468\n",
      "     32            0.6854        1.0033       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        \u001b[94m1.1056\u001b[0m  0.0005  0.8366\n",
      "     33            0.8042        \u001b[32m0.7221\u001b[0m       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.9104\u001b[0m  0.0005  0.7966\n",
      "     34            \u001b[36m0.8521\u001b[0m        \u001b[32m0.6308\u001b[0m       \u001b[35m0.6979\u001b[0m            \u001b[31m0.6979\u001b[0m        \u001b[94m0.8358\u001b[0m  0.0006  0.8590\n",
      "     35            \u001b[36m0.8771\u001b[0m        \u001b[32m0.5346\u001b[0m       \u001b[35m0.7083\u001b[0m            \u001b[31m0.7083\u001b[0m        \u001b[94m0.8156\u001b[0m  0.0006  0.8365\n",
      "     36            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4029\u001b[0m       \u001b[35m0.7396\u001b[0m            \u001b[31m0.7396\u001b[0m        \u001b[94m0.7830\u001b[0m  0.0007  0.8890\n",
      "     37            \u001b[36m0.9292\u001b[0m        \u001b[32m0.3492\u001b[0m       0.7396            0.7396        0.7996  0.0007  0.8898\n",
      "     38            \u001b[36m0.9437\u001b[0m        \u001b[32m0.3375\u001b[0m       0.7188            0.7188        0.7864  0.0007  0.8211\n",
      "     39            \u001b[36m0.9625\u001b[0m        \u001b[32m0.2961\u001b[0m       0.7188            0.7188        \u001b[94m0.7436\u001b[0m  0.0007  0.9933\n",
      "     40            \u001b[36m0.9729\u001b[0m        \u001b[32m0.2647\u001b[0m       \u001b[35m0.7917\u001b[0m            \u001b[31m0.7917\u001b[0m        \u001b[94m0.6999\u001b[0m  0.0007  0.9740\n",
      "     41            \u001b[36m0.9854\u001b[0m        \u001b[32m0.2040\u001b[0m       0.7812            0.7812        0.7070  0.0007  1.0941\n",
      "     42            0.9854        \u001b[32m0.1964\u001b[0m       0.7083            0.7083        0.7717  0.0007  0.9004\n",
      "     43            \u001b[36m0.9896\u001b[0m        \u001b[32m0.1722\u001b[0m       0.7292            0.7292        0.7802  0.0006  0.8682\n",
      "     44            \u001b[36m0.9958\u001b[0m        0.1818       0.7500            0.7500        0.7574  0.0006  0.8957\n",
      "     45            0.9958        \u001b[32m0.1467\u001b[0m       0.7500            0.7500        0.7353  0.0005  0.8109\n",
      "     46            \u001b[36m0.9979\u001b[0m        0.1476       0.7604            0.7604        0.7441  0.0005  0.8911\n",
      "     47            0.9979        \u001b[32m0.1395\u001b[0m       0.7604            0.7604        0.7504  0.0004  0.7968\n",
      "     48            0.9979        0.1440       0.7604            0.7604        0.7459  0.0004  0.7982\n",
      "     49            0.9979        \u001b[32m0.1177\u001b[0m       0.7604            0.7604        0.7535  0.0003  0.9049\n",
      "     50            0.9979        0.1314       0.7708            0.7708        0.7555  0.0003  0.9551\n"
     ]
    }
   ],
   "source": [
    "### Split finetune set into finetune_train and finetune_valid\n",
    "fine_tune_set_by_subj = fine_tune_set.split('subject')\n",
    "\n",
    "data_amount_step = 20\n",
    "results_columns = ['valid_accuracy',]\n",
    "dict_results = {}\n",
    "\n",
    "### Finetune for each subject in the finetune set\n",
    "for finetune_subj_id, finetune_subj_set in fine_tune_set_by_subj.items():\n",
    "    \n",
    "    finetune_splitted_lst_by_run = list(finetune_subj_set.split('run').values())\n",
    "    finetune_subj_train_set = BaseConcatDataset(finetune_splitted_lst_by_run[:-1])\n",
    "    finetune_subj_valid_set = BaseConcatDataset(finetune_splitted_lst_by_run[-1:])\n",
    "\n",
    "    ### Baseline accuracy on the finetune_valid set\n",
    "    finetune_valid_predicted = cur_clf.predict(finetune_subj_valid_set)\n",
    "    finetune_valid_true = np.array(finetune_subj_valid_set.get_metadata().target)\n",
    "    finetune_baseline_correct = np.equal(finetune_valid_predicted, finetune_valid_true)\n",
    "    finetune_baseline_acc = np.sum(finetune_baseline_correct) / len(finetune_baseline_correct)\n",
    "    print(f'Before finetuning for subject {finetune_subj_id}, the baseline accuracy is {finetune_baseline_acc}')\n",
    "\n",
    "    dict_subj_results = {0: finetune_baseline_acc}\n",
    "\n",
    "    ### Finetune with different amount of new data\n",
    "    finetune_trials_num = len(finetune_subj_train_set.get_metadata())\n",
    "    for finetune_training_data_amount in np.arange(1, finetune_trials_num // data_amount_step) * data_amount_step:\n",
    "\n",
    "        ## Get current finetune samples\n",
    "        cur_finetune_subj_train_subset = get_subset(finetune_subj_train_set, finetune_training_data_amount)\n",
    "\n",
    "        finetune_model = ShallowFBCSPNet(\n",
    "            n_chans,\n",
    "            n_classes,\n",
    "            input_window_samples=input_window_samples,\n",
    "            final_conv_length='auto',\n",
    "        )\n",
    "\n",
    "        finetune_lr = 0.0625 * 0.01\n",
    "        finetune_weight_decay = 0\n",
    "        finetune_batch_size = int(min(finetune_training_data_amount // 2, 64))\n",
    "        finetune_n_epochs = 20\n",
    "        \n",
    "        new_clf = EEGClassifier(\n",
    "            finetune_model,\n",
    "            criterion=torch.nn.NLLLoss,\n",
    "            optimizer=torch.optim.AdamW,\n",
    "            train_split=predefined_split(finetune_subj_valid_set), \n",
    "            optimizer__lr=finetune_lr,\n",
    "            optimizer__weight_decay=finetune_weight_decay,\n",
    "            batch_size=finetune_batch_size,\n",
    "            callbacks=[\n",
    "                \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=finetune_n_epochs - 1)),\n",
    "            ],\n",
    "            device=device,\n",
    "            classes=classes,\n",
    "        )\n",
    "        new_clf.initialize()\n",
    "        \n",
    "        ## Load pretrained model\n",
    "        new_clf.load_params(f_params=os.path.join(results_dir, f'{exp_name}_model.pkl'), \n",
    "                            f_optimizer=os.path.join(results_dir, f'{exp_name}_opt.pkl'), \n",
    "                            f_history=os.path.join(results_dir, f'{exp_name}_history.json'))\n",
    "\n",
    "        ## Continue training / finetuning\n",
    "        print(f'Fine tuning model for subject {finetune_subj_id} with {finetune_training_data_amount} trials')\n",
    "        _ = new_clf.partial_fit(finetune_subj_train_set, y=None, epochs=finetune_n_epochs)\n",
    "\n",
    "        ## Get results after fine tuning\n",
    "        df = pd.DataFrame(new_clf.history[:, results_columns], columns=results_columns,\n",
    "                          # index=new_clf.history[:, 'epoch'],\n",
    "                         )\n",
    "\n",
    "        cur_final_acc = np.mean(df.tail(5).valid_accuracy)\n",
    "        dict_subj_results.update({finetune_training_data_amount: cur_final_acc})\n",
    "\n",
    "    dict_results.update({finetune_subj_id: dict_subj_results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'ShallowFBCSPNet_BNCI2014_001_finetuning_1'\n",
    "file_path = os.path.join(results_dir, f'{file_name}.pkl')\n",
    "\n",
    "with open(f'{results_dir}\\\\{file_name}.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
    "#     with open(file_path, 'rb') as f:\n",
    "#         baseline_2_1 = pickle.load(f)\n",
    "#     print(\"Dictionary loaded successfully.\")\n",
    "# else:\n",
    "#     print(f\"Error: File '{file_path}' does not exist or is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.356250</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.610417</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.412500</td>\n",
       "      <td>0.710417</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.783333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.387500</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.591667</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.362500</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.589583</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.568750</td>\n",
       "      <td>0.768750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.404167</td>\n",
       "      <td>0.622917</td>\n",
       "      <td>0.591667</td>\n",
       "      <td>0.770833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.589583</td>\n",
       "      <td>0.585417</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.412500</td>\n",
       "      <td>0.597917</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.768750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.620833</td>\n",
       "      <td>0.579167</td>\n",
       "      <td>0.756250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.393750</td>\n",
       "      <td>0.597917</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.768750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.577083</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.758333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.397917</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.589583</td>\n",
       "      <td>0.752083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.597917</td>\n",
       "      <td>0.577083</td>\n",
       "      <td>0.764583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.608333</td>\n",
       "      <td>0.595833</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.408333</td>\n",
       "      <td>0.577083</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.760417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.597917</td>\n",
       "      <td>0.587500</td>\n",
       "      <td>0.745833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>0.360417</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.587500</td>\n",
       "      <td>0.747917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.610417</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.802083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.629167</td>\n",
       "      <td>0.581250</td>\n",
       "      <td>0.756250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.412500</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.764583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>0.422917</td>\n",
       "      <td>0.591667</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.762500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.785417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>0.393750</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.572917</td>\n",
       "      <td>0.762500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            6         7         8         9\n",
       "0    0.270833  0.343750  0.479167  0.437500\n",
       "20   0.356250  0.666667  0.610417  0.733333\n",
       "40   0.412500  0.710417  0.550000  0.783333\n",
       "60   0.387500  0.662500  0.591667  0.766667\n",
       "80   0.362500  0.616667  0.589583  0.781250\n",
       "100  0.416667  0.645833  0.568750  0.768750\n",
       "120  0.404167  0.622917  0.591667  0.770833\n",
       "140  0.400000  0.589583  0.585417  0.750000\n",
       "160  0.412500  0.597917  0.604167  0.768750\n",
       "180  0.395833  0.620833  0.579167  0.756250\n",
       "200  0.393750  0.597917  0.593750  0.768750\n",
       "220  0.400000  0.577083  0.583333  0.758333\n",
       "240  0.397917  0.625000  0.589583  0.752083\n",
       "260  0.400000  0.597917  0.577083  0.764583\n",
       "280  0.400000  0.608333  0.595833  0.766667\n",
       "300  0.408333  0.577083  0.583333  0.760417\n",
       "320  0.400000  0.597917  0.587500  0.745833\n",
       "340  0.360417  0.625000  0.587500  0.747917\n",
       "360  0.383333  0.610417  0.575000  0.802083\n",
       "380  0.400000  0.629167  0.581250  0.756250\n",
       "400  0.412500  0.641667  0.600000  0.764583\n",
       "420  0.422917  0.591667  0.593750  0.762500\n",
       "440  0.395833  0.612500  0.583333  0.785417\n",
       "460  0.393750  0.656250  0.572917  0.762500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(dict_results)\n",
    "display(df_results)\n",
    "\n",
    "subject_averaged_df = df_results.mean(axis=1)\n",
    "# Calculate the standard error of the mean\n",
    "std_err_df = df_results.sem(axis=1)\n",
    "# Calculate the confidence interval (95% confidence level)\n",
    "conf_interval_df = stats.t.interval(0.95, len(df_results.columns) - 1, loc=subject_averaged_df, scale=std_err_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns\n",
    "\n",
    "for subj_id, subj_res in dict_results.items():\n",
    "    ax1.plot(subj_res.keys(), subj_res.values(), label=f'Subject {subj_id}')\n",
    "\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('Amount of fine tuning data (trials, 4 secs each)')\n",
    "ax1.set_ylabel('Validation Accuracy')\n",
    "\n",
    "ax2.plot(subject_averaged_df, label='Subject averaged')\n",
    "ax2.fill_between(subject_averaged_df.index, conf_interval_df[0], conf_interval_df[1], color='b', alpha=0.3, label='95% CI')\n",
    "ax2.legend()\n",
    "ax2.set_xlabel('Amount of fine tuning data (trials, 4 secs each)')\n",
    "\n",
    "plt.suptitle('ShallowFBCSPNet on BNCI2014_001 Dataset \\n Fine-tuning for Subject 6~9 (Pretrained on Subject 1~5)')\n",
    "\n",
    "plt.savefig(os.path.join(results_dir, f'{file_name}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
