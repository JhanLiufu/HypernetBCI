{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Baseline Experiment 1\n",
    "\n",
    "Train model from scratch for each subject. \n",
    "\n",
    "Model: BSFShallowNet\n",
    "\n",
    "Dataset: BCI Competitin IV 2a, BCNI2014001 via MOABB library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from braindecode.datasets import MOABBDataset\n",
    "from numpy import multiply\n",
    "from braindecode.preprocessing import (Preprocessor,\n",
    "                                       exponential_moving_standardize,\n",
    "                                       preprocess)\n",
    "from braindecode.preprocessing import create_windows_from_events\n",
    "import torch\n",
    "from braindecode.models import ShallowFBCSPNet\n",
    "from braindecode.util import set_random_seeds\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "from braindecode import EEGClassifier\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import os\n",
    "import pickle\n",
    "from matplotlib.lines import Line2D\n",
    "# from braindecode.visualization import plot_confusion_matrix\n",
    "\n",
    "from braindecode.datasets import BaseConcatDataset\n",
    "from braindecode.datasets.base import EEGWindowsDataset\n",
    "from braindecode.preprocessing.windowers import _create_windows_from_events\n",
    "import numpy as np\n",
    "import mne\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing the data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A06T.mat' to file 'C:\\Users\\mengz\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A06T.mat'.\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\urllib3\\connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "100%|#############################################| 44.6M/44.6M [00:00<?, ?B/s]\n",
      "SHA256 hash of downloaded file: 4dc3be1b0d60279134d1220323c73c68cf73799339a7fb224087a3c560a9a7e2\n",
      "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
      "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A06E.mat' to file 'C:\\Users\\mengz\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A06E.mat'.\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\urllib3\\connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "100%|#############################################| 43.4M/43.4M [00:00<?, ?B/s]\n",
      "SHA256 hash of downloaded file: bf67a40621b74b6af7a986c2f6edfff7fc2bbbca237aadd07b575893032998d1\n",
      "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
      "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A07T.mat' to file 'C:\\Users\\mengz\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A07T.mat'.\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\urllib3\\connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "100%|#############################################| 42.8M/42.8M [00:00<?, ?B/s]\n",
      "SHA256 hash of downloaded file: 43b6bbef0be78f0ac2b66cb2d9679091f1f5b7f0a5d4ebef73d2c7cc8e11aa96\n",
      "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
      "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A07E.mat' to file 'C:\\Users\\mengz\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A07E.mat'.\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\urllib3\\connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "100%|#####################################| 42.2M/42.2M [00:00<00:00, 42.1GB/s]\n",
      "SHA256 hash of downloaded file: b9aaec73dcee002fab84ee98e938039a67bf6a3cbf4fc86d5d8df198cfe4c323\n",
      "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
      "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A08T.mat' to file 'C:\\Users\\mengz\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A08T.mat'.\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\urllib3\\connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "100%|#####################################| 45.0M/45.0M [00:00<00:00, 44.3GB/s]\n",
      "SHA256 hash of downloaded file: 7a4b3bd602d5bc307d3f4527fca2cf076659e94aca584dd64f6286fd413a82f2\n",
      "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
      "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A08E.mat' to file 'C:\\Users\\mengz\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A08E.mat'.\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\urllib3\\connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "100%|#############################################| 46.3M/46.3M [00:00<?, ?B/s]\n",
      "SHA256 hash of downloaded file: 0eedbd89790c7d621c8eef68065ddecf80d437bbbcf60321d9253e2305f294f7\n",
      "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
      "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A09T.mat' to file 'C:\\Users\\mengz\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A09T.mat'.\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\urllib3\\connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "100%|#####################################| 44.8M/44.8M [00:00<00:00, 2.86GB/s]\n",
      "SHA256 hash of downloaded file: b28d8a262c779c8cad9cc80ee6aa9c5691cfa6617c03befe490a090347ebd15c\n",
      "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
      "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A09E.mat' to file 'C:\\Users\\mengz\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A09E.mat'.\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\urllib3\\connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "100%|#############################################| 44.8M/44.8M [00:00<?, ?B/s]\n",
      "SHA256 hash of downloaded file: 5d79649a42df9d51215def8ffbdaf1c3f76c54b88b9bbaae721e8c6fd972cc36\n",
      "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n"
     ]
    }
   ],
   "source": [
    "# subject_id = 3\n",
    "dataset = MOABBDataset(dataset_name=\"BNCI2014_001\", subject_ids=list(range(1, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\preprocessing\\preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
      "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<braindecode.datasets.moabb.MOABBDataset at 0x250604d8590>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_cut_hz = 4.  # low cut frequency for filtering\n",
    "high_cut_hz = 38.  # high cut frequency for filtering\n",
    "# Parameters for exponential moving standardization\n",
    "factor_new = 1e-3\n",
    "init_block_size = 1000\n",
    "# Factor to convert from V to uV\n",
    "factor = 1e6\n",
    "\n",
    "preprocessors = [\n",
    "    Preprocessor('pick_types', eeg=True, meg=False, stim=False),  # Keep EEG sensors\n",
    "    Preprocessor(lambda data: multiply(data, factor)),  # Convert from V to uV\n",
    "    Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz),  # Bandpass filter\n",
    "    Preprocessor(exponential_moving_standardize,  # Exponential moving standardization\n",
    "                 factor_new=factor_new, init_block_size=init_block_size)\n",
    "]\n",
    "\n",
    "# Transform the data\n",
    "preprocess(dataset, preprocessors, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Compute Windows\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n"
     ]
    }
   ],
   "source": [
    "trial_start_offset_seconds = -0.5\n",
    "# Extract sampling frequency, check that they are same in all datasets\n",
    "sfreq = dataset.datasets[0].raw.info['sfreq']\n",
    "assert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])\n",
    "# Calculate the trial start offset in samples.\n",
    "trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
    "\n",
    "# Create windows using braindecode function for this. It needs parameters to define how\n",
    "# trials should be used.\n",
    "windows_dataset = create_windows_from_events(\n",
    "    dataset,\n",
    "    trial_start_offset_samples=trial_start_offset_samples,\n",
    "    trial_stop_offset_samples=0,\n",
    "    preload=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset(window_set, target_trial_num):\n",
    "    new_ds_lst = []\n",
    "    \n",
    "    for ds in window_set.datasets:\n",
    "        cur_run_trial_num = len(ds.metadata)\n",
    "        if target_trial_num > cur_run_trial_num:\n",
    "            new_ds_lst.append(ds)\n",
    "            target_trial_num -= cur_run_trial_num\n",
    "        else:\n",
    "            new_ds_lst.append(EEGWindowsDataset(ds.raw, ds.metadata[:target_trial_num], description=ds.description[:target_trial_num]))\n",
    "            break\n",
    "\n",
    "    return BaseConcatDataset(new_ds_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "if cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "# Set random seed to be able to roughly reproduce results\n",
    "# Note that with cudnn benchmark set to True, GPU indeterminism\n",
    "# may still make results substantially different between runs.\n",
    "# To obtain more consistent results at the cost of increased computation time,\n",
    "# you can set `cudnn_benchmark=False` in `set_random_seeds`\n",
    "# or remove `torch.backends.cudnn.benchmark = True`\n",
    "seed = 20200220\n",
    "set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "n_classes = 4\n",
    "classes = list(range(n_classes))\n",
    "# Extract number of chans and time steps from dataset\n",
    "n_chans = train_set[0][0].shape[0]\n",
    "input_window_samples = train_set[0][0].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for subject 1 with 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3000\u001b[0m        \u001b[32m2.2982\u001b[0m       \u001b[35m0.2188\u001b[0m            \u001b[31m0.2188\u001b[0m        \u001b[94m3.4029\u001b[0m  0.0006  0.3629\n",
      "      2            0.3000        \u001b[32m1.4780\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m2.9980\u001b[0m  0.0006  0.3207\n",
      "      3            0.3000        \u001b[32m0.5364\u001b[0m       0.2500            0.2500        3.1499  0.0006  0.3303\n",
      "      4            \u001b[36m0.4000\u001b[0m        \u001b[32m0.5146\u001b[0m       0.2431            0.2431        3.2865  0.0006  0.3356\n",
      "      5            \u001b[36m0.8000\u001b[0m        \u001b[32m0.4008\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        3.5582  0.0006  0.3323\n",
      "      6            0.8000        \u001b[32m0.1817\u001b[0m       0.2465            0.2465        3.6862  0.0006  0.3342\n",
      "      7            0.8000        \u001b[32m0.0910\u001b[0m       0.2431            0.2431        3.7328  0.0006  0.3130\n",
      "      8            \u001b[36m0.9000\u001b[0m        0.1503       0.2396            0.2396        3.6721  0.0006  0.3206\n",
      "      9            0.9000        \u001b[32m0.0632\u001b[0m       0.2431            0.2431        3.5261  0.0006  0.3129\n",
      "     10            0.9000        \u001b[32m0.0436\u001b[0m       0.2326            0.2326        3.3766  0.0005  0.3014\n",
      "     11            0.9000        0.0643       0.2326            0.2326        3.2028  0.0005  0.3170\n",
      "     12            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0391\u001b[0m       0.2431            0.2431        3.0254  0.0005  0.3206\n",
      "     13            1.0000        \u001b[32m0.0315\u001b[0m       0.2431            0.2431        \u001b[94m2.8580\u001b[0m  0.0005  0.3273\n",
      "     14            1.0000        \u001b[32m0.0246\u001b[0m       0.2431            0.2431        \u001b[94m2.7121\u001b[0m  0.0005  0.3027\n",
      "     15            1.0000        \u001b[32m0.0152\u001b[0m       0.2222            0.2222        \u001b[94m2.5867\u001b[0m  0.0004  0.3025\n",
      "     16            1.0000        0.0224       0.2257            0.2257        \u001b[94m2.4801\u001b[0m  0.0004  0.3079\n",
      "     17            1.0000        0.0515       0.2326            0.2326        \u001b[94m2.3895\u001b[0m  0.0004  0.2958\n",
      "     18            1.0000        \u001b[32m0.0103\u001b[0m       0.2361            0.2361        \u001b[94m2.3126\u001b[0m  0.0004  0.3011\n",
      "     19            1.0000        0.0119       0.2292            0.2292        \u001b[94m2.2505\u001b[0m  0.0004  0.3234\n",
      "     20            1.0000        0.0248       0.2326            0.2326        \u001b[94m2.2054\u001b[0m  0.0003  0.3179\n",
      "     21            1.0000        \u001b[32m0.0073\u001b[0m       0.2292            0.2292        \u001b[94m2.1706\u001b[0m  0.0003  0.3106\n",
      "     22            1.0000        0.0173       0.2326            0.2326        \u001b[94m2.1449\u001b[0m  0.0003  0.3293\n",
      "     23            1.0000        0.0387       0.2292            0.2292        \u001b[94m2.1240\u001b[0m  0.0002  0.4495\n",
      "     24            1.0000        0.0099       0.2396            0.2396        \u001b[94m2.1083\u001b[0m  0.0002  0.4331\n",
      "     25            1.0000        0.0134       0.2431            0.2431        \u001b[94m2.0986\u001b[0m  0.0002  0.4355\n",
      "     26            1.0000        0.0330       0.2431            0.2431        \u001b[94m2.0898\u001b[0m  0.0002  0.4472\n",
      "     27            1.0000        0.0078       0.2465            0.2465        \u001b[94m2.0840\u001b[0m  0.0002  0.4490\n",
      "     28            1.0000        0.0148       0.2500            0.2500        \u001b[94m2.0782\u001b[0m  0.0001  0.4085\n",
      "     29            1.0000        0.0105       0.2500            0.2500        \u001b[94m2.0736\u001b[0m  0.0001  0.3525\n",
      "     30            1.0000        0.0149       0.2500            0.2500        \u001b[94m2.0723\u001b[0m  0.0001  0.3405\n",
      "     31            1.0000        0.0162       0.2500            0.2500        \u001b[94m2.0714\u001b[0m  0.0001  0.3574\n",
      "     32            1.0000        0.0177       0.2569            0.2569        \u001b[94m2.0708\u001b[0m  0.0001  0.3436\n",
      "     33            1.0000        0.0088       0.2569            0.2569        \u001b[94m2.0704\u001b[0m  0.0000  0.3360\n",
      "     34            1.0000        0.0140       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        2.0710  0.0000  0.4300\n",
      "     35            1.0000        0.0175       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        2.0719  0.0000  0.3648\n",
      "     36            1.0000        0.0083       0.2639            0.2639        2.0724  0.0000  0.3536\n",
      "     37            1.0000        0.0103       0.2639            0.2639        2.0715  0.0000  0.3380\n",
      "     38            1.0000        0.0103       0.2639            0.2639        2.0718  0.0000  0.3340\n",
      "     39            1.0000        0.0083       0.2639            0.2639        2.0712  0.0000  0.3757\n",
      "     40            1.0000        0.0161       0.2639            0.2639        \u001b[94m2.0700\u001b[0m  0.0000  0.3500\n",
      "Training model for subject 1 with 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.5500\u001b[0m        \u001b[32m1.6643\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        \u001b[94m1.4681\u001b[0m  0.0006  0.2991\n",
      "      2            \u001b[36m0.9000\u001b[0m        \u001b[32m1.0510\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        1.7377  0.0006  0.2931\n",
      "      3            0.9000        \u001b[32m0.7156\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        1.9787  0.0006  0.2911\n",
      "      4            0.8500        \u001b[32m0.5472\u001b[0m       0.3333            0.3333        2.2011  0.0006  0.2866\n",
      "      5            0.9000        \u001b[32m0.2576\u001b[0m       0.3229            0.3229        2.2855  0.0006  0.2783\n",
      "      6            0.9000        \u001b[32m0.1668\u001b[0m       0.3160            0.3160        2.3164  0.0006  0.2652\n",
      "      7            0.9000        0.2726       0.3299            0.3299        2.2012  0.0006  0.2752\n",
      "      8            0.9000        0.2080       0.3194            0.3194        2.0953  0.0006  0.2819\n",
      "      9            \u001b[36m0.9500\u001b[0m        \u001b[32m0.1183\u001b[0m       0.3403            0.3403        1.9573  0.0006  0.2599\n",
      "     10            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1155\u001b[0m       0.3368            0.3368        1.8395  0.0005  0.2625\n",
      "     11            1.0000        \u001b[32m0.0550\u001b[0m       0.3403            0.3403        1.7397  0.0005  0.2669\n",
      "     12            1.0000        0.0695       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        1.6782  0.0005  0.2590\n",
      "     13            1.0000        0.0595       0.3472            0.3472        1.6504  0.0005  0.2607\n",
      "     14            1.0000        \u001b[32m0.0408\u001b[0m       0.3507            0.3507        1.6376  0.0005  0.2690\n",
      "     15            1.0000        0.0519       0.3507            0.3507        1.6338  0.0004  0.2688\n",
      "     16            1.0000        0.0523       0.3507            0.3507        1.6361  0.0004  0.2748\n",
      "     17            1.0000        \u001b[32m0.0405\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        1.6442  0.0004  0.2784\n",
      "     18            1.0000        0.0700       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        1.6644  0.0004  0.2684\n",
      "     19            1.0000        \u001b[32m0.0206\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        1.6812  0.0004  0.2749\n",
      "     20            1.0000        0.0262       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        1.6937  0.0003  0.2618\n",
      "     21            1.0000        0.0260       0.3646            0.3646        1.7016  0.0003  0.2514\n",
      "     22            1.0000        \u001b[32m0.0135\u001b[0m       0.3646            0.3646        1.7083  0.0003  0.2640\n",
      "     23            1.0000        0.0161       0.3646            0.3646        1.7148  0.0002  0.2923\n",
      "     24            1.0000        0.0216       0.3646            0.3646        1.7182  0.0002  0.2639\n",
      "     25            1.0000        0.0164       0.3646            0.3646        1.7197  0.0002  0.2687\n",
      "     26            1.0000        0.0203       0.3646            0.3646        1.7219  0.0002  0.2733\n",
      "     27            1.0000        0.0362       0.3611            0.3611        1.7218  0.0002  0.2767\n",
      "     28            1.0000        0.0193       0.3611            0.3611        1.7213  0.0001  0.2542\n",
      "     29            1.0000        0.0304       0.3542            0.3542        1.7197  0.0001  0.2768\n",
      "     30            1.0000        0.0241       0.3542            0.3542        1.7195  0.0001  0.2607\n",
      "     31            1.0000        0.0216       0.3576            0.3576        1.7179  0.0001  0.3618\n",
      "     32            1.0000        0.0242       0.3576            0.3576        1.7167  0.0001  0.3486\n",
      "     33            1.0000        0.0178       0.3576            0.3576        1.7145  0.0000  0.3542\n",
      "     34            1.0000        \u001b[32m0.0095\u001b[0m       0.3576            0.3576        1.7128  0.0000  0.3386\n",
      "     35            1.0000        0.0180       0.3576            0.3576        1.7117  0.0000  0.3458\n",
      "     36            1.0000        0.0274       0.3611            0.3611        1.7100  0.0000  0.3871\n",
      "     37            1.0000        0.0323       0.3611            0.3611        1.7089  0.0000  0.3713\n",
      "     38            1.0000        0.0190       0.3611            0.3611        1.7082  0.0000  0.2603\n",
      "     39            1.0000        0.0170       0.3611            0.3611        1.7071  0.0000  0.2788\n",
      "     40            1.0000        0.0200       0.3646            0.3646        1.7067  0.0000  0.2643\n",
      "Training model for subject 1 with 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2667\u001b[0m        \u001b[32m1.6866\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.0832\u001b[0m  0.0006  0.2723\n",
      "      2            \u001b[36m0.3000\u001b[0m        \u001b[32m1.2495\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m3.0234\u001b[0m  0.0006  0.2540\n",
      "      3            \u001b[36m0.4667\u001b[0m        \u001b[32m0.9231\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m2.5564\u001b[0m  0.0006  0.2520\n",
      "      4            \u001b[36m0.7333\u001b[0m        \u001b[32m0.5831\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m2.2145\u001b[0m  0.0006  0.2622\n",
      "      5            \u001b[36m0.8000\u001b[0m        \u001b[32m0.4505\u001b[0m       0.2778            0.2778        \u001b[94m2.1134\u001b[0m  0.0006  0.2618\n",
      "      6            0.8000        \u001b[32m0.4270\u001b[0m       0.2847            0.2847        2.1452  0.0006  0.2660\n",
      "      7            0.8000        \u001b[32m0.3991\u001b[0m       0.2847            0.2847        2.3188  0.0006  0.2626\n",
      "      8            0.7333        \u001b[32m0.3503\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        2.5425  0.0006  0.2595\n",
      "      9            0.7000        \u001b[32m0.2059\u001b[0m       0.2951            0.2951        2.7238  0.0006  0.2927\n",
      "     10            0.8000        \u001b[32m0.1908\u001b[0m       0.2951            0.2951        2.7680  0.0005  0.2546\n",
      "     11            0.8000        \u001b[32m0.1151\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        2.7203  0.0005  0.2638\n",
      "     12            \u001b[36m0.9000\u001b[0m        \u001b[32m0.1029\u001b[0m       0.3021            0.3021        2.6457  0.0005  0.2619\n",
      "     13            0.9000        0.1281       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        2.5220  0.0005  0.2661\n",
      "     14            \u001b[36m0.9667\u001b[0m        0.1032       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        2.3825  0.0005  0.2669\n",
      "     15            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0675\u001b[0m       0.3125            0.3125        2.2693  0.0004  0.2588\n",
      "     16            1.0000        0.0741       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        2.1733  0.0004  0.2615\n",
      "     17            1.0000        \u001b[32m0.0596\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m2.1055\u001b[0m  0.0004  0.2513\n",
      "     18            1.0000        \u001b[32m0.0367\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m2.0598\u001b[0m  0.0004  0.2612\n",
      "     19            1.0000        0.0866       0.3160            0.3160        \u001b[94m2.0278\u001b[0m  0.0004  0.3640\n",
      "     20            1.0000        0.0707       0.3125            0.3125        \u001b[94m2.0003\u001b[0m  0.0003  0.2625\n",
      "     21            1.0000        0.0454       0.3264            0.3264        \u001b[94m1.9830\u001b[0m  0.0003  0.2788\n",
      "     22            1.0000        0.0537       0.3264            0.3264        \u001b[94m1.9710\u001b[0m  0.0003  0.2622\n",
      "     23            1.0000        0.0482       0.3264            0.3264        \u001b[94m1.9607\u001b[0m  0.0002  0.2487\n",
      "     24            1.0000        0.0459       0.3264            0.3264        \u001b[94m1.9540\u001b[0m  0.0002  0.2526\n",
      "     25            1.0000        0.0596       0.3264            0.3264        \u001b[94m1.9478\u001b[0m  0.0002  0.2584\n",
      "     26            1.0000        0.0491       0.3264            0.3264        \u001b[94m1.9425\u001b[0m  0.0002  0.2610\n",
      "     27            1.0000        0.0702       0.3229            0.3229        \u001b[94m1.9379\u001b[0m  0.0002  0.2506\n",
      "     28            1.0000        \u001b[32m0.0227\u001b[0m       0.3229            0.3229        \u001b[94m1.9340\u001b[0m  0.0001  0.2630\n",
      "     29            1.0000        0.0485       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m1.9310\u001b[0m  0.0001  0.2767\n",
      "     30            1.0000        0.0415       0.3299            0.3299        \u001b[94m1.9275\u001b[0m  0.0001  0.2659\n",
      "     31            1.0000        0.0344       0.3299            0.3299        \u001b[94m1.9252\u001b[0m  0.0001  0.2686\n",
      "     32            1.0000        0.0461       0.3299            0.3299        \u001b[94m1.9226\u001b[0m  0.0001  0.2587\n",
      "     33            1.0000        0.0316       0.3299            0.3299        \u001b[94m1.9204\u001b[0m  0.0000  0.2613\n",
      "     34            1.0000        0.0383       0.3299            0.3299        \u001b[94m1.9186\u001b[0m  0.0000  0.2650\n",
      "     35            1.0000        0.0439       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m1.9168\u001b[0m  0.0000  0.2788\n",
      "     36            1.0000        0.0293       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m1.9146\u001b[0m  0.0000  0.2611\n",
      "     37            1.0000        0.0371       0.3368            0.3368        \u001b[94m1.9135\u001b[0m  0.0000  0.2653\n",
      "     38            1.0000        0.0355       0.3368            0.3368        \u001b[94m1.9122\u001b[0m  0.0000  0.2518\n",
      "     39            1.0000        \u001b[32m0.0219\u001b[0m       0.3368            0.3368        \u001b[94m1.9109\u001b[0m  0.0000  0.2607\n",
      "     40            1.0000        0.0295       0.3368            0.3368        \u001b[94m1.9100\u001b[0m  0.0000  0.2617\n",
      "Training model for subject 1 with 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3750\u001b[0m        \u001b[32m1.5510\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m1.8893\u001b[0m  0.0006  0.2695\n",
      "      2            \u001b[36m0.6500\u001b[0m        \u001b[32m1.1274\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        1.8937  0.0006  0.2808\n",
      "      3            \u001b[36m0.6750\u001b[0m        \u001b[32m0.8179\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        1.9248  0.0006  0.2781\n",
      "      4            \u001b[36m0.7750\u001b[0m        \u001b[32m0.7220\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        1.9064  0.0006  0.3460\n",
      "      5            \u001b[36m0.8000\u001b[0m        \u001b[32m0.5466\u001b[0m       0.3194            0.3194        1.9940  0.0006  0.3442\n",
      "      6            0.8000        \u001b[32m0.4462\u001b[0m       0.3264            0.3264        2.0511  0.0006  0.3288\n",
      "      7            0.8000        0.4617       0.3229            0.3229        2.0902  0.0006  0.3522\n",
      "      8            0.8000        \u001b[32m0.4007\u001b[0m       0.3056            0.3056        2.1159  0.0006  0.3196\n",
      "      9            \u001b[36m0.8500\u001b[0m        \u001b[32m0.2764\u001b[0m       0.3160            0.3160        2.1030  0.0006  0.3504\n",
      "     10            0.8500        \u001b[32m0.2207\u001b[0m       0.3160            0.3160        2.0942  0.0005  0.3501\n",
      "     11            0.8500        0.3087       0.3264            0.3264        2.0752  0.0005  0.3250\n",
      "     12            \u001b[36m0.9000\u001b[0m        \u001b[32m0.1685\u001b[0m       0.3333            0.3333        2.0594  0.0005  0.2723\n",
      "     13            \u001b[36m0.9500\u001b[0m        \u001b[32m0.1448\u001b[0m       0.3229            0.3229        2.0358  0.0005  0.2634\n",
      "     14            \u001b[36m0.9750\u001b[0m        \u001b[32m0.1297\u001b[0m       0.3299            0.3299        1.9842  0.0005  0.2763\n",
      "     15            0.9750        0.1418       0.3299            0.3299        1.9394  0.0004  0.2657\n",
      "     16            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0933\u001b[0m       0.3194            0.3194        1.9023  0.0004  0.3001\n",
      "     17            1.0000        0.1516       0.3333            0.3333        \u001b[94m1.8623\u001b[0m  0.0004  0.2757\n",
      "     18            1.0000        0.1024       0.3229            0.3229        \u001b[94m1.8256\u001b[0m  0.0004  0.2620\n",
      "     19            1.0000        0.1122       0.3403            0.3403        \u001b[94m1.8003\u001b[0m  0.0004  0.2692\n",
      "     20            1.0000        0.0968       0.3368            0.3368        \u001b[94m1.7756\u001b[0m  0.0003  0.2753\n",
      "     21            1.0000        \u001b[32m0.0699\u001b[0m       0.3333            0.3333        \u001b[94m1.7577\u001b[0m  0.0003  0.2777\n",
      "     22            1.0000        0.0755       0.3333            0.3333        \u001b[94m1.7427\u001b[0m  0.0003  0.2679\n",
      "     23            1.0000        0.0853       0.3333            0.3333        \u001b[94m1.7325\u001b[0m  0.0002  0.2624\n",
      "     24            1.0000        \u001b[32m0.0595\u001b[0m       0.3368            0.3368        \u001b[94m1.7244\u001b[0m  0.0002  0.2626\n",
      "     25            1.0000        \u001b[32m0.0533\u001b[0m       0.3368            0.3368        \u001b[94m1.7179\u001b[0m  0.0002  0.2654\n",
      "     26            1.0000        0.0598       0.3403            0.3403        \u001b[94m1.7139\u001b[0m  0.0002  0.2696\n",
      "     27            1.0000        \u001b[32m0.0517\u001b[0m       0.3368            0.3368        \u001b[94m1.7115\u001b[0m  0.0002  0.2708\n",
      "     28            1.0000        0.0758       0.3368            0.3368        \u001b[94m1.7103\u001b[0m  0.0001  0.3734\n",
      "     29            1.0000        0.0906       0.3368            0.3368        \u001b[94m1.7101\u001b[0m  0.0001  0.2740\n",
      "     30            1.0000        0.0670       0.3368            0.3368        1.7104  0.0001  0.2577\n",
      "     31            1.0000        0.0569       0.3368            0.3368        1.7110  0.0001  0.2763\n",
      "     32            1.0000        0.0692       0.3368            0.3368        1.7111  0.0001  0.2693\n",
      "     33            1.0000        0.0561       0.3333            0.3333        1.7116  0.0000  0.2779\n",
      "     34            1.0000        \u001b[32m0.0488\u001b[0m       0.3333            0.3333        1.7119  0.0000  0.2768\n",
      "     35            1.0000        0.0577       0.3333            0.3333        1.7122  0.0000  0.2846\n",
      "     36            1.0000        0.0572       0.3333            0.3333        1.7124  0.0000  0.2620\n",
      "     37            1.0000        0.0567       0.3333            0.3333        1.7126  0.0000  0.2783\n",
      "     38            1.0000        0.0522       0.3333            0.3333        1.7129  0.0000  0.2653\n",
      "     39            1.0000        0.0628       0.3333            0.3333        1.7129  0.0000  0.2685\n",
      "     40            1.0000        0.0652       0.3333            0.3333        1.7130  0.0000  0.2759\n",
      "Training model for subject 1 with 50 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3200\u001b[0m        \u001b[32m1.6192\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m1.9510\u001b[0m  0.0006  0.2812\n",
      "      2            \u001b[36m0.6600\u001b[0m        \u001b[32m1.0424\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        \u001b[94m1.6901\u001b[0m  0.0006  0.2721\n",
      "      3            0.6200        \u001b[32m0.9915\u001b[0m       0.2917            0.2917        1.7181  0.0006  0.2922\n",
      "      4            0.4800        \u001b[32m0.8046\u001b[0m       0.2778            0.2778        2.0094  0.0006  0.2800\n",
      "      5            0.4400        \u001b[32m0.7574\u001b[0m       0.2604            0.2604        2.3615  0.0006  0.2846\n",
      "      6            0.4600        \u001b[32m0.4981\u001b[0m       0.2674            0.2674        2.4746  0.0006  0.2931\n",
      "      7            0.4600        \u001b[32m0.4901\u001b[0m       0.2708            0.2708        2.5536  0.0006  0.2931\n",
      "      8            0.5200        \u001b[32m0.4664\u001b[0m       0.2743            0.2743        2.5831  0.0006  0.2723\n",
      "      9            0.5200        \u001b[32m0.4244\u001b[0m       0.2743            0.2743        2.6338  0.0006  0.2917\n",
      "     10            0.6000        \u001b[32m0.2812\u001b[0m       0.2778            0.2778        2.5813  0.0005  0.2962\n",
      "     11            \u001b[36m0.7600\u001b[0m        0.2914       0.2951            0.2951        2.4488  0.0005  0.2899\n",
      "     12            \u001b[36m0.8600\u001b[0m        \u001b[32m0.2403\u001b[0m       0.3056            0.3056        2.2936  0.0005  0.2880\n",
      "     13            \u001b[36m0.9000\u001b[0m        0.2686       0.3160            0.3160        2.1344  0.0005  0.3403\n",
      "     14            \u001b[36m0.9800\u001b[0m        \u001b[32m0.1904\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        2.0083  0.0005  0.3678\n",
      "     15            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1862\u001b[0m       0.3264            0.3264        1.9138  0.0004  0.3594\n",
      "     16            1.0000        \u001b[32m0.1451\u001b[0m       0.3194            0.3194        1.8496  0.0004  0.3625\n",
      "     17            1.0000        0.2099       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        1.8114  0.0004  0.3941\n",
      "     18            1.0000        0.1766       0.3264            0.3264        1.7882  0.0004  0.3448\n",
      "     19            1.0000        \u001b[32m0.1373\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        1.7687  0.0004  0.3876\n",
      "     20            1.0000        0.1447       0.3403            0.3403        1.7552  0.0003  0.4024\n",
      "     21            1.0000        \u001b[32m0.1313\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        1.7433  0.0003  0.3044\n",
      "     22            1.0000        \u001b[32m0.1292\u001b[0m       0.3403            0.3403        1.7344  0.0003  0.2936\n",
      "     23            1.0000        0.1424       0.3438            0.3438        1.7264  0.0002  0.2931\n",
      "     24            1.0000        \u001b[32m0.1188\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        1.7188  0.0002  0.3011\n",
      "     25            1.0000        0.1435       0.3472            0.3472        1.7158  0.0002  0.2970\n",
      "     26            1.0000        0.1440       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        1.7157  0.0002  0.2773\n",
      "     27            1.0000        0.1245       0.3576            0.3576        1.7166  0.0002  0.2847\n",
      "     28            1.0000        \u001b[32m0.0883\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        1.7165  0.0001  0.2806\n",
      "     29            1.0000        0.1015       0.3611            0.3611        1.7164  0.0001  0.2972\n",
      "     30            1.0000        0.1043       0.3576            0.3576        1.7160  0.0001  0.3109\n",
      "     31            1.0000        0.0972       0.3576            0.3576        1.7146  0.0001  0.2827\n",
      "     32            1.0000        0.1082       0.3611            0.3611        1.7122  0.0001  0.2928\n",
      "     33            1.0000        0.0980       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        1.7103  0.0000  0.2839\n",
      "     34            1.0000        0.0969       0.3646            0.3646        1.7080  0.0000  0.2827\n",
      "     35            1.0000        0.1051       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        1.7060  0.0000  0.2930\n",
      "     36            1.0000        0.0958       0.3681            0.3681        1.7038  0.0000  0.2956\n",
      "     37            1.0000        0.1028       0.3681            0.3681        1.7017  0.0000  0.2872\n",
      "     38            1.0000        0.0981       0.3681            0.3681        1.7001  0.0000  0.2914\n",
      "     39            1.0000        0.1008       0.3646            0.3646        1.6989  0.0000  0.2939\n",
      "     40            1.0000        \u001b[32m0.0832\u001b[0m       0.3681            0.3681        1.6978  0.0000  0.2875\n",
      "Training model for subject 1 with 60 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2833\u001b[0m        \u001b[32m1.4789\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m2.8053\u001b[0m  0.0006  0.2937\n",
      "      2            \u001b[36m0.4833\u001b[0m        \u001b[32m1.2301\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m1.7438\u001b[0m  0.0006  0.3049\n",
      "      3            \u001b[36m0.5333\u001b[0m        \u001b[32m0.8592\u001b[0m       0.2917            0.2917        1.7776  0.0006  0.2886\n",
      "      4            0.2667        0.8672       0.2500            0.2500        2.9712  0.0006  0.2905\n",
      "      5            0.2333        \u001b[32m0.6956\u001b[0m       0.2500            0.2500        3.7268  0.0006  0.2970\n",
      "      6            0.2333        \u001b[32m0.6456\u001b[0m       0.2500            0.2500        4.1003  0.0006  0.3029\n",
      "      7            0.2333        \u001b[32m0.5226\u001b[0m       0.2500            0.2500        4.1660  0.0006  0.3085\n",
      "      8            0.2667        \u001b[32m0.4734\u001b[0m       0.2500            0.2500        3.8728  0.0006  0.2881\n",
      "      9            0.3667        \u001b[32m0.4309\u001b[0m       0.2500            0.2500        3.4866  0.0006  0.2847\n",
      "     10            0.3833        \u001b[32m0.4022\u001b[0m       0.2569            0.2569        3.1718  0.0005  0.2934\n",
      "     11            0.5167        \u001b[32m0.3668\u001b[0m       0.2743            0.2743        2.8496  0.0005  0.2952\n",
      "     12            \u001b[36m0.6167\u001b[0m        \u001b[32m0.3241\u001b[0m       0.2847            0.2847        2.5492  0.0005  0.2856\n",
      "     13            \u001b[36m0.7833\u001b[0m        \u001b[32m0.2557\u001b[0m       0.2917            0.2917        2.3299  0.0005  0.3944\n",
      "     14            \u001b[36m0.8833\u001b[0m        \u001b[32m0.2368\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        2.1200  0.0005  0.3128\n",
      "     15            \u001b[36m0.9667\u001b[0m        \u001b[32m0.1986\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        1.9654  0.0004  0.2932\n",
      "     16            \u001b[36m0.9833\u001b[0m        0.2027       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        1.8675  0.0004  0.2939\n",
      "     17            0.9833        \u001b[32m0.1828\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        1.7955  0.0004  0.3179\n",
      "     18            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1683\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        1.7440  0.0004  0.3359\n",
      "     19            1.0000        \u001b[32m0.1650\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.7145\u001b[0m  0.0004  0.3478\n",
      "     20            1.0000        \u001b[32m0.1545\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.6888\u001b[0m  0.0003  0.4038\n",
      "     21            1.0000        0.1772       0.3611            0.3611        \u001b[94m1.6680\u001b[0m  0.0003  0.3564\n",
      "     22            1.0000        \u001b[32m0.1082\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.6542\u001b[0m  0.0003  0.3848\n",
      "     23            1.0000        0.1397       0.3646            0.3646        \u001b[94m1.6445\u001b[0m  0.0002  0.3630\n",
      "     24            1.0000        0.2026       0.3542            0.3542        \u001b[94m1.6383\u001b[0m  0.0002  0.3749\n",
      "     25            1.0000        0.1433       0.3507            0.3507        \u001b[94m1.6354\u001b[0m  0.0002  0.4115\n",
      "     26            1.0000        0.1348       0.3576            0.3576        \u001b[94m1.6320\u001b[0m  0.0002  0.3890\n",
      "     27            1.0000        0.1160       0.3542            0.3542        \u001b[94m1.6276\u001b[0m  0.0002  0.3116\n",
      "     28            1.0000        0.1129       0.3542            0.3542        \u001b[94m1.6250\u001b[0m  0.0001  0.3106\n",
      "     29            1.0000        0.1281       0.3542            0.3542        \u001b[94m1.6241\u001b[0m  0.0001  0.2966\n",
      "     30            1.0000        0.1613       0.3576            0.3576        \u001b[94m1.6237\u001b[0m  0.0001  0.3325\n",
      "     31            1.0000        0.1089       0.3542            0.3542        1.6242  0.0001  0.3172\n",
      "     32            1.0000        0.1092       0.3542            0.3542        1.6250  0.0001  0.2918\n",
      "     33            1.0000        0.1198       0.3542            0.3542        1.6257  0.0000  0.3249\n",
      "     34            1.0000        0.1500       0.3542            0.3542        1.6263  0.0000  0.3334\n",
      "     35            1.0000        \u001b[32m0.0826\u001b[0m       0.3542            0.3542        1.6267  0.0000  0.2932\n",
      "     36            1.0000        0.1116       0.3576            0.3576        1.6266  0.0000  0.2938\n",
      "     37            1.0000        0.1040       0.3576            0.3576        1.6263  0.0000  0.2956\n",
      "     38            1.0000        0.0936       0.3576            0.3576        1.6261  0.0000  0.3007\n",
      "     39            1.0000        0.1000       0.3542            0.3542        1.6259  0.0000  0.2937\n",
      "     40            1.0000        0.0955       0.3542            0.3542        1.6258  0.0000  0.2964\n",
      "Training model for subject 1 with 70 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3571\u001b[0m        \u001b[32m1.8348\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m3.2455\u001b[0m  0.0006  0.3183\n",
      "      2            0.2857        \u001b[32m1.3973\u001b[0m       0.2708            0.2708        \u001b[94m2.5342\u001b[0m  0.0006  0.2971\n",
      "      3            0.2714        \u001b[32m1.2372\u001b[0m       0.2569            0.2569        2.6616  0.0006  0.2966\n",
      "      4            0.2714        \u001b[32m0.8937\u001b[0m       0.2569            0.2569        2.7762  0.0006  0.3205\n",
      "      5            0.3143        \u001b[32m0.8270\u001b[0m       0.2569            0.2569        2.7172  0.0006  0.3062\n",
      "      6            \u001b[36m0.4000\u001b[0m        \u001b[32m0.7794\u001b[0m       0.2639            0.2639        2.6927  0.0006  0.3193\n",
      "      7            \u001b[36m0.4714\u001b[0m        \u001b[32m0.6229\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        2.7173  0.0006  0.3115\n",
      "      8            \u001b[36m0.5286\u001b[0m        \u001b[32m0.5857\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        2.6449  0.0006  0.3079\n",
      "      9            \u001b[36m0.6429\u001b[0m        \u001b[32m0.5229\u001b[0m       0.2778            0.2778        \u001b[94m2.4970\u001b[0m  0.0006  0.3035\n",
      "     10            \u001b[36m0.7857\u001b[0m        \u001b[32m0.4259\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        \u001b[94m2.3072\u001b[0m  0.0005  0.3103\n",
      "     11            \u001b[36m0.8571\u001b[0m        \u001b[32m0.3828\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m2.1433\u001b[0m  0.0005  0.2973\n",
      "     12            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3519\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m2.0340\u001b[0m  0.0005  0.3159\n",
      "     13            \u001b[36m0.9571\u001b[0m        \u001b[32m0.3011\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.9269\u001b[0m  0.0005  0.3113\n",
      "     14            0.9571        0.3334       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.8582\u001b[0m  0.0005  0.3274\n",
      "     15            \u001b[36m0.9714\u001b[0m        \u001b[32m0.2333\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.7928\u001b[0m  0.0004  0.3034\n",
      "     16            \u001b[36m0.9857\u001b[0m        0.2775       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.7367\u001b[0m  0.0004  0.3085\n",
      "     17            \u001b[36m1.0000\u001b[0m        0.2822       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.7050\u001b[0m  0.0004  0.3044\n",
      "     18            1.0000        \u001b[32m0.2026\u001b[0m       0.3924            0.3924        \u001b[94m1.6824\u001b[0m  0.0004  0.3258\n",
      "     19            1.0000        0.2479       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.6693\u001b[0m  0.0004  0.3234\n",
      "     20            1.0000        \u001b[32m0.1836\u001b[0m       0.3924            0.3924        \u001b[94m1.6588\u001b[0m  0.0003  0.4104\n",
      "     21            1.0000        \u001b[32m0.1664\u001b[0m       0.3924            0.3924        \u001b[94m1.6483\u001b[0m  0.0003  0.3346\n",
      "     22            1.0000        0.2246       0.3854            0.3854        \u001b[94m1.6399\u001b[0m  0.0003  0.3776\n",
      "     23            1.0000        0.1935       0.3854            0.3854        \u001b[94m1.6313\u001b[0m  0.0002  0.3748\n",
      "     24            1.0000        0.1695       0.3924            0.3924        \u001b[94m1.6270\u001b[0m  0.0002  0.3909\n",
      "     25            1.0000        \u001b[32m0.1522\u001b[0m       0.3889            0.3889        \u001b[94m1.6253\u001b[0m  0.0002  0.3649\n",
      "     26            1.0000        0.1867       0.3819            0.3819        1.6265  0.0002  0.3703\n",
      "     27            1.0000        \u001b[32m0.1519\u001b[0m       0.3854            0.3854        1.6266  0.0002  0.4144\n",
      "     28            1.0000        \u001b[32m0.1440\u001b[0m       0.3819            0.3819        1.6277  0.0001  0.3954\n",
      "     29            1.0000        0.1457       0.3819            0.3819        1.6286  0.0001  0.3643\n",
      "     30            1.0000        \u001b[32m0.1214\u001b[0m       0.3854            0.3854        1.6287  0.0001  0.3323\n",
      "     31            1.0000        0.1434       0.3819            0.3819        1.6285  0.0001  0.3133\n",
      "     32            1.0000        0.1416       0.3785            0.3785        1.6287  0.0001  0.3182\n",
      "     33            1.0000        0.1301       0.3785            0.3785        1.6291  0.0000  0.3120\n",
      "     34            1.0000        0.1891       0.3750            0.3750        1.6290  0.0000  0.3124\n",
      "     35            1.0000        0.1401       0.3750            0.3750        1.6289  0.0000  0.3210\n",
      "     36            1.0000        0.1385       0.3715            0.3715        1.6288  0.0000  0.3090\n",
      "     37            1.0000        0.1540       0.3715            0.3715        1.6288  0.0000  0.3291\n",
      "     38            1.0000        0.1714       0.3715            0.3715        1.6284  0.0000  0.3466\n",
      "     39            1.0000        \u001b[32m0.1155\u001b[0m       0.3715            0.3715        1.6282  0.0000  0.3125\n",
      "     40            1.0000        0.1737       0.3715            0.3715        1.6277  0.0000  0.3191\n",
      "Training model for subject 1 with 80 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.4125\u001b[0m        \u001b[32m1.7256\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m1.9120\u001b[0m  0.0006  0.3274\n",
      "      2            0.2625        \u001b[32m1.4013\u001b[0m       0.2500            0.2500        3.0230  0.0006  0.4159\n",
      "      3            0.2625        \u001b[32m0.9630\u001b[0m       0.2500            0.2500        4.1212  0.0006  0.3225\n",
      "      4            0.2625        1.0230       0.2500            0.2500        4.3392  0.0006  0.3251\n",
      "      5            0.2625        \u001b[32m0.8383\u001b[0m       0.2500            0.2500        4.4880  0.0006  0.3209\n",
      "      6            0.2625        \u001b[32m0.7436\u001b[0m       0.2500            0.2500        4.4216  0.0006  0.3279\n",
      "      7            0.2625        \u001b[32m0.6357\u001b[0m       0.2500            0.2500        4.0953  0.0006  0.3304\n",
      "      8            0.3250        \u001b[32m0.5622\u001b[0m       0.2500            0.2500        3.7997  0.0006  0.3521\n",
      "      9            0.3250        \u001b[32m0.5506\u001b[0m       0.2535            0.2535        3.5737  0.0006  0.3221\n",
      "     10            0.3625        \u001b[32m0.5111\u001b[0m       0.2535            0.2535        3.3999  0.0005  0.3356\n",
      "     11            \u001b[36m0.4375\u001b[0m        0.5130       0.2569            0.2569        3.2033  0.0005  0.3280\n",
      "     12            \u001b[36m0.5375\u001b[0m        \u001b[32m0.4102\u001b[0m       0.2743            0.2743        2.9367  0.0005  0.3272\n",
      "     13            \u001b[36m0.6250\u001b[0m        \u001b[32m0.3905\u001b[0m       0.2812            0.2812        2.6735  0.0005  0.3360\n",
      "     14            \u001b[36m0.7375\u001b[0m        \u001b[32m0.3277\u001b[0m       0.2882            0.2882        2.4298  0.0005  0.3282\n",
      "     15            \u001b[36m0.8000\u001b[0m        \u001b[32m0.3112\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        2.2165  0.0004  0.3277\n",
      "     16            \u001b[36m0.9000\u001b[0m        \u001b[32m0.2840\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        2.0544  0.0004  0.3249\n",
      "     17            \u001b[36m0.9125\u001b[0m        \u001b[32m0.2565\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        1.9256  0.0004  0.3165\n",
      "     18            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2365\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.8221\u001b[0m  0.0004  0.3369\n",
      "     19            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2181\u001b[0m       0.3750            0.3750        \u001b[94m1.7437\u001b[0m  0.0004  0.3582\n",
      "     20            \u001b[36m0.9875\u001b[0m        \u001b[32m0.1946\u001b[0m       0.3750            0.3750        \u001b[94m1.6931\u001b[0m  0.0003  0.3313\n",
      "     21            \u001b[36m1.0000\u001b[0m        0.2450       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.6595\u001b[0m  0.0003  0.3366\n",
      "     22            1.0000        \u001b[32m0.1886\u001b[0m       0.3715            0.3715        \u001b[94m1.6361\u001b[0m  0.0003  0.3556\n",
      "     23            1.0000        0.2315       0.3646            0.3646        \u001b[94m1.6128\u001b[0m  0.0002  0.3960\n",
      "     24            1.0000        \u001b[32m0.1836\u001b[0m       0.3681            0.3681        \u001b[94m1.5979\u001b[0m  0.0002  0.3816\n",
      "     25            1.0000        \u001b[32m0.1760\u001b[0m       0.3681            0.3681        \u001b[94m1.5811\u001b[0m  0.0002  0.3961\n",
      "     26            1.0000        \u001b[32m0.1593\u001b[0m       0.3681            0.3681        \u001b[94m1.5674\u001b[0m  0.0002  0.3791\n",
      "     27            1.0000        \u001b[32m0.1518\u001b[0m       0.3681            0.3681        \u001b[94m1.5556\u001b[0m  0.0002  0.3789\n",
      "     28            1.0000        \u001b[32m0.1219\u001b[0m       0.3715            0.3715        \u001b[94m1.5477\u001b[0m  0.0001  0.3998\n",
      "     29            1.0000        0.1480       0.3750            0.3750        \u001b[94m1.5390\u001b[0m  0.0001  0.5292\n",
      "     30            1.0000        \u001b[32m0.1208\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.5325\u001b[0m  0.0001  0.3297\n",
      "     31            1.0000        0.1827       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.5274\u001b[0m  0.0001  0.3436\n",
      "     32            1.0000        0.1426       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.5233\u001b[0m  0.0001  0.3212\n",
      "     33            1.0000        0.1555       0.4028            0.4028        \u001b[94m1.5192\u001b[0m  0.0000  0.3269\n",
      "     34            1.0000        0.1413       0.4028            0.4028        \u001b[94m1.5159\u001b[0m  0.0000  0.3273\n",
      "     35            1.0000        0.1404       0.3958            0.3958        \u001b[94m1.5137\u001b[0m  0.0000  0.3421\n",
      "     36            1.0000        0.1443       0.3993            0.3993        \u001b[94m1.5118\u001b[0m  0.0000  0.3241\n",
      "     37            1.0000        0.1456       0.4028            0.4028        \u001b[94m1.5106\u001b[0m  0.0000  0.3362\n",
      "     38            1.0000        \u001b[32m0.1197\u001b[0m       0.4028            0.4028        \u001b[94m1.5095\u001b[0m  0.0000  0.3610\n",
      "     39            1.0000        0.1499       0.4062            0.4062        \u001b[94m1.5088\u001b[0m  0.0000  0.3435\n",
      "     40            1.0000        0.1607       0.4062            0.4062        \u001b[94m1.5081\u001b[0m  0.0000  0.3379\n",
      "Training model for subject 1 with 90 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3111\u001b[0m        \u001b[32m1.8406\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m2.2227\u001b[0m  0.0006  0.3449\n",
      "      2            0.2556        \u001b[32m1.3969\u001b[0m       0.2535            0.2535        2.9164  0.0006  0.3257\n",
      "      3            0.3111        \u001b[32m1.1027\u001b[0m       0.2708            0.2708        2.6382  0.0006  0.3447\n",
      "      4            \u001b[36m0.3556\u001b[0m        \u001b[32m1.0358\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        2.5076  0.0006  0.3452\n",
      "      5            \u001b[36m0.4444\u001b[0m        \u001b[32m0.8992\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        2.2347  0.0006  0.3230\n",
      "      6            \u001b[36m0.5778\u001b[0m        \u001b[32m0.8612\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m2.0431\u001b[0m  0.0006  0.3385\n",
      "      7            \u001b[36m0.5889\u001b[0m        \u001b[32m0.7937\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m2.0179\u001b[0m  0.0006  0.3357\n",
      "      8            \u001b[36m0.6444\u001b[0m        \u001b[32m0.7433\u001b[0m       0.3229            0.3229        \u001b[94m2.0118\u001b[0m  0.0006  0.3770\n",
      "      9            \u001b[36m0.6889\u001b[0m        \u001b[32m0.6170\u001b[0m       0.3264            0.3264        2.0260  0.0006  0.3501\n",
      "     10            \u001b[36m0.7778\u001b[0m        \u001b[32m0.5955\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        2.0312  0.0005  0.3308\n",
      "     11            \u001b[36m0.8444\u001b[0m        \u001b[32m0.4689\u001b[0m       0.3299            0.3299        \u001b[94m2.0066\u001b[0m  0.0005  0.3398\n",
      "     12            0.8444        \u001b[32m0.4386\u001b[0m       0.3403            0.3403        \u001b[94m1.9573\u001b[0m  0.0005  0.3386\n",
      "     13            \u001b[36m0.8667\u001b[0m        0.4548       0.3264            0.3264        \u001b[94m1.9145\u001b[0m  0.0005  0.3285\n",
      "     14            \u001b[36m0.9222\u001b[0m        \u001b[32m0.4293\u001b[0m       0.3403            0.3403        \u001b[94m1.8308\u001b[0m  0.0005  0.3464\n",
      "     15            \u001b[36m0.9444\u001b[0m        \u001b[32m0.3643\u001b[0m       0.3299            0.3299        \u001b[94m1.7659\u001b[0m  0.0004  0.3347\n",
      "     16            0.9444        \u001b[32m0.3466\u001b[0m       0.3333            0.3333        \u001b[94m1.7217\u001b[0m  0.0004  0.3241\n",
      "     17            \u001b[36m0.9667\u001b[0m        0.4476       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.6931\u001b[0m  0.0004  0.3369\n",
      "     18            \u001b[36m0.9889\u001b[0m        \u001b[32m0.3279\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.6828\u001b[0m  0.0004  0.3596\n",
      "     19            0.9889        \u001b[32m0.2574\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.6743\u001b[0m  0.0004  0.3485\n",
      "     20            \u001b[36m1.0000\u001b[0m        0.2862       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.6683\u001b[0m  0.0003  0.3496\n",
      "     21            1.0000        \u001b[32m0.2458\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.6593\u001b[0m  0.0003  0.3559\n",
      "     22            1.0000        \u001b[32m0.2247\u001b[0m       0.3681            0.3681        \u001b[94m1.6454\u001b[0m  0.0003  0.3860\n",
      "     23            1.0000        0.2280       0.3646            0.3646        \u001b[94m1.6331\u001b[0m  0.0002  0.4179\n",
      "     24            1.0000        0.2305       0.3611            0.3611        \u001b[94m1.6208\u001b[0m  0.0002  0.4166\n",
      "     25            1.0000        0.2557       0.3646            0.3646        \u001b[94m1.6108\u001b[0m  0.0002  0.3855\n",
      "     26            1.0000        \u001b[32m0.2217\u001b[0m       0.3611            0.3611        \u001b[94m1.6048\u001b[0m  0.0002  0.3982\n",
      "     27            1.0000        \u001b[32m0.2141\u001b[0m       0.3611            0.3611        \u001b[94m1.5992\u001b[0m  0.0002  0.4621\n",
      "     28            1.0000        0.2355       0.3646            0.3646        \u001b[94m1.5935\u001b[0m  0.0001  0.4433\n",
      "     29            1.0000        \u001b[32m0.1948\u001b[0m       0.3681            0.3681        \u001b[94m1.5908\u001b[0m  0.0001  0.3523\n",
      "     30            1.0000        0.2173       0.3646            0.3646        \u001b[94m1.5896\u001b[0m  0.0001  0.3453\n",
      "     31            1.0000        0.2453       0.3611            0.3611        1.5901  0.0001  0.3469\n",
      "     32            1.0000        0.1957       0.3611            0.3611        1.5908  0.0001  0.4435\n",
      "     33            1.0000        \u001b[32m0.1915\u001b[0m       0.3611            0.3611        1.5907  0.0000  0.3348\n",
      "     34            1.0000        0.1954       0.3611            0.3611        \u001b[94m1.5894\u001b[0m  0.0000  0.3393\n",
      "     35            1.0000        0.2023       0.3611            0.3611        \u001b[94m1.5880\u001b[0m  0.0000  0.3394\n",
      "     36            1.0000        0.2012       0.3576            0.3576        \u001b[94m1.5863\u001b[0m  0.0000  0.3604\n",
      "     37            1.0000        \u001b[32m0.1832\u001b[0m       0.3576            0.3576        \u001b[94m1.5847\u001b[0m  0.0000  0.3455\n",
      "     38            1.0000        \u001b[32m0.1771\u001b[0m       0.3576            0.3576        \u001b[94m1.5835\u001b[0m  0.0000  0.3503\n",
      "     39            1.0000        0.1963       0.3576            0.3576        \u001b[94m1.5824\u001b[0m  0.0000  0.3408\n",
      "     40            1.0000        0.2191       0.3576            0.3576        \u001b[94m1.5816\u001b[0m  0.0000  0.3377\n",
      "Training model for subject 1 with 100 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.6355\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.0259\u001b[0m  0.0006  0.3491\n",
      "      2            \u001b[36m0.4000\u001b[0m        \u001b[32m1.3069\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m2.3674\u001b[0m  0.0006  0.3421\n",
      "      3            0.3300        \u001b[32m1.1612\u001b[0m       0.3056            0.3056        2.5432  0.0006  0.3442\n",
      "      4            0.3900        \u001b[32m1.0088\u001b[0m       0.3160            0.3160        2.7386  0.0006  0.3480\n",
      "      5            \u001b[36m0.4500\u001b[0m        \u001b[32m0.8566\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        2.9246  0.0006  0.3548\n",
      "      6            \u001b[36m0.4800\u001b[0m        \u001b[32m0.7835\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        3.0054  0.0006  0.3568\n",
      "      7            0.4800        \u001b[32m0.7024\u001b[0m       0.3333            0.3333        3.0494  0.0006  0.3528\n",
      "      8            0.4800        \u001b[32m0.6258\u001b[0m       0.3194            0.3194        2.9569  0.0006  0.3460\n",
      "      9            \u001b[36m0.4900\u001b[0m        \u001b[32m0.5917\u001b[0m       0.3160            0.3160        2.7808  0.0006  0.3455\n",
      "     10            \u001b[36m0.5000\u001b[0m        \u001b[32m0.5332\u001b[0m       0.3160            0.3160        2.5412  0.0005  0.3489\n",
      "     11            \u001b[36m0.5600\u001b[0m        \u001b[32m0.5315\u001b[0m       0.3194            0.3194        \u001b[94m2.3169\u001b[0m  0.0005  0.3443\n",
      "     12            \u001b[36m0.6000\u001b[0m        \u001b[32m0.3962\u001b[0m       0.3194            0.3194        \u001b[94m2.1362\u001b[0m  0.0005  0.3402\n",
      "     13            \u001b[36m0.7000\u001b[0m        0.4081       0.3333            0.3333        \u001b[94m1.9811\u001b[0m  0.0005  0.3556\n",
      "     14            \u001b[36m0.7400\u001b[0m        \u001b[32m0.3776\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.8689\u001b[0m  0.0005  0.3490\n",
      "     15            \u001b[36m0.8000\u001b[0m        \u001b[32m0.3422\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.7677\u001b[0m  0.0004  0.3618\n",
      "     16            \u001b[36m0.8900\u001b[0m        0.3613       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.6838\u001b[0m  0.0004  0.3623\n",
      "     17            \u001b[36m0.9300\u001b[0m        \u001b[32m0.3030\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.6271\u001b[0m  0.0004  0.3522\n",
      "     18            \u001b[36m0.9900\u001b[0m        \u001b[32m0.2893\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.5879\u001b[0m  0.0004  0.4041\n",
      "     19            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2684\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.5534\u001b[0m  0.0004  0.4230\n",
      "     20            1.0000        \u001b[32m0.2509\u001b[0m       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.5204\u001b[0m  0.0003  0.4496\n",
      "     21            1.0000        \u001b[32m0.2462\u001b[0m       0.4167            0.4167        \u001b[94m1.4942\u001b[0m  0.0003  0.4084\n",
      "     22            1.0000        \u001b[32m0.1892\u001b[0m       0.4167            0.4167        \u001b[94m1.4795\u001b[0m  0.0003  0.4048\n",
      "     23            1.0000        0.2580       0.4167            0.4167        \u001b[94m1.4719\u001b[0m  0.0002  0.4540\n",
      "     24            1.0000        0.2456       0.4097            0.4097        \u001b[94m1.4677\u001b[0m  0.0002  0.4394\n",
      "     25            1.0000        0.2000       0.4062            0.4062        \u001b[94m1.4573\u001b[0m  0.0002  0.3730\n",
      "     26            1.0000        0.2572       0.4028            0.4028        \u001b[94m1.4448\u001b[0m  0.0002  0.3731\n",
      "     27            1.0000        0.2073       0.4028            0.4028        \u001b[94m1.4351\u001b[0m  0.0002  0.3947\n",
      "     28            1.0000        0.2174       0.4028            0.4028        \u001b[94m1.4286\u001b[0m  0.0001  0.3820\n",
      "     29            1.0000        0.2058       0.3993            0.3993        \u001b[94m1.4250\u001b[0m  0.0001  0.3762\n",
      "     30            1.0000        \u001b[32m0.1847\u001b[0m       0.3924            0.3924        \u001b[94m1.4226\u001b[0m  0.0001  0.3918\n",
      "     31            1.0000        \u001b[32m0.1829\u001b[0m       0.3924            0.3924        \u001b[94m1.4200\u001b[0m  0.0001  0.3911\n",
      "     32            1.0000        0.2202       0.3924            0.3924        \u001b[94m1.4188\u001b[0m  0.0001  0.3627\n",
      "     33            1.0000        0.1958       0.3924            0.3924        \u001b[94m1.4178\u001b[0m  0.0000  0.3725\n",
      "     34            1.0000        \u001b[32m0.1445\u001b[0m       0.3924            0.3924        \u001b[94m1.4171\u001b[0m  0.0000  0.4590\n",
      "     35            1.0000        0.2146       0.3924            0.3924        \u001b[94m1.4164\u001b[0m  0.0000  0.3992\n",
      "     36            1.0000        0.2277       0.3924            0.3924        \u001b[94m1.4161\u001b[0m  0.0000  0.3852\n",
      "     37            1.0000        0.2128       0.3924            0.3924        \u001b[94m1.4155\u001b[0m  0.0000  0.3639\n",
      "     38            1.0000        0.1994       0.3924            0.3924        \u001b[94m1.4149\u001b[0m  0.0000  0.3913\n",
      "     39            1.0000        0.2134       0.3958            0.3958        \u001b[94m1.4142\u001b[0m  0.0000  0.3568\n",
      "     40            1.0000        0.1582       0.3958            0.3958        \u001b[94m1.4138\u001b[0m  0.0000  0.3665\n",
      "Training model for subject 1 with 110 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3455\u001b[0m        \u001b[32m1.4293\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m2.8555\u001b[0m  0.0006  0.4119\n",
      "      2            \u001b[36m0.4091\u001b[0m        \u001b[32m1.1691\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        2.9555  0.0006  0.4104\n",
      "      3            0.3909        \u001b[32m0.9489\u001b[0m       0.3299            0.3299        2.9597  0.0006  0.4026\n",
      "      4            \u001b[36m0.4182\u001b[0m        \u001b[32m0.9035\u001b[0m       0.3438            0.3438        \u001b[94m2.7840\u001b[0m  0.0006  0.3651\n",
      "      5            \u001b[36m0.4273\u001b[0m        \u001b[32m0.7751\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m2.7362\u001b[0m  0.0006  0.3565\n",
      "      6            \u001b[36m0.4727\u001b[0m        \u001b[32m0.6970\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m2.5510\u001b[0m  0.0006  0.3626\n",
      "      7            \u001b[36m0.5000\u001b[0m        0.7125       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m2.3283\u001b[0m  0.0006  0.3941\n",
      "      8            \u001b[36m0.5455\u001b[0m        \u001b[32m0.6326\u001b[0m       0.3785            0.3785        \u001b[94m2.1151\u001b[0m  0.0006  0.3660\n",
      "      9            \u001b[36m0.6636\u001b[0m        \u001b[32m0.6036\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.9232\u001b[0m  0.0006  0.3612\n",
      "     10            \u001b[36m0.7364\u001b[0m        \u001b[32m0.5897\u001b[0m       0.3611            0.3611        \u001b[94m1.8071\u001b[0m  0.0005  0.4115\n",
      "     11            \u001b[36m0.8273\u001b[0m        \u001b[32m0.5399\u001b[0m       0.3715            0.3715        \u001b[94m1.7596\u001b[0m  0.0005  0.3542\n",
      "     12            \u001b[36m0.8545\u001b[0m        \u001b[32m0.5121\u001b[0m       0.3681            0.3681        \u001b[94m1.7497\u001b[0m  0.0005  0.3915\n",
      "     13            \u001b[36m0.9455\u001b[0m        \u001b[32m0.4504\u001b[0m       0.3715            0.3715        \u001b[94m1.7125\u001b[0m  0.0005  0.4311\n",
      "     14            \u001b[36m0.9545\u001b[0m        \u001b[32m0.4151\u001b[0m       0.3750            0.3750        1.7157  0.0005  0.4798\n",
      "     15            0.9545        \u001b[32m0.3613\u001b[0m       0.3715            0.3715        1.7163  0.0004  0.4265\n",
      "     16            \u001b[36m0.9727\u001b[0m        \u001b[32m0.3573\u001b[0m       0.3750            0.3750        \u001b[94m1.6965\u001b[0m  0.0004  0.4483\n",
      "     17            \u001b[36m0.9818\u001b[0m        \u001b[32m0.3203\u001b[0m       0.3819            0.3819        \u001b[94m1.6773\u001b[0m  0.0004  0.4266\n",
      "     18            0.9818        \u001b[32m0.3178\u001b[0m       0.3854            0.3854        \u001b[94m1.6513\u001b[0m  0.0004  0.4966\n",
      "     19            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2984\u001b[0m       0.3854            0.3854        \u001b[94m1.6205\u001b[0m  0.0004  0.4091\n",
      "     20            1.0000        0.3548       0.3889            0.3889        \u001b[94m1.5925\u001b[0m  0.0003  0.3763\n",
      "     21            1.0000        0.3143       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.5710\u001b[0m  0.0003  0.3737\n",
      "     22            1.0000        \u001b[32m0.2498\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.5526\u001b[0m  0.0003  0.3704\n",
      "     23            1.0000        0.2803       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.5396\u001b[0m  0.0002  0.3722\n",
      "     24            1.0000        \u001b[32m0.2441\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.5265\u001b[0m  0.0002  0.3620\n",
      "     25            1.0000        0.3187       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.5139\u001b[0m  0.0002  0.3642\n",
      "     26            1.0000        \u001b[32m0.2178\u001b[0m       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.5056\u001b[0m  0.0002  0.3623\n",
      "     27            1.0000        \u001b[32m0.2119\u001b[0m       0.4167            0.4167        \u001b[94m1.5025\u001b[0m  0.0002  0.3812\n",
      "     28            1.0000        \u001b[32m0.1920\u001b[0m       0.4167            0.4167        \u001b[94m1.5020\u001b[0m  0.0001  0.3656\n",
      "     29            1.0000        \u001b[32m0.1744\u001b[0m       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.5010\u001b[0m  0.0001  0.3629\n",
      "     30            1.0000        0.2750       0.4167            0.4167        1.5029  0.0001  0.3711\n",
      "     31            1.0000        0.2211       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        1.5047  0.0001  0.3863\n",
      "     32            1.0000        0.2014       0.4236            0.4236        1.5056  0.0001  0.3718\n",
      "     33            1.0000        0.2028       0.4167            0.4167        1.5060  0.0000  0.4851\n",
      "     34            1.0000        0.2073       0.4132            0.4132        1.5051  0.0000  0.3723\n",
      "     35            1.0000        0.2179       0.4097            0.4097        1.5040  0.0000  0.3605\n",
      "     36            1.0000        0.1977       0.4097            0.4097        1.5026  0.0000  0.3960\n",
      "     37            1.0000        \u001b[32m0.1655\u001b[0m       0.4097            0.4097        \u001b[94m1.5009\u001b[0m  0.0000  0.3799\n",
      "     38            1.0000        0.1894       0.4132            0.4132        \u001b[94m1.4993\u001b[0m  0.0000  0.3621\n",
      "     39            1.0000        0.2162       0.4097            0.4097        \u001b[94m1.4978\u001b[0m  0.0000  0.3683\n",
      "     40            1.0000        \u001b[32m0.1649\u001b[0m       0.4132            0.4132        \u001b[94m1.4967\u001b[0m  0.0000  0.3606\n",
      "Training model for subject 1 with 120 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2583\u001b[0m        \u001b[32m1.4914\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m5.9704\u001b[0m  0.0006  0.3913\n",
      "      2            0.2583        \u001b[32m1.3150\u001b[0m       0.2500            0.2500        \u001b[94m5.2339\u001b[0m  0.0006  0.3704\n",
      "      3            \u001b[36m0.2750\u001b[0m        \u001b[32m1.1486\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m4.3426\u001b[0m  0.0006  0.3711\n",
      "      4            \u001b[36m0.3500\u001b[0m        \u001b[32m1.0295\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m3.9981\u001b[0m  0.0006  0.3706\n",
      "      5            \u001b[36m0.3667\u001b[0m        \u001b[32m0.8681\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        4.1046  0.0006  0.4026\n",
      "      6            \u001b[36m0.4333\u001b[0m        \u001b[32m0.8376\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m3.9607\u001b[0m  0.0006  0.3794\n",
      "      7            \u001b[36m0.4667\u001b[0m        \u001b[32m0.7527\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m3.7859\u001b[0m  0.0006  0.4736\n",
      "      8            0.4667        \u001b[32m0.7126\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m3.5903\u001b[0m  0.0006  0.4947\n",
      "      9            \u001b[36m0.4833\u001b[0m        \u001b[32m0.6951\u001b[0m       0.3472            0.3472        \u001b[94m3.3318\u001b[0m  0.0006  0.4272\n",
      "     10            0.4833        \u001b[32m0.6510\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m3.0612\u001b[0m  0.0005  0.4187\n",
      "     11            \u001b[36m0.5250\u001b[0m        \u001b[32m0.5804\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m2.7500\u001b[0m  0.0005  0.4407\n",
      "     12            \u001b[36m0.5583\u001b[0m        \u001b[32m0.5017\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m2.4698\u001b[0m  0.0005  0.5622\n",
      "     13            \u001b[36m0.6250\u001b[0m        \u001b[32m0.4720\u001b[0m       0.3785            0.3785        \u001b[94m2.2693\u001b[0m  0.0005  0.3936\n",
      "     14            \u001b[36m0.6667\u001b[0m        \u001b[32m0.3624\u001b[0m       0.3715            0.3715        \u001b[94m2.1024\u001b[0m  0.0005  0.3965\n",
      "     15            \u001b[36m0.7250\u001b[0m        0.4467       0.3646            0.3646        \u001b[94m1.9703\u001b[0m  0.0004  0.3838\n",
      "     16            \u001b[36m0.8333\u001b[0m        0.4028       0.3646            0.3646        \u001b[94m1.8784\u001b[0m  0.0004  0.3770\n",
      "     17            \u001b[36m0.8917\u001b[0m        \u001b[32m0.3489\u001b[0m       0.3646            0.3646        \u001b[94m1.8102\u001b[0m  0.0004  0.3869\n",
      "     18            \u001b[36m0.9500\u001b[0m        0.3639       0.3646            0.3646        \u001b[94m1.7406\u001b[0m  0.0004  0.3896\n",
      "     19            \u001b[36m0.9583\u001b[0m        0.3821       0.3681            0.3681        \u001b[94m1.6823\u001b[0m  0.0004  0.3915\n",
      "     20            \u001b[36m0.9833\u001b[0m        \u001b[32m0.3464\u001b[0m       0.3750            0.3750        \u001b[94m1.6344\u001b[0m  0.0003  0.3871\n",
      "     21            \u001b[36m0.9917\u001b[0m        \u001b[32m0.3402\u001b[0m       0.3681            0.3681        \u001b[94m1.5989\u001b[0m  0.0003  0.4118\n",
      "     22            0.9917        \u001b[32m0.3071\u001b[0m       0.3785            0.3785        \u001b[94m1.5765\u001b[0m  0.0003  0.3707\n",
      "     23            \u001b[36m1.0000\u001b[0m        0.3169       0.3785            0.3785        \u001b[94m1.5629\u001b[0m  0.0002  0.3854\n",
      "     24            1.0000        \u001b[32m0.2809\u001b[0m       0.3785            0.3785        \u001b[94m1.5569\u001b[0m  0.0002  0.3799\n",
      "     25            1.0000        \u001b[32m0.2507\u001b[0m       0.3819            0.3819        \u001b[94m1.5528\u001b[0m  0.0002  0.3806\n",
      "     26            1.0000        0.3530       0.3750            0.3750        \u001b[94m1.5444\u001b[0m  0.0002  0.3792\n",
      "     27            1.0000        0.3016       0.3646            0.3646        \u001b[94m1.5351\u001b[0m  0.0002  0.3800\n",
      "     28            1.0000        0.2564       0.3715            0.3715        \u001b[94m1.5251\u001b[0m  0.0001  0.3972\n",
      "     29            1.0000        \u001b[32m0.2438\u001b[0m       0.3681            0.3681        \u001b[94m1.5185\u001b[0m  0.0001  0.3783\n",
      "     30            1.0000        0.2505       0.3646            0.3646        \u001b[94m1.5151\u001b[0m  0.0001  0.3864\n",
      "     31            1.0000        0.2661       0.3681            0.3681        \u001b[94m1.5127\u001b[0m  0.0001  0.3759\n",
      "     32            1.0000        0.2657       0.3715            0.3715        \u001b[94m1.5117\u001b[0m  0.0001  0.4669\n",
      "     33            1.0000        0.2686       0.3715            0.3715        \u001b[94m1.5115\u001b[0m  0.0000  0.4099\n",
      "     34            1.0000        \u001b[32m0.2308\u001b[0m       0.3715            0.3715        \u001b[94m1.5110\u001b[0m  0.0000  0.3728\n",
      "     35            1.0000        0.2407       0.3715            0.3715        \u001b[94m1.5105\u001b[0m  0.0000  0.3874\n",
      "     36            1.0000        0.2385       0.3785            0.3785        \u001b[94m1.5096\u001b[0m  0.0000  0.3914\n",
      "     37            1.0000        \u001b[32m0.2240\u001b[0m       0.3785            0.3785        \u001b[94m1.5087\u001b[0m  0.0000  0.3870\n",
      "     38            1.0000        0.2519       0.3854            0.3854        \u001b[94m1.5077\u001b[0m  0.0000  0.3858\n",
      "     39            1.0000        0.2496       0.3854            0.3854        \u001b[94m1.5069\u001b[0m  0.0000  0.4033\n",
      "     40            1.0000        0.2407       0.3819            0.3819        \u001b[94m1.5062\u001b[0m  0.0000  0.4604\n",
      "Training model for subject 1 with 130 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3154\u001b[0m        \u001b[32m1.5731\u001b[0m       \u001b[35m0.2292\u001b[0m            \u001b[31m0.2292\u001b[0m        \u001b[94m1.8580\u001b[0m  0.0006  0.4783\n",
      "      2            \u001b[36m0.3615\u001b[0m        \u001b[32m1.4398\u001b[0m       \u001b[35m0.2396\u001b[0m            \u001b[31m0.2396\u001b[0m        2.4832  0.0006  0.4484\n",
      "      3            0.2923        \u001b[32m1.2508\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        2.7122  0.0006  0.4493\n",
      "      4            \u001b[36m0.4385\u001b[0m        \u001b[32m1.0642\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        2.5325  0.0006  0.4799\n",
      "      5            \u001b[36m0.4923\u001b[0m        \u001b[32m0.9814\u001b[0m       0.3542            0.3542        2.4231  0.0006  0.4936\n",
      "      6            0.4615        \u001b[32m0.9541\u001b[0m       0.3472            0.3472        2.3673  0.0006  0.4836\n",
      "      7            \u001b[36m0.5000\u001b[0m        \u001b[32m0.8550\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        2.2141  0.0006  0.3870\n",
      "      8            \u001b[36m0.5462\u001b[0m        \u001b[32m0.7763\u001b[0m       0.3646            0.3646        2.1304  0.0006  0.3868\n",
      "      9            \u001b[36m0.5615\u001b[0m        \u001b[32m0.6323\u001b[0m       0.3646            0.3646        2.0753  0.0006  0.3844\n",
      "     10            \u001b[36m0.5769\u001b[0m        \u001b[32m0.5777\u001b[0m       0.3507            0.3507        2.0037  0.0005  0.3816\n",
      "     11            \u001b[36m0.6154\u001b[0m        \u001b[32m0.5620\u001b[0m       0.3576            0.3576        1.9414  0.0005  0.3803\n",
      "     12            \u001b[36m0.6692\u001b[0m        0.6156       0.3646            0.3646        1.8604  0.0005  0.3799\n",
      "     13            \u001b[36m0.7538\u001b[0m        0.5859       0.3611            0.3611        \u001b[94m1.8140\u001b[0m  0.0005  0.3974\n",
      "     14            \u001b[36m0.8615\u001b[0m        0.5624       0.3542            0.3542        \u001b[94m1.7353\u001b[0m  0.0005  0.3842\n",
      "     15            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4929\u001b[0m       0.3715            0.3715        \u001b[94m1.6752\u001b[0m  0.0004  0.3808\n",
      "     16            \u001b[36m0.9385\u001b[0m        \u001b[32m0.4013\u001b[0m       0.3681            0.3681        \u001b[94m1.6114\u001b[0m  0.0004  0.3809\n",
      "     17            \u001b[36m0.9615\u001b[0m        0.4393       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.5624\u001b[0m  0.0004  0.3815\n",
      "     18            \u001b[36m0.9769\u001b[0m        \u001b[32m0.3820\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.5388\u001b[0m  0.0004  0.4001\n",
      "     19            0.9769        \u001b[32m0.3723\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.5174\u001b[0m  0.0004  0.3797\n",
      "     20            \u001b[36m0.9846\u001b[0m        \u001b[32m0.3335\u001b[0m       0.3854            0.3854        \u001b[94m1.4974\u001b[0m  0.0003  0.3836\n",
      "     21            0.9846        \u001b[32m0.3241\u001b[0m       0.3750            0.3750        \u001b[94m1.4787\u001b[0m  0.0003  0.3790\n",
      "     22            0.9846        \u001b[32m0.2907\u001b[0m       0.3785            0.3785        \u001b[94m1.4694\u001b[0m  0.0003  0.4021\n",
      "     23            \u001b[36m0.9923\u001b[0m        0.3088       0.3750            0.3750        \u001b[94m1.4647\u001b[0m  0.0002  0.3922\n",
      "     24            0.9923        0.3282       0.3715            0.3715        \u001b[94m1.4588\u001b[0m  0.0002  0.3870\n",
      "     25            \u001b[36m1.0000\u001b[0m        0.3343       0.3750            0.3750        \u001b[94m1.4509\u001b[0m  0.0002  0.3910\n",
      "     26            1.0000        0.3107       0.3785            0.3785        \u001b[94m1.4444\u001b[0m  0.0002  0.3902\n",
      "     27            1.0000        \u001b[32m0.2808\u001b[0m       0.3819            0.3819        \u001b[94m1.4393\u001b[0m  0.0002  0.3875\n",
      "     28            1.0000        \u001b[32m0.2748\u001b[0m       0.3854            0.3854        1.4404  0.0001  0.3935\n",
      "     29            1.0000        \u001b[32m0.2742\u001b[0m       0.3854            0.3854        1.4417  0.0001  0.3865\n",
      "     30            1.0000        \u001b[32m0.2669\u001b[0m       0.3819            0.3819        1.4407  0.0001  0.4030\n",
      "     31            1.0000        \u001b[32m0.2569\u001b[0m       0.3785            0.3785        \u001b[94m1.4390\u001b[0m  0.0001  0.4099\n",
      "     32            1.0000        0.2787       0.3854            0.3854        \u001b[94m1.4374\u001b[0m  0.0001  0.4647\n",
      "     33            1.0000        0.2902       0.3889            0.3889        \u001b[94m1.4358\u001b[0m  0.0000  0.4972\n",
      "     34            1.0000        \u001b[32m0.2395\u001b[0m       0.3889            0.3889        \u001b[94m1.4338\u001b[0m  0.0000  0.4410\n",
      "     35            1.0000        0.2809       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.4322\u001b[0m  0.0000  0.5585\n",
      "     36            1.0000        0.2678       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.4311\u001b[0m  0.0000  0.5130\n",
      "     37            1.0000        0.2965       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.4298\u001b[0m  0.0000  0.5056\n",
      "     38            1.0000        0.3039       0.4062            0.4062        \u001b[94m1.4290\u001b[0m  0.0000  0.4069\n",
      "     39            1.0000        \u001b[32m0.2337\u001b[0m       0.4062            0.4062        \u001b[94m1.4282\u001b[0m  0.0000  0.3894\n",
      "     40            1.0000        0.2403       0.3993            0.3993        \u001b[94m1.4274\u001b[0m  0.0000  0.3900\n",
      "Training model for subject 1 with 140 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.5812\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.4005\u001b[0m  0.0006  0.3910\n",
      "      2            0.2500        \u001b[32m1.3813\u001b[0m       0.2500            0.2500        4.7601  0.0006  0.3863\n",
      "      3            0.2500        \u001b[32m1.2194\u001b[0m       0.2500            0.2500        4.4699  0.0006  0.3886\n",
      "      4            \u001b[36m0.2714\u001b[0m        \u001b[32m1.0539\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m4.3069\u001b[0m  0.0006  0.3871\n",
      "      5            0.2714        \u001b[32m1.0482\u001b[0m       0.2604            0.2604        4.4426  0.0006  0.3991\n",
      "      6            0.2714        \u001b[32m0.8851\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        4.3613  0.0006  0.4014\n",
      "      7            \u001b[36m0.3000\u001b[0m        \u001b[32m0.8635\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m4.0287\u001b[0m  0.0006  0.3841\n",
      "      8            \u001b[36m0.3286\u001b[0m        \u001b[32m0.7810\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        \u001b[94m3.7679\u001b[0m  0.0006  0.4023\n",
      "      9            \u001b[36m0.3643\u001b[0m        \u001b[32m0.7012\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m3.4957\u001b[0m  0.0006  0.3846\n",
      "     10            \u001b[36m0.4000\u001b[0m        \u001b[32m0.6393\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m3.1163\u001b[0m  0.0005  0.3857\n",
      "     11            \u001b[36m0.4357\u001b[0m        0.6698       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m2.7837\u001b[0m  0.0005  0.3850\n",
      "     12            \u001b[36m0.4929\u001b[0m        \u001b[32m0.6272\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m2.5402\u001b[0m  0.0005  0.3801\n",
      "     13            \u001b[36m0.5071\u001b[0m        \u001b[32m0.5173\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m2.3364\u001b[0m  0.0005  0.4131\n",
      "     14            \u001b[36m0.5857\u001b[0m        0.5274       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m2.1277\u001b[0m  0.0005  0.3816\n",
      "     15            \u001b[36m0.6571\u001b[0m        \u001b[32m0.4814\u001b[0m       0.3681            0.3681        \u001b[94m1.9460\u001b[0m  0.0004  0.3749\n",
      "     16            \u001b[36m0.7357\u001b[0m        \u001b[32m0.4527\u001b[0m       0.3681            0.3681        \u001b[94m1.7956\u001b[0m  0.0004  0.3891\n",
      "     17            \u001b[36m0.8643\u001b[0m        0.4780       0.3785            0.3785        \u001b[94m1.6815\u001b[0m  0.0004  0.3989\n",
      "     18            \u001b[36m0.8929\u001b[0m        \u001b[32m0.3595\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.6030\u001b[0m  0.0004  0.3982\n",
      "     19            \u001b[36m0.9214\u001b[0m        0.3814       0.3924            0.3924        \u001b[94m1.5569\u001b[0m  0.0004  0.3966\n",
      "     20            \u001b[36m0.9429\u001b[0m        \u001b[32m0.3406\u001b[0m       0.3924            0.3924        \u001b[94m1.5247\u001b[0m  0.0003  0.3967\n",
      "     21            \u001b[36m0.9643\u001b[0m        0.4579       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.5044\u001b[0m  0.0003  0.3849\n",
      "     22            \u001b[36m0.9714\u001b[0m        0.3730       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.4943\u001b[0m  0.0003  0.4103\n",
      "     23            \u001b[36m0.9786\u001b[0m        \u001b[32m0.3366\u001b[0m       0.4062            0.4062        \u001b[94m1.4804\u001b[0m  0.0002  0.4455\n",
      "     24            \u001b[36m0.9857\u001b[0m        0.3517       0.4062            0.4062        \u001b[94m1.4697\u001b[0m  0.0002  0.4511\n",
      "     25            \u001b[36m0.9929\u001b[0m        \u001b[32m0.3314\u001b[0m       0.4097            0.4097        \u001b[94m1.4629\u001b[0m  0.0002  0.4469\n",
      "     26            0.9929        \u001b[32m0.3213\u001b[0m       0.4062            0.4062        \u001b[94m1.4544\u001b[0m  0.0002  0.4323\n",
      "     27            0.9929        0.3254       0.4132            0.4132        \u001b[94m1.4504\u001b[0m  0.0002  0.4738\n",
      "     28            0.9929        0.3409       0.4097            0.4097        \u001b[94m1.4461\u001b[0m  0.0001  0.4835\n",
      "     29            0.9929        \u001b[32m0.3007\u001b[0m       0.4097            0.4097        \u001b[94m1.4412\u001b[0m  0.0001  0.4690\n",
      "     30            0.9929        0.3338       0.4132            0.4132        \u001b[94m1.4366\u001b[0m  0.0001  0.4012\n",
      "     31            0.9929        \u001b[32m0.2904\u001b[0m       0.4097            0.4097        \u001b[94m1.4331\u001b[0m  0.0001  0.3978\n",
      "     32            \u001b[36m1.0000\u001b[0m        0.3324       0.4097            0.4097        \u001b[94m1.4285\u001b[0m  0.0001  0.3808\n",
      "     33            1.0000        \u001b[32m0.2519\u001b[0m       0.4132            0.4132        \u001b[94m1.4251\u001b[0m  0.0000  0.3987\n",
      "     34            1.0000        0.2914       0.4062            0.4062        \u001b[94m1.4222\u001b[0m  0.0000  0.3808\n",
      "     35            1.0000        0.2753       0.4132            0.4132        \u001b[94m1.4194\u001b[0m  0.0000  0.4793\n",
      "     36            1.0000        0.3279       0.4132            0.4132        \u001b[94m1.4175\u001b[0m  0.0000  0.3857\n",
      "     37            1.0000        0.2630       0.4132            0.4132        \u001b[94m1.4166\u001b[0m  0.0000  0.4278\n",
      "     38            1.0000        0.3168       0.4097            0.4097        \u001b[94m1.4152\u001b[0m  0.0000  0.3981\n",
      "     39            1.0000        0.3248       0.4097            0.4097        \u001b[94m1.4143\u001b[0m  0.0000  0.3973\n",
      "     40            1.0000        0.2601       0.4097            0.4097        \u001b[94m1.4136\u001b[0m  0.0000  0.3968\n",
      "Training model for subject 1 with 150 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2933\u001b[0m        \u001b[32m1.5054\u001b[0m       \u001b[35m0.2431\u001b[0m            \u001b[31m0.2431\u001b[0m        \u001b[94m2.7703\u001b[0m  0.0006  0.3860\n",
      "      2            \u001b[36m0.3067\u001b[0m        \u001b[32m1.2618\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m2.2154\u001b[0m  0.0006  0.3923\n",
      "      3            \u001b[36m0.3667\u001b[0m        \u001b[32m1.0390\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m2.0781\u001b[0m  0.0006  0.3998\n",
      "      4            \u001b[36m0.3933\u001b[0m        \u001b[32m1.0387\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        2.3894  0.0006  0.3802\n",
      "      5            \u001b[36m0.4333\u001b[0m        \u001b[32m0.9434\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        2.5954  0.0006  0.3846\n",
      "      6            \u001b[36m0.5067\u001b[0m        \u001b[32m0.8419\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        2.4991  0.0006  0.3816\n",
      "      7            0.5000        \u001b[32m0.7914\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        2.4142  0.0006  0.3841\n",
      "      8            0.5067        \u001b[32m0.6638\u001b[0m       0.3889            0.3889        2.3347  0.0006  0.3948\n",
      "      9            \u001b[36m0.5467\u001b[0m        0.6944       0.3785            0.3785        2.1836  0.0006  0.3861\n",
      "     10            \u001b[36m0.5933\u001b[0m        0.6882       0.3854            0.3854        \u001b[94m1.9965\u001b[0m  0.0005  0.3941\n",
      "     11            \u001b[36m0.6533\u001b[0m        \u001b[32m0.5450\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.8655\u001b[0m  0.0005  0.3881\n",
      "     12            \u001b[36m0.7467\u001b[0m        0.5600       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.7610\u001b[0m  0.0005  0.4078\n",
      "     13            \u001b[36m0.8000\u001b[0m        0.5749       0.4062            0.4062        \u001b[94m1.6915\u001b[0m  0.0005  0.4517\n",
      "     14            \u001b[36m0.8400\u001b[0m        \u001b[32m0.5346\u001b[0m       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.6391\u001b[0m  0.0005  0.4412\n",
      "     15            \u001b[36m0.8733\u001b[0m        \u001b[32m0.4602\u001b[0m       0.4167            0.4167        \u001b[94m1.5743\u001b[0m  0.0004  0.4497\n",
      "     16            \u001b[36m0.8933\u001b[0m        \u001b[32m0.4273\u001b[0m       0.4097            0.4097        \u001b[94m1.5313\u001b[0m  0.0004  0.4971\n",
      "     17            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3953\u001b[0m       0.4132            0.4132        \u001b[94m1.5100\u001b[0m  0.0004  0.4806\n",
      "     18            \u001b[36m0.9533\u001b[0m        \u001b[32m0.3844\u001b[0m       0.4097            0.4097        \u001b[94m1.4990\u001b[0m  0.0004  0.4571\n",
      "     19            \u001b[36m0.9667\u001b[0m        0.4074       0.3993            0.3993        \u001b[94m1.4718\u001b[0m  0.0004  0.5062\n",
      "     20            0.9667        0.4287       0.3889            0.3889        \u001b[94m1.4465\u001b[0m  0.0003  0.4116\n",
      "     21            \u001b[36m0.9800\u001b[0m        \u001b[32m0.3600\u001b[0m       0.4028            0.4028        \u001b[94m1.4239\u001b[0m  0.0003  0.3981\n",
      "     22            \u001b[36m0.9867\u001b[0m        \u001b[32m0.3354\u001b[0m       0.4132            0.4132        \u001b[94m1.3985\u001b[0m  0.0003  0.4145\n",
      "     23            \u001b[36m0.9933\u001b[0m        0.3801       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.3781\u001b[0m  0.0002  0.3822\n",
      "     24            0.9933        0.3493       0.4306            0.4306        \u001b[94m1.3661\u001b[0m  0.0002  0.3977\n",
      "     25            0.9933        \u001b[32m0.3071\u001b[0m       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.3568\u001b[0m  0.0002  0.3906\n",
      "     26            0.9933        0.3366       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.3484\u001b[0m  0.0002  0.3993\n",
      "     27            0.9933        \u001b[32m0.2979\u001b[0m       0.4479            0.4479        \u001b[94m1.3422\u001b[0m  0.0002  0.3952\n",
      "     28            0.9933        0.3166       0.4444            0.4444        \u001b[94m1.3383\u001b[0m  0.0001  0.4012\n",
      "     29            0.9933        0.3029       0.4444            0.4444        \u001b[94m1.3342\u001b[0m  0.0001  0.3964\n",
      "     30            0.9933        \u001b[32m0.2927\u001b[0m       0.4479            0.4479        \u001b[94m1.3314\u001b[0m  0.0001  0.4026\n",
      "     31            0.9933        \u001b[32m0.2247\u001b[0m       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.3276\u001b[0m  0.0001  0.3922\n",
      "     32            0.9933        0.2968       0.4514            0.4514        \u001b[94m1.3247\u001b[0m  0.0001  0.4009\n",
      "     33            0.9933        0.2759       0.4514            0.4514        \u001b[94m1.3220\u001b[0m  0.0000  0.3975\n",
      "     34            0.9933        0.2557       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.3198\u001b[0m  0.0000  0.3999\n",
      "     35            0.9933        0.3559       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.3186\u001b[0m  0.0000  0.5005\n",
      "     36            0.9933        0.3314       0.4583            0.4583        \u001b[94m1.3178\u001b[0m  0.0000  0.3919\n",
      "     37            0.9933        0.3022       0.4583            0.4583        \u001b[94m1.3168\u001b[0m  0.0000  0.3864\n",
      "     38            0.9933        0.3046       0.4583            0.4583        \u001b[94m1.3165\u001b[0m  0.0000  0.3987\n",
      "     39            0.9933        0.2868       0.4583            0.4583        \u001b[94m1.3162\u001b[0m  0.0000  0.3954\n",
      "     40            0.9933        0.3440       0.4583            0.4583        1.3165  0.0000  0.3983\n",
      "Training model for subject 1 with 160 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3187\u001b[0m        \u001b[32m1.7572\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m3.1536\u001b[0m  0.0006  0.3951\n",
      "      2            0.3000        \u001b[32m1.3609\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m2.9402\u001b[0m  0.0006  0.3848\n",
      "      3            0.2687        \u001b[32m1.2740\u001b[0m       0.2604            0.2604        3.2860  0.0006  0.3814\n",
      "      4            0.3000        \u001b[32m1.1226\u001b[0m       0.2743            0.2743        3.0267  0.0006  0.4388\n",
      "      5            \u001b[36m0.3312\u001b[0m        \u001b[32m0.9948\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        2.9455  0.0006  0.4228\n",
      "      6            \u001b[36m0.3438\u001b[0m        \u001b[32m0.9777\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        2.9698  0.0006  0.4482\n",
      "      7            \u001b[36m0.3937\u001b[0m        \u001b[32m0.8233\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m2.9364\u001b[0m  0.0006  0.4507\n",
      "      8            0.3688        0.9017       0.3021            0.3021        2.9618  0.0006  0.4496\n",
      "      9            0.3688        \u001b[32m0.7247\u001b[0m       0.3021            0.3021        2.9667  0.0006  0.4733\n",
      "     10            0.3438        0.7508       0.2917            0.2917        3.0202  0.0005  0.5217\n",
      "     11            0.3500        \u001b[32m0.6656\u001b[0m       0.2917            0.2917        2.9775  0.0005  0.4234\n",
      "     12            0.3563        \u001b[32m0.6254\u001b[0m       0.2951            0.2951        \u001b[94m2.8622\u001b[0m  0.0005  0.3846\n",
      "     13            0.3937        0.6476       0.2917            0.2917        \u001b[94m2.6424\u001b[0m  0.0005  0.3837\n",
      "     14            \u001b[36m0.4437\u001b[0m        0.6632       0.3090            0.3090        \u001b[94m2.3953\u001b[0m  0.0005  0.3817\n",
      "     15            \u001b[36m0.4813\u001b[0m        \u001b[32m0.5589\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m2.1910\u001b[0m  0.0004  0.3981\n",
      "     16            \u001b[36m0.5625\u001b[0m        \u001b[32m0.5367\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m1.9701\u001b[0m  0.0004  0.3824\n",
      "     17            \u001b[36m0.7000\u001b[0m        \u001b[32m0.5153\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.7807\u001b[0m  0.0004  0.3837\n",
      "     18            \u001b[36m0.8187\u001b[0m        \u001b[32m0.5153\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.6176\u001b[0m  0.0004  0.3809\n",
      "     19            \u001b[36m0.8625\u001b[0m        \u001b[32m0.5004\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.5197\u001b[0m  0.0004  0.3992\n",
      "     20            \u001b[36m0.9062\u001b[0m        \u001b[32m0.4547\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.4656\u001b[0m  0.0003  0.3824\n",
      "     21            \u001b[36m0.9313\u001b[0m        0.4975       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4327\u001b[0m  0.0003  0.4002\n",
      "     22            \u001b[36m0.9437\u001b[0m        \u001b[32m0.4332\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.4039\u001b[0m  0.0003  0.4000\n",
      "     23            0.9437        0.4682       0.3993            0.3993        \u001b[94m1.3882\u001b[0m  0.0002  0.4117\n",
      "     24            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4307\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.3793\u001b[0m  0.0002  0.3978\n",
      "     25            \u001b[36m0.9688\u001b[0m        0.4503       0.4062            0.4062        \u001b[94m1.3607\u001b[0m  0.0002  0.3981\n",
      "     26            0.9688        \u001b[32m0.3927\u001b[0m       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.3436\u001b[0m  0.0002  0.3961\n",
      "     27            0.9688        \u001b[32m0.3888\u001b[0m       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.3335\u001b[0m  0.0002  0.3996\n",
      "     28            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3860\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.3268\u001b[0m  0.0001  0.4014\n",
      "     29            0.9750        0.3971       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.3217\u001b[0m  0.0001  0.3951\n",
      "     30            0.9688        0.4053       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.3174\u001b[0m  0.0001  0.3985\n",
      "     31            0.9688        \u001b[32m0.3642\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.3129\u001b[0m  0.0001  0.4155\n",
      "     32            0.9688        \u001b[32m0.3334\u001b[0m       0.4479            0.4479        \u001b[94m1.3104\u001b[0m  0.0001  0.3979\n",
      "     33            0.9750        0.3479       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.3081\u001b[0m  0.0000  0.3830\n",
      "     34            \u001b[36m0.9812\u001b[0m        0.3559       0.4514            0.4514        \u001b[94m1.3072\u001b[0m  0.0000  0.3982\n",
      "     35            0.9812        0.3946       0.4514            0.4514        \u001b[94m1.3061\u001b[0m  0.0000  0.3970\n",
      "     36            0.9812        0.3588       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.3052\u001b[0m  0.0000  0.5686\n",
      "     37            0.9812        0.3569       0.4549            0.4549        \u001b[94m1.3045\u001b[0m  0.0000  0.5034\n",
      "     38            0.9812        0.3833       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.3045\u001b[0m  0.0000  0.4525\n",
      "     39            0.9812        0.3865       0.4549            0.4549        \u001b[94m1.3038\u001b[0m  0.0000  0.4660\n",
      "     40            0.9812        0.4006       0.4549            0.4549        \u001b[94m1.3033\u001b[0m  0.0000  0.5420\n",
      "Training model for subject 1 with 170 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2412\u001b[0m        \u001b[32m1.6783\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m2.9122\u001b[0m  0.0006  0.3944\n",
      "      2            0.2412        \u001b[32m1.4955\u001b[0m       0.2465            0.2465        \u001b[94m2.1993\u001b[0m  0.0006  0.4007\n",
      "      3            \u001b[36m0.3588\u001b[0m        \u001b[32m1.1467\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m1.9698\u001b[0m  0.0006  0.3888\n",
      "      4            \u001b[36m0.4176\u001b[0m        1.1500       0.2361            0.2361        2.0626  0.0006  0.3965\n",
      "      5            0.3000        \u001b[32m1.0927\u001b[0m       0.2465            0.2465        2.3896  0.0006  0.3828\n",
      "      6            0.2706        \u001b[32m0.9622\u001b[0m       0.2535            0.2535        2.5708  0.0006  0.3809\n",
      "      7            0.2647        \u001b[32m0.9556\u001b[0m       0.2535            0.2535        2.7709  0.0006  0.3847\n",
      "      8            0.2588        0.9864       0.2535            0.2535        2.7940  0.0006  0.3955\n",
      "      9            0.2588        \u001b[32m0.8058\u001b[0m       0.2535            0.2535        2.6680  0.0006  0.4008\n",
      "     10            0.2765        \u001b[32m0.7131\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        2.4137  0.0005  0.3962\n",
      "     11            0.3176        0.7639       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        2.2442  0.0005  0.3848\n",
      "     12            0.3824        0.7217       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        2.0856  0.0005  0.3968\n",
      "     13            \u001b[36m0.5471\u001b[0m        \u001b[32m0.7089\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m1.8645\u001b[0m  0.0005  0.3850\n",
      "     14            \u001b[36m0.6588\u001b[0m        \u001b[32m0.7081\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m1.7211\u001b[0m  0.0005  0.3954\n",
      "     15            \u001b[36m0.7824\u001b[0m        \u001b[32m0.6263\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.6008\u001b[0m  0.0004  0.4005\n",
      "     16            \u001b[36m0.8412\u001b[0m        \u001b[32m0.5924\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.5409\u001b[0m  0.0004  0.3813\n",
      "     17            \u001b[36m0.8706\u001b[0m        \u001b[32m0.5805\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.4956\u001b[0m  0.0004  0.3842\n",
      "     18            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4979\u001b[0m       0.3750            0.3750        \u001b[94m1.4453\u001b[0m  0.0004  0.4113\n",
      "     19            \u001b[36m0.9412\u001b[0m        0.5157       0.3785            0.3785        \u001b[94m1.4114\u001b[0m  0.0004  0.3855\n",
      "     20            0.9353        0.5591       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.3976\u001b[0m  0.0003  0.3960\n",
      "     21            0.9235        0.5139       0.3889            0.3889        \u001b[94m1.3836\u001b[0m  0.0003  0.3868\n",
      "     22            0.9294        \u001b[32m0.4623\u001b[0m       0.3889            0.3889        \u001b[94m1.3732\u001b[0m  0.0003  0.3796\n",
      "     23            0.9294        0.4638       0.3924            0.3924        \u001b[94m1.3646\u001b[0m  0.0002  0.4008\n",
      "     24            0.9353        \u001b[32m0.4606\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.3590\u001b[0m  0.0002  0.3803\n",
      "     25            \u001b[36m0.9529\u001b[0m        0.4818       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.3549\u001b[0m  0.0002  0.4006\n",
      "     26            \u001b[36m0.9647\u001b[0m        0.4869       0.4028            0.4028        \u001b[94m1.3529\u001b[0m  0.0002  0.4731\n",
      "     27            \u001b[36m0.9706\u001b[0m        \u001b[32m0.4374\u001b[0m       0.4028            0.4028        \u001b[94m1.3476\u001b[0m  0.0002  0.4221\n",
      "     28            0.9706        \u001b[32m0.4282\u001b[0m       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.3422\u001b[0m  0.0001  0.4525\n",
      "     29            0.9706        0.5028       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.3393\u001b[0m  0.0001  0.4806\n",
      "     30            0.9706        \u001b[32m0.3804\u001b[0m       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.3370\u001b[0m  0.0001  0.4579\n",
      "     31            0.9647        0.3916       0.4340            0.4340        \u001b[94m1.3351\u001b[0m  0.0001  0.4882\n",
      "     32            0.9647        0.4228       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.3328\u001b[0m  0.0001  0.3901\n",
      "     33            0.9647        0.4316       0.4444            0.4444        \u001b[94m1.3309\u001b[0m  0.0000  0.4171\n",
      "     34            0.9647        0.3917       0.4410            0.4410        \u001b[94m1.3291\u001b[0m  0.0000  0.3983\n",
      "     35            0.9647        0.4227       0.4410            0.4410        \u001b[94m1.3278\u001b[0m  0.0000  0.4009\n",
      "     36            0.9706        0.4170       0.4375            0.4375        \u001b[94m1.3267\u001b[0m  0.0000  0.3808\n",
      "     37            \u001b[36m0.9765\u001b[0m        0.4124       0.4375            0.4375        \u001b[94m1.3267\u001b[0m  0.0000  0.3994\n",
      "     38            0.9765        \u001b[32m0.3800\u001b[0m       0.4375            0.4375        \u001b[94m1.3262\u001b[0m  0.0000  0.3834\n",
      "     39            0.9765        0.3909       0.4340            0.4340        \u001b[94m1.3262\u001b[0m  0.0000  0.4936\n",
      "     40            0.9706        0.3938       0.4306            0.4306        \u001b[94m1.3260\u001b[0m  0.0000  0.4023\n",
      "Training model for subject 1 with 180 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.7915\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m5.6283\u001b[0m  0.0006  0.4028\n",
      "      2            \u001b[36m0.3500\u001b[0m        \u001b[32m1.4047\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        \u001b[94m4.0597\u001b[0m  0.0006  0.3846\n",
      "      3            0.2611        \u001b[32m1.2848\u001b[0m       0.2569            0.2569        4.3555  0.0006  0.4113\n",
      "      4            0.3000        \u001b[32m1.1809\u001b[0m       0.2604            0.2604        \u001b[94m3.9243\u001b[0m  0.0006  0.3867\n",
      "      5            \u001b[36m0.3556\u001b[0m        \u001b[32m1.0905\u001b[0m       0.2639            0.2639        \u001b[94m3.5954\u001b[0m  0.0006  0.3768\n",
      "      6            \u001b[36m0.4389\u001b[0m        \u001b[32m1.0210\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m3.0936\u001b[0m  0.0006  0.3892\n",
      "      7            0.4278        \u001b[32m0.8662\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m2.7450\u001b[0m  0.0006  0.3909\n",
      "      8            \u001b[36m0.4556\u001b[0m        0.9261       0.2986            0.2986        \u001b[94m2.5038\u001b[0m  0.0006  0.4026\n",
      "      9            \u001b[36m0.4611\u001b[0m        0.8991       0.2917            0.2917        \u001b[94m2.2436\u001b[0m  0.0006  0.3938\n",
      "     10            \u001b[36m0.5167\u001b[0m        \u001b[32m0.8531\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m2.0105\u001b[0m  0.0005  0.3895\n",
      "     11            \u001b[36m0.5778\u001b[0m        0.9106       0.3160            0.3160        \u001b[94m1.8443\u001b[0m  0.0005  0.3919\n",
      "     12            \u001b[36m0.6167\u001b[0m        \u001b[32m0.7064\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m1.7418\u001b[0m  0.0005  0.3912\n",
      "     13            \u001b[36m0.6833\u001b[0m        0.7457       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.5907\u001b[0m  0.0005  0.4053\n",
      "     14            \u001b[36m0.7389\u001b[0m        0.7308       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.4950\u001b[0m  0.0005  0.4087\n",
      "     15            \u001b[36m0.7611\u001b[0m        \u001b[32m0.6184\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4345\u001b[0m  0.0004  0.3961\n",
      "     16            \u001b[36m0.8056\u001b[0m        \u001b[32m0.6020\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.3925\u001b[0m  0.0004  0.4422\n",
      "     17            \u001b[36m0.8333\u001b[0m        0.6033       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.3705\u001b[0m  0.0004  0.4658\n",
      "     18            \u001b[36m0.8500\u001b[0m        0.6057       0.4097            0.4097        \u001b[94m1.3631\u001b[0m  0.0004  0.4521\n",
      "     19            \u001b[36m0.8556\u001b[0m        \u001b[32m0.5438\u001b[0m       0.4097            0.4097        \u001b[94m1.3530\u001b[0m  0.0004  0.4640\n",
      "     20            \u001b[36m0.8667\u001b[0m        0.6015       0.4201            0.4201        \u001b[94m1.3497\u001b[0m  0.0003  0.4600\n",
      "     21            \u001b[36m0.8722\u001b[0m        0.6451       0.4271            0.4271        \u001b[94m1.3436\u001b[0m  0.0003  0.4877\n",
      "     22            \u001b[36m0.9000\u001b[0m        0.5514       0.4201            0.4201        \u001b[94m1.3355\u001b[0m  0.0003  0.3901\n",
      "     23            \u001b[36m0.9333\u001b[0m        0.6290       0.4236            0.4236        \u001b[94m1.3229\u001b[0m  0.0002  0.4016\n",
      "     24            0.9278        \u001b[32m0.4762\u001b[0m       0.4271            0.4271        \u001b[94m1.3092\u001b[0m  0.0002  0.3820\n",
      "     25            0.9333        \u001b[32m0.4384\u001b[0m       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.3003\u001b[0m  0.0002  0.3835\n",
      "     26            \u001b[36m0.9389\u001b[0m        0.5701       0.4410            0.4410        \u001b[94m1.2918\u001b[0m  0.0002  0.4092\n",
      "     27            0.9389        \u001b[32m0.4168\u001b[0m       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.2835\u001b[0m  0.0002  0.3897\n",
      "     28            \u001b[36m0.9444\u001b[0m        0.5508       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.2779\u001b[0m  0.0001  0.3872\n",
      "     29            0.9444        0.4804       0.4479            0.4479        \u001b[94m1.2718\u001b[0m  0.0001  0.3839\n",
      "     30            0.9444        \u001b[32m0.3965\u001b[0m       0.4410            0.4410        \u001b[94m1.2672\u001b[0m  0.0001  0.3988\n",
      "     31            \u001b[36m0.9556\u001b[0m        0.4263       0.4444            0.4444        \u001b[94m1.2617\u001b[0m  0.0001  0.4034\n",
      "     32            0.9556        0.4955       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.2557\u001b[0m  0.0001  0.3850\n",
      "     33            0.9556        \u001b[32m0.3790\u001b[0m       0.4549            0.4549        \u001b[94m1.2514\u001b[0m  0.0000  0.3903\n",
      "     34            0.9556        0.4418       0.4514            0.4514        \u001b[94m1.2491\u001b[0m  0.0000  0.3851\n",
      "     35            0.9556        0.3990       0.4479            0.4479        \u001b[94m1.2471\u001b[0m  0.0000  0.3886\n",
      "     36            0.9556        0.4792       0.4410            0.4410        \u001b[94m1.2454\u001b[0m  0.0000  0.4035\n",
      "     37            0.9556        0.4382       0.4340            0.4340        \u001b[94m1.2440\u001b[0m  0.0000  0.3932\n",
      "     38            0.9556        0.3983       0.4340            0.4340        \u001b[94m1.2427\u001b[0m  0.0000  0.3870\n",
      "     39            0.9556        0.4168       0.4340            0.4340        \u001b[94m1.2417\u001b[0m  0.0000  0.4890\n",
      "     40            0.9556        0.4326       0.4340            0.4340        \u001b[94m1.2410\u001b[0m  0.0000  0.3929\n",
      "Training model for subject 1 with 190 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2526\u001b[0m        \u001b[32m1.7845\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.4720\u001b[0m  0.0006  0.3968\n",
      "      2            0.2526        \u001b[32m1.3613\u001b[0m       0.2500            0.2500        \u001b[94m3.6054\u001b[0m  0.0006  0.3779\n",
      "      3            \u001b[36m0.2684\u001b[0m        1.5185       0.2500            0.2500        \u001b[94m2.5414\u001b[0m  0.0006  0.3829\n",
      "      4            \u001b[36m0.3474\u001b[0m        \u001b[32m1.0997\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m2.4695\u001b[0m  0.0006  0.3834\n",
      "      5            0.2579        1.1364       0.2465            0.2465        2.5694  0.0006  0.3827\n",
      "      6            0.2947        \u001b[32m1.0934\u001b[0m       0.2396            0.2396        2.4892  0.0006  0.5064\n",
      "      7            \u001b[36m0.3842\u001b[0m        \u001b[32m0.9973\u001b[0m       0.2604            0.2604        \u001b[94m2.3374\u001b[0m  0.0006  0.4868\n",
      "      8            \u001b[36m0.4368\u001b[0m        \u001b[32m0.9000\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m2.1080\u001b[0m  0.0006  0.4740\n",
      "      9            \u001b[36m0.4579\u001b[0m        \u001b[32m0.8910\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m1.8928\u001b[0m  0.0006  0.4543\n",
      "     10            \u001b[36m0.5053\u001b[0m        1.0099       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m1.7245\u001b[0m  0.0005  0.4855\n",
      "     11            \u001b[36m0.5947\u001b[0m        \u001b[32m0.7646\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.6156\u001b[0m  0.0005  0.4976\n",
      "     12            \u001b[36m0.6632\u001b[0m        0.7878       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.5014\u001b[0m  0.0005  0.3882\n",
      "     13            \u001b[36m0.7368\u001b[0m        0.7668       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.4518\u001b[0m  0.0005  0.4022\n",
      "     14            \u001b[36m0.7579\u001b[0m        0.8015       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.4304\u001b[0m  0.0005  0.3863\n",
      "     15            \u001b[36m0.7947\u001b[0m        \u001b[32m0.7255\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.3952\u001b[0m  0.0004  0.3941\n",
      "     16            \u001b[36m0.8368\u001b[0m        \u001b[32m0.7050\u001b[0m       0.3889            0.3889        \u001b[94m1.3699\u001b[0m  0.0004  0.4056\n",
      "     17            \u001b[36m0.8737\u001b[0m        \u001b[32m0.6465\u001b[0m       0.3819            0.3819        \u001b[94m1.3549\u001b[0m  0.0004  0.3764\n",
      "     18            0.8684        \u001b[32m0.6389\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.3430\u001b[0m  0.0004  0.3864\n",
      "     19            \u001b[36m0.8895\u001b[0m        0.6724       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.3298\u001b[0m  0.0004  0.3926\n",
      "     20            0.8842        \u001b[32m0.5998\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.3180\u001b[0m  0.0003  0.4056\n",
      "     21            \u001b[36m0.8947\u001b[0m        \u001b[32m0.5349\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.3090\u001b[0m  0.0003  0.4056\n",
      "     22            \u001b[36m0.9053\u001b[0m        0.6102       0.4062            0.4062        \u001b[94m1.3032\u001b[0m  0.0003  0.4008\n",
      "     23            0.9053        0.5690       0.4062            0.4062        \u001b[94m1.2976\u001b[0m  0.0002  0.4106\n",
      "     24            \u001b[36m0.9158\u001b[0m        \u001b[32m0.5006\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.2924\u001b[0m  0.0002  0.4133\n",
      "     25            \u001b[36m0.9263\u001b[0m        0.5884       0.4236            0.4236        \u001b[94m1.2885\u001b[0m  0.0002  0.3859\n",
      "     26            0.9263        0.5026       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.2833\u001b[0m  0.0002  0.3960\n",
      "     27            0.9263        0.5267       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.2796\u001b[0m  0.0002  0.3817\n",
      "     28            \u001b[36m0.9316\u001b[0m        \u001b[32m0.4841\u001b[0m       0.4340            0.4340        \u001b[94m1.2774\u001b[0m  0.0001  0.4029\n",
      "     29            \u001b[36m0.9368\u001b[0m        \u001b[32m0.4621\u001b[0m       0.4340            0.4340        \u001b[94m1.2754\u001b[0m  0.0001  0.3974\n",
      "     30            0.9368        0.4735       0.4340            0.4340        \u001b[94m1.2743\u001b[0m  0.0001  0.4010\n",
      "     31            0.9316        0.4809       0.4271            0.4271        \u001b[94m1.2735\u001b[0m  0.0001  0.3994\n",
      "     32            0.9368        \u001b[32m0.4408\u001b[0m       0.4271            0.4271        \u001b[94m1.2730\u001b[0m  0.0001  0.3984\n",
      "     33            0.9316        \u001b[32m0.4226\u001b[0m       0.4236            0.4236        \u001b[94m1.2725\u001b[0m  0.0000  0.3843\n",
      "     34            0.9368        0.4624       0.4236            0.4236        \u001b[94m1.2721\u001b[0m  0.0000  0.3987\n",
      "     35            0.9368        0.5041       0.4271            0.4271        \u001b[94m1.2718\u001b[0m  0.0000  0.3998\n",
      "     36            0.9368        0.4344       0.4375            0.4375        \u001b[94m1.2716\u001b[0m  0.0000  0.4710\n",
      "     37            0.9368        0.5195       0.4375            0.4375        \u001b[94m1.2715\u001b[0m  0.0000  0.5150\n",
      "     38            0.9368        0.4431       0.4375            0.4375        \u001b[94m1.2712\u001b[0m  0.0000  0.5357\n",
      "     39            0.9368        0.4350       0.4375            0.4375        \u001b[94m1.2709\u001b[0m  0.0000  0.4686\n",
      "     40            0.9368        0.5441       0.4375            0.4375        1.2710  0.0000  0.4654\n",
      "Training model for subject 1 with 200 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3500\u001b[0m        \u001b[32m1.6239\u001b[0m       \u001b[35m0.2431\u001b[0m            \u001b[31m0.2431\u001b[0m        \u001b[94m2.3148\u001b[0m  0.0006  0.5024\n",
      "      2            \u001b[36m0.3850\u001b[0m        \u001b[32m1.2479\u001b[0m       0.2396            0.2396        3.1220  0.0006  0.5156\n",
      "      3            0.3800        \u001b[32m1.1752\u001b[0m       0.2396            0.2396        2.9049  0.0006  0.4794\n",
      "      4            \u001b[36m0.4000\u001b[0m        \u001b[32m1.0115\u001b[0m       0.2361            0.2361        \u001b[94m2.3116\u001b[0m  0.0006  0.5064\n",
      "      5            \u001b[36m0.4700\u001b[0m        \u001b[32m0.9886\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m1.9387\u001b[0m  0.0006  0.5093\n",
      "      6            0.4550        \u001b[32m0.9236\u001b[0m       0.2535            0.2535        \u001b[94m1.9101\u001b[0m  0.0006  0.5147\n",
      "      7            0.4400        \u001b[32m0.8747\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m1.8924\u001b[0m  0.0006  0.4901\n",
      "      8            \u001b[36m0.5200\u001b[0m        \u001b[32m0.8372\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m1.8257\u001b[0m  0.0006  0.4892\n",
      "      9            \u001b[36m0.6450\u001b[0m        \u001b[32m0.7564\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m1.6321\u001b[0m  0.0006  0.4984\n",
      "     10            \u001b[36m0.7150\u001b[0m        \u001b[32m0.6737\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.5123\u001b[0m  0.0005  0.4863\n",
      "     11            \u001b[36m0.8050\u001b[0m        \u001b[32m0.6290\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.4155\u001b[0m  0.0005  0.4806\n",
      "     12            \u001b[36m0.8900\u001b[0m        \u001b[32m0.5978\u001b[0m       0.3993            0.3993        \u001b[94m1.3592\u001b[0m  0.0005  0.5036\n",
      "     13            \u001b[36m0.9200\u001b[0m        \u001b[32m0.5289\u001b[0m       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.3278\u001b[0m  0.0005  0.5109\n",
      "     14            \u001b[36m0.9350\u001b[0m        0.5444       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.3079\u001b[0m  0.0005  0.5292\n",
      "     15            \u001b[36m0.9600\u001b[0m        \u001b[32m0.4874\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.2979\u001b[0m  0.0004  0.5021\n",
      "     16            0.9500        \u001b[32m0.4790\u001b[0m       0.4479            0.4479        \u001b[94m1.2920\u001b[0m  0.0004  0.4911\n",
      "     17            \u001b[36m0.9650\u001b[0m        \u001b[32m0.4780\u001b[0m       0.4479            0.4479        \u001b[94m1.2886\u001b[0m  0.0004  0.4831\n",
      "     18            0.9500        \u001b[32m0.4757\u001b[0m       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.2828\u001b[0m  0.0004  0.5078\n",
      "     19            \u001b[36m0.9700\u001b[0m        \u001b[32m0.4089\u001b[0m       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        \u001b[94m1.2705\u001b[0m  0.0004  0.5014\n",
      "     20            \u001b[36m0.9750\u001b[0m        0.4636       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        \u001b[94m1.2644\u001b[0m  0.0003  0.4992\n",
      "     21            \u001b[36m0.9850\u001b[0m        \u001b[32m0.3874\u001b[0m       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        1.2675  0.0003  0.6881\n",
      "     22            0.9850        \u001b[32m0.3722\u001b[0m       0.4826            0.4826        1.2686  0.0003  0.5688\n",
      "     23            0.9850        \u001b[32m0.3540\u001b[0m       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        1.2668  0.0002  0.6047\n",
      "     24            \u001b[36m0.9950\u001b[0m        \u001b[32m0.3101\u001b[0m       0.4757            0.4757        \u001b[94m1.2636\u001b[0m  0.0002  0.6306\n",
      "     25            0.9950        0.3267       0.4861            0.4861        \u001b[94m1.2590\u001b[0m  0.0002  0.5150\n",
      "     26            0.9900        \u001b[32m0.3053\u001b[0m       0.4757            0.4757        1.2610  0.0002  0.5197\n",
      "     27            0.9900        0.3185       0.4757            0.4757        1.2614  0.0002  0.5012\n",
      "     28            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3041\u001b[0m       0.4826            0.4826        1.2602  0.0001  0.5081\n",
      "     29            1.0000        0.3195       0.4757            0.4757        1.2596  0.0001  0.5001\n",
      "     30            1.0000        \u001b[32m0.3012\u001b[0m       0.4826            0.4826        \u001b[94m1.2578\u001b[0m  0.0001  0.5025\n",
      "     31            1.0000        \u001b[32m0.2823\u001b[0m       0.4792            0.4792        \u001b[94m1.2554\u001b[0m  0.0001  0.4822\n",
      "     32            0.9950        0.3325       0.4826            0.4826        \u001b[94m1.2536\u001b[0m  0.0001  0.4811\n",
      "     33            1.0000        0.2887       0.4826            0.4826        \u001b[94m1.2529\u001b[0m  0.0000  0.6127\n",
      "     34            0.9950        \u001b[32m0.2702\u001b[0m       0.4826            0.4826        \u001b[94m1.2524\u001b[0m  0.0000  0.5331\n",
      "     35            0.9950        0.2780       0.4861            0.4861        \u001b[94m1.2519\u001b[0m  0.0000  0.5428\n",
      "     36            0.9950        0.3260       0.4861            0.4861        \u001b[94m1.2516\u001b[0m  0.0000  0.5509\n",
      "     37            0.9950        \u001b[32m0.2502\u001b[0m       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        \u001b[94m1.2511\u001b[0m  0.0000  0.6240\n",
      "     38            0.9950        0.2965       0.4861            0.4861        \u001b[94m1.2509\u001b[0m  0.0000  0.5670\n",
      "     39            0.9950        0.2705       0.4861            0.4861        \u001b[94m1.2509\u001b[0m  0.0000  0.5524\n",
      "     40            0.9950        0.2655       0.4861            0.4861        \u001b[94m1.2508\u001b[0m  0.0000  0.5377\n",
      "Training model for subject 1 with 210 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2524\u001b[0m        \u001b[32m1.6638\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.6374\u001b[0m  0.0006  0.4857\n",
      "      2            \u001b[36m0.2810\u001b[0m        \u001b[32m1.2732\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m2.2906\u001b[0m  0.0006  0.4872\n",
      "      3            \u001b[36m0.4143\u001b[0m        \u001b[32m1.1415\u001b[0m       0.2535            0.2535        \u001b[94m2.2559\u001b[0m  0.0006  0.8317\n",
      "      4            0.4000        \u001b[32m1.0325\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        2.5813  0.0006  0.6436\n",
      "      5            0.3143        \u001b[32m0.9689\u001b[0m       0.2674            0.2674        2.5964  0.0006  0.5772\n",
      "      6            0.3238        \u001b[32m0.9493\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        2.4171  0.0006  0.6074\n",
      "      7            0.4095        \u001b[32m0.8608\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m2.0560\u001b[0m  0.0006  0.6559\n",
      "      8            \u001b[36m0.4905\u001b[0m        \u001b[32m0.8214\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m1.9081\u001b[0m  0.0006  0.5940\n",
      "      9            \u001b[36m0.6000\u001b[0m        \u001b[32m0.7573\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m1.7318\u001b[0m  0.0006  0.5158\n",
      "     10            \u001b[36m0.6381\u001b[0m        \u001b[32m0.7253\u001b[0m       0.3194            0.3194        \u001b[94m1.6872\u001b[0m  0.0005  0.5320\n",
      "     11            \u001b[36m0.6762\u001b[0m        \u001b[32m0.7083\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.6166\u001b[0m  0.0005  0.4991\n",
      "     12            \u001b[36m0.7714\u001b[0m        \u001b[32m0.6478\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.5064\u001b[0m  0.0005  0.4853\n",
      "     13            \u001b[36m0.8238\u001b[0m        \u001b[32m0.5991\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.4340\u001b[0m  0.0005  0.5058\n",
      "     14            \u001b[36m0.8762\u001b[0m        \u001b[32m0.5156\u001b[0m       0.4028            0.4028        \u001b[94m1.3821\u001b[0m  0.0005  0.4945\n",
      "     15            \u001b[36m0.9095\u001b[0m        0.6108       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.3550\u001b[0m  0.0004  0.4950\n",
      "     16            \u001b[36m0.9143\u001b[0m        0.5613       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.3348\u001b[0m  0.0004  0.5694\n",
      "     17            \u001b[36m0.9238\u001b[0m        0.5229       0.4236            0.4236        \u001b[94m1.3224\u001b[0m  0.0004  0.6639\n",
      "     18            0.9238        \u001b[32m0.4363\u001b[0m       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.3160\u001b[0m  0.0004  0.5332\n",
      "     19            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4320\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.3076\u001b[0m  0.0004  0.5193\n",
      "     20            \u001b[36m0.9429\u001b[0m        \u001b[32m0.4242\u001b[0m       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.2960\u001b[0m  0.0003  0.5435\n",
      "     21            \u001b[36m0.9476\u001b[0m        0.4459       \u001b[35m0.4722\u001b[0m            \u001b[31m0.4722\u001b[0m        \u001b[94m1.2804\u001b[0m  0.0003  0.5458\n",
      "     22            \u001b[36m0.9667\u001b[0m        0.4371       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        \u001b[94m1.2757\u001b[0m  0.0003  0.5100\n",
      "     23            0.9667        \u001b[32m0.3981\u001b[0m       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        \u001b[94m1.2702\u001b[0m  0.0002  0.5210\n",
      "     24            0.9667        0.4168       0.4826            0.4826        \u001b[94m1.2612\u001b[0m  0.0002  0.5307\n",
      "     25            \u001b[36m0.9762\u001b[0m        \u001b[32m0.3844\u001b[0m       \u001b[35m0.4931\u001b[0m            \u001b[31m0.4931\u001b[0m        \u001b[94m1.2577\u001b[0m  0.0002  0.5447\n",
      "     26            0.9762        \u001b[32m0.3258\u001b[0m       \u001b[35m0.5000\u001b[0m            \u001b[31m0.5000\u001b[0m        \u001b[94m1.2563\u001b[0m  0.0002  0.5503\n",
      "     27            0.9714        0.3783       0.4931            0.4931        \u001b[94m1.2525\u001b[0m  0.0002  0.6067\n",
      "     28            0.9714        0.3523       0.4931            0.4931        \u001b[94m1.2478\u001b[0m  0.0001  0.6076\n",
      "     29            \u001b[36m0.9810\u001b[0m        0.3601       0.4965            0.4965        \u001b[94m1.2459\u001b[0m  0.0001  0.6114\n",
      "     30            0.9810        0.3537       0.5000            0.5000        \u001b[94m1.2441\u001b[0m  0.0001  0.7471\n",
      "     31            \u001b[36m0.9905\u001b[0m        0.3612       0.5000            0.5000        \u001b[94m1.2435\u001b[0m  0.0001  0.4979\n",
      "     32            0.9905        0.3685       0.5000            0.5000        1.2437  0.0001  0.4908\n",
      "     33            0.9905        \u001b[32m0.3002\u001b[0m       0.5000            0.5000        1.2444  0.0000  0.5186\n",
      "     34            0.9905        0.3482       0.5000            0.5000        1.2451  0.0000  0.5026\n",
      "     35            0.9905        0.3260       0.4965            0.4965        1.2453  0.0000  0.5075\n",
      "     36            0.9905        0.3769       0.5000            0.5000        1.2453  0.0000  0.5005\n",
      "     37            0.9905        0.3276       0.4965            0.4965        1.2448  0.0000  0.5002\n",
      "     38            0.9905        0.3648       0.4965            0.4965        1.2447  0.0000  0.5171\n",
      "     39            0.9905        0.3196       0.4931            0.4931        1.2448  0.0000  0.5274\n",
      "     40            0.9905        0.3097       0.4931            0.4931        1.2446  0.0000  0.5051\n",
      "Training model for subject 1 with 220 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2591\u001b[0m        \u001b[32m1.6372\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.8500\u001b[0m  0.0006  0.4916\n",
      "      2            0.2591        \u001b[32m1.4226\u001b[0m       0.2500            0.2500        \u001b[94m3.3534\u001b[0m  0.0006  0.4905\n",
      "      3            \u001b[36m0.3091\u001b[0m        \u001b[32m1.1151\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m2.0554\u001b[0m  0.0006  0.4939\n",
      "      4            \u001b[36m0.3182\u001b[0m        \u001b[32m1.0252\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        2.0989  0.0006  0.4882\n",
      "      5            0.2773        \u001b[32m0.9809\u001b[0m       0.2674            0.2674        2.2641  0.0006  0.4859\n",
      "      6            0.2727        \u001b[32m0.9057\u001b[0m       0.2604            0.2604        2.3631  0.0006  0.5083\n",
      "      7            \u001b[36m0.3591\u001b[0m        0.9374       0.2535            0.2535        \u001b[94m2.0448\u001b[0m  0.0006  0.4848\n",
      "      8            \u001b[36m0.4455\u001b[0m        \u001b[32m0.8646\u001b[0m       0.2674            0.2674        \u001b[94m1.8359\u001b[0m  0.0006  0.4986\n",
      "      9            \u001b[36m0.5955\u001b[0m        \u001b[32m0.7631\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m1.5869\u001b[0m  0.0006  0.4925\n",
      "     10            \u001b[36m0.7364\u001b[0m        \u001b[32m0.7050\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.3985\u001b[0m  0.0005  0.5161\n",
      "     11            \u001b[36m0.8273\u001b[0m        \u001b[32m0.6613\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.3080\u001b[0m  0.0005  0.6141\n",
      "     12            \u001b[36m0.8545\u001b[0m        0.6662       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.2770\u001b[0m  0.0005  0.5630\n",
      "     13            \u001b[36m0.8727\u001b[0m        \u001b[32m0.6465\u001b[0m       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        \u001b[94m1.2507\u001b[0m  0.0005  0.5792\n",
      "     14            \u001b[36m0.8773\u001b[0m        \u001b[32m0.5865\u001b[0m       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        \u001b[94m1.2238\u001b[0m  0.0005  0.6055\n",
      "     15            \u001b[36m0.9091\u001b[0m        \u001b[32m0.5643\u001b[0m       \u001b[35m0.5069\u001b[0m            \u001b[31m0.5069\u001b[0m        \u001b[94m1.1978\u001b[0m  0.0004  0.5214\n",
      "     16            \u001b[36m0.9136\u001b[0m        \u001b[32m0.4825\u001b[0m       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        \u001b[94m1.1885\u001b[0m  0.0004  0.5103\n",
      "     17            0.9136        0.5317       0.5174            0.5174        1.1916  0.0004  0.4868\n",
      "     18            \u001b[36m0.9455\u001b[0m        0.4909       0.5174            0.5174        1.1937  0.0004  0.4845\n",
      "     19            \u001b[36m0.9591\u001b[0m        0.5183       0.5069            0.5069        1.1915  0.0004  0.5113\n",
      "     20            \u001b[36m0.9727\u001b[0m        \u001b[32m0.4347\u001b[0m       0.5139            0.5139        1.1907  0.0003  0.4978\n",
      "     21            0.9727        \u001b[32m0.4330\u001b[0m       0.5208            0.5208        \u001b[94m1.1877\u001b[0m  0.0003  0.4869\n",
      "     22            0.9682        \u001b[32m0.4097\u001b[0m       0.5208            0.5208        \u001b[94m1.1806\u001b[0m  0.0003  0.4985\n",
      "     23            0.9727        0.4162       0.5174            0.5174        \u001b[94m1.1714\u001b[0m  0.0002  0.4968\n",
      "     24            0.9682        \u001b[32m0.3609\u001b[0m       \u001b[35m0.5486\u001b[0m            \u001b[31m0.5486\u001b[0m        \u001b[94m1.1642\u001b[0m  0.0002  0.5034\n",
      "     25            \u001b[36m0.9773\u001b[0m        0.3785       0.5451            0.5451        \u001b[94m1.1607\u001b[0m  0.0002  0.4937\n",
      "     26            0.9773        0.3826       0.5451            0.5451        1.1614  0.0002  0.5165\n",
      "     27            \u001b[36m0.9818\u001b[0m        0.3746       0.5417            0.5417        1.1608  0.0002  0.4891\n",
      "     28            \u001b[36m0.9864\u001b[0m        \u001b[32m0.3584\u001b[0m       0.5278            0.5278        1.1641  0.0001  0.5045\n",
      "     29            0.9864        \u001b[32m0.3358\u001b[0m       0.5243            0.5243        1.1679  0.0001  0.5959\n",
      "     30            0.9818        \u001b[32m0.3274\u001b[0m       0.5069            0.5069        1.1695  0.0001  0.5252\n",
      "     31            0.9818        0.3749       0.5069            0.5069        1.1698  0.0001  0.4923\n",
      "     32            0.9818        0.3375       0.5139            0.5139        1.1704  0.0001  0.4993\n",
      "     33            0.9818        0.3368       0.5174            0.5174        1.1700  0.0000  0.4927\n",
      "     34            0.9818        \u001b[32m0.3032\u001b[0m       0.5243            0.5243        1.1690  0.0000  0.5495\n",
      "     35            0.9818        0.3299       0.5243            0.5243        1.1680  0.0000  0.5987\n",
      "     36            0.9818        0.3296       0.5243            0.5243        1.1675  0.0000  0.5703\n",
      "     37            0.9818        0.3544       0.5278            0.5278        1.1670  0.0000  0.5923\n",
      "     38            0.9818        0.3285       0.5278            0.5278        1.1667  0.0000  0.6755\n",
      "     39            0.9864        \u001b[32m0.2905\u001b[0m       0.5243            0.5243        1.1667  0.0000  0.5098\n",
      "     40            0.9864        0.3497       0.5243            0.5243        1.1667  0.0000  0.5200\n",
      "Training model for subject 1 with 230 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3304\u001b[0m        \u001b[32m1.6078\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m3.1373\u001b[0m  0.0006  0.4969\n",
      "      2            0.3000        \u001b[32m1.3594\u001b[0m       0.2708            0.2708        \u001b[94m3.0488\u001b[0m  0.0006  0.4902\n",
      "      3            \u001b[36m0.4000\u001b[0m        \u001b[32m1.1586\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m2.4433\u001b[0m  0.0006  0.5100\n",
      "      4            \u001b[36m0.4696\u001b[0m        \u001b[32m1.0609\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m1.9640\u001b[0m  0.0006  0.5108\n",
      "      5            0.3739        \u001b[32m1.0157\u001b[0m       0.2743            0.2743        \u001b[94m1.8866\u001b[0m  0.0006  0.4833\n",
      "      6            0.3348        \u001b[32m0.9392\u001b[0m       0.2674            0.2674        2.1833  0.0006  0.4875\n",
      "      7            0.3522        0.9573       0.2674            0.2674        2.1134  0.0006  0.4890\n",
      "      8            \u001b[36m0.4783\u001b[0m        \u001b[32m0.8041\u001b[0m       0.2847            0.2847        \u001b[94m1.8605\u001b[0m  0.0006  0.5001\n",
      "      9            \u001b[36m0.6261\u001b[0m        0.8403       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.6186\u001b[0m  0.0006  0.4787\n",
      "     10            \u001b[36m0.7217\u001b[0m        \u001b[32m0.7187\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.4772\u001b[0m  0.0005  0.4996\n",
      "     11            \u001b[36m0.7783\u001b[0m        \u001b[32m0.6796\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.3932\u001b[0m  0.0005  0.4871\n",
      "     12            0.7652        \u001b[32m0.6399\u001b[0m       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        1.3932  0.0005  0.4986\n",
      "     13            \u001b[36m0.8130\u001b[0m        \u001b[32m0.6271\u001b[0m       0.4236            0.4236        \u001b[94m1.3556\u001b[0m  0.0005  0.4966\n",
      "     14            \u001b[36m0.8435\u001b[0m        0.6755       0.4375            0.4375        \u001b[94m1.3160\u001b[0m  0.0005  0.4982\n",
      "     15            \u001b[36m0.8957\u001b[0m        0.6314       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2788\u001b[0m  0.0004  0.4875\n",
      "     16            \u001b[36m0.9304\u001b[0m        \u001b[32m0.5742\u001b[0m       0.4514            0.4514        \u001b[94m1.2556\u001b[0m  0.0004  0.5041\n",
      "     17            \u001b[36m0.9478\u001b[0m        \u001b[32m0.5450\u001b[0m       0.4514            0.4514        \u001b[94m1.2516\u001b[0m  0.0004  0.4805\n",
      "     18            \u001b[36m0.9522\u001b[0m        \u001b[32m0.5093\u001b[0m       0.4583            0.4583        1.2548  0.0004  0.5686\n",
      "     19            0.9522        0.5180       0.4549            0.4549        1.2583  0.0004  0.6054\n",
      "     20            \u001b[36m0.9609\u001b[0m        \u001b[32m0.4930\u001b[0m       \u001b[35m0.4722\u001b[0m            \u001b[31m0.4722\u001b[0m        1.2518  0.0003  0.5840\n",
      "     21            \u001b[36m0.9696\u001b[0m        \u001b[32m0.4189\u001b[0m       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        \u001b[94m1.2467\u001b[0m  0.0003  0.5594\n",
      "     22            \u001b[36m0.9783\u001b[0m        0.4779       0.4757            0.4757        \u001b[94m1.2411\u001b[0m  0.0003  0.6365\n",
      "     23            0.9739        0.4570       0.4688            0.4688        \u001b[94m1.2353\u001b[0m  0.0002  0.5139\n",
      "     24            0.9783        \u001b[32m0.4152\u001b[0m       0.4757            0.4757        1.2361  0.0002  0.4993\n",
      "     25            \u001b[36m0.9826\u001b[0m        \u001b[32m0.4071\u001b[0m       0.4722            0.4722        1.2359  0.0002  0.4860\n",
      "     26            0.9826        0.4412       0.4688            0.4688        1.2355  0.0002  0.4983\n",
      "     27            0.9826        \u001b[32m0.3919\u001b[0m       0.4722            0.4722        \u001b[94m1.2331\u001b[0m  0.0002  0.4857\n",
      "     28            0.9826        \u001b[32m0.3786\u001b[0m       0.4757            0.4757        \u001b[94m1.2318\u001b[0m  0.0001  0.4830\n",
      "     29            0.9783        0.3966       0.4757            0.4757        \u001b[94m1.2308\u001b[0m  0.0001  0.4897\n",
      "     30            0.9783        \u001b[32m0.3478\u001b[0m       0.4792            0.4792        \u001b[94m1.2291\u001b[0m  0.0001  0.4956\n",
      "     31            0.9783        0.3587       0.4792            0.4792        1.2291  0.0001  0.5723\n",
      "     32            0.9783        0.3679       0.4792            0.4792        \u001b[94m1.2283\u001b[0m  0.0001  0.4977\n",
      "     33            0.9783        \u001b[32m0.3339\u001b[0m       0.4792            0.4792        \u001b[94m1.2275\u001b[0m  0.0000  0.5033\n",
      "     34            0.9826        0.3787       0.4792            0.4792        \u001b[94m1.2262\u001b[0m  0.0000  0.4958\n",
      "     35            0.9826        0.3995       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        \u001b[94m1.2255\u001b[0m  0.0000  0.4971\n",
      "     36            0.9826        0.3688       0.4792            0.4792        \u001b[94m1.2251\u001b[0m  0.0000  0.5042\n",
      "     37            0.9826        0.3587       0.4826            0.4826        \u001b[94m1.2247\u001b[0m  0.0000  0.4929\n",
      "     38            0.9826        0.4145       0.4792            0.4792        \u001b[94m1.2243\u001b[0m  0.0000  0.5013\n",
      "     39            0.9783        0.3972       0.4792            0.4792        \u001b[94m1.2239\u001b[0m  0.0000  0.4874\n",
      "     40            0.9783        0.3664       0.4792            0.4792        1.2239  0.0000  0.4887\n",
      "Training model for subject 1 with 240 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.6125\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m5.5449\u001b[0m  0.0006  0.5151\n",
      "      2            0.2500        \u001b[32m1.3235\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m4.3986\u001b[0m  0.0006  0.5669\n",
      "      3            \u001b[36m0.3875\u001b[0m        \u001b[32m1.1944\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m3.5050\u001b[0m  0.0006  0.6033\n",
      "      4            0.3458        \u001b[32m1.1420\u001b[0m       0.2535            0.2535        \u001b[94m3.0809\u001b[0m  0.0006  0.6080\n",
      "      5            \u001b[36m0.4125\u001b[0m        \u001b[32m1.0487\u001b[0m       0.2743            0.2743        \u001b[94m2.6677\u001b[0m  0.0006  0.5870\n",
      "      6            0.4000        \u001b[32m1.0288\u001b[0m       0.2778            0.2778        \u001b[94m2.3827\u001b[0m  0.0006  0.5396\n",
      "      7            \u001b[36m0.4542\u001b[0m        \u001b[32m0.8880\u001b[0m       0.2847            0.2847        \u001b[94m2.0631\u001b[0m  0.0006  0.5301\n",
      "      8            \u001b[36m0.5625\u001b[0m        \u001b[32m0.8759\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m1.7010\u001b[0m  0.0006  0.4956\n",
      "      9            \u001b[36m0.5917\u001b[0m        \u001b[32m0.8116\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.5393\u001b[0m  0.0006  0.5043\n",
      "     10            \u001b[36m0.7083\u001b[0m        0.8179       0.3785            0.3785        \u001b[94m1.4682\u001b[0m  0.0005  0.5139\n",
      "     11            \u001b[36m0.7708\u001b[0m        \u001b[32m0.7584\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.4304\u001b[0m  0.0005  0.4965\n",
      "     12            \u001b[36m0.8125\u001b[0m        \u001b[32m0.6617\u001b[0m       0.3785            0.3785        \u001b[94m1.4012\u001b[0m  0.0005  0.4884\n",
      "     13            \u001b[36m0.8500\u001b[0m        0.6989       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.3802\u001b[0m  0.0005  0.5069\n",
      "     14            \u001b[36m0.8875\u001b[0m        \u001b[32m0.5765\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.3501\u001b[0m  0.0005  0.5119\n",
      "     15            \u001b[36m0.9042\u001b[0m        0.6293       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.3257\u001b[0m  0.0004  0.4875\n",
      "     16            \u001b[36m0.9250\u001b[0m        0.5951       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.3072\u001b[0m  0.0004  0.4978\n",
      "     17            0.9208        0.5808       0.4201            0.4201        \u001b[94m1.2949\u001b[0m  0.0004  0.4987\n",
      "     18            \u001b[36m0.9417\u001b[0m        \u001b[32m0.5493\u001b[0m       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.2914\u001b[0m  0.0004  0.5172\n",
      "     19            0.9417        0.5529       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.2871\u001b[0m  0.0004  0.4817\n",
      "     20            \u001b[36m0.9542\u001b[0m        \u001b[32m0.5109\u001b[0m       0.4271            0.4271        1.2938  0.0003  0.5144\n",
      "     21            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4264\u001b[0m       0.4236            0.4236        1.3014  0.0003  0.5062\n",
      "     22            \u001b[36m0.9750\u001b[0m        0.5005       0.4201            0.4201        1.3011  0.0003  0.4953\n",
      "     23            \u001b[36m0.9792\u001b[0m        0.4580       0.4340            0.4340        1.2983  0.0002  0.5138\n",
      "     24            0.9792        0.4291       0.4306            0.4306        1.2963  0.0002  0.5062\n",
      "     25            \u001b[36m0.9875\u001b[0m        \u001b[32m0.4105\u001b[0m       0.4410            0.4410        \u001b[94m1.2869\u001b[0m  0.0002  0.5547\n",
      "     26            0.9875        \u001b[32m0.3791\u001b[0m       0.4340            0.4340        \u001b[94m1.2814\u001b[0m  0.0002  0.6048\n",
      "     27            0.9875        0.3936       0.4306            0.4306        \u001b[94m1.2775\u001b[0m  0.0002  0.6026\n",
      "     28            0.9875        \u001b[32m0.3464\u001b[0m       0.4375            0.4375        \u001b[94m1.2722\u001b[0m  0.0001  0.5763\n",
      "     29            0.9875        0.4360       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.2678\u001b[0m  0.0001  0.6251\n",
      "     30            0.9875        0.3902       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.2648\u001b[0m  0.0001  0.5123\n",
      "     31            0.9875        0.3876       0.4444            0.4444        \u001b[94m1.2633\u001b[0m  0.0001  0.5943\n",
      "     32            0.9875        0.3782       0.4479            0.4479        \u001b[94m1.2630\u001b[0m  0.0001  0.4965\n",
      "     33            0.9875        0.3565       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.2625\u001b[0m  0.0000  0.5023\n",
      "     34            0.9875        0.3841       0.4514            0.4514        \u001b[94m1.2617\u001b[0m  0.0000  0.5017\n",
      "     35            0.9875        0.3641       0.4514            0.4514        \u001b[94m1.2615\u001b[0m  0.0000  0.4941\n",
      "     36            0.9875        0.3748       0.4479            0.4479        \u001b[94m1.2613\u001b[0m  0.0000  0.5025\n",
      "     37            \u001b[36m0.9917\u001b[0m        0.3483       0.4479            0.4479        \u001b[94m1.2608\u001b[0m  0.0000  0.5005\n",
      "     38            0.9917        0.4186       0.4514            0.4514        \u001b[94m1.2603\u001b[0m  0.0000  0.4958\n",
      "     39            0.9917        0.3627       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.2596\u001b[0m  0.0000  0.5292\n",
      "     40            0.9917        0.3805       0.4549            0.4549        \u001b[94m1.2596\u001b[0m  0.0000  0.4891\n",
      "Training model for subject 1 with 250 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2520\u001b[0m        \u001b[32m1.6983\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.0342\u001b[0m  0.0006  0.4896\n",
      "      2            \u001b[36m0.2560\u001b[0m        \u001b[32m1.3243\u001b[0m       0.2500            0.2500        \u001b[94m3.0389\u001b[0m  0.0006  0.4829\n",
      "      3            \u001b[36m0.2800\u001b[0m        \u001b[32m1.2546\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m2.6406\u001b[0m  0.0006  0.4852\n",
      "      4            \u001b[36m0.2880\u001b[0m        \u001b[32m1.1199\u001b[0m       0.2569            0.2569        \u001b[94m2.4924\u001b[0m  0.0006  0.5020\n",
      "      5            \u001b[36m0.3240\u001b[0m        \u001b[32m1.0100\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m2.4495\u001b[0m  0.0006  0.4976\n",
      "      6            \u001b[36m0.4480\u001b[0m        \u001b[32m0.8831\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m2.2101\u001b[0m  0.0006  0.4874\n",
      "      7            \u001b[36m0.4880\u001b[0m        0.9055       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m2.0085\u001b[0m  0.0006  0.4857\n",
      "      8            \u001b[36m0.5680\u001b[0m        \u001b[32m0.7950\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.7201\u001b[0m  0.0006  0.5269\n",
      "      9            \u001b[36m0.6520\u001b[0m        0.8146       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.4905\u001b[0m  0.0006  0.5844\n",
      "     10            \u001b[36m0.7120\u001b[0m        \u001b[32m0.7905\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.3957\u001b[0m  0.0005  0.6096\n",
      "     11            \u001b[36m0.7680\u001b[0m        \u001b[32m0.7277\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.3235\u001b[0m  0.0005  0.6029\n",
      "     12            \u001b[36m0.8200\u001b[0m        \u001b[32m0.6253\u001b[0m       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.2922\u001b[0m  0.0005  0.6683\n",
      "     13            \u001b[36m0.8480\u001b[0m        0.6412       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.2774\u001b[0m  0.0005  0.4954\n",
      "     14            \u001b[36m0.8520\u001b[0m        \u001b[32m0.6097\u001b[0m       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.2657\u001b[0m  0.0005  0.5217\n",
      "     15            \u001b[36m0.8720\u001b[0m        \u001b[32m0.5718\u001b[0m       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.2504\u001b[0m  0.0004  0.5076\n",
      "     16            \u001b[36m0.8920\u001b[0m        \u001b[32m0.5530\u001b[0m       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        \u001b[94m1.2302\u001b[0m  0.0004  0.5115\n",
      "     17            \u001b[36m0.9120\u001b[0m        \u001b[32m0.5439\u001b[0m       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        \u001b[94m1.2203\u001b[0m  0.0004  0.5052\n",
      "     18            \u001b[36m0.9320\u001b[0m        \u001b[32m0.5246\u001b[0m       0.4792            0.4792        \u001b[94m1.2192\u001b[0m  0.0004  0.4932\n",
      "     19            \u001b[36m0.9520\u001b[0m        \u001b[32m0.4723\u001b[0m       0.4826            0.4826        1.2250  0.0004  0.4980\n",
      "     20            \u001b[36m0.9640\u001b[0m        0.4831       0.4583            0.4583        1.2318  0.0003  0.4889\n",
      "     21            \u001b[36m0.9680\u001b[0m        \u001b[32m0.4504\u001b[0m       0.4688            0.4688        1.2322  0.0003  0.4918\n",
      "     22            0.9680        0.4768       0.4688            0.4688        1.2224  0.0003  0.5026\n",
      "     23            \u001b[36m0.9720\u001b[0m        \u001b[32m0.3847\u001b[0m       0.4653            0.4653        \u001b[94m1.2075\u001b[0m  0.0002  0.4895\n",
      "     24            0.9720        \u001b[32m0.3811\u001b[0m       0.4757            0.4757        \u001b[94m1.1976\u001b[0m  0.0002  0.5040\n",
      "     25            0.9680        0.4242       0.4861            0.4861        \u001b[94m1.1972\u001b[0m  0.0002  0.4892\n",
      "     26            0.9720        0.4166       0.4826            0.4826        1.1977  0.0002  0.5027\n",
      "     27            \u001b[36m0.9800\u001b[0m        \u001b[32m0.3712\u001b[0m       0.4861            0.4861        \u001b[94m1.1944\u001b[0m  0.0002  0.5082\n",
      "     28            0.9800        0.3913       0.4826            0.4826        \u001b[94m1.1919\u001b[0m  0.0001  0.4999\n",
      "     29            \u001b[36m0.9840\u001b[0m        \u001b[32m0.3640\u001b[0m       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        \u001b[94m1.1883\u001b[0m  0.0001  0.5868\n",
      "     30            0.9840        0.3814       \u001b[35m0.4931\u001b[0m            \u001b[31m0.4931\u001b[0m        \u001b[94m1.1851\u001b[0m  0.0001  0.4942\n",
      "     31            0.9840        0.3837       0.4931            0.4931        \u001b[94m1.1836\u001b[0m  0.0001  0.5282\n",
      "     32            0.9840        \u001b[32m0.3543\u001b[0m       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        \u001b[94m1.1828\u001b[0m  0.0001  0.5665\n",
      "     33            0.9840        0.3790       \u001b[35m0.5000\u001b[0m            \u001b[31m0.5000\u001b[0m        \u001b[94m1.1825\u001b[0m  0.0000  0.6087\n",
      "     34            0.9800        \u001b[32m0.3273\u001b[0m       0.5000            0.5000        \u001b[94m1.1819\u001b[0m  0.0000  0.6343\n",
      "     35            0.9800        0.3560       0.4965            0.4965        \u001b[94m1.1817\u001b[0m  0.0000  0.6241\n",
      "     36            0.9800        0.3419       0.4965            0.4965        \u001b[94m1.1814\u001b[0m  0.0000  0.4963\n",
      "     37            0.9800        0.3330       0.4965            0.4965        \u001b[94m1.1814\u001b[0m  0.0000  0.5057\n",
      "     38            0.9800        0.3635       0.4965            0.4965        1.1815  0.0000  0.5089\n",
      "     39            0.9800        0.3541       0.4965            0.4965        1.1819  0.0000  0.5037\n",
      "     40            0.9800        0.3289       0.4965            0.4965        1.1817  0.0000  0.5033\n",
      "Training model for subject 1 with 260 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2846\u001b[0m        \u001b[32m1.5813\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m2.4485\u001b[0m  0.0006  0.6248\n",
      "      2            0.2462        \u001b[32m1.2413\u001b[0m       0.2535            0.2535        2.8871  0.0006  0.7289\n",
      "      3            0.2846        \u001b[32m1.0888\u001b[0m       0.2569            0.2569        2.8229  0.0006  0.6860\n",
      "      4            \u001b[36m0.4077\u001b[0m        \u001b[32m1.0079\u001b[0m       0.2396            0.2396        \u001b[94m2.3271\u001b[0m  0.0006  0.6030\n",
      "      5            0.3615        \u001b[32m0.8978\u001b[0m       0.2569            0.2569        \u001b[94m2.0657\u001b[0m  0.0006  0.6125\n",
      "      6            \u001b[36m0.4231\u001b[0m        \u001b[32m0.8503\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m1.8590\u001b[0m  0.0006  0.5956\n",
      "      7            \u001b[36m0.5731\u001b[0m        \u001b[32m0.7764\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m1.5983\u001b[0m  0.0006  0.5816\n",
      "      8            \u001b[36m0.7462\u001b[0m        \u001b[32m0.6359\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.3523\u001b[0m  0.0006  0.6612\n",
      "      9            \u001b[36m0.8769\u001b[0m        0.6388       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        \u001b[94m1.2395\u001b[0m  0.0006  0.6166\n",
      "     10            \u001b[36m0.9154\u001b[0m        \u001b[32m0.5926\u001b[0m       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        \u001b[94m1.2088\u001b[0m  0.0005  0.6446\n",
      "     11            0.9038        0.6047       0.4653            0.4653        1.2167  0.0005  0.6215\n",
      "     12            \u001b[36m0.9308\u001b[0m        \u001b[32m0.5213\u001b[0m       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        \u001b[94m1.1970\u001b[0m  0.0005  0.7156\n",
      "     13            \u001b[36m0.9462\u001b[0m        0.5377       \u001b[35m0.5174\u001b[0m            \u001b[31m0.5174\u001b[0m        \u001b[94m1.1839\u001b[0m  0.0005  0.7244\n",
      "     14            \u001b[36m0.9654\u001b[0m        \u001b[32m0.4476\u001b[0m       \u001b[35m0.5278\u001b[0m            \u001b[31m0.5278\u001b[0m        \u001b[94m1.1623\u001b[0m  0.0005  0.7176\n",
      "     15            0.9577        0.4562       0.5278            0.5278        \u001b[94m1.1438\u001b[0m  0.0004  0.7493\n",
      "     16            \u001b[36m0.9731\u001b[0m        \u001b[32m0.4426\u001b[0m       0.5174            0.5174        \u001b[94m1.1435\u001b[0m  0.0004  0.6056\n",
      "     17            \u001b[36m0.9846\u001b[0m        \u001b[32m0.4070\u001b[0m       0.5174            0.5174        \u001b[94m1.1315\u001b[0m  0.0004  0.5852\n",
      "     18            \u001b[36m0.9885\u001b[0m        \u001b[32m0.3380\u001b[0m       0.5139            0.5139        \u001b[94m1.1240\u001b[0m  0.0004  0.5963\n",
      "     19            \u001b[36m0.9962\u001b[0m        0.3394       \u001b[35m0.5382\u001b[0m            \u001b[31m0.5382\u001b[0m        \u001b[94m1.1203\u001b[0m  0.0004  0.6039\n",
      "     20            0.9962        \u001b[32m0.3299\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.1208  0.0003  0.5915\n",
      "     21            0.9962        0.3335       \u001b[35m0.5660\u001b[0m            \u001b[31m0.5660\u001b[0m        \u001b[94m1.1115\u001b[0m  0.0003  0.6018\n",
      "     22            0.9962        \u001b[32m0.2925\u001b[0m       0.5660            0.5660        \u001b[94m1.1082\u001b[0m  0.0003  0.6036\n",
      "     23            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2843\u001b[0m       0.5625            0.5625        \u001b[94m1.1046\u001b[0m  0.0002  0.7184\n",
      "     24            1.0000        \u001b[32m0.2682\u001b[0m       0.5625            0.5625        1.1050  0.0002  0.6163\n",
      "     25            1.0000        \u001b[32m0.2558\u001b[0m       0.5590            0.5590        1.1054  0.0002  0.5935\n",
      "     26            1.0000        0.2970       0.5521            0.5521        \u001b[94m1.1023\u001b[0m  0.0002  0.5961\n",
      "     27            1.0000        0.2741       0.5660            0.5660        \u001b[94m1.1001\u001b[0m  0.0002  0.6088\n",
      "     28            1.0000        \u001b[32m0.2401\u001b[0m       \u001b[35m0.5694\u001b[0m            \u001b[31m0.5694\u001b[0m        \u001b[94m1.0983\u001b[0m  0.0001  0.6081\n",
      "     29            1.0000        0.2641       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.0997  0.0001  0.5915\n",
      "     30            1.0000        0.2652       0.5729            0.5729        1.1001  0.0001  0.5873\n",
      "     31            1.0000        0.2432       0.5660            0.5660        1.1000  0.0001  0.5857\n",
      "     32            1.0000        0.2594       0.5694            0.5694        1.0996  0.0001  0.7184\n",
      "     33            1.0000        0.2717       \u001b[35m0.5764\u001b[0m            \u001b[31m0.5764\u001b[0m        1.1002  0.0000  0.7513\n",
      "     34            1.0000        0.2447       0.5694            0.5694        1.1013  0.0000  0.7011\n",
      "     35            1.0000        0.2592       0.5694            0.5694        1.1020  0.0000  0.7480\n",
      "     36            1.0000        0.2458       0.5694            0.5694        1.1020  0.0000  0.6107\n",
      "     37            1.0000        0.2475       0.5694            0.5694        1.1018  0.0000  0.6027\n",
      "     38            1.0000        \u001b[32m0.2261\u001b[0m       0.5694            0.5694        1.1019  0.0000  0.5971\n",
      "     39            1.0000        0.2523       0.5729            0.5729        1.1018  0.0000  0.6080\n",
      "     40            1.0000        0.2401       0.5729            0.5729        1.1020  0.0000  0.6034\n",
      "Training model for subject 1 with 270 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2778\u001b[0m        \u001b[32m1.6688\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m3.2888\u001b[0m  0.0006  0.5997\n",
      "      2            \u001b[36m0.3481\u001b[0m        \u001b[32m1.3090\u001b[0m       0.2535            0.2535        \u001b[94m1.9551\u001b[0m  0.0006  0.5905\n",
      "      3            \u001b[36m0.3926\u001b[0m        \u001b[32m1.1552\u001b[0m       0.2500            0.2500        \u001b[94m1.9214\u001b[0m  0.0006  0.5910\n",
      "      4            \u001b[36m0.4148\u001b[0m        \u001b[32m1.0051\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        2.1161  0.0006  0.6073\n",
      "      5            \u001b[36m0.5185\u001b[0m        1.0182       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        \u001b[94m1.8816\u001b[0m  0.0006  0.5924\n",
      "      6            \u001b[36m0.6185\u001b[0m        \u001b[32m0.8445\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.5891\u001b[0m  0.0006  0.5833\n",
      "      7            \u001b[36m0.7333\u001b[0m        \u001b[32m0.8281\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.4195\u001b[0m  0.0006  0.6050\n",
      "      8            \u001b[36m0.7852\u001b[0m        \u001b[32m0.8265\u001b[0m       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.3190\u001b[0m  0.0006  0.6066\n",
      "      9            \u001b[36m0.8407\u001b[0m        \u001b[32m0.7595\u001b[0m       0.4306            0.4306        \u001b[94m1.2776\u001b[0m  0.0006  0.5983\n",
      "     10            \u001b[36m0.8593\u001b[0m        \u001b[32m0.6748\u001b[0m       0.4271            0.4271        \u001b[94m1.2640\u001b[0m  0.0005  0.6150\n",
      "     11            \u001b[36m0.8815\u001b[0m        \u001b[32m0.6352\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.2325\u001b[0m  0.0005  0.6186\n",
      "     12            \u001b[36m0.9037\u001b[0m        \u001b[32m0.6245\u001b[0m       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2236\u001b[0m  0.0005  0.7139\n",
      "     13            \u001b[36m0.9111\u001b[0m        \u001b[32m0.5192\u001b[0m       0.4583            0.4583        1.2334  0.0005  0.7338\n",
      "     14            \u001b[36m0.9148\u001b[0m        0.5514       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        1.2318  0.0005  0.7335\n",
      "     15            \u001b[36m0.9519\u001b[0m        \u001b[32m0.4994\u001b[0m       0.4583            0.4583        1.2273  0.0004  0.7601\n",
      "     16            \u001b[36m0.9630\u001b[0m        0.5065       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        1.2327  0.0004  0.6114\n",
      "     17            0.9593        \u001b[32m0.4391\u001b[0m       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        1.2466  0.0004  0.6071\n",
      "     18            0.9630        0.4727       \u001b[35m0.4931\u001b[0m            \u001b[31m0.4931\u001b[0m        1.2317  0.0004  0.5878\n",
      "     19            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3923\u001b[0m       0.4861            0.4861        \u001b[94m1.2189\u001b[0m  0.0004  0.6196\n",
      "     20            0.9593        \u001b[32m0.3918\u001b[0m       \u001b[35m0.5069\u001b[0m            \u001b[31m0.5069\u001b[0m        \u001b[94m1.2103\u001b[0m  0.0003  0.5913\n",
      "     21            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3534\u001b[0m       \u001b[35m0.5139\u001b[0m            \u001b[31m0.5139\u001b[0m        \u001b[94m1.2007\u001b[0m  0.0003  0.6130\n",
      "     22            \u001b[36m0.9815\u001b[0m        \u001b[32m0.3208\u001b[0m       \u001b[35m0.5174\u001b[0m            \u001b[31m0.5174\u001b[0m        \u001b[94m1.1980\u001b[0m  0.0003  0.5914\n",
      "     23            \u001b[36m0.9852\u001b[0m        0.3614       \u001b[35m0.5243\u001b[0m            \u001b[31m0.5243\u001b[0m        \u001b[94m1.1927\u001b[0m  0.0002  0.6938\n",
      "     24            \u001b[36m0.9889\u001b[0m        \u001b[32m0.3179\u001b[0m       0.5174            0.5174        \u001b[94m1.1833\u001b[0m  0.0002  0.6069\n",
      "     25            \u001b[36m0.9926\u001b[0m        \u001b[32m0.3071\u001b[0m       0.5139            0.5139        \u001b[94m1.1745\u001b[0m  0.0002  0.6192\n",
      "     26            0.9926        0.3330       0.5243            0.5243        \u001b[94m1.1709\u001b[0m  0.0002  0.6062\n",
      "     27            \u001b[36m0.9963\u001b[0m        0.3083       0.5243            0.5243        \u001b[94m1.1709\u001b[0m  0.0002  0.6038\n",
      "     28            0.9926        \u001b[32m0.2937\u001b[0m       0.5243            0.5243        1.1724  0.0001  0.6009\n",
      "     29            0.9926        \u001b[32m0.2790\u001b[0m       0.5243            0.5243        1.1729  0.0001  0.6077\n",
      "     30            0.9926        \u001b[32m0.2757\u001b[0m       0.5243            0.5243        1.1717  0.0001  0.5899\n",
      "     31            0.9963        0.2773       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        1.1712  0.0001  0.6399\n",
      "     32            0.9963        \u001b[32m0.2742\u001b[0m       0.5278            0.5278        \u001b[94m1.1705\u001b[0m  0.0001  0.6982\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.2829       0.5278            0.5278        \u001b[94m1.1688\u001b[0m  0.0000  0.7508\n",
      "     34            1.0000        0.2756       0.5278            0.5278        \u001b[94m1.1681\u001b[0m  0.0000  0.7577\n",
      "     35            1.0000        0.3121       0.5278            0.5278        \u001b[94m1.1675\u001b[0m  0.0000  0.6856\n",
      "     36            1.0000        \u001b[32m0.2659\u001b[0m       0.5278            0.5278        \u001b[94m1.1670\u001b[0m  0.0000  0.5949\n",
      "     37            1.0000        \u001b[32m0.2489\u001b[0m       0.5278            0.5278        1.1676  0.0000  0.5920\n",
      "     38            1.0000        0.2953       0.5278            0.5278        1.1672  0.0000  0.5932\n",
      "     39            1.0000        0.2510       0.5278            0.5278        1.1676  0.0000  0.6187\n",
      "     40            1.0000        0.2671       0.5278            0.5278        \u001b[94m1.1670\u001b[0m  0.0000  0.5999\n",
      "Training model for subject 2 with 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.1000\u001b[0m        \u001b[32m1.6764\u001b[0m       \u001b[35m0.2465\u001b[0m            \u001b[31m0.2465\u001b[0m        \u001b[94m2.8658\u001b[0m  0.0006  0.3146\n",
      "      2            \u001b[36m0.6000\u001b[0m        \u001b[32m0.6832\u001b[0m       0.2465            0.2465        \u001b[94m1.7711\u001b[0m  0.0006  0.3297\n",
      "      3            \u001b[36m0.8000\u001b[0m        \u001b[32m0.6482\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        2.0558  0.0006  0.3104\n",
      "      4            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3663\u001b[0m       0.2639            0.2639        2.5975  0.0006  0.3183\n",
      "      5            0.9000        \u001b[32m0.2358\u001b[0m       0.2500            0.2500        2.9248  0.0006  0.3239\n",
      "      6            0.8000        \u001b[32m0.2097\u001b[0m       0.2639            0.2639        3.1030  0.0006  0.3266\n",
      "      7            0.9000        \u001b[32m0.0959\u001b[0m       0.2500            0.2500        3.1470  0.0006  0.3158\n",
      "      8            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0538\u001b[0m       0.2431            0.2431        3.0864  0.0006  0.3107\n",
      "      9            1.0000        0.0850       0.2431            0.2431        2.9736  0.0006  0.3260\n",
      "     10            1.0000        0.1745       0.2639            0.2639        2.8817  0.0005  0.3202\n",
      "     11            1.0000        0.0643       0.2500            0.2500        2.8437  0.0005  0.3069\n",
      "     12            1.0000        \u001b[32m0.0143\u001b[0m       0.2500            0.2500        2.8234  0.0005  0.3259\n",
      "     13            1.0000        0.0310       0.2535            0.2535        2.7981  0.0005  0.3333\n",
      "     14            1.0000        0.0226       0.2535            0.2535        2.7646  0.0005  0.3108\n",
      "     15            1.0000        0.0200       0.2569            0.2569        2.7225  0.0004  0.3246\n",
      "     16            1.0000        0.0215       0.2569            0.2569        2.6789  0.0004  0.3195\n",
      "     17            1.0000        0.0184       0.2569            0.2569        2.6436  0.0004  0.3235\n",
      "     18            1.0000        0.0314       0.2569            0.2569        2.6057  0.0004  0.3111\n",
      "     19            1.0000        0.0170       0.2535            0.2535        2.5779  0.0004  0.3171\n",
      "     20            1.0000        0.0193       0.2569            0.2569        2.5461  0.0003  0.3241\n",
      "     21            1.0000        \u001b[32m0.0119\u001b[0m       0.2604            0.2604        2.5180  0.0003  0.3267\n",
      "     22            1.0000        0.0163       0.2639            0.2639        2.4941  0.0003  0.3160\n",
      "     23            1.0000        0.0138       0.2639            0.2639        2.4767  0.0002  0.3106\n",
      "     24            1.0000        0.0130       0.2639            0.2639        2.4615  0.0002  0.3885\n",
      "     25            1.0000        \u001b[32m0.0096\u001b[0m       0.2639            0.2639        2.4494  0.0002  0.4207\n",
      "     26            1.0000        0.0205       0.2674            0.2674        2.4401  0.0002  0.4303\n",
      "     27            1.0000        \u001b[32m0.0086\u001b[0m       0.2674            0.2674        2.4331  0.0002  0.4170\n",
      "     28            1.0000        0.0127       0.2674            0.2674        2.4289  0.0001  0.4256\n",
      "     29            1.0000        0.0110       0.2674            0.2674        2.4230  0.0001  0.4446\n",
      "     30            1.0000        0.0226       0.2674            0.2674        2.4158  0.0001  0.3157\n",
      "     31            1.0000        0.0185       0.2639            0.2639        2.4082  0.0001  0.4198\n",
      "     32            1.0000        0.0208       0.2639            0.2639        2.3999  0.0001  0.3250\n",
      "     33            1.0000        0.0122       0.2639            0.2639        2.3920  0.0000  0.3241\n",
      "     34            1.0000        0.0100       0.2639            0.2639        2.3885  0.0000  0.3110\n",
      "     35            1.0000        0.0105       0.2639            0.2639        2.3865  0.0000  0.3155\n",
      "     36            1.0000        \u001b[32m0.0077\u001b[0m       0.2639            0.2639        2.3842  0.0000  0.3089\n",
      "     37            1.0000        0.0177       0.2639            0.2639        2.3808  0.0000  0.3264\n",
      "     38            1.0000        0.0099       0.2639            0.2639        2.3777  0.0000  0.3170\n",
      "     39            1.0000        0.0639       0.2639            0.2639        2.3807  0.0000  0.3603\n",
      "     40            1.0000        0.0146       0.2639            0.2639        2.3842  0.0000  0.3080\n",
      "Training model for subject 2 with 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.4500\u001b[0m        \u001b[32m1.9317\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m1.7197\u001b[0m  0.0006  0.2828\n",
      "      2            \u001b[36m0.6500\u001b[0m        \u001b[32m1.1463\u001b[0m       0.2535            0.2535        1.8980  0.0006  0.2634\n",
      "      3            \u001b[36m0.8000\u001b[0m        \u001b[32m0.7197\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        2.1333  0.0006  0.2631\n",
      "      4            \u001b[36m0.8500\u001b[0m        \u001b[32m0.5055\u001b[0m       0.2604            0.2604        2.1738  0.0006  0.2810\n",
      "      5            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3787\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        2.1794  0.0006  0.2610\n",
      "      6            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2689\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        2.1362  0.0006  0.2771\n",
      "      7            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2299\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        2.0456  0.0006  0.2671\n",
      "      8            1.0000        \u001b[32m0.1869\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        2.0062  0.0006  0.2518\n",
      "      9            1.0000        0.2467       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        1.9989  0.0006  0.2592\n",
      "     10            1.0000        \u001b[32m0.1293\u001b[0m       0.3368            0.3368        2.0184  0.0005  0.2639\n",
      "     11            1.0000        \u001b[32m0.1062\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        2.0143  0.0005  0.2820\n",
      "     12            1.0000        0.1312       0.3368            0.3368        1.9894  0.0005  0.2600\n",
      "     13            1.0000        \u001b[32m0.0519\u001b[0m       0.3403            0.3403        1.9793  0.0005  0.2773\n",
      "     14            1.0000        0.0606       0.3368            0.3368        1.9739  0.0005  0.2696\n",
      "     15            1.0000        \u001b[32m0.0450\u001b[0m       0.3438            0.3438        1.9685  0.0004  0.2782\n",
      "     16            1.0000        0.0684       0.3403            0.3403        1.9627  0.0004  0.2767\n",
      "     17            1.0000        \u001b[32m0.0338\u001b[0m       0.3229            0.3229        1.9582  0.0004  0.2945\n",
      "     18            1.0000        \u001b[32m0.0330\u001b[0m       0.3229            0.3229        1.9574  0.0004  0.2711\n",
      "     19            1.0000        0.0472       0.3264            0.3264        1.9542  0.0004  0.2917\n",
      "     20            1.0000        \u001b[32m0.0306\u001b[0m       0.3264            0.3264        1.9566  0.0003  0.2767\n",
      "     21            1.0000        0.0386       0.3299            0.3299        1.9566  0.0003  0.2854\n",
      "     22            1.0000        \u001b[32m0.0288\u001b[0m       0.3333            0.3333        1.9564  0.0003  0.2583\n",
      "     23            1.0000        \u001b[32m0.0251\u001b[0m       0.3264            0.3264        1.9544  0.0002  0.2607\n",
      "     24            1.0000        \u001b[32m0.0209\u001b[0m       0.3264            0.3264        1.9558  0.0002  0.2845\n",
      "     25            1.0000        0.0514       0.3264            0.3264        1.9545  0.0002  0.2845\n",
      "     26            1.0000        0.0317       0.3229            0.3229        1.9552  0.0002  0.2605\n",
      "     27            1.0000        0.0387       0.3264            0.3264        1.9561  0.0002  0.2769\n",
      "     28            1.0000        0.0410       0.3264            0.3264        1.9572  0.0001  0.2719\n",
      "     29            1.0000        \u001b[32m0.0146\u001b[0m       0.3299            0.3299        1.9598  0.0001  0.2718\n",
      "     30            1.0000        \u001b[32m0.0144\u001b[0m       0.3299            0.3299        1.9618  0.0001  0.2909\n",
      "     31            1.0000        0.0152       0.3299            0.3299        1.9632  0.0001  0.2863\n",
      "     32            1.0000        \u001b[32m0.0136\u001b[0m       0.3299            0.3299        1.9647  0.0001  0.3166\n",
      "     33            1.0000        \u001b[32m0.0131\u001b[0m       0.3299            0.3299        1.9654  0.0000  0.3666\n",
      "     34            1.0000        0.0154       0.3299            0.3299        1.9666  0.0000  0.3870\n",
      "     35            1.0000        0.0228       0.3299            0.3299        1.9670  0.0000  0.3776\n",
      "     36            1.0000        0.0288       0.3299            0.3299        1.9682  0.0000  0.3671\n",
      "     37            1.0000        0.0246       0.3299            0.3299        1.9692  0.0000  0.3804\n",
      "     38            1.0000        0.0194       0.3299            0.3299        1.9698  0.0000  0.3779\n",
      "     39            1.0000        0.0305       0.3299            0.3299        1.9703  0.0000  0.3842\n",
      "     40            1.0000        0.0213       0.3299            0.3299        1.9701  0.0000  0.3430\n",
      "Training model for subject 2 with 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3000\u001b[0m        \u001b[32m1.7166\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m2.1777\u001b[0m  0.0006  0.3011\n",
      "      2            \u001b[36m0.4667\u001b[0m        \u001b[32m1.0649\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        2.8704  0.0006  0.2667\n",
      "      3            0.4667        \u001b[32m0.7622\u001b[0m       0.3264            0.3264        3.3644  0.0006  0.3004\n",
      "      4            0.4667        \u001b[32m0.5444\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        3.4712  0.0006  0.2692\n",
      "      5            0.4667        \u001b[32m0.5136\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        3.3519  0.0006  0.2797\n",
      "      6            0.4667        \u001b[32m0.2840\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        3.2639  0.0006  0.2667\n",
      "      7            \u001b[36m0.5333\u001b[0m        \u001b[32m0.2520\u001b[0m       0.3646            0.3646        3.0975  0.0006  0.2809\n",
      "      8            \u001b[36m0.6000\u001b[0m        \u001b[32m0.1739\u001b[0m       0.3611            0.3611        2.9685  0.0006  0.2845\n",
      "      9            \u001b[36m0.6333\u001b[0m        0.1815       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        2.8516  0.0006  0.2821\n",
      "     10            \u001b[36m0.6667\u001b[0m        0.1856       0.3681            0.3681        2.7160  0.0005  0.2666\n",
      "     11            \u001b[36m0.7333\u001b[0m        \u001b[32m0.1230\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        2.5752  0.0005  0.2668\n",
      "     12            \u001b[36m0.8000\u001b[0m        \u001b[32m0.1180\u001b[0m       0.3715            0.3715        2.4321  0.0005  0.3665\n",
      "     13            \u001b[36m0.8333\u001b[0m        \u001b[32m0.1088\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        2.2937  0.0005  0.2690\n",
      "     14            \u001b[36m0.9000\u001b[0m        0.1368       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m2.1711\u001b[0m  0.0005  0.2678\n",
      "     15            \u001b[36m0.9333\u001b[0m        \u001b[32m0.0764\u001b[0m       0.3819            0.3819        \u001b[94m2.0640\u001b[0m  0.0004  0.2684\n",
      "     16            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0757\u001b[0m       0.3785            0.3785        \u001b[94m1.9814\u001b[0m  0.0004  0.2612\n",
      "     17            1.0000        \u001b[32m0.0586\u001b[0m       0.3542            0.3542        \u001b[94m1.9156\u001b[0m  0.0004  0.2609\n",
      "     18            1.0000        0.0648       0.3576            0.3576        \u001b[94m1.8618\u001b[0m  0.0004  0.2694\n",
      "     19            1.0000        \u001b[32m0.0491\u001b[0m       0.3646            0.3646        \u001b[94m1.8202\u001b[0m  0.0004  0.2594\n",
      "     20            1.0000        0.0566       0.3681            0.3681        \u001b[94m1.7899\u001b[0m  0.0003  0.2608\n",
      "     21            1.0000        0.0747       0.3681            0.3681        \u001b[94m1.7684\u001b[0m  0.0003  0.2529\n",
      "     22            1.0000        \u001b[32m0.0411\u001b[0m       0.3715            0.3715        \u001b[94m1.7534\u001b[0m  0.0003  0.2640\n",
      "     23            1.0000        \u001b[32m0.0343\u001b[0m       0.3750            0.3750        \u001b[94m1.7428\u001b[0m  0.0002  0.2755\n",
      "     24            1.0000        0.0401       0.3750            0.3750        \u001b[94m1.7358\u001b[0m  0.0002  0.2790\n",
      "     25            1.0000        0.0419       0.3785            0.3785        \u001b[94m1.7311\u001b[0m  0.0002  0.2690\n",
      "     26            1.0000        0.0364       0.3750            0.3750        \u001b[94m1.7281\u001b[0m  0.0002  0.2747\n",
      "     27            1.0000        0.0374       0.3750            0.3750        \u001b[94m1.7260\u001b[0m  0.0002  0.2785\n",
      "     28            1.0000        \u001b[32m0.0325\u001b[0m       0.3785            0.3785        \u001b[94m1.7245\u001b[0m  0.0001  0.2750\n",
      "     29            1.0000        0.0552       0.3715            0.3715        \u001b[94m1.7240\u001b[0m  0.0001  0.2697\n",
      "     30            1.0000        \u001b[32m0.0228\u001b[0m       0.3715            0.3715        \u001b[94m1.7238\u001b[0m  0.0001  0.2606\n",
      "     31            1.0000        0.0552       0.3715            0.3715        1.7241  0.0001  0.2663\n",
      "     32            1.0000        0.0276       0.3715            0.3715        1.7247  0.0001  0.2670\n",
      "     33            1.0000        0.0299       0.3715            0.3715        1.7251  0.0000  0.2595\n",
      "     34            1.0000        0.0366       0.3715            0.3715        1.7254  0.0000  0.2775\n",
      "     35            1.0000        0.0409       0.3715            0.3715        1.7256  0.0000  0.2682\n",
      "     36            1.0000        0.0412       0.3681            0.3681        1.7256  0.0000  0.2609\n",
      "     37            1.0000        0.0353       0.3681            0.3681        1.7256  0.0000  0.2604\n",
      "     38            1.0000        0.0415       0.3646            0.3646        1.7256  0.0000  0.2665\n",
      "     39            1.0000        0.0306       0.3646            0.3646        1.7254  0.0000  0.2541\n",
      "     40            1.0000        0.0667       0.3681            0.3681        1.7254  0.0000  0.2581\n",
      "Training model for subject 2 with 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3000\u001b[0m        \u001b[32m1.7146\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.3237\u001b[0m  0.0006  0.2713\n",
      "      2            \u001b[36m0.3500\u001b[0m        \u001b[32m1.3062\u001b[0m       0.2500            0.2500        \u001b[94m2.7874\u001b[0m  0.0006  0.2733\n",
      "      3            \u001b[36m0.5500\u001b[0m        \u001b[32m0.9850\u001b[0m       0.2465            0.2465        \u001b[94m2.5437\u001b[0m  0.0006  0.2743\n",
      "      4            \u001b[36m0.6250\u001b[0m        \u001b[32m0.8534\u001b[0m       0.2396            0.2396        2.6543  0.0006  0.2932\n",
      "      5            0.6250        \u001b[32m0.6384\u001b[0m       0.2257            0.2257        2.6024  0.0006  0.3489\n",
      "      6            0.6250        \u001b[32m0.5033\u001b[0m       0.2500            0.2500        \u001b[94m2.3946\u001b[0m  0.0006  0.3432\n",
      "      7            \u001b[36m0.6500\u001b[0m        \u001b[32m0.3894\u001b[0m       0.2500            0.2500        \u001b[94m2.2623\u001b[0m  0.0006  0.3609\n",
      "      8            \u001b[36m0.7500\u001b[0m        \u001b[32m0.3656\u001b[0m       0.2431            0.2431        \u001b[94m2.1293\u001b[0m  0.0006  0.3401\n",
      "      9            \u001b[36m0.7750\u001b[0m        \u001b[32m0.3297\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m2.0627\u001b[0m  0.0006  0.3374\n",
      "     10            \u001b[36m0.8500\u001b[0m        \u001b[32m0.2467\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        2.0638  0.0005  0.3492\n",
      "     11            \u001b[36m0.8750\u001b[0m        \u001b[32m0.2462\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m2.0479\u001b[0m  0.0005  0.3449\n",
      "     12            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2327\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m2.0341\u001b[0m  0.0005  0.2971\n",
      "     13            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1890\u001b[0m       0.2917            0.2917        \u001b[94m1.9799\u001b[0m  0.0005  0.2862\n",
      "     14            1.0000        \u001b[32m0.1420\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m1.9125\u001b[0m  0.0005  0.2769\n",
      "     15            1.0000        0.1579       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m1.8585\u001b[0m  0.0004  0.2767\n",
      "     16            1.0000        0.1916       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m1.8092\u001b[0m  0.0004  0.2694\n",
      "     17            1.0000        0.1577       0.3194            0.3194        \u001b[94m1.7653\u001b[0m  0.0004  0.2810\n",
      "     18            1.0000        0.1651       0.3194            0.3194        \u001b[94m1.7321\u001b[0m  0.0004  0.2775\n",
      "     19            1.0000        \u001b[32m0.1121\u001b[0m       0.3125            0.3125        \u001b[94m1.7062\u001b[0m  0.0004  0.2524\n",
      "     20            1.0000        \u001b[32m0.1082\u001b[0m       0.3056            0.3056        \u001b[94m1.6910\u001b[0m  0.0003  0.2652\n",
      "     21            1.0000        0.1378       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m1.6832\u001b[0m  0.0003  0.3854\n",
      "     22            1.0000        \u001b[32m0.0837\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.6771\u001b[0m  0.0003  0.2687\n",
      "     23            1.0000        0.0979       0.3368            0.3368        \u001b[94m1.6725\u001b[0m  0.0002  0.2675\n",
      "     24            1.0000        0.1246       0.3264            0.3264        \u001b[94m1.6679\u001b[0m  0.0002  0.2771\n",
      "     25            1.0000        0.0934       0.3264            0.3264        \u001b[94m1.6651\u001b[0m  0.0002  0.2777\n",
      "     26            1.0000        0.1038       0.3229            0.3229        \u001b[94m1.6632\u001b[0m  0.0002  0.2823\n",
      "     27            1.0000        0.0853       0.3194            0.3194        \u001b[94m1.6622\u001b[0m  0.0002  0.2617\n",
      "     28            1.0000        \u001b[32m0.0556\u001b[0m       0.3229            0.3229        \u001b[94m1.6614\u001b[0m  0.0001  0.2766\n",
      "     29            1.0000        0.0853       0.3194            0.3194        \u001b[94m1.6607\u001b[0m  0.0001  0.2681\n",
      "     30            1.0000        0.0766       0.3229            0.3229        \u001b[94m1.6602\u001b[0m  0.0001  0.2660\n",
      "     31            1.0000        0.0892       0.3299            0.3299        \u001b[94m1.6595\u001b[0m  0.0001  0.2603\n",
      "     32            1.0000        0.0953       0.3403            0.3403        \u001b[94m1.6591\u001b[0m  0.0001  0.2675\n",
      "     33            1.0000        0.0788       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.6587\u001b[0m  0.0000  0.2665\n",
      "     34            1.0000        0.0621       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.6581\u001b[0m  0.0000  0.2763\n",
      "     35            1.0000        \u001b[32m0.0524\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.6577\u001b[0m  0.0000  0.2618\n",
      "     36            1.0000        0.0574       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.6572\u001b[0m  0.0000  0.2669\n",
      "     37            1.0000        0.0704       0.3576            0.3576        \u001b[94m1.6567\u001b[0m  0.0000  0.2788\n",
      "     38            1.0000        0.0830       0.3576            0.3576        \u001b[94m1.6562\u001b[0m  0.0000  0.2623\n",
      "     39            1.0000        \u001b[32m0.0445\u001b[0m       0.3507            0.3507        \u001b[94m1.6559\u001b[0m  0.0000  0.2803\n",
      "     40            1.0000        0.0604       0.3507            0.3507        \u001b[94m1.6556\u001b[0m  0.0000  0.2666\n",
      "Training model for subject 2 with 50 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2600\u001b[0m        \u001b[32m2.0314\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m7.6882\u001b[0m  0.0006  0.2984\n",
      "      2            0.2600        \u001b[32m1.5088\u001b[0m       0.2500            0.2500        \u001b[94m7.0345\u001b[0m  0.0006  0.2735\n",
      "      3            0.2600        \u001b[32m0.9580\u001b[0m       0.2500            0.2500        \u001b[94m6.6867\u001b[0m  0.0006  0.2898\n",
      "      4            0.2600        \u001b[32m0.8566\u001b[0m       0.2500            0.2500        \u001b[94m6.6003\u001b[0m  0.0006  0.2750\n",
      "      5            0.2600        \u001b[32m0.7320\u001b[0m       0.2500            0.2500        \u001b[94m6.4477\u001b[0m  0.0006  0.2967\n",
      "      6            0.2600        \u001b[32m0.5998\u001b[0m       0.2500            0.2500        \u001b[94m6.1140\u001b[0m  0.0006  0.2841\n",
      "      7            0.2600        \u001b[32m0.3845\u001b[0m       0.2500            0.2500        \u001b[94m5.6146\u001b[0m  0.0006  0.2810\n",
      "      8            0.2600        0.5512       0.2500            0.2500        \u001b[94m5.0556\u001b[0m  0.0006  0.2959\n",
      "      9            \u001b[36m0.3200\u001b[0m        \u001b[32m0.3623\u001b[0m       0.2500            0.2500        \u001b[94m4.4244\u001b[0m  0.0006  0.2858\n",
      "     10            \u001b[36m0.3400\u001b[0m        0.3906       0.2500            0.2500        \u001b[94m3.8270\u001b[0m  0.0005  0.2774\n",
      "     11            \u001b[36m0.4800\u001b[0m        \u001b[32m0.2740\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m3.3450\u001b[0m  0.0005  0.2927\n",
      "     12            \u001b[36m0.6400\u001b[0m        0.2753       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m2.9809\u001b[0m  0.0005  0.2878\n",
      "     13            \u001b[36m0.7000\u001b[0m        \u001b[32m0.2267\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m2.6843\u001b[0m  0.0005  0.2922\n",
      "     14            \u001b[36m0.8600\u001b[0m        0.2304       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m2.4278\u001b[0m  0.0005  0.3393\n",
      "     15            \u001b[36m0.9600\u001b[0m        \u001b[32m0.2227\u001b[0m       0.2986            0.2986        \u001b[94m2.2200\u001b[0m  0.0004  0.3383\n",
      "     16            0.9600        \u001b[32m0.1199\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m2.0685\u001b[0m  0.0004  0.3697\n",
      "     17            \u001b[36m0.9800\u001b[0m        0.1840       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m1.9696\u001b[0m  0.0004  0.3657\n",
      "     18            0.9800        0.1686       0.3229            0.3229        \u001b[94m1.8963\u001b[0m  0.0004  0.3610\n",
      "     19            \u001b[36m1.0000\u001b[0m        0.1499       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m1.8500\u001b[0m  0.0004  0.3603\n",
      "     20            1.0000        0.1238       0.3264            0.3264        \u001b[94m1.8160\u001b[0m  0.0003  0.4176\n",
      "     21            1.0000        0.1415       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m1.7847\u001b[0m  0.0003  0.3560\n",
      "     22            1.0000        0.1248       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.7636\u001b[0m  0.0003  0.3212\n",
      "     23            1.0000        0.1387       0.3403            0.3403        \u001b[94m1.7499\u001b[0m  0.0002  0.2938\n",
      "     24            1.0000        \u001b[32m0.0992\u001b[0m       0.3368            0.3368        \u001b[94m1.7397\u001b[0m  0.0002  0.2922\n",
      "     25            1.0000        \u001b[32m0.0866\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.7313\u001b[0m  0.0002  0.2881\n",
      "     26            1.0000        0.1007       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.7235\u001b[0m  0.0002  0.2932\n",
      "     27            1.0000        0.1232       0.3438            0.3438        \u001b[94m1.7165\u001b[0m  0.0002  0.3867\n",
      "     28            1.0000        0.1194       0.3438            0.3438        \u001b[94m1.7093\u001b[0m  0.0001  0.2782\n",
      "     29            1.0000        0.1004       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.7035\u001b[0m  0.0001  0.2753\n",
      "     30            1.0000        0.0905       0.3542            0.3542        \u001b[94m1.6992\u001b[0m  0.0001  0.2938\n",
      "     31            1.0000        0.1013       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.6959\u001b[0m  0.0001  0.2880\n",
      "     32            1.0000        0.0923       0.3542            0.3542        \u001b[94m1.6938\u001b[0m  0.0001  0.2911\n",
      "     33            1.0000        0.1036       0.3542            0.3542        \u001b[94m1.6927\u001b[0m  0.0000  0.2821\n",
      "     34            1.0000        0.0992       0.3576            0.3576        \u001b[94m1.6918\u001b[0m  0.0000  0.2827\n",
      "     35            1.0000        \u001b[32m0.0674\u001b[0m       0.3542            0.3542        \u001b[94m1.6909\u001b[0m  0.0000  0.2780\n",
      "     36            1.0000        0.1069       0.3507            0.3507        \u001b[94m1.6902\u001b[0m  0.0000  0.2942\n",
      "     37            1.0000        0.1007       0.3507            0.3507        \u001b[94m1.6896\u001b[0m  0.0000  0.3011\n",
      "     38            1.0000        0.0947       0.3507            0.3507        \u001b[94m1.6890\u001b[0m  0.0000  0.2945\n",
      "     39            1.0000        0.1035       0.3542            0.3542        \u001b[94m1.6886\u001b[0m  0.0000  0.2941\n",
      "     40            1.0000        0.1131       0.3542            0.3542        \u001b[94m1.6883\u001b[0m  0.0000  0.2850\n",
      "Training model for subject 2 with 60 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2667\u001b[0m        \u001b[32m1.9712\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m1.7138\u001b[0m  0.0006  0.3045\n",
      "      2            \u001b[36m0.3833\u001b[0m        \u001b[32m1.3589\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        2.1570  0.0006  0.2906\n",
      "      3            0.2667        \u001b[32m1.1690\u001b[0m       0.2569            0.2569        3.3170  0.0006  0.2877\n",
      "      4            0.2500        \u001b[32m0.9566\u001b[0m       0.2535            0.2535        3.8084  0.0006  0.2958\n",
      "      5            0.2500        \u001b[32m0.7743\u001b[0m       0.2535            0.2535        4.1520  0.0006  0.2929\n",
      "      6            0.2667        \u001b[32m0.7015\u001b[0m       0.2535            0.2535        3.9485  0.0006  0.3006\n",
      "      7            0.3000        \u001b[32m0.6377\u001b[0m       0.2535            0.2535        3.6613  0.0006  0.2957\n",
      "      8            \u001b[36m0.4000\u001b[0m        \u001b[32m0.6344\u001b[0m       0.2569            0.2569        3.4114  0.0006  0.2944\n",
      "      9            \u001b[36m0.4333\u001b[0m        \u001b[32m0.4406\u001b[0m       0.2639            0.2639        3.1562  0.0006  0.3028\n",
      "     10            \u001b[36m0.4667\u001b[0m        0.4514       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        2.8942  0.0005  0.2790\n",
      "     11            \u001b[36m0.5667\u001b[0m        \u001b[32m0.4112\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        2.6759  0.0005  0.3060\n",
      "     12            \u001b[36m0.7000\u001b[0m        \u001b[32m0.3167\u001b[0m       0.2951            0.2951        2.4547  0.0005  0.2897\n",
      "     13            \u001b[36m0.7667\u001b[0m        0.3854       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        2.2600  0.0005  0.2947\n",
      "     14            \u001b[36m0.8500\u001b[0m        \u001b[32m0.2833\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        2.0674  0.0005  0.2947\n",
      "     15            \u001b[36m0.9167\u001b[0m        \u001b[32m0.2583\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        1.9179  0.0004  0.2862\n",
      "     16            \u001b[36m0.9667\u001b[0m        \u001b[32m0.2069\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        1.8151  0.0004  0.2931\n",
      "     17            \u001b[36m0.9833\u001b[0m        0.2561       0.3403            0.3403        1.7505  0.0004  0.2885\n",
      "     18            \u001b[36m1.0000\u001b[0m        0.2487       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.7043\u001b[0m  0.0004  0.3076\n",
      "     19            1.0000        \u001b[32m0.2044\u001b[0m       0.3507            0.3507        \u001b[94m1.6711\u001b[0m  0.0004  0.2957\n",
      "     20            1.0000        0.2218       0.3507            0.3507        \u001b[94m1.6489\u001b[0m  0.0003  0.3414\n",
      "     21            1.0000        \u001b[32m0.1927\u001b[0m       0.3507            0.3507        \u001b[94m1.6330\u001b[0m  0.0003  0.3676\n",
      "     22            1.0000        0.2245       0.3576            0.3576        \u001b[94m1.6165\u001b[0m  0.0003  0.3892\n",
      "     23            1.0000        \u001b[32m0.1652\u001b[0m       0.3576            0.3576        \u001b[94m1.6041\u001b[0m  0.0002  0.3631\n",
      "     24            1.0000        \u001b[32m0.1523\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.5950\u001b[0m  0.0002  0.3766\n",
      "     25            1.0000        0.1644       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.5869\u001b[0m  0.0002  0.3611\n",
      "     26            1.0000        \u001b[32m0.1521\u001b[0m       0.3576            0.3576        \u001b[94m1.5813\u001b[0m  0.0002  0.4167\n",
      "     27            1.0000        0.1724       0.3646            0.3646        \u001b[94m1.5774\u001b[0m  0.0002  0.3919\n",
      "     28            1.0000        \u001b[32m0.1409\u001b[0m       0.3542            0.3542        \u001b[94m1.5747\u001b[0m  0.0001  0.3049\n",
      "     29            1.0000        0.1814       0.3542            0.3542        \u001b[94m1.5723\u001b[0m  0.0001  0.2968\n",
      "     30            1.0000        0.1505       0.3472            0.3472        \u001b[94m1.5708\u001b[0m  0.0001  0.2957\n",
      "     31            1.0000        \u001b[32m0.1388\u001b[0m       0.3472            0.3472        \u001b[94m1.5694\u001b[0m  0.0001  0.2871\n",
      "     32            1.0000        0.1409       0.3507            0.3507        \u001b[94m1.5684\u001b[0m  0.0001  0.2904\n",
      "     33            1.0000        \u001b[32m0.1294\u001b[0m       0.3507            0.3507        \u001b[94m1.5676\u001b[0m  0.0000  0.2979\n",
      "     34            1.0000        0.1464       0.3507            0.3507        \u001b[94m1.5667\u001b[0m  0.0000  0.4097\n",
      "     35            1.0000        \u001b[32m0.1193\u001b[0m       0.3542            0.3542        \u001b[94m1.5662\u001b[0m  0.0000  0.2927\n",
      "     36            1.0000        0.1833       0.3542            0.3542        \u001b[94m1.5652\u001b[0m  0.0000  0.3029\n",
      "     37            1.0000        0.1441       0.3542            0.3542        \u001b[94m1.5644\u001b[0m  0.0000  0.2961\n",
      "     38            1.0000        0.1296       0.3576            0.3576        \u001b[94m1.5638\u001b[0m  0.0000  0.2912\n",
      "     39            1.0000        0.1296       0.3576            0.3576        \u001b[94m1.5635\u001b[0m  0.0000  0.3020\n",
      "     40            1.0000        0.1334       0.3611            0.3611        \u001b[94m1.5634\u001b[0m  0.0000  0.2944\n",
      "Training model for subject 2 with 70 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3571\u001b[0m        \u001b[32m1.6102\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m3.7281\u001b[0m  0.0006  0.3145\n",
      "      2            \u001b[36m0.4286\u001b[0m        \u001b[32m1.2440\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m3.4295\u001b[0m  0.0006  0.3068\n",
      "      3            \u001b[36m0.4571\u001b[0m        \u001b[32m1.1932\u001b[0m       0.3438            0.3438        \u001b[94m2.6204\u001b[0m  0.0006  0.3091\n",
      "      4            0.4286        \u001b[32m0.8372\u001b[0m       0.2743            0.2743        \u001b[94m2.4810\u001b[0m  0.0006  0.3137\n",
      "      5            0.4143        \u001b[32m0.7831\u001b[0m       0.2639            0.2639        2.5495  0.0006  0.3197\n",
      "      6            0.4286        \u001b[32m0.6613\u001b[0m       0.2639            0.2639        2.5844  0.0006  0.3055\n",
      "      7            0.4571        0.6772       0.2639            0.2639        2.6502  0.0006  0.3132\n",
      "      8            \u001b[36m0.4857\u001b[0m        \u001b[32m0.4996\u001b[0m       0.2674            0.2674        2.6542  0.0006  0.3033\n",
      "      9            \u001b[36m0.5571\u001b[0m        \u001b[32m0.4687\u001b[0m       0.2674            0.2674        2.6144  0.0006  0.3063\n",
      "     10            \u001b[36m0.6286\u001b[0m        \u001b[32m0.3827\u001b[0m       0.2812            0.2812        \u001b[94m2.4719\u001b[0m  0.0005  0.3170\n",
      "     11            \u001b[36m0.6857\u001b[0m        0.4153       0.2917            0.2917        \u001b[94m2.3752\u001b[0m  0.0005  0.3275\n",
      "     12            \u001b[36m0.7714\u001b[0m        0.4401       0.3021            0.3021        \u001b[94m2.2671\u001b[0m  0.0005  0.3099\n",
      "     13            \u001b[36m0.8286\u001b[0m        \u001b[32m0.3419\u001b[0m       0.3160            0.3160        \u001b[94m2.1616\u001b[0m  0.0005  0.3199\n",
      "     14            \u001b[36m0.8571\u001b[0m        \u001b[32m0.2966\u001b[0m       0.3333            0.3333        \u001b[94m2.0298\u001b[0m  0.0005  0.3111\n",
      "     15            \u001b[36m0.9286\u001b[0m        \u001b[32m0.2538\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.9063\u001b[0m  0.0004  0.3144\n",
      "     16            0.9286        0.2598       0.3542            0.3542        \u001b[94m1.8068\u001b[0m  0.0004  0.3164\n",
      "     17            \u001b[36m0.9571\u001b[0m        \u001b[32m0.2339\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.7189\u001b[0m  0.0004  0.3123\n",
      "     18            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2153\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.6685\u001b[0m  0.0004  0.3125\n",
      "     19            1.0000        \u001b[32m0.1657\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.6373\u001b[0m  0.0004  0.3187\n",
      "     20            1.0000        0.2190       0.4062            0.4062        \u001b[94m1.6211\u001b[0m  0.0003  0.3083\n",
      "     21            1.0000        0.1936       0.4062            0.4062        \u001b[94m1.6112\u001b[0m  0.0003  0.3165\n",
      "     22            1.0000        0.1936       0.3924            0.3924        \u001b[94m1.6066\u001b[0m  0.0003  0.3427\n",
      "     23            1.0000        \u001b[32m0.1538\u001b[0m       0.3924            0.3924        \u001b[94m1.6028\u001b[0m  0.0002  0.3424\n",
      "     24            1.0000        0.1643       0.3993            0.3993        \u001b[94m1.5979\u001b[0m  0.0002  0.3839\n",
      "     25            1.0000        \u001b[32m0.1458\u001b[0m       0.3993            0.3993        \u001b[94m1.5941\u001b[0m  0.0002  0.3714\n",
      "     26            1.0000        \u001b[32m0.1333\u001b[0m       0.3889            0.3889        \u001b[94m1.5907\u001b[0m  0.0002  0.3811\n",
      "     27            1.0000        0.1675       0.3924            0.3924        \u001b[94m1.5896\u001b[0m  0.0002  0.4093\n",
      "     28            1.0000        0.1498       0.3924            0.3924        \u001b[94m1.5877\u001b[0m  0.0001  0.3817\n",
      "     29            1.0000        0.1358       0.3924            0.3924        \u001b[94m1.5854\u001b[0m  0.0001  0.4255\n",
      "     30            1.0000        0.1543       0.3924            0.3924        \u001b[94m1.5824\u001b[0m  0.0001  0.4008\n",
      "     31            1.0000        \u001b[32m0.1313\u001b[0m       0.3924            0.3924        \u001b[94m1.5791\u001b[0m  0.0001  0.3351\n",
      "     32            1.0000        \u001b[32m0.1211\u001b[0m       0.3889            0.3889        \u001b[94m1.5755\u001b[0m  0.0001  0.3252\n",
      "     33            1.0000        0.1314       0.3889            0.3889        \u001b[94m1.5733\u001b[0m  0.0000  0.3009\n",
      "     34            1.0000        0.1392       0.3924            0.3924        \u001b[94m1.5717\u001b[0m  0.0000  0.3125\n",
      "     35            1.0000        \u001b[32m0.1155\u001b[0m       0.3924            0.3924        \u001b[94m1.5706\u001b[0m  0.0000  0.3117\n",
      "     36            1.0000        0.1807       0.3924            0.3924        \u001b[94m1.5696\u001b[0m  0.0000  0.3232\n",
      "     37            1.0000        0.1297       0.3993            0.3993        \u001b[94m1.5687\u001b[0m  0.0000  0.3244\n",
      "     38            1.0000        0.1434       0.3993            0.3993        \u001b[94m1.5682\u001b[0m  0.0000  0.3116\n",
      "     39            1.0000        0.1563       0.3993            0.3993        \u001b[94m1.5679\u001b[0m  0.0000  0.3510\n",
      "     40            1.0000        0.1492       0.3993            0.3993        \u001b[94m1.5675\u001b[0m  0.0000  0.3099\n",
      "Training model for subject 2 with 80 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.8796\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.5585\u001b[0m  0.0006  0.4324\n",
      "      2            0.2500        \u001b[32m1.4332\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m2.4087\u001b[0m  0.0006  0.3194\n",
      "      3            \u001b[36m0.4375\u001b[0m        \u001b[32m1.1367\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m1.8271\u001b[0m  0.0006  0.3297\n",
      "      4            \u001b[36m0.5000\u001b[0m        \u001b[32m0.9563\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        1.9306  0.0006  0.3341\n",
      "      5            \u001b[36m0.5125\u001b[0m        \u001b[32m0.7877\u001b[0m       0.3299            0.3299        2.1122  0.0006  0.3229\n",
      "      6            0.5125        \u001b[32m0.7466\u001b[0m       0.3021            0.3021        2.2139  0.0006  0.3220\n",
      "      7            \u001b[36m0.5625\u001b[0m        \u001b[32m0.7134\u001b[0m       0.3194            0.3194        2.2076  0.0006  0.3425\n",
      "      8            \u001b[36m0.6750\u001b[0m        \u001b[32m0.6593\u001b[0m       0.3264            0.3264        2.1192  0.0006  0.3117\n",
      "      9            \u001b[36m0.7125\u001b[0m        \u001b[32m0.5506\u001b[0m       0.3438            0.3438        2.0777  0.0006  0.3230\n",
      "     10            \u001b[36m0.7625\u001b[0m        \u001b[32m0.4702\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        2.0055  0.0005  0.3082\n",
      "     11            \u001b[36m0.8000\u001b[0m        0.4735       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        1.9530  0.0005  0.3154\n",
      "     12            \u001b[36m0.8500\u001b[0m        \u001b[32m0.4478\u001b[0m       0.3646            0.3646        1.8847  0.0005  0.3305\n",
      "     13            \u001b[36m0.8625\u001b[0m        \u001b[32m0.4113\u001b[0m       0.3646            0.3646        1.8471  0.0005  0.3262\n",
      "     14            \u001b[36m0.8875\u001b[0m        \u001b[32m0.3097\u001b[0m       0.3715            0.3715        \u001b[94m1.7996\u001b[0m  0.0005  0.3210\n",
      "     15            \u001b[36m0.9250\u001b[0m        0.3265       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.7717\u001b[0m  0.0004  0.3269\n",
      "     16            \u001b[36m0.9375\u001b[0m        \u001b[32m0.2937\u001b[0m       0.3681            0.3681        \u001b[94m1.7542\u001b[0m  0.0004  0.3292\n",
      "     17            0.9375        0.3202       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.7238\u001b[0m  0.0004  0.3375\n",
      "     18            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2817\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.6997\u001b[0m  0.0004  0.3241\n",
      "     19            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2331\u001b[0m       0.3958            0.3958        \u001b[94m1.6714\u001b[0m  0.0004  0.3326\n",
      "     20            0.9750        \u001b[32m0.2089\u001b[0m       0.3958            0.3958        \u001b[94m1.6423\u001b[0m  0.0003  0.3285\n",
      "     21            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1985\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.6205\u001b[0m  0.0003  0.3121\n",
      "     22            1.0000        \u001b[32m0.1945\u001b[0m       0.4028            0.4028        \u001b[94m1.6018\u001b[0m  0.0003  0.3359\n",
      "     23            1.0000        \u001b[32m0.1909\u001b[0m       0.4028            0.4028        \u001b[94m1.5888\u001b[0m  0.0002  0.3289\n",
      "     24            1.0000        \u001b[32m0.1763\u001b[0m       0.3958            0.3958        \u001b[94m1.5806\u001b[0m  0.0002  0.4054\n",
      "     25            1.0000        0.2283       0.3958            0.3958        \u001b[94m1.5747\u001b[0m  0.0002  0.3810\n",
      "     26            1.0000        0.1896       0.4062            0.4062        \u001b[94m1.5672\u001b[0m  0.0002  0.4737\n",
      "     27            1.0000        \u001b[32m0.1670\u001b[0m       0.3993            0.3993        \u001b[94m1.5628\u001b[0m  0.0002  0.3860\n",
      "     28            1.0000        0.1677       0.4028            0.4028        \u001b[94m1.5627\u001b[0m  0.0001  0.3796\n",
      "     29            1.0000        \u001b[32m0.1660\u001b[0m       0.3993            0.3993        \u001b[94m1.5624\u001b[0m  0.0001  0.4157\n",
      "     30            1.0000        0.1998       0.3993            0.3993        1.5624  0.0001  0.4535\n",
      "     31            1.0000        0.1831       0.3958            0.3958        1.5625  0.0001  0.3574\n",
      "     32            1.0000        \u001b[32m0.1530\u001b[0m       0.3958            0.3958        \u001b[94m1.5617\u001b[0m  0.0001  0.3409\n",
      "     33            1.0000        \u001b[32m0.1515\u001b[0m       0.3993            0.3993        \u001b[94m1.5612\u001b[0m  0.0000  0.3351\n",
      "     34            1.0000        0.1516       0.3993            0.3993        1.5616  0.0000  0.3279\n",
      "     35            1.0000        \u001b[32m0.1395\u001b[0m       0.3958            0.3958        1.5620  0.0000  0.3283\n",
      "     36            1.0000        0.1502       0.3958            0.3958        1.5626  0.0000  0.3383\n",
      "     37            1.0000        \u001b[32m0.1150\u001b[0m       0.3889            0.3889        1.5631  0.0000  0.3410\n",
      "     38            1.0000        0.1465       0.3889            0.3889        1.5636  0.0000  0.3321\n",
      "     39            1.0000        0.1695       0.3854            0.3854        1.5643  0.0000  0.3275\n",
      "     40            1.0000        0.1364       0.3854            0.3854        1.5644  0.0000  0.3461\n",
      "Training model for subject 2 with 90 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3000\u001b[0m        \u001b[32m1.6404\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m2.1741\u001b[0m  0.0006  0.3472\n",
      "      2            \u001b[36m0.3333\u001b[0m        \u001b[32m1.2656\u001b[0m       0.2569            0.2569        2.3852  0.0006  0.3488\n",
      "      3            \u001b[36m0.4222\u001b[0m        \u001b[32m1.0576\u001b[0m       0.2639            0.2639        2.8403  0.0006  0.3383\n",
      "      4            \u001b[36m0.4556\u001b[0m        \u001b[32m1.0214\u001b[0m       0.2743            0.2743        2.8956  0.0006  0.3288\n",
      "      5            0.4556        \u001b[32m0.9147\u001b[0m       0.2812            0.2812        2.7807  0.0006  0.3438\n",
      "      6            0.4333        \u001b[32m0.7764\u001b[0m       0.2708            0.2708        2.6914  0.0006  0.3381\n",
      "      7            \u001b[36m0.4778\u001b[0m        \u001b[32m0.6680\u001b[0m       0.2778            0.2778        2.4740  0.0006  0.3234\n",
      "      8            \u001b[36m0.5778\u001b[0m        \u001b[32m0.6597\u001b[0m       0.2917            0.2917        2.1819  0.0006  0.3481\n",
      "      9            \u001b[36m0.6667\u001b[0m        \u001b[32m0.5702\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m1.9868\u001b[0m  0.0006  0.3614\n",
      "     10            \u001b[36m0.7778\u001b[0m        0.5823       0.3125            0.3125        \u001b[94m1.8461\u001b[0m  0.0005  0.3448\n",
      "     11            \u001b[36m0.8333\u001b[0m        \u001b[32m0.5393\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m1.7428\u001b[0m  0.0005  0.3404\n",
      "     12            \u001b[36m0.8556\u001b[0m        \u001b[32m0.5285\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.6780\u001b[0m  0.0005  0.3391\n",
      "     13            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4240\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.6399\u001b[0m  0.0005  0.3492\n",
      "     14            \u001b[36m0.9333\u001b[0m        0.4315       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.6107\u001b[0m  0.0005  0.3444\n",
      "     15            \u001b[36m0.9556\u001b[0m        \u001b[32m0.3859\u001b[0m       0.3611            0.3611        \u001b[94m1.5791\u001b[0m  0.0004  0.3463\n",
      "     16            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3794\u001b[0m       0.3611            0.3611        \u001b[94m1.5533\u001b[0m  0.0004  0.3461\n",
      "     17            \u001b[36m0.9778\u001b[0m        \u001b[32m0.2730\u001b[0m       0.3611            0.3611        \u001b[94m1.5331\u001b[0m  0.0004  0.3391\n",
      "     18            \u001b[36m1.0000\u001b[0m        0.2958       0.3681            0.3681        \u001b[94m1.5235\u001b[0m  0.0004  0.3381\n",
      "     19            1.0000        \u001b[32m0.2620\u001b[0m       0.3681            0.3681        \u001b[94m1.5209\u001b[0m  0.0004  0.3459\n",
      "     20            1.0000        0.2978       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.5195\u001b[0m  0.0003  0.3452\n",
      "     21            1.0000        0.2904       0.3750            0.3750        \u001b[94m1.5141\u001b[0m  0.0003  0.3517\n",
      "     22            1.0000        \u001b[32m0.2508\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.5101\u001b[0m  0.0003  0.3709\n",
      "     23            1.0000        0.2593       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.5077\u001b[0m  0.0002  0.4073\n",
      "     24            1.0000        \u001b[32m0.2460\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.5064\u001b[0m  0.0002  0.4175\n",
      "     25            1.0000        0.2474       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        1.5067  0.0002  0.4042\n",
      "     26            1.0000        \u001b[32m0.1864\u001b[0m       0.4132            0.4132        1.5100  0.0002  0.3863\n",
      "     27            1.0000        0.2241       0.4028            0.4028        1.5148  0.0002  0.3998\n",
      "     28            1.0000        0.1998       0.4028            0.4028        1.5194  0.0001  0.4120\n",
      "     29            1.0000        \u001b[32m0.1821\u001b[0m       0.4028            0.4028        1.5239  0.0001  0.4617\n",
      "     30            1.0000        \u001b[32m0.1717\u001b[0m       0.4028            0.4028        1.5291  0.0001  0.3520\n",
      "     31            1.0000        0.1846       0.3958            0.3958        1.5332  0.0001  0.3611\n",
      "     32            1.0000        0.1969       0.3993            0.3993        1.5363  0.0001  0.3629\n",
      "     33            1.0000        \u001b[32m0.1639\u001b[0m       0.3889            0.3889        1.5380  0.0000  0.3325\n",
      "     34            1.0000        0.1950       0.3993            0.3993        1.5384  0.0000  0.3558\n",
      "     35            1.0000        0.2145       0.3993            0.3993        1.5388  0.0000  0.3410\n",
      "     36            1.0000        \u001b[32m0.1393\u001b[0m       0.3993            0.3993        1.5388  0.0000  0.3469\n",
      "     37            1.0000        0.1885       0.3993            0.3993        1.5388  0.0000  0.3461\n",
      "     38            1.0000        0.1866       0.4028            0.4028        1.5389  0.0000  0.3317\n",
      "     39            1.0000        0.1952       0.4028            0.4028        1.5388  0.0000  0.3530\n",
      "     40            1.0000        0.2045       0.4028            0.4028        1.5388  0.0000  0.3489\n",
      "Training model for subject 2 with 100 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2700\u001b[0m        \u001b[32m1.6762\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m2.5424\u001b[0m  0.0006  0.3503\n",
      "      2            \u001b[36m0.4800\u001b[0m        \u001b[32m1.3822\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.8950\u001b[0m  0.0006  0.3649\n",
      "      3            0.4600        \u001b[32m1.2005\u001b[0m       0.2847            0.2847        1.9301  0.0006  0.3502\n",
      "      4            0.4100        \u001b[32m0.9310\u001b[0m       0.2674            0.2674        2.1181  0.0006  0.3774\n",
      "      5            0.3600        \u001b[32m0.9032\u001b[0m       0.2743            0.2743        2.3338  0.0006  0.3619\n",
      "      6            0.4200        \u001b[32m0.8066\u001b[0m       0.2778            0.2778        2.4198  0.0006  0.3565\n",
      "      7            0.4200        \u001b[32m0.7737\u001b[0m       0.2986            0.2986        2.4754  0.0006  0.3545\n",
      "      8            0.4700        0.8110       0.3021            0.3021        2.4805  0.0006  0.3506\n",
      "      9            \u001b[36m0.5000\u001b[0m        \u001b[32m0.6275\u001b[0m       0.3125            0.3125        2.3808  0.0006  0.3621\n",
      "     10            \u001b[36m0.5400\u001b[0m        \u001b[32m0.5126\u001b[0m       0.3299            0.3299        2.3375  0.0005  0.3619\n",
      "     11            \u001b[36m0.5900\u001b[0m        0.5245       0.3403            0.3403        2.2634  0.0005  0.3501\n",
      "     12            \u001b[36m0.6700\u001b[0m        \u001b[32m0.5055\u001b[0m       0.3403            0.3403        2.1561  0.0005  0.3587\n",
      "     13            \u001b[36m0.7800\u001b[0m        \u001b[32m0.4409\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        2.0485  0.0005  0.3532\n",
      "     14            \u001b[36m0.8300\u001b[0m        \u001b[32m0.3794\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        1.9295  0.0005  0.3574\n",
      "     15            \u001b[36m0.8900\u001b[0m        \u001b[32m0.3476\u001b[0m       0.3611            0.3611        \u001b[94m1.8145\u001b[0m  0.0004  0.3647\n",
      "     16            \u001b[36m0.9300\u001b[0m        0.3941       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.7262\u001b[0m  0.0004  0.3773\n",
      "     17            \u001b[36m0.9500\u001b[0m        0.3569       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.6697\u001b[0m  0.0004  0.3656\n",
      "     18            \u001b[36m0.9600\u001b[0m        \u001b[32m0.3186\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.6308\u001b[0m  0.0004  0.3484\n",
      "     19            \u001b[36m0.9900\u001b[0m        0.3266       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.5973\u001b[0m  0.0004  0.3938\n",
      "     20            0.9900        \u001b[32m0.2949\u001b[0m       0.3993            0.3993        \u001b[94m1.5721\u001b[0m  0.0003  0.4663\n",
      "     21            0.9900        \u001b[32m0.2589\u001b[0m       0.3924            0.3924        \u001b[94m1.5518\u001b[0m  0.0003  0.4390\n",
      "     22            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2290\u001b[0m       0.3785            0.3785        \u001b[94m1.5349\u001b[0m  0.0003  0.4192\n",
      "     23            1.0000        0.2942       0.3715            0.3715        \u001b[94m1.5239\u001b[0m  0.0002  0.4566\n",
      "     24            1.0000        0.2544       0.3646            0.3646        \u001b[94m1.5162\u001b[0m  0.0002  0.4508\n",
      "     25            1.0000        0.2891       0.3681            0.3681        \u001b[94m1.5138\u001b[0m  0.0002  0.5599\n",
      "     26            1.0000        0.2399       0.3750            0.3750        \u001b[94m1.5099\u001b[0m  0.0002  0.3657\n",
      "     27            1.0000        0.2786       0.3785            0.3785        \u001b[94m1.5059\u001b[0m  0.0002  0.3786\n",
      "     28            1.0000        \u001b[32m0.1729\u001b[0m       0.3715            0.3715        \u001b[94m1.5033\u001b[0m  0.0001  0.3803\n",
      "     29            1.0000        0.2107       0.3715            0.3715        \u001b[94m1.5009\u001b[0m  0.0001  0.3630\n",
      "     30            1.0000        0.2147       0.3785            0.3785        \u001b[94m1.4982\u001b[0m  0.0001  0.3556\n",
      "     31            1.0000        0.2155       0.3819            0.3819        \u001b[94m1.4954\u001b[0m  0.0001  0.3547\n",
      "     32            1.0000        0.2206       0.3750            0.3750        \u001b[94m1.4941\u001b[0m  0.0001  0.3687\n",
      "     33            1.0000        0.2152       0.3715            0.3715        \u001b[94m1.4931\u001b[0m  0.0000  0.3617\n",
      "     34            1.0000        0.2136       0.3750            0.3750        \u001b[94m1.4921\u001b[0m  0.0000  0.3648\n",
      "     35            1.0000        0.2293       0.3819            0.3819        \u001b[94m1.4912\u001b[0m  0.0000  0.3636\n",
      "     36            1.0000        0.1987       0.3854            0.3854        \u001b[94m1.4907\u001b[0m  0.0000  0.3752\n",
      "     37            1.0000        0.1785       0.3854            0.3854        \u001b[94m1.4903\u001b[0m  0.0000  0.3576\n",
      "     38            1.0000        0.2294       0.3854            0.3854        \u001b[94m1.4902\u001b[0m  0.0000  0.3558\n",
      "     39            1.0000        0.2366       0.3854            0.3854        \u001b[94m1.4901\u001b[0m  0.0000  0.3846\n",
      "     40            1.0000        0.2175       0.3819            0.3819        \u001b[94m1.4900\u001b[0m  0.0000  0.3615\n",
      "Training model for subject 2 with 110 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3727\u001b[0m        \u001b[32m1.6886\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m1.7587\u001b[0m  0.0006  0.3748\n",
      "      2            \u001b[36m0.4091\u001b[0m        \u001b[32m1.3339\u001b[0m       0.2778            0.2778        \u001b[94m1.6658\u001b[0m  0.0006  0.3674\n",
      "      3            0.3909        \u001b[32m1.1153\u001b[0m       0.2743            0.2743        1.7082  0.0006  0.4592\n",
      "      4            0.3182        \u001b[32m1.0170\u001b[0m       0.2743            0.2743        2.0533  0.0006  0.3819\n",
      "      5            \u001b[36m0.4364\u001b[0m        \u001b[32m0.8957\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        2.0639  0.0006  0.4362\n",
      "      6            \u001b[36m0.4818\u001b[0m        \u001b[32m0.8505\u001b[0m       0.3229            0.3229        2.1551  0.0006  0.3593\n",
      "      7            \u001b[36m0.5818\u001b[0m        \u001b[32m0.7734\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        1.9909  0.0006  0.3637\n",
      "      8            \u001b[36m0.6364\u001b[0m        \u001b[32m0.6984\u001b[0m       0.3576            0.3576        1.8494  0.0006  0.3570\n",
      "      9            \u001b[36m0.7455\u001b[0m        \u001b[32m0.6696\u001b[0m       0.3507            0.3507        1.7507  0.0006  0.3694\n",
      "     10            \u001b[36m0.8727\u001b[0m        0.6793       0.3333            0.3333        1.7074  0.0005  0.3687\n",
      "     11            \u001b[36m0.8818\u001b[0m        \u001b[32m0.5172\u001b[0m       0.3368            0.3368        1.7286  0.0005  0.3621\n",
      "     12            \u001b[36m0.9091\u001b[0m        0.6137       0.3438            0.3438        1.7334  0.0005  0.3637\n",
      "     13            0.9091        \u001b[32m0.4922\u001b[0m       0.3368            0.3368        1.7295  0.0005  0.3623\n",
      "     14            0.9091        \u001b[32m0.4535\u001b[0m       0.3472            0.3472        1.7277  0.0005  0.4257\n",
      "     15            0.9091        \u001b[32m0.4155\u001b[0m       0.3507            0.3507        1.7278  0.0004  0.4169\n",
      "     16            \u001b[36m0.9364\u001b[0m        \u001b[32m0.4004\u001b[0m       0.3472            0.3472        1.7127  0.0004  0.4434\n",
      "     17            \u001b[36m0.9545\u001b[0m        \u001b[32m0.3426\u001b[0m       0.3472            0.3472        1.6930  0.0004  0.4177\n",
      "     18            \u001b[36m1.0000\u001b[0m        0.3957       0.3542            0.3542        \u001b[94m1.6562\u001b[0m  0.0004  0.4465\n",
      "     19            1.0000        \u001b[32m0.3034\u001b[0m       0.3576            0.3576        \u001b[94m1.6201\u001b[0m  0.0004  0.4905\n",
      "     20            1.0000        0.3242       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.5958\u001b[0m  0.0003  0.4480\n",
      "     21            1.0000        \u001b[32m0.2955\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.5833\u001b[0m  0.0003  0.3887\n",
      "     22            1.0000        0.3187       0.3681            0.3681        \u001b[94m1.5731\u001b[0m  0.0003  0.3816\n",
      "     23            1.0000        0.3203       0.3611            0.3611        \u001b[94m1.5660\u001b[0m  0.0002  0.3626\n",
      "     24            1.0000        \u001b[32m0.2271\u001b[0m       0.3576            0.3576        \u001b[94m1.5583\u001b[0m  0.0002  0.3639\n",
      "     25            1.0000        0.2968       0.3542            0.3542        \u001b[94m1.5530\u001b[0m  0.0002  0.3666\n",
      "     26            1.0000        0.2685       0.3542            0.3542        \u001b[94m1.5476\u001b[0m  0.0002  0.3775\n",
      "     27            1.0000        0.2868       0.3542            0.3542        \u001b[94m1.5430\u001b[0m  0.0002  0.3702\n",
      "     28            1.0000        0.2829       0.3542            0.3542        \u001b[94m1.5374\u001b[0m  0.0001  0.3549\n",
      "     29            1.0000        0.2569       0.3576            0.3576        \u001b[94m1.5313\u001b[0m  0.0001  0.3572\n",
      "     30            1.0000        0.2491       0.3576            0.3576        \u001b[94m1.5269\u001b[0m  0.0001  0.3602\n",
      "     31            1.0000        \u001b[32m0.2213\u001b[0m       0.3576            0.3576        \u001b[94m1.5240\u001b[0m  0.0001  0.3653\n",
      "     32            1.0000        0.2267       0.3542            0.3542        \u001b[94m1.5205\u001b[0m  0.0001  0.3624\n",
      "     33            1.0000        \u001b[32m0.2120\u001b[0m       0.3507            0.3507        \u001b[94m1.5178\u001b[0m  0.0000  0.3626\n",
      "     34            1.0000        0.2522       0.3507            0.3507        \u001b[94m1.5157\u001b[0m  0.0000  0.3840\n",
      "     35            1.0000        0.2542       0.3507            0.3507        \u001b[94m1.5140\u001b[0m  0.0000  0.3746\n",
      "     36            1.0000        0.2354       0.3507            0.3507        \u001b[94m1.5131\u001b[0m  0.0000  0.4845\n",
      "     37            1.0000        0.2301       0.3507            0.3507        \u001b[94m1.5125\u001b[0m  0.0000  0.3582\n",
      "     38            1.0000        \u001b[32m0.2116\u001b[0m       0.3507            0.3507        \u001b[94m1.5120\u001b[0m  0.0000  0.3572\n",
      "     39            1.0000        0.2441       0.3542            0.3542        \u001b[94m1.5114\u001b[0m  0.0000  0.3700\n",
      "     40            1.0000        0.2286       0.3542            0.3542        \u001b[94m1.5109\u001b[0m  0.0000  0.3683\n",
      "Training model for subject 2 with 120 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2333\u001b[0m        \u001b[32m1.5016\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.5408\u001b[0m  0.0006  0.3946\n",
      "      2            \u001b[36m0.3333\u001b[0m        \u001b[32m1.2464\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m1.7660\u001b[0m  0.0006  0.3790\n",
      "      3            \u001b[36m0.5000\u001b[0m        \u001b[32m1.1684\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        1.8661  0.0006  0.3802\n",
      "      4            \u001b[36m0.5250\u001b[0m        \u001b[32m0.9408\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        1.9812  0.0006  0.3789\n",
      "      5            \u001b[36m0.5667\u001b[0m        \u001b[32m0.8286\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        1.8953  0.0006  0.3811\n",
      "      6            \u001b[36m0.6250\u001b[0m        \u001b[32m0.8049\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        1.7971  0.0006  0.3778\n",
      "      7            \u001b[36m0.6917\u001b[0m        \u001b[32m0.6926\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.6463\u001b[0m  0.0006  0.3702\n",
      "      8            \u001b[36m0.7083\u001b[0m        \u001b[32m0.6591\u001b[0m       0.3819            0.3819        \u001b[94m1.6123\u001b[0m  0.0006  0.4218\n",
      "      9            0.7083        \u001b[32m0.6122\u001b[0m       0.3785            0.3785        1.6334  0.0006  0.4379\n",
      "     10            \u001b[36m0.7417\u001b[0m        \u001b[32m0.5773\u001b[0m       0.3715            0.3715        1.6633  0.0005  0.4844\n",
      "     11            0.7417        \u001b[32m0.5065\u001b[0m       0.3646            0.3646        1.6900  0.0005  0.4214\n",
      "     12            \u001b[36m0.7667\u001b[0m        0.5244       0.3472            0.3472        1.7046  0.0005  0.4313\n",
      "     13            \u001b[36m0.7833\u001b[0m        \u001b[32m0.4568\u001b[0m       0.3611            0.3611        1.7188  0.0005  0.4589\n",
      "     14            \u001b[36m0.8167\u001b[0m        \u001b[32m0.4326\u001b[0m       0.3715            0.3715        1.7120  0.0005  0.4650\n",
      "     15            \u001b[36m0.8583\u001b[0m        \u001b[32m0.4263\u001b[0m       0.3681            0.3681        1.6912  0.0004  0.3856\n",
      "     16            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3682\u001b[0m       0.3819            0.3819        1.6596  0.0004  0.3970\n",
      "     17            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3578\u001b[0m       0.3750            0.3750        1.6398  0.0004  0.3809\n",
      "     18            \u001b[36m0.9583\u001b[0m        \u001b[32m0.3434\u001b[0m       0.3750            0.3750        1.6215  0.0004  0.3808\n",
      "     19            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3044\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        1.6124  0.0004  0.3810\n",
      "     20            \u001b[36m0.9750\u001b[0m        0.3533       0.3681            0.3681        \u001b[94m1.5972\u001b[0m  0.0003  0.3800\n",
      "     21            \u001b[36m0.9833\u001b[0m        \u001b[32m0.3006\u001b[0m       0.3715            0.3715        \u001b[94m1.5847\u001b[0m  0.0003  0.3975\n",
      "     22            0.9833        \u001b[32m0.2715\u001b[0m       0.3750            0.3750        \u001b[94m1.5704\u001b[0m  0.0003  0.3936\n",
      "     23            \u001b[36m0.9917\u001b[0m        0.2752       0.3785            0.3785        \u001b[94m1.5565\u001b[0m  0.0002  0.3699\n",
      "     24            \u001b[36m1.0000\u001b[0m        0.3070       0.3785            0.3785        \u001b[94m1.5442\u001b[0m  0.0002  0.3912\n",
      "     25            1.0000        0.3359       0.3854            0.3854        \u001b[94m1.5379\u001b[0m  0.0002  0.3912\n",
      "     26            1.0000        \u001b[32m0.2658\u001b[0m       0.3819            0.3819        \u001b[94m1.5304\u001b[0m  0.0002  0.3843\n",
      "     27            1.0000        \u001b[32m0.2358\u001b[0m       0.3854            0.3854        \u001b[94m1.5235\u001b[0m  0.0002  0.3753\n",
      "     28            1.0000        0.2408       0.3854            0.3854        \u001b[94m1.5196\u001b[0m  0.0001  0.3868\n",
      "     29            1.0000        0.2647       0.3785            0.3785        \u001b[94m1.5164\u001b[0m  0.0001  0.3839\n",
      "     30            1.0000        0.2577       0.3819            0.3819        \u001b[94m1.5129\u001b[0m  0.0001  0.3807\n",
      "     31            1.0000        0.2437       0.3785            0.3785        \u001b[94m1.5097\u001b[0m  0.0001  0.3898\n",
      "     32            1.0000        \u001b[32m0.2123\u001b[0m       0.3750            0.3750        \u001b[94m1.5066\u001b[0m  0.0001  0.3818\n",
      "     33            1.0000        0.2316       0.3819            0.3819        \u001b[94m1.5047\u001b[0m  0.0000  0.3810\n",
      "     34            1.0000        0.2458       0.3819            0.3819        \u001b[94m1.5028\u001b[0m  0.0000  0.3804\n",
      "     35            1.0000        0.2455       0.3785            0.3785        \u001b[94m1.5010\u001b[0m  0.0000  0.3790\n",
      "     36            1.0000        0.2238       0.3819            0.3819        \u001b[94m1.4997\u001b[0m  0.0000  0.3840\n",
      "     37            1.0000        0.2360       0.3785            0.3785        \u001b[94m1.4983\u001b[0m  0.0000  0.3770\n",
      "     38            1.0000        0.2212       0.3819            0.3819        \u001b[94m1.4971\u001b[0m  0.0000  0.3887\n",
      "     39            1.0000        0.2157       0.3854            0.3854        \u001b[94m1.4960\u001b[0m  0.0000  0.3866\n",
      "     40            1.0000        0.2571       0.3889            0.3889        \u001b[94m1.4950\u001b[0m  0.0000  0.3909\n",
      "Training model for subject 2 with 130 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2615\u001b[0m        \u001b[32m1.5392\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.5063\u001b[0m  0.0006  0.5082\n",
      "      2            \u001b[36m0.3385\u001b[0m        \u001b[32m1.2707\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m2.4956\u001b[0m  0.0006  0.4621\n",
      "      3            \u001b[36m0.4231\u001b[0m        \u001b[32m1.0921\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m2.2488\u001b[0m  0.0006  0.4832\n",
      "      4            0.4231        \u001b[32m1.0238\u001b[0m       0.2708            0.2708        \u001b[94m1.9605\u001b[0m  0.0006  0.4558\n",
      "      5            0.3615        \u001b[32m0.9061\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        1.9936  0.0006  0.4799\n",
      "      6            0.3231        0.9148       0.2882            0.2882        2.1980  0.0006  0.4821\n",
      "      7            0.3231        \u001b[32m0.7531\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        2.3925  0.0006  0.5018\n",
      "      8            0.3462        0.7677       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        2.4006  0.0006  0.3928\n",
      "      9            0.3769        \u001b[32m0.6887\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        2.3045  0.0006  0.4042\n",
      "     10            \u001b[36m0.4846\u001b[0m        \u001b[32m0.6201\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        2.1115  0.0005  0.3873\n",
      "     11            \u001b[36m0.6000\u001b[0m        \u001b[32m0.5344\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.9034\u001b[0m  0.0005  0.3829\n",
      "     12            \u001b[36m0.7077\u001b[0m        0.5476       0.3785            0.3785        \u001b[94m1.7780\u001b[0m  0.0005  0.3861\n",
      "     13            \u001b[36m0.8077\u001b[0m        \u001b[32m0.5287\u001b[0m       0.3681            0.3681        \u001b[94m1.6840\u001b[0m  0.0005  0.3937\n",
      "     14            \u001b[36m0.9154\u001b[0m        \u001b[32m0.5285\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.6222\u001b[0m  0.0005  0.4036\n",
      "     15            \u001b[36m0.9462\u001b[0m        \u001b[32m0.4181\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.5822\u001b[0m  0.0004  0.3874\n",
      "     16            \u001b[36m0.9692\u001b[0m        0.4608       0.3854            0.3854        \u001b[94m1.5535\u001b[0m  0.0004  0.3872\n",
      "     17            \u001b[36m0.9846\u001b[0m        0.4318       0.3854            0.3854        \u001b[94m1.5091\u001b[0m  0.0004  0.3997\n",
      "     18            0.9846        \u001b[32m0.3779\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4854\u001b[0m  0.0004  0.3806\n",
      "     19            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3601\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.4775\u001b[0m  0.0004  0.3826\n",
      "     20            1.0000        \u001b[32m0.3506\u001b[0m       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        1.4784  0.0003  0.3956\n",
      "     21            1.0000        \u001b[32m0.3437\u001b[0m       0.4167            0.4167        1.4833  0.0003  0.3966\n",
      "     22            1.0000        \u001b[32m0.3158\u001b[0m       0.4097            0.4097        1.4868  0.0003  0.3990\n",
      "     23            1.0000        \u001b[32m0.3142\u001b[0m       0.4167            0.4167        1.4854  0.0002  0.4125\n",
      "     24            1.0000        \u001b[32m0.3033\u001b[0m       0.4167            0.4167        1.4858  0.0002  0.3993\n",
      "     25            1.0000        \u001b[32m0.2966\u001b[0m       0.4062            0.4062        1.4877  0.0002  0.4105\n",
      "     26            1.0000        0.3019       0.4028            0.4028        1.4887  0.0002  0.3988\n",
      "     27            1.0000        0.3380       0.3993            0.3993        1.4890  0.0002  0.3798\n",
      "     28            1.0000        \u001b[32m0.2544\u001b[0m       0.4062            0.4062        1.4880  0.0001  0.3986\n",
      "     29            1.0000        0.2906       0.4132            0.4132        1.4859  0.0001  0.3966\n",
      "     30            1.0000        \u001b[32m0.2203\u001b[0m       0.4097            0.4097        1.4862  0.0001  0.3990\n",
      "     31            1.0000        0.3157       0.3993            0.3993        1.4862  0.0001  0.3959\n",
      "     32            1.0000        0.2701       0.4028            0.4028        1.4845  0.0001  0.3847\n",
      "     33            1.0000        0.2491       0.4028            0.4028        1.4832  0.0000  0.4401\n",
      "     34            1.0000        0.2711       0.4028            0.4028        1.4821  0.0000  0.4668\n",
      "     35            1.0000        0.2265       0.4062            0.4062        1.4807  0.0000  0.4631\n",
      "     36            1.0000        0.3159       0.4062            0.4062        1.4800  0.0000  0.4486\n",
      "     37            1.0000        0.2564       0.4097            0.4097        1.4795  0.0000  0.5110\n",
      "     38            1.0000        \u001b[32m0.2061\u001b[0m       0.4167            0.4167        1.4790  0.0000  0.4912\n",
      "     39            1.0000        0.2346       0.4167            0.4167        1.4785  0.0000  0.4478\n",
      "     40            1.0000        0.2520       0.4132            0.4132        1.4781  0.0000  0.3910\n",
      "Training model for subject 2 with 140 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3214\u001b[0m        \u001b[32m1.5901\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m3.4003\u001b[0m  0.0006  0.4007\n",
      "      2            \u001b[36m0.4214\u001b[0m        \u001b[32m1.2947\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m2.8299\u001b[0m  0.0006  0.4016\n",
      "      3            0.4071        \u001b[32m1.1903\u001b[0m       0.2882            0.2882        \u001b[94m2.6812\u001b[0m  0.0006  0.3873\n",
      "      4            \u001b[36m0.4857\u001b[0m        \u001b[32m0.9936\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m2.3395\u001b[0m  0.0006  0.3749\n",
      "      5            \u001b[36m0.5643\u001b[0m        \u001b[32m0.9221\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m2.2255\u001b[0m  0.0006  0.3885\n",
      "      6            0.5500        \u001b[32m0.8364\u001b[0m       0.3472            0.3472        2.2297  0.0006  0.3904\n",
      "      7            0.5143        \u001b[32m0.8107\u001b[0m       0.3403            0.3403        2.2856  0.0006  0.3751\n",
      "      8            0.5000        \u001b[32m0.7143\u001b[0m       0.3403            0.3403        2.2894  0.0006  0.3867\n",
      "      9            0.4929        0.7645       0.3438            0.3438        2.2712  0.0006  0.3889\n",
      "     10            0.5143        \u001b[32m0.6372\u001b[0m       0.3368            0.3368        \u001b[94m2.1618\u001b[0m  0.0005  0.3880\n",
      "     11            0.5286        \u001b[32m0.5461\u001b[0m       0.3438            0.3438        \u001b[94m2.0673\u001b[0m  0.0005  0.3872\n",
      "     12            \u001b[36m0.6000\u001b[0m        0.5642       0.3542            0.3542        \u001b[94m1.9555\u001b[0m  0.0005  0.3856\n",
      "     13            \u001b[36m0.6429\u001b[0m        \u001b[32m0.5335\u001b[0m       0.3646            0.3646        \u001b[94m1.8665\u001b[0m  0.0005  0.3850\n",
      "     14            \u001b[36m0.7286\u001b[0m        \u001b[32m0.4978\u001b[0m       0.3715            0.3715        \u001b[94m1.7536\u001b[0m  0.0005  0.5079\n",
      "     15            \u001b[36m0.8143\u001b[0m        \u001b[32m0.4511\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.6703\u001b[0m  0.0004  0.3880\n",
      "     16            \u001b[36m0.8500\u001b[0m        \u001b[32m0.4288\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.6239\u001b[0m  0.0004  0.3872\n",
      "     17            \u001b[36m0.9000\u001b[0m        0.4426       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.5775\u001b[0m  0.0004  0.4022\n",
      "     18            \u001b[36m0.9143\u001b[0m        0.4398       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.5450\u001b[0m  0.0004  0.3859\n",
      "     19            \u001b[36m0.9286\u001b[0m        \u001b[32m0.3736\u001b[0m       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.5122\u001b[0m  0.0004  0.3856\n",
      "     20            \u001b[36m0.9643\u001b[0m        0.3742       0.4097            0.4097        \u001b[94m1.4812\u001b[0m  0.0003  0.3865\n",
      "     21            \u001b[36m0.9714\u001b[0m        0.3827       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.4514\u001b[0m  0.0003  0.3831\n",
      "     22            \u001b[36m0.9929\u001b[0m        0.3961       0.4375            0.4375        \u001b[94m1.4364\u001b[0m  0.0003  0.4137\n",
      "     23            0.9929        \u001b[32m0.3459\u001b[0m       0.4271            0.4271        \u001b[94m1.4248\u001b[0m  0.0002  0.4236\n",
      "     24            0.9929        0.3463       0.4340            0.4340        \u001b[94m1.4181\u001b[0m  0.0002  0.4045\n",
      "     25            0.9929        0.3995       0.4340            0.4340        \u001b[94m1.4120\u001b[0m  0.0002  0.4772\n",
      "     26            \u001b[36m1.0000\u001b[0m        0.3470       0.4375            0.4375        \u001b[94m1.4069\u001b[0m  0.0002  0.4935\n",
      "     27            1.0000        \u001b[32m0.3092\u001b[0m       0.4410            0.4410        \u001b[94m1.4044\u001b[0m  0.0002  0.4728\n",
      "     28            1.0000        0.3478       0.4340            0.4340        \u001b[94m1.4017\u001b[0m  0.0001  0.5029\n",
      "     29            1.0000        \u001b[32m0.3072\u001b[0m       0.4340            0.4340        \u001b[94m1.3998\u001b[0m  0.0001  0.4688\n",
      "     30            1.0000        \u001b[32m0.3014\u001b[0m       0.4410            0.4410        \u001b[94m1.3975\u001b[0m  0.0001  0.4980\n",
      "     31            1.0000        \u001b[32m0.2853\u001b[0m       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.3946\u001b[0m  0.0001  0.3874\n",
      "     32            1.0000        \u001b[32m0.2680\u001b[0m       0.4375            0.4375        \u001b[94m1.3916\u001b[0m  0.0001  0.4146\n",
      "     33            1.0000        0.2783       0.4375            0.4375        \u001b[94m1.3899\u001b[0m  0.0000  0.3868\n",
      "     34            1.0000        0.2911       0.4410            0.4410        \u001b[94m1.3896\u001b[0m  0.0000  0.3989\n",
      "     35            1.0000        0.2797       0.4410            0.4410        \u001b[94m1.3886\u001b[0m  0.0000  0.3874\n",
      "     36            1.0000        \u001b[32m0.2373\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.3871\u001b[0m  0.0000  0.3985\n",
      "     37            1.0000        0.3116       0.4479            0.4479        \u001b[94m1.3868\u001b[0m  0.0000  0.3812\n",
      "     38            1.0000        0.2731       0.4479            0.4479        \u001b[94m1.3862\u001b[0m  0.0000  0.3987\n",
      "     39            1.0000        0.2976       0.4479            0.4479        \u001b[94m1.3855\u001b[0m  0.0000  0.3808\n",
      "     40            1.0000        0.2420       0.4479            0.4479        \u001b[94m1.3850\u001b[0m  0.0000  0.3956\n",
      "Training model for subject 2 with 150 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2533\u001b[0m        \u001b[32m1.5025\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.5784\u001b[0m  0.0006  0.3939\n",
      "      2            0.2467        \u001b[32m1.3380\u001b[0m       0.2500            0.2500        4.4236  0.0006  0.4132\n",
      "      3            0.2467        \u001b[32m1.1780\u001b[0m       0.2500            0.2500        4.7093  0.0006  0.3955\n",
      "      4            0.2467        \u001b[32m1.1116\u001b[0m       0.2500            0.2500        4.4035  0.0006  0.3817\n",
      "      5            \u001b[36m0.2800\u001b[0m        \u001b[32m1.0238\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        3.8483  0.0006  0.3974\n",
      "      6            \u001b[36m0.3400\u001b[0m        \u001b[32m0.8893\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m3.4885\u001b[0m  0.0006  0.3848\n",
      "      7            \u001b[36m0.4133\u001b[0m        \u001b[32m0.8784\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m3.2560\u001b[0m  0.0006  0.4252\n",
      "      8            \u001b[36m0.4667\u001b[0m        0.8845       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m3.0253\u001b[0m  0.0006  0.3846\n",
      "      9            \u001b[36m0.4867\u001b[0m        \u001b[32m0.8276\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m2.8638\u001b[0m  0.0006  0.4129\n",
      "     10            0.4800        \u001b[32m0.8101\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m2.7995\u001b[0m  0.0005  0.3819\n",
      "     11            0.4800        \u001b[32m0.7273\u001b[0m       0.3854            0.3854        \u001b[94m2.6277\u001b[0m  0.0005  0.3824\n",
      "     12            \u001b[36m0.5133\u001b[0m        \u001b[32m0.5858\u001b[0m       0.3854            0.3854        \u001b[94m2.4467\u001b[0m  0.0005  0.3973\n",
      "     13            \u001b[36m0.5667\u001b[0m        \u001b[32m0.5702\u001b[0m       0.3750            0.3750        \u001b[94m2.2225\u001b[0m  0.0005  0.3977\n",
      "     14            \u001b[36m0.6600\u001b[0m        0.6237       0.3819            0.3819        \u001b[94m2.0086\u001b[0m  0.0005  0.3813\n",
      "     15            \u001b[36m0.7267\u001b[0m        \u001b[32m0.5452\u001b[0m       0.3715            0.3715        \u001b[94m1.8374\u001b[0m  0.0004  0.3805\n",
      "     16            \u001b[36m0.8067\u001b[0m        0.5871       0.3924            0.3924        \u001b[94m1.7125\u001b[0m  0.0004  0.4465\n",
      "     17            \u001b[36m0.8333\u001b[0m        \u001b[32m0.5036\u001b[0m       0.3854            0.3854        \u001b[94m1.6542\u001b[0m  0.0004  0.4642\n",
      "     18            \u001b[36m0.8400\u001b[0m        \u001b[32m0.4652\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.6054\u001b[0m  0.0004  0.4826\n",
      "     19            \u001b[36m0.8800\u001b[0m        0.4922       0.3924            0.3924        \u001b[94m1.5764\u001b[0m  0.0004  0.5735\n",
      "     20            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3871\u001b[0m       0.3889            0.3889        \u001b[94m1.5378\u001b[0m  0.0003  0.4582\n",
      "     21            \u001b[36m0.9400\u001b[0m        0.4591       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.4929\u001b[0m  0.0003  0.4768\n",
      "     22            \u001b[36m0.9467\u001b[0m        \u001b[32m0.3604\u001b[0m       0.3993            0.3993        \u001b[94m1.4593\u001b[0m  0.0003  0.3834\n",
      "     23            \u001b[36m0.9600\u001b[0m        \u001b[32m0.3490\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.4306\u001b[0m  0.0002  0.3954\n",
      "     24            \u001b[36m0.9933\u001b[0m        0.3957       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.4066\u001b[0m  0.0002  0.4020\n",
      "     25            0.9933        0.3695       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.3894\u001b[0m  0.0002  0.3976\n",
      "     26            0.9933        0.4120       0.4062            0.4062        \u001b[94m1.3781\u001b[0m  0.0002  0.3806\n",
      "     27            0.9867        0.3755       0.4028            0.4028        \u001b[94m1.3732\u001b[0m  0.0002  0.3832\n",
      "     28            0.9867        \u001b[32m0.3242\u001b[0m       0.4167            0.4167        \u001b[94m1.3685\u001b[0m  0.0001  0.3978\n",
      "     29            0.9933        0.3406       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.3642\u001b[0m  0.0001  0.3811\n",
      "     30            0.9933        0.3255       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.3607\u001b[0m  0.0001  0.3822\n",
      "     31            0.9933        \u001b[32m0.3049\u001b[0m       0.4306            0.4306        \u001b[94m1.3571\u001b[0m  0.0001  0.3815\n",
      "     32            \u001b[36m1.0000\u001b[0m        0.3891       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.3547\u001b[0m  0.0001  0.4113\n",
      "     33            1.0000        0.3136       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.3527\u001b[0m  0.0000  0.3859\n",
      "     34            1.0000        0.3139       0.4410            0.4410        \u001b[94m1.3524\u001b[0m  0.0000  0.3995\n",
      "     35            1.0000        \u001b[32m0.2818\u001b[0m       0.4375            0.4375        \u001b[94m1.3523\u001b[0m  0.0000  0.3982\n",
      "     36            1.0000        0.3910       0.4410            0.4410        \u001b[94m1.3514\u001b[0m  0.0000  0.3969\n",
      "     37            1.0000        0.3395       0.4410            0.4410        \u001b[94m1.3507\u001b[0m  0.0000  0.3975\n",
      "     38            1.0000        \u001b[32m0.2730\u001b[0m       0.4410            0.4410        \u001b[94m1.3498\u001b[0m  0.0000  0.3990\n",
      "     39            1.0000        0.3002       0.4444            0.4444        \u001b[94m1.3493\u001b[0m  0.0000  0.3981\n",
      "     40            1.0000        0.3103       0.4444            0.4444        \u001b[94m1.3488\u001b[0m  0.0000  0.3974\n",
      "Training model for subject 2 with 160 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3625\u001b[0m        \u001b[32m1.6098\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m3.5517\u001b[0m  0.0006  0.3878\n",
      "      2            0.2875        \u001b[32m1.4059\u001b[0m       0.2812            0.2812        4.5644  0.0006  0.4019\n",
      "      3            0.3625        \u001b[32m1.2167\u001b[0m       0.3056            0.3056        4.3683  0.0006  0.3975\n",
      "      4            \u001b[36m0.4062\u001b[0m        \u001b[32m1.0054\u001b[0m       0.3194            0.3194        3.7004  0.0006  0.3834\n",
      "      5            0.3563        \u001b[32m0.9730\u001b[0m       0.2882            0.2882        \u001b[94m3.4868\u001b[0m  0.0006  0.3837\n",
      "      6            0.3500        1.0282       0.2812            0.2812        \u001b[94m3.3522\u001b[0m  0.0006  0.4054\n",
      "      7            0.3063        \u001b[32m0.8793\u001b[0m       0.2639            0.2639        3.4163  0.0006  0.4515\n",
      "      8            0.3063        0.8802       0.2604            0.2604        3.3612  0.0006  0.4714\n",
      "      9            0.3063        \u001b[32m0.8554\u001b[0m       0.2674            0.2674        \u001b[94m3.2209\u001b[0m  0.0006  0.4644\n",
      "     10            0.3000        \u001b[32m0.8334\u001b[0m       0.2674            0.2674        3.2352  0.0005  0.4513\n",
      "     11            0.3125        \u001b[32m0.7403\u001b[0m       0.2708            0.2708        \u001b[94m3.0232\u001b[0m  0.0005  0.4715\n",
      "     12            0.4062        \u001b[32m0.7393\u001b[0m       0.2917            0.2917        \u001b[94m2.6961\u001b[0m  0.0005  0.4763\n",
      "     13            \u001b[36m0.4813\u001b[0m        \u001b[32m0.6971\u001b[0m       0.3056            0.3056        \u001b[94m2.3865\u001b[0m  0.0005  0.3922\n",
      "     14            \u001b[36m0.5375\u001b[0m        \u001b[32m0.6632\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m2.1050\u001b[0m  0.0005  0.4118\n",
      "     15            \u001b[36m0.6438\u001b[0m        \u001b[32m0.6046\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.9099\u001b[0m  0.0004  0.4012\n",
      "     16            \u001b[36m0.7188\u001b[0m        \u001b[32m0.5912\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.7953\u001b[0m  0.0004  0.3817\n",
      "     17            \u001b[36m0.7375\u001b[0m        \u001b[32m0.5132\u001b[0m       0.3646            0.3646        \u001b[94m1.7271\u001b[0m  0.0004  0.3995\n",
      "     18            \u001b[36m0.8125\u001b[0m        0.5709       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.6742\u001b[0m  0.0004  0.4126\n",
      "     19            \u001b[36m0.8562\u001b[0m        \u001b[32m0.4936\u001b[0m       0.3785            0.3785        \u001b[94m1.6135\u001b[0m  0.0004  0.3963\n",
      "     20            \u001b[36m0.8875\u001b[0m        0.5365       0.3819            0.3819        \u001b[94m1.5646\u001b[0m  0.0003  0.4015\n",
      "     21            \u001b[36m0.9313\u001b[0m        0.5401       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.5292\u001b[0m  0.0003  0.4043\n",
      "     22            \u001b[36m0.9625\u001b[0m        \u001b[32m0.4551\u001b[0m       0.3750            0.3750        \u001b[94m1.4995\u001b[0m  0.0003  0.5033\n",
      "     23            \u001b[36m0.9688\u001b[0m        0.4576       0.3854            0.3854        \u001b[94m1.4786\u001b[0m  0.0002  0.4023\n",
      "     24            0.9688        0.4651       0.3854            0.3854        \u001b[94m1.4664\u001b[0m  0.0002  0.3968\n",
      "     25            0.9688        \u001b[32m0.4123\u001b[0m       0.3854            0.3854        \u001b[94m1.4571\u001b[0m  0.0002  0.3900\n",
      "     26            \u001b[36m0.9750\u001b[0m        0.4411       0.3924            0.3924        \u001b[94m1.4465\u001b[0m  0.0002  0.4035\n",
      "     27            \u001b[36m0.9812\u001b[0m        \u001b[32m0.4026\u001b[0m       0.3924            0.3924        \u001b[94m1.4373\u001b[0m  0.0002  0.4185\n",
      "     28            0.9812        \u001b[32m0.3459\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.4310\u001b[0m  0.0001  0.4093\n",
      "     29            0.9812        0.4054       0.3889            0.3889        \u001b[94m1.4262\u001b[0m  0.0001  0.4076\n",
      "     30            0.9812        0.3596       0.3889            0.3889        \u001b[94m1.4228\u001b[0m  0.0001  0.4034\n",
      "     31            \u001b[36m0.9875\u001b[0m        0.3752       0.3924            0.3924        \u001b[94m1.4215\u001b[0m  0.0001  0.4065\n",
      "     32            0.9812        0.4210       0.3924            0.3924        1.4217  0.0001  0.3913\n",
      "     33            0.9875        0.3922       0.3958            0.3958        \u001b[94m1.4201\u001b[0m  0.0000  0.4094\n",
      "     34            0.9875        0.3728       0.3993            0.3993        \u001b[94m1.4194\u001b[0m  0.0000  0.3846\n",
      "     35            0.9875        0.4065       0.3993            0.3993        \u001b[94m1.4189\u001b[0m  0.0000  0.3926\n",
      "     36            0.9875        0.4073       0.3993            0.3993        1.4190  0.0000  0.3867\n",
      "     37            0.9875        \u001b[32m0.3445\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.4176\u001b[0m  0.0000  0.4406\n",
      "     38            0.9875        0.3619       0.3958            0.3958        \u001b[94m1.4165\u001b[0m  0.0000  0.4645\n",
      "     39            0.9875        0.4098       0.3993            0.3993        \u001b[94m1.4161\u001b[0m  0.0000  0.4841\n",
      "     40            0.9875        0.4206       0.4028            0.4028        \u001b[94m1.4156\u001b[0m  0.0000  0.4490\n",
      "Training model for subject 2 with 170 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2588\u001b[0m        \u001b[32m1.7583\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m4.5462\u001b[0m  0.0006  0.4810\n",
      "      2            \u001b[36m0.3706\u001b[0m        \u001b[32m1.2937\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m3.5075\u001b[0m  0.0006  0.4686\n",
      "      3            0.2588        \u001b[32m1.2766\u001b[0m       0.2569            0.2569        \u001b[94m3.4170\u001b[0m  0.0006  0.4074\n",
      "      4            0.2471        \u001b[32m1.1265\u001b[0m       0.2500            0.2500        4.2025  0.0006  0.4005\n",
      "      5            0.2471        \u001b[32m0.9907\u001b[0m       0.2500            0.2500        4.9416  0.0006  0.3991\n",
      "      6            0.2471        \u001b[32m0.9208\u001b[0m       0.2500            0.2500        5.0931  0.0006  0.3984\n",
      "      7            0.2471        \u001b[32m0.8947\u001b[0m       0.2535            0.2535        4.6898  0.0006  0.5576\n",
      "      8            0.2647        0.9036       0.2569            0.2569        4.1086  0.0006  0.3933\n",
      "      9            0.3000        \u001b[32m0.8667\u001b[0m       0.2708            0.2708        3.4948  0.0006  0.3885\n",
      "     10            0.3529        \u001b[32m0.7819\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m2.9619\u001b[0m  0.0005  0.3867\n",
      "     11            \u001b[36m0.4353\u001b[0m        \u001b[32m0.7292\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m2.4598\u001b[0m  0.0005  0.3717\n",
      "     12            \u001b[36m0.5588\u001b[0m        \u001b[32m0.7026\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m2.0855\u001b[0m  0.0005  0.3744\n",
      "     13            \u001b[36m0.6353\u001b[0m        0.7062       0.3438            0.3438        \u001b[94m1.8884\u001b[0m  0.0005  0.4035\n",
      "     14            \u001b[36m0.6765\u001b[0m        0.7167       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.7617\u001b[0m  0.0005  0.3732\n",
      "     15            \u001b[36m0.7471\u001b[0m        \u001b[32m0.5895\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.6849\u001b[0m  0.0004  0.3872\n",
      "     16            0.7412        \u001b[32m0.5542\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.6440\u001b[0m  0.0004  0.4087\n",
      "     17            \u001b[36m0.7588\u001b[0m        \u001b[32m0.5393\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.5966\u001b[0m  0.0004  0.4044\n",
      "     18            \u001b[36m0.7882\u001b[0m        \u001b[32m0.4978\u001b[0m       0.3819            0.3819        \u001b[94m1.5621\u001b[0m  0.0004  0.3945\n",
      "     19            \u001b[36m0.8471\u001b[0m        0.5629       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.5055\u001b[0m  0.0004  0.3903\n",
      "     20            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4789\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.4531\u001b[0m  0.0003  0.4028\n",
      "     21            \u001b[36m0.9176\u001b[0m        0.4857       0.3924            0.3924        \u001b[94m1.4092\u001b[0m  0.0003  0.4034\n",
      "     22            \u001b[36m0.9235\u001b[0m        \u001b[32m0.4759\u001b[0m       0.3889            0.3889        \u001b[94m1.3768\u001b[0m  0.0003  0.3931\n",
      "     23            \u001b[36m0.9294\u001b[0m        \u001b[32m0.4207\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.3548\u001b[0m  0.0002  0.3904\n",
      "     24            0.9294        0.4647       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.3395\u001b[0m  0.0002  0.4074\n",
      "     25            \u001b[36m0.9471\u001b[0m        \u001b[32m0.4095\u001b[0m       0.4306            0.4306        \u001b[94m1.3306\u001b[0m  0.0002  0.3898\n",
      "     26            \u001b[36m0.9529\u001b[0m        0.4166       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.3261\u001b[0m  0.0002  0.4058\n",
      "     27            0.9529        0.4117       0.4375            0.4375        \u001b[94m1.3226\u001b[0m  0.0002  0.4383\n",
      "     28            \u001b[36m0.9588\u001b[0m        0.4273       0.4340            0.4340        \u001b[94m1.3225\u001b[0m  0.0001  0.5459\n",
      "     29            0.9588        0.4358       0.4410            0.4410        \u001b[94m1.3218\u001b[0m  0.0001  0.4711\n",
      "     30            0.9588        \u001b[32m0.3799\u001b[0m       0.4444            0.4444        \u001b[94m1.3210\u001b[0m  0.0001  0.4960\n",
      "     31            0.9588        \u001b[32m0.3678\u001b[0m       0.4375            0.4375        \u001b[94m1.3200\u001b[0m  0.0001  0.4723\n",
      "     32            0.9588        0.3740       0.4375            0.4375        \u001b[94m1.3186\u001b[0m  0.0001  0.4896\n",
      "     33            0.9588        0.4318       0.4375            0.4375        \u001b[94m1.3174\u001b[0m  0.0000  0.3901\n",
      "     34            0.9588        0.3886       0.4375            0.4375        \u001b[94m1.3163\u001b[0m  0.0000  0.4005\n",
      "     35            0.9529        \u001b[32m0.3651\u001b[0m       0.4375            0.4375        \u001b[94m1.3143\u001b[0m  0.0000  0.3974\n",
      "     36            0.9529        0.3970       0.4444            0.4444        \u001b[94m1.3128\u001b[0m  0.0000  0.4153\n",
      "     37            0.9529        0.4000       0.4444            0.4444        \u001b[94m1.3115\u001b[0m  0.0000  0.3979\n",
      "     38            0.9529        \u001b[32m0.3576\u001b[0m       0.4410            0.4410        \u001b[94m1.3103\u001b[0m  0.0000  0.4019\n",
      "     39            0.9529        0.4054       0.4444            0.4444        \u001b[94m1.3095\u001b[0m  0.0000  0.3967\n",
      "     40            0.9529        0.3736       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.3087\u001b[0m  0.0000  0.3982\n",
      "Training model for subject 2 with 180 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2722\u001b[0m        \u001b[32m1.5525\u001b[0m       \u001b[35m0.2431\u001b[0m            \u001b[31m0.2431\u001b[0m        \u001b[94m2.7221\u001b[0m  0.0006  0.4004\n",
      "      2            \u001b[36m0.3833\u001b[0m        \u001b[32m1.4410\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m1.8733\u001b[0m  0.0006  0.4027\n",
      "      3            0.3611        \u001b[32m1.2743\u001b[0m       0.2882            0.2882        1.8844  0.0006  0.3882\n",
      "      4            0.3389        \u001b[32m1.2217\u001b[0m       0.2431            0.2431        2.5379  0.0006  0.4145\n",
      "      5            0.3444        \u001b[32m1.0131\u001b[0m       0.2500            0.2500        2.9778  0.0006  0.3883\n",
      "      6            0.3000        1.0250       0.2569            0.2569        3.1655  0.0006  0.3927\n",
      "      7            0.2833        \u001b[32m0.9715\u001b[0m       0.2674            0.2674        3.3200  0.0006  0.3865\n",
      "      8            0.2889        \u001b[32m0.8837\u001b[0m       0.2708            0.2708        3.1210  0.0006  0.3901\n",
      "      9            0.2944        \u001b[32m0.8377\u001b[0m       0.2778            0.2778        2.7712  0.0006  0.3880\n",
      "     10            0.3222        \u001b[32m0.8131\u001b[0m       0.2743            0.2743        2.5000  0.0005  0.3759\n",
      "     11            0.3556        \u001b[32m0.7961\u001b[0m       0.2882            0.2882        2.2473  0.0005  0.3870\n",
      "     12            \u001b[36m0.4444\u001b[0m        \u001b[32m0.7784\u001b[0m       0.2812            0.2812        2.0208  0.0005  0.3943\n",
      "     13            \u001b[36m0.5389\u001b[0m        0.7933       0.2917            0.2917        \u001b[94m1.8127\u001b[0m  0.0005  0.3864\n",
      "     14            \u001b[36m0.6611\u001b[0m        \u001b[32m0.7016\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.6620\u001b[0m  0.0005  0.3946\n",
      "     15            \u001b[36m0.7500\u001b[0m        \u001b[32m0.6873\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.5478\u001b[0m  0.0004  0.4031\n",
      "     16            \u001b[36m0.7722\u001b[0m        \u001b[32m0.6545\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.5017\u001b[0m  0.0004  0.3936\n",
      "     17            \u001b[36m0.8111\u001b[0m        \u001b[32m0.5812\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.4607\u001b[0m  0.0004  0.4655\n",
      "     18            \u001b[36m0.8556\u001b[0m        0.5864       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.4233\u001b[0m  0.0004  0.4385\n",
      "     19            \u001b[36m0.8889\u001b[0m        \u001b[32m0.5086\u001b[0m       0.4167            0.4167        \u001b[94m1.3935\u001b[0m  0.0004  0.4798\n",
      "     20            \u001b[36m0.9000\u001b[0m        0.5295       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.3668\u001b[0m  0.0003  0.4525\n",
      "     21            \u001b[36m0.9111\u001b[0m        0.5269       0.4236            0.4236        \u001b[94m1.3463\u001b[0m  0.0003  0.4419\n",
      "     22            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4981\u001b[0m       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.3274\u001b[0m  0.0003  0.5001\n",
      "     23            \u001b[36m0.9500\u001b[0m        0.5220       0.4306            0.4306        \u001b[94m1.3128\u001b[0m  0.0002  0.4241\n",
      "     24            0.9389        \u001b[32m0.4661\u001b[0m       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.3017\u001b[0m  0.0002  0.4006\n",
      "     25            0.9444        0.4944       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.2928\u001b[0m  0.0002  0.3842\n",
      "     26            \u001b[36m0.9667\u001b[0m        0.4930       0.4549            0.4549        \u001b[94m1.2883\u001b[0m  0.0002  0.4009\n",
      "     27            \u001b[36m0.9722\u001b[0m        \u001b[32m0.4338\u001b[0m       0.4514            0.4514        \u001b[94m1.2842\u001b[0m  0.0002  0.3974\n",
      "     28            0.9667        0.4346       0.4479            0.4479        \u001b[94m1.2815\u001b[0m  0.0001  0.3986\n",
      "     29            0.9667        \u001b[32m0.4154\u001b[0m       0.4444            0.4444        \u001b[94m1.2799\u001b[0m  0.0001  0.3988\n",
      "     30            0.9722        \u001b[32m0.3630\u001b[0m       0.4514            0.4514        \u001b[94m1.2786\u001b[0m  0.0001  0.3851\n",
      "     31            0.9722        0.4272       0.4514            0.4514        \u001b[94m1.2772\u001b[0m  0.0001  0.3806\n",
      "     32            0.9722        0.4535       0.4549            0.4549        \u001b[94m1.2758\u001b[0m  0.0001  0.3838\n",
      "     33            0.9722        0.4258       0.4583            0.4583        \u001b[94m1.2748\u001b[0m  0.0000  0.4960\n",
      "     34            0.9722        0.4387       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2734\u001b[0m  0.0000  0.3947\n",
      "     35            0.9722        0.4008       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.2725\u001b[0m  0.0000  0.4056\n",
      "     36            0.9722        \u001b[32m0.3388\u001b[0m       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2714\u001b[0m  0.0000  0.3908\n",
      "     37            0.9667        0.4392       0.4688            0.4688        \u001b[94m1.2709\u001b[0m  0.0000  0.3907\n",
      "     38            0.9667        0.4302       0.4688            0.4688        \u001b[94m1.2706\u001b[0m  0.0000  0.3892\n",
      "     39            0.9667        0.4280       0.4653            0.4653        \u001b[94m1.2700\u001b[0m  0.0000  0.3931\n",
      "     40            0.9667        0.4620       0.4653            0.4653        \u001b[94m1.2698\u001b[0m  0.0000  0.3948\n",
      "Training model for subject 2 with 190 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2526\u001b[0m        \u001b[32m1.7805\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.9670\u001b[0m  0.0006  0.4016\n",
      "      2            \u001b[36m0.2579\u001b[0m        \u001b[32m1.4375\u001b[0m       0.2500            0.2500        \u001b[94m3.2371\u001b[0m  0.0006  0.3885\n",
      "      3            0.2579        \u001b[32m1.2969\u001b[0m       0.2500            0.2500        \u001b[94m2.9607\u001b[0m  0.0006  0.3843\n",
      "      4            \u001b[36m0.2632\u001b[0m        \u001b[32m1.1664\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m2.8580\u001b[0m  0.0006  0.3981\n",
      "      5            \u001b[36m0.3842\u001b[0m        \u001b[32m1.1144\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m2.6177\u001b[0m  0.0006  0.3831\n",
      "      6            \u001b[36m0.4842\u001b[0m        \u001b[32m1.0784\u001b[0m       0.2604            0.2604        \u001b[94m2.3810\u001b[0m  0.0006  0.3826\n",
      "      7            0.4842        \u001b[32m1.0420\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m2.2936\u001b[0m  0.0006  0.4454\n",
      "      8            0.4842        \u001b[32m0.8883\u001b[0m       0.2604            0.2604        \u001b[94m2.1441\u001b[0m  0.0006  0.5293\n",
      "      9            \u001b[36m0.5316\u001b[0m        \u001b[32m0.8289\u001b[0m       0.2535            0.2535        \u001b[94m1.9409\u001b[0m  0.0006  0.4752\n",
      "     10            \u001b[36m0.6000\u001b[0m        0.9121       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m1.7727\u001b[0m  0.0005  0.5183\n",
      "     11            \u001b[36m0.6316\u001b[0m        0.8572       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m1.6284\u001b[0m  0.0005  0.5957\n",
      "     12            \u001b[36m0.6895\u001b[0m        0.8480       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m1.5290\u001b[0m  0.0005  0.4026\n",
      "     13            \u001b[36m0.7579\u001b[0m        0.8653       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.4749\u001b[0m  0.0005  0.4032\n",
      "     14            \u001b[36m0.7789\u001b[0m        \u001b[32m0.7746\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.4603\u001b[0m  0.0005  0.3927\n",
      "     15            \u001b[36m0.7842\u001b[0m        \u001b[32m0.7481\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        1.4650  0.0004  0.3891\n",
      "     16            \u001b[36m0.8158\u001b[0m        \u001b[32m0.6505\u001b[0m       0.3611            0.3611        1.4668  0.0004  0.4048\n",
      "     17            0.8158        0.6885       0.3715            0.3715        1.4627  0.0004  0.3935\n",
      "     18            \u001b[36m0.8474\u001b[0m        \u001b[32m0.6276\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.4334\u001b[0m  0.0004  0.4032\n",
      "     19            \u001b[36m0.8737\u001b[0m        0.6759       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.3921\u001b[0m  0.0004  0.3821\n",
      "     20            \u001b[36m0.8789\u001b[0m        \u001b[32m0.6246\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.3542\u001b[0m  0.0003  0.4045\n",
      "     21            \u001b[36m0.9105\u001b[0m        \u001b[32m0.5929\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.3241\u001b[0m  0.0003  0.3980\n",
      "     22            \u001b[36m0.9158\u001b[0m        \u001b[32m0.5857\u001b[0m       0.4132            0.4132        \u001b[94m1.3082\u001b[0m  0.0003  0.3995\n",
      "     23            \u001b[36m0.9263\u001b[0m        \u001b[32m0.5352\u001b[0m       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.2985\u001b[0m  0.0002  0.3948\n",
      "     24            0.9263        0.5824       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.2904\u001b[0m  0.0002  0.4017\n",
      "     25            \u001b[36m0.9368\u001b[0m        \u001b[32m0.4895\u001b[0m       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.2841\u001b[0m  0.0002  0.3999\n",
      "     26            0.9368        0.6522       0.4306            0.4306        \u001b[94m1.2810\u001b[0m  0.0002  0.3992\n",
      "     27            \u001b[36m0.9421\u001b[0m        0.5291       0.4306            0.4306        \u001b[94m1.2790\u001b[0m  0.0002  0.3826\n",
      "     28            0.9368        \u001b[32m0.4773\u001b[0m       0.4306            0.4306        \u001b[94m1.2766\u001b[0m  0.0001  0.4017\n",
      "     29            \u001b[36m0.9526\u001b[0m        0.5543       0.4271            0.4271        \u001b[94m1.2732\u001b[0m  0.0001  0.3973\n",
      "     30            0.9526        0.4864       0.4271            0.4271        \u001b[94m1.2701\u001b[0m  0.0001  0.3942\n",
      "     31            0.9526        0.5237       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.2662\u001b[0m  0.0001  0.4067\n",
      "     32            0.9474        \u001b[32m0.4368\u001b[0m       0.4306            0.4306        \u001b[94m1.2628\u001b[0m  0.0001  0.4021\n",
      "     33            0.9474        0.4999       0.4340            0.4340        \u001b[94m1.2607\u001b[0m  0.0000  0.4876\n",
      "     34            0.9474        0.5368       0.4306            0.4306        \u001b[94m1.2589\u001b[0m  0.0000  0.3896\n",
      "     35            0.9474        0.5420       0.4306            0.4306        \u001b[94m1.2584\u001b[0m  0.0000  0.3925\n",
      "     36            0.9474        0.5291       0.4340            0.4340        \u001b[94m1.2578\u001b[0m  0.0000  0.4538\n",
      "     37            0.9474        0.4720       0.4340            0.4340        \u001b[94m1.2575\u001b[0m  0.0000  0.4684\n",
      "     38            0.9474        0.4614       0.4340            0.4340        \u001b[94m1.2572\u001b[0m  0.0000  0.4678\n",
      "     39            0.9474        0.4715       0.4340            0.4340        \u001b[94m1.2567\u001b[0m  0.0000  0.4634\n",
      "     40            0.9474        0.4700       0.4340            0.4340        \u001b[94m1.2566\u001b[0m  0.0000  0.4746\n",
      "Training model for subject 2 with 200 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.5847\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.0374\u001b[0m  0.0006  0.6192\n",
      "      2            0.2500        \u001b[32m1.3106\u001b[0m       0.2500            0.2500        \u001b[94m2.7259\u001b[0m  0.0006  0.4937\n",
      "      3            \u001b[36m0.3950\u001b[0m        \u001b[32m1.1568\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m2.0095\u001b[0m  0.0006  0.5080\n",
      "      4            \u001b[36m0.4800\u001b[0m        \u001b[32m1.0115\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m1.8422\u001b[0m  0.0006  0.4919\n",
      "      5            \u001b[36m0.5600\u001b[0m        \u001b[32m0.8725\u001b[0m       0.2847            0.2847        \u001b[94m1.7008\u001b[0m  0.0006  0.4844\n",
      "      6            \u001b[36m0.5750\u001b[0m        \u001b[32m0.8058\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        1.7344  0.0006  0.5071\n",
      "      7            \u001b[36m0.6400\u001b[0m        0.8215       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        \u001b[94m1.6182\u001b[0m  0.0006  0.5123\n",
      "      8            \u001b[36m0.7200\u001b[0m        \u001b[32m0.7545\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.4754\u001b[0m  0.0006  0.4893\n",
      "      9            \u001b[36m0.8100\u001b[0m        \u001b[32m0.7241\u001b[0m       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.3669\u001b[0m  0.0006  0.5017\n",
      "     10            \u001b[36m0.8650\u001b[0m        \u001b[32m0.5868\u001b[0m       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.2937\u001b[0m  0.0005  0.4979\n",
      "     11            \u001b[36m0.8950\u001b[0m        0.6372       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.2538\u001b[0m  0.0005  0.5057\n",
      "     12            0.8750        \u001b[32m0.5710\u001b[0m       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2377\u001b[0m  0.0005  0.4929\n",
      "     13            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5335\u001b[0m       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        \u001b[94m1.2203\u001b[0m  0.0005  0.5004\n",
      "     14            \u001b[36m0.9250\u001b[0m        \u001b[32m0.5087\u001b[0m       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        \u001b[94m1.2084\u001b[0m  0.0005  0.5006\n",
      "     15            \u001b[36m0.9600\u001b[0m        \u001b[32m0.4776\u001b[0m       0.4722            0.4722        \u001b[94m1.2058\u001b[0m  0.0004  0.5035\n",
      "     16            0.9600        0.4931       0.4757            0.4757        \u001b[94m1.1998\u001b[0m  0.0004  0.5009\n",
      "     17            \u001b[36m0.9750\u001b[0m        \u001b[32m0.4423\u001b[0m       0.4757            0.4757        \u001b[94m1.1924\u001b[0m  0.0004  0.4994\n",
      "     18            \u001b[36m0.9800\u001b[0m        \u001b[32m0.4145\u001b[0m       0.4896            0.4896        \u001b[94m1.1862\u001b[0m  0.0004  0.4957\n",
      "     19            \u001b[36m0.9850\u001b[0m        \u001b[32m0.3760\u001b[0m       \u001b[35m0.4931\u001b[0m            \u001b[31m0.4931\u001b[0m        1.1875  0.0004  0.5047\n",
      "     20            0.9850        0.3872       \u001b[35m0.5069\u001b[0m            \u001b[31m0.5069\u001b[0m        1.1897  0.0003  0.4969\n",
      "     21            0.9850        \u001b[32m0.3688\u001b[0m       0.4965            0.4965        1.1887  0.0003  0.5272\n",
      "     22            \u001b[36m0.9900\u001b[0m        \u001b[32m0.3354\u001b[0m       0.5000            0.5000        1.1902  0.0003  0.5672\n",
      "     23            0.9900        0.3531       0.5069            0.5069        1.1890  0.0002  0.6110\n",
      "     24            \u001b[36m0.9950\u001b[0m        0.3596       0.5035            0.5035        1.1864  0.0002  0.6684\n",
      "     25            0.9950        \u001b[32m0.3262\u001b[0m       0.5069            0.5069        \u001b[94m1.1838\u001b[0m  0.0002  0.6452\n",
      "     26            \u001b[36m1.0000\u001b[0m        0.3397       \u001b[35m0.5139\u001b[0m            \u001b[31m0.5139\u001b[0m        \u001b[94m1.1826\u001b[0m  0.0002  0.5305\n",
      "     27            1.0000        \u001b[32m0.2534\u001b[0m       0.5069            0.5069        1.1836  0.0002  0.5302\n",
      "     28            1.0000        0.2626       0.5104            0.5104        \u001b[94m1.1825\u001b[0m  0.0001  0.5418\n",
      "     29            1.0000        0.3047       \u001b[35m0.5174\u001b[0m            \u001b[31m0.5174\u001b[0m        \u001b[94m1.1824\u001b[0m  0.0001  0.5310\n",
      "     30            1.0000        0.3195       0.5104            0.5104        \u001b[94m1.1792\u001b[0m  0.0001  0.6424\n",
      "     31            1.0000        0.2690       0.5139            0.5139        \u001b[94m1.1767\u001b[0m  0.0001  0.5322\n",
      "     32            1.0000        0.2881       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        \u001b[94m1.1735\u001b[0m  0.0001  0.4989\n",
      "     33            1.0000        0.3071       0.5139            0.5139        \u001b[94m1.1715\u001b[0m  0.0000  0.5356\n",
      "     34            1.0000        0.3351       0.5174            0.5174        \u001b[94m1.1694\u001b[0m  0.0000  0.5350\n",
      "     35            1.0000        0.2944       0.5139            0.5139        \u001b[94m1.1680\u001b[0m  0.0000  0.5502\n",
      "     36            1.0000        0.2985       0.5139            0.5139        \u001b[94m1.1674\u001b[0m  0.0000  0.5452\n",
      "     37            1.0000        \u001b[32m0.2302\u001b[0m       0.5104            0.5104        \u001b[94m1.1671\u001b[0m  0.0000  0.5147\n",
      "     38            1.0000        0.2734       0.5104            0.5104        \u001b[94m1.1670\u001b[0m  0.0000  0.5247\n",
      "     39            1.0000        0.2769       0.5104            0.5104        \u001b[94m1.1669\u001b[0m  0.0000  0.6256\n",
      "     40            1.0000        0.3021       0.5104            0.5104        \u001b[94m1.1669\u001b[0m  0.0000  0.5284\n",
      "Training model for subject 2 with 210 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2667\u001b[0m        \u001b[32m1.6556\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m2.7123\u001b[0m  0.0006  0.5095\n",
      "      2            0.2524        \u001b[32m1.3626\u001b[0m       0.2500            0.2500        \u001b[94m2.5728\u001b[0m  0.0006  0.6030\n",
      "      3            \u001b[36m0.3238\u001b[0m        \u001b[32m1.1783\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        \u001b[94m2.2232\u001b[0m  0.0006  0.4966\n",
      "      4            \u001b[36m0.3571\u001b[0m        \u001b[32m1.1095\u001b[0m       0.2743            0.2743        \u001b[94m1.9819\u001b[0m  0.0006  0.5974\n",
      "      5            \u001b[36m0.5333\u001b[0m        \u001b[32m0.9894\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        2.0300  0.0006  0.5992\n",
      "      6            0.5190        \u001b[32m0.9300\u001b[0m       0.2708            0.2708        2.1825  0.0006  0.5905\n",
      "      7            \u001b[36m0.5714\u001b[0m        \u001b[32m0.8425\u001b[0m       0.2604            0.2604        2.0061  0.0006  0.6589\n",
      "      8            \u001b[36m0.6286\u001b[0m        0.8768       0.2708            0.2708        \u001b[94m1.8077\u001b[0m  0.0006  0.6451\n",
      "      9            \u001b[36m0.7000\u001b[0m        \u001b[32m0.8333\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m1.6527\u001b[0m  0.0006  0.6339\n",
      "     10            \u001b[36m0.7476\u001b[0m        \u001b[32m0.7772\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.5501\u001b[0m  0.0005  0.5419\n",
      "     11            \u001b[36m0.8143\u001b[0m        \u001b[32m0.7363\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.4508\u001b[0m  0.0005  0.5251\n",
      "     12            \u001b[36m0.8714\u001b[0m        \u001b[32m0.6636\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.3929\u001b[0m  0.0005  0.6165\n",
      "     13            \u001b[36m0.8857\u001b[0m        \u001b[32m0.6286\u001b[0m       0.3785            0.3785        \u001b[94m1.3801\u001b[0m  0.0005  0.5122\n",
      "     14            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5507\u001b[0m       0.3715            0.3715        \u001b[94m1.3609\u001b[0m  0.0005  0.4881\n",
      "     15            \u001b[36m0.9190\u001b[0m        0.6708       0.3819            0.3819        \u001b[94m1.3390\u001b[0m  0.0004  0.5257\n",
      "     16            \u001b[36m0.9381\u001b[0m        0.5591       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.3066\u001b[0m  0.0004  0.5565\n",
      "     17            \u001b[36m0.9524\u001b[0m        \u001b[32m0.5366\u001b[0m       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.2828\u001b[0m  0.0004  0.5772\n",
      "     18            0.9429        \u001b[32m0.5152\u001b[0m       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.2689\u001b[0m  0.0004  0.5316\n",
      "     19            0.9429        \u001b[32m0.5016\u001b[0m       0.4479            0.4479        \u001b[94m1.2591\u001b[0m  0.0004  0.5020\n",
      "     20            0.9524        \u001b[32m0.4867\u001b[0m       0.4410            0.4410        \u001b[94m1.2570\u001b[0m  0.0003  0.5009\n",
      "     21            \u001b[36m0.9619\u001b[0m        \u001b[32m0.4318\u001b[0m       0.4514            0.4514        1.2615  0.0003  0.5262\n",
      "     22            0.9619        \u001b[32m0.3929\u001b[0m       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        1.2585  0.0003  0.6081\n",
      "     23            \u001b[36m0.9667\u001b[0m        0.4863       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2514\u001b[0m  0.0002  0.5236\n",
      "     24            0.9667        0.3988       0.4653            0.4653        \u001b[94m1.2430\u001b[0m  0.0002  0.5579\n",
      "     25            0.9619        0.4257       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        \u001b[94m1.2341\u001b[0m  0.0002  0.6381\n",
      "     26            0.9667        0.3966       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        \u001b[94m1.2306\u001b[0m  0.0002  1.2590\n",
      "     27            0.9667        \u001b[32m0.3913\u001b[0m       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        \u001b[94m1.2267\u001b[0m  0.0002  1.1627\n",
      "     28            \u001b[36m0.9714\u001b[0m        \u001b[32m0.3844\u001b[0m       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        \u001b[94m1.2241\u001b[0m  0.0001  0.6696\n",
      "     29            \u001b[36m0.9762\u001b[0m        \u001b[32m0.3763\u001b[0m       0.4826            0.4826        \u001b[94m1.2203\u001b[0m  0.0001  0.7147\n",
      "     30            \u001b[36m0.9810\u001b[0m        0.4129       0.4792            0.4792        \u001b[94m1.2179\u001b[0m  0.0001  0.4925\n",
      "     31            0.9810        \u001b[32m0.2970\u001b[0m       0.4826            0.4826        \u001b[94m1.2166\u001b[0m  0.0001  0.5297\n",
      "     32            0.9810        0.3604       0.4826            0.4826        \u001b[94m1.2154\u001b[0m  0.0001  0.5435\n",
      "     33            0.9810        0.3206       0.4792            0.4792        \u001b[94m1.2143\u001b[0m  0.0000  0.5350\n",
      "     34            \u001b[36m0.9857\u001b[0m        0.3607       0.4792            0.4792        \u001b[94m1.2133\u001b[0m  0.0000  0.5151\n",
      "     35            \u001b[36m0.9905\u001b[0m        0.3371       0.4826            0.4826        \u001b[94m1.2127\u001b[0m  0.0000  0.5137\n",
      "     36            0.9905        0.3513       0.4792            0.4792        \u001b[94m1.2121\u001b[0m  0.0000  0.8997\n",
      "     37            0.9905        0.3099       0.4792            0.4792        \u001b[94m1.2120\u001b[0m  0.0000  0.6080\n",
      "     38            0.9905        0.3623       0.4792            0.4792        \u001b[94m1.2117\u001b[0m  0.0000  0.6164\n",
      "     39            0.9905        0.3538       0.4792            0.4792        \u001b[94m1.2115\u001b[0m  0.0000  0.7112\n",
      "     40            0.9905        0.3537       0.4792            0.4792        1.2116  0.0000  0.9206\n",
      "Training model for subject 2 with 220 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2455\u001b[0m        \u001b[32m1.6208\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m2.1816\u001b[0m  0.0006  0.6560\n",
      "      2            \u001b[36m0.3455\u001b[0m        \u001b[32m1.3692\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        \u001b[94m1.6482\u001b[0m  0.0006  0.5457\n",
      "      3            \u001b[36m0.5045\u001b[0m        \u001b[32m1.2031\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.5155\u001b[0m  0.0006  0.6275\n",
      "      4            \u001b[36m0.5636\u001b[0m        \u001b[32m1.0433\u001b[0m       0.3194            0.3194        1.5264  0.0006  0.6688\n",
      "      5            0.4636        \u001b[32m1.0039\u001b[0m       0.3021            0.3021        1.7568  0.0006  0.8327\n",
      "      6            0.4273        \u001b[32m0.9062\u001b[0m       0.2917            0.2917        1.8154  0.0006  0.6460\n",
      "      7            0.5136        \u001b[32m0.8115\u001b[0m       0.3160            0.3160        1.6416  0.0006  0.7767\n",
      "      8            \u001b[36m0.6409\u001b[0m        0.9130       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4465\u001b[0m  0.0006  0.9538\n",
      "      9            \u001b[36m0.7636\u001b[0m        \u001b[32m0.7627\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.3193\u001b[0m  0.0006  0.5044\n",
      "     10            \u001b[36m0.8864\u001b[0m        \u001b[32m0.6981\u001b[0m       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.2674\u001b[0m  0.0005  0.5313\n",
      "     11            \u001b[36m0.8955\u001b[0m        \u001b[32m0.6216\u001b[0m       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.2574\u001b[0m  0.0005  0.6485\n",
      "     12            \u001b[36m0.9000\u001b[0m        0.7766       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2571\u001b[0m  0.0005  0.5000\n",
      "     13            \u001b[36m0.9318\u001b[0m        \u001b[32m0.5481\u001b[0m       0.4410            0.4410        \u001b[94m1.2474\u001b[0m  0.0005  0.7394\n",
      "     14            0.9318        0.6046       0.4236            0.4236        1.2498  0.0005  0.6541\n",
      "     15            \u001b[36m0.9409\u001b[0m        \u001b[32m0.5173\u001b[0m       0.4340            0.4340        \u001b[94m1.2397\u001b[0m  0.0004  0.5039\n",
      "     16            \u001b[36m0.9545\u001b[0m        \u001b[32m0.4811\u001b[0m       0.4514            0.4514        \u001b[94m1.2163\u001b[0m  0.0004  0.5063\n",
      "     17            \u001b[36m0.9636\u001b[0m        0.4935       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        \u001b[94m1.1976\u001b[0m  0.0004  0.6950\n",
      "     18            0.9636        \u001b[32m0.4297\u001b[0m       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        \u001b[94m1.1923\u001b[0m  0.0004  0.6914\n",
      "     19            \u001b[36m0.9682\u001b[0m        0.4891       0.4861            0.4861        1.1982  0.0004  0.4865\n",
      "     20            \u001b[36m0.9818\u001b[0m        \u001b[32m0.4283\u001b[0m       0.4861            0.4861        1.2030  0.0003  0.6454\n",
      "     21            0.9818        \u001b[32m0.4029\u001b[0m       0.4792            0.4792        1.2034  0.0003  0.5000\n",
      "     22            \u001b[36m0.9864\u001b[0m        \u001b[32m0.3674\u001b[0m       0.4722            0.4722        1.2050  0.0003  0.5000\n",
      "     23            0.9864        0.4033       0.4792            0.4792        1.2007  0.0002  0.4852\n",
      "     24            0.9864        0.3685       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        1.1958  0.0002  0.5012\n",
      "     25            0.9864        0.3691       0.4826            0.4826        \u001b[94m1.1883\u001b[0m  0.0002  0.9603\n",
      "     26            \u001b[36m0.9909\u001b[0m        \u001b[32m0.3281\u001b[0m       0.4896            0.4896        \u001b[94m1.1805\u001b[0m  0.0002  0.7327\n",
      "     27            \u001b[36m1.0000\u001b[0m        0.3746       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        \u001b[94m1.1788\u001b[0m  0.0002  0.7322\n",
      "     28            1.0000        \u001b[32m0.3039\u001b[0m       \u001b[35m0.5000\u001b[0m            \u001b[31m0.5000\u001b[0m        \u001b[94m1.1778\u001b[0m  0.0001  0.5561\n",
      "     29            1.0000        0.3157       0.4965            0.4965        \u001b[94m1.1767\u001b[0m  0.0001  0.4992\n",
      "     30            1.0000        0.3428       0.4965            0.4965        \u001b[94m1.1764\u001b[0m  0.0001  0.6046\n",
      "     31            1.0000        \u001b[32m0.2898\u001b[0m       0.4965            0.4965        1.1770  0.0001  0.4847\n",
      "     32            1.0000        0.3287       0.5000            0.5000        1.1773  0.0001  0.5045\n",
      "     33            1.0000        0.3081       0.5000            0.5000        1.1771  0.0000  0.5008\n",
      "     34            1.0000        0.3060       \u001b[35m0.5035\u001b[0m            \u001b[31m0.5035\u001b[0m        1.1765  0.0000  0.7433\n",
      "     35            1.0000        \u001b[32m0.2814\u001b[0m       0.5035            0.5035        \u001b[94m1.1761\u001b[0m  0.0000  0.5357\n",
      "     36            1.0000        0.2976       0.5035            0.5035        1.1762  0.0000  0.5000\n",
      "     37            1.0000        0.2943       0.5035            0.5035        \u001b[94m1.1758\u001b[0m  0.0000  0.5707\n",
      "     38            1.0000        0.3166       0.5035            0.5035        \u001b[94m1.1757\u001b[0m  0.0000  0.5176\n",
      "     39            1.0000        0.3016       \u001b[35m0.5069\u001b[0m            \u001b[31m0.5069\u001b[0m        1.1758  0.0000  0.5436\n",
      "     40            1.0000        0.2957       0.5035            0.5035        \u001b[94m1.1755\u001b[0m  0.0000  0.6648\n",
      "Training model for subject 2 with 230 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3174\u001b[0m        \u001b[32m1.7129\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m3.2313\u001b[0m  0.0006  0.6660\n",
      "      2            0.2565        \u001b[32m1.3677\u001b[0m       0.2535            0.2535        3.6225  0.0006  0.5313\n",
      "      3            \u001b[36m0.4000\u001b[0m        \u001b[32m1.2359\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m2.9674\u001b[0m  0.0006  0.5002\n",
      "      4            0.3000        \u001b[32m1.0731\u001b[0m       0.2778            0.2778        \u001b[94m2.6186\u001b[0m  0.0006  0.5996\n",
      "      5            0.2696        \u001b[32m1.0394\u001b[0m       0.2500            0.2500        2.6527  0.0006  0.7275\n",
      "      6            0.2696        \u001b[32m0.9142\u001b[0m       0.2500            0.2500        2.6530  0.0006  0.6721\n",
      "      7            0.3261        1.0184       0.2847            0.2847        \u001b[94m2.3568\u001b[0m  0.0006  0.7622\n",
      "      8            \u001b[36m0.4217\u001b[0m        \u001b[32m0.8465\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m2.0439\u001b[0m  0.0006  0.7387\n",
      "      9            \u001b[36m0.5478\u001b[0m        \u001b[32m0.7995\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m1.7754\u001b[0m  0.0006  0.6459\n",
      "     10            \u001b[36m0.6130\u001b[0m        \u001b[32m0.7684\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.5898\u001b[0m  0.0005  0.5156\n",
      "     11            \u001b[36m0.7261\u001b[0m        \u001b[32m0.7161\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.4770\u001b[0m  0.0005  0.5779\n",
      "     12            \u001b[36m0.7913\u001b[0m        0.7257       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.4172\u001b[0m  0.0005  0.5000\n",
      "     13            \u001b[36m0.8304\u001b[0m        \u001b[32m0.6937\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.3927\u001b[0m  0.0005  0.5000\n",
      "     14            \u001b[36m0.8826\u001b[0m        \u001b[32m0.6835\u001b[0m       0.3993            0.3993        \u001b[94m1.3681\u001b[0m  0.0005  0.5051\n",
      "     15            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6168\u001b[0m       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.3383\u001b[0m  0.0004  0.5349\n",
      "     16            \u001b[36m0.9261\u001b[0m        \u001b[32m0.5740\u001b[0m       0.4236            0.4236        \u001b[94m1.3167\u001b[0m  0.0004  0.5160\n",
      "     17            0.9043        \u001b[32m0.5477\u001b[0m       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.2997\u001b[0m  0.0004  0.5149\n",
      "     18            \u001b[36m0.9348\u001b[0m        0.5709       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.2869\u001b[0m  0.0004  0.6154\n",
      "     19            \u001b[36m0.9435\u001b[0m        0.5581       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.2844\u001b[0m  0.0004  0.5174\n",
      "     20            \u001b[36m0.9522\u001b[0m        \u001b[32m0.5299\u001b[0m       0.4410            0.4410        1.2882  0.0003  0.6171\n",
      "     21            \u001b[36m0.9739\u001b[0m        \u001b[32m0.4553\u001b[0m       0.4306            0.4306        1.2953  0.0003  0.6368\n",
      "     22            0.9739        \u001b[32m0.4447\u001b[0m       0.4271            0.4271        1.2939  0.0003  0.5098\n",
      "     23            \u001b[36m0.9783\u001b[0m        \u001b[32m0.4230\u001b[0m       0.4271            0.4271        1.2912  0.0002  0.5096\n",
      "     24            \u001b[36m0.9826\u001b[0m        \u001b[32m0.3834\u001b[0m       0.4375            0.4375        1.2917  0.0002  0.5180\n",
      "     25            \u001b[36m0.9870\u001b[0m        0.4352       0.4410            0.4410        1.2929  0.0002  0.6784\n",
      "     26            0.9870        0.3977       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        1.2886  0.0002  0.7351\n",
      "     27            0.9870        0.4235       0.4479            0.4479        \u001b[94m1.2831\u001b[0m  0.0002  0.7127\n",
      "     28            0.9870        0.4059       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2788\u001b[0m  0.0001  0.7333\n",
      "     29            0.9870        \u001b[32m0.3748\u001b[0m       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.2764\u001b[0m  0.0001  0.7202\n",
      "     30            0.9870        \u001b[32m0.3690\u001b[0m       0.4653            0.4653        \u001b[94m1.2743\u001b[0m  0.0001  0.5327\n",
      "     31            0.9826        \u001b[32m0.3379\u001b[0m       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2729\u001b[0m  0.0001  0.5326\n",
      "     32            0.9870        0.3933       0.4653            0.4653        \u001b[94m1.2721\u001b[0m  0.0001  0.5992\n",
      "     33            \u001b[36m0.9957\u001b[0m        0.3566       0.4653            0.4653        \u001b[94m1.2716\u001b[0m  0.0000  0.6574\n",
      "     34            0.9957        \u001b[32m0.3178\u001b[0m       0.4688            0.4688        \u001b[94m1.2715\u001b[0m  0.0000  0.6505\n",
      "     35            0.9957        0.3449       \u001b[35m0.4722\u001b[0m            \u001b[31m0.4722\u001b[0m        \u001b[94m1.2710\u001b[0m  0.0000  0.4844\n",
      "     36            0.9957        0.3345       0.4722            0.4722        \u001b[94m1.2710\u001b[0m  0.0000  0.5001\n",
      "     37            0.9957        0.3846       0.4722            0.4722        \u001b[94m1.2707\u001b[0m  0.0000  0.4848\n",
      "     38            0.9957        0.3610       0.4722            0.4722        \u001b[94m1.2706\u001b[0m  0.0000  0.5046\n",
      "     39            0.9957        0.4378       0.4722            0.4722        1.2706  0.0000  0.4968\n",
      "     40            0.9957        0.3765       0.4722            0.4722        \u001b[94m1.2702\u001b[0m  0.0000  0.5259\n",
      "Training model for subject 2 with 240 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2583\u001b[0m        \u001b[32m1.6805\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m2.5597\u001b[0m  0.0006  0.4892\n",
      "      2            \u001b[36m0.4125\u001b[0m        \u001b[32m1.3845\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m1.8384\u001b[0m  0.0006  0.4916\n",
      "      3            \u001b[36m0.4667\u001b[0m        \u001b[32m1.1723\u001b[0m       0.2847            0.2847        \u001b[94m1.7561\u001b[0m  0.0006  0.4874\n",
      "      4            0.4292        \u001b[32m1.1376\u001b[0m       0.2812            0.2812        2.1868  0.0006  0.6153\n",
      "      5            0.4125        \u001b[32m0.9185\u001b[0m       0.2778            0.2778        2.4262  0.0006  0.5164\n",
      "      6            0.4333        0.9808       0.2812            0.2812        2.3482  0.0006  0.5160\n",
      "      7            \u001b[36m0.4917\u001b[0m        \u001b[32m0.9096\u001b[0m       0.2882            0.2882        1.9848  0.0006  0.5072\n",
      "      8            \u001b[36m0.5708\u001b[0m        0.9399       0.2917            0.2917        \u001b[94m1.7413\u001b[0m  0.0006  0.5161\n",
      "      9            \u001b[36m0.6375\u001b[0m        \u001b[32m0.8312\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m1.5084\u001b[0m  0.0006  0.5782\n",
      "     10            \u001b[36m0.7292\u001b[0m        \u001b[32m0.7921\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.3664\u001b[0m  0.0005  0.5782\n",
      "     11            \u001b[36m0.8042\u001b[0m        \u001b[32m0.7516\u001b[0m       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.2898\u001b[0m  0.0005  0.5625\n",
      "     12            \u001b[36m0.8583\u001b[0m        \u001b[32m0.6743\u001b[0m       0.4236            0.4236        \u001b[94m1.2551\u001b[0m  0.0005  0.5938\n",
      "     13            \u001b[36m0.8875\u001b[0m        \u001b[32m0.6522\u001b[0m       0.4062            0.4062        \u001b[94m1.2547\u001b[0m  0.0005  0.5157\n",
      "     14            0.8875        \u001b[32m0.6101\u001b[0m       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        1.2568  0.0005  0.5161\n",
      "     15            \u001b[36m0.9000\u001b[0m        0.6244       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.2465\u001b[0m  0.0004  0.4848\n",
      "     16            \u001b[36m0.9250\u001b[0m        0.6110       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2140\u001b[0m  0.0004  0.5212\n",
      "     17            0.9083        \u001b[32m0.5310\u001b[0m       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        \u001b[94m1.1976\u001b[0m  0.0004  0.5000\n",
      "     18            0.8917        \u001b[32m0.5307\u001b[0m       \u001b[35m0.5104\u001b[0m            \u001b[31m0.5104\u001b[0m        \u001b[94m1.1937\u001b[0m  0.0004  0.5328\n",
      "     19            0.8958        \u001b[32m0.4662\u001b[0m       0.5035            0.5035        \u001b[94m1.1841\u001b[0m  0.0004  0.4997\n",
      "     20            \u001b[36m0.9417\u001b[0m        0.5097       0.5069            0.5069        \u001b[94m1.1678\u001b[0m  0.0003  0.5000\n",
      "     21            \u001b[36m0.9542\u001b[0m        0.4977       \u001b[35m0.5174\u001b[0m            \u001b[31m0.5174\u001b[0m        \u001b[94m1.1651\u001b[0m  0.0003  0.4849\n",
      "     22            0.9500        0.4769       \u001b[35m0.5243\u001b[0m            \u001b[31m0.5243\u001b[0m        1.1654  0.0003  0.5001\n",
      "     23            \u001b[36m0.9708\u001b[0m        \u001b[32m0.4473\u001b[0m       0.5174            0.5174        1.1701  0.0002  0.5003\n",
      "     24            0.9625        \u001b[32m0.4407\u001b[0m       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        1.1780  0.0002  0.4848\n",
      "     25            0.9625        \u001b[32m0.4245\u001b[0m       0.5208            0.5208        1.1738  0.0002  0.5093\n",
      "     26            0.9667        0.4561       \u001b[35m0.5451\u001b[0m            \u001b[31m0.5451\u001b[0m        \u001b[94m1.1640\u001b[0m  0.0002  0.5940\n",
      "     27            0.9708        \u001b[32m0.3907\u001b[0m       \u001b[35m0.5486\u001b[0m            \u001b[31m0.5486\u001b[0m        \u001b[94m1.1614\u001b[0m  0.0002  0.6415\n",
      "     28            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3857\u001b[0m       0.5451            0.5451        1.1631  0.0001  0.5164\n",
      "     29            0.9750        0.3875       0.5382            0.5382        1.1631  0.0001  0.6140\n",
      "     30            0.9750        \u001b[32m0.3488\u001b[0m       0.5382            0.5382        1.1642  0.0001  0.5000\n",
      "     31            \u001b[36m0.9792\u001b[0m        0.4193       0.5382            0.5382        1.1634  0.0001  0.5161\n",
      "     32            0.9792        \u001b[32m0.3354\u001b[0m       0.5417            0.5417        1.1617  0.0001  0.7033\n",
      "     33            \u001b[36m0.9833\u001b[0m        0.3446       0.5451            0.5451        \u001b[94m1.1612\u001b[0m  0.0000  0.5782\n",
      "     34            0.9833        0.3473       0.5417            0.5417        \u001b[94m1.1606\u001b[0m  0.0000  0.6568\n",
      "     35            0.9833        0.3461       0.5486            0.5486        \u001b[94m1.1594\u001b[0m  0.0000  0.7041\n",
      "     36            0.9833        0.3365       0.5486            0.5486        \u001b[94m1.1589\u001b[0m  0.0000  0.5000\n",
      "     37            0.9833        0.3497       0.5451            0.5451        \u001b[94m1.1582\u001b[0m  0.0000  0.5935\n",
      "     38            0.9833        0.3952       0.5486            0.5486        \u001b[94m1.1576\u001b[0m  0.0000  0.5000\n",
      "     39            0.9833        0.3555       0.5486            0.5486        1.1576  0.0000  0.5157\n",
      "     40            0.9833        0.3744       0.5486            0.5486        \u001b[94m1.1573\u001b[0m  0.0000  0.6002\n",
      "Training model for subject 2 with 250 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2520\u001b[0m        \u001b[32m1.5804\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.2246\u001b[0m  0.0006  0.6244\n",
      "      2            0.2520        \u001b[32m1.2819\u001b[0m       0.2465            0.2465        \u001b[94m3.0323\u001b[0m  0.0006  0.5158\n",
      "      3            \u001b[36m0.3880\u001b[0m        \u001b[32m1.2335\u001b[0m       0.2396            0.2396        \u001b[94m2.5947\u001b[0m  0.0006  0.5338\n",
      "      4            \u001b[36m0.4000\u001b[0m        \u001b[32m1.1110\u001b[0m       0.2431            0.2431        \u001b[94m2.2934\u001b[0m  0.0006  0.5557\n",
      "      5            \u001b[36m0.4280\u001b[0m        \u001b[32m1.0822\u001b[0m       0.2326            0.2326        \u001b[94m2.0487\u001b[0m  0.0006  0.5911\n",
      "      6            0.4120        \u001b[32m0.9487\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m1.8460\u001b[0m  0.0006  0.5836\n",
      "      7            \u001b[36m0.4440\u001b[0m        0.9638       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        \u001b[94m1.7763\u001b[0m  0.0006  0.5611\n",
      "      8            \u001b[36m0.5320\u001b[0m        \u001b[32m0.9193\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m1.6519\u001b[0m  0.0006  0.5503\n",
      "      9            \u001b[36m0.6440\u001b[0m        \u001b[32m0.8784\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m1.5277\u001b[0m  0.0006  0.5276\n",
      "     10            \u001b[36m0.6840\u001b[0m        \u001b[32m0.7936\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.4913\u001b[0m  0.0005  0.5326\n",
      "     11            \u001b[36m0.7320\u001b[0m        0.8401       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.4421\u001b[0m  0.0005  0.5323\n",
      "     12            \u001b[36m0.7880\u001b[0m        \u001b[32m0.7261\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4109\u001b[0m  0.0005  0.5488\n",
      "     13            0.7720        \u001b[32m0.6624\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.3868\u001b[0m  0.0005  0.6090\n",
      "     14            \u001b[36m0.8480\u001b[0m        0.6722       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.3261\u001b[0m  0.0005  0.6169\n",
      "     15            \u001b[36m0.8720\u001b[0m        \u001b[32m0.6104\u001b[0m       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.2881\u001b[0m  0.0004  0.5699\n",
      "     16            \u001b[36m0.8960\u001b[0m        0.6130       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.2832\u001b[0m  0.0004  0.6066\n",
      "     17            \u001b[36m0.9120\u001b[0m        \u001b[32m0.5567\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.2788\u001b[0m  0.0004  0.5948\n",
      "     18            \u001b[36m0.9200\u001b[0m        0.6269       0.4444            0.4444        \u001b[94m1.2723\u001b[0m  0.0004  0.5672\n",
      "     19            \u001b[36m0.9400\u001b[0m        \u001b[32m0.5559\u001b[0m       0.4306            0.4306        \u001b[94m1.2651\u001b[0m  0.0004  0.5067\n",
      "     20            \u001b[36m0.9520\u001b[0m        0.5588       0.4375            0.4375        \u001b[94m1.2616\u001b[0m  0.0003  0.5372\n",
      "     21            0.9520        \u001b[32m0.5445\u001b[0m       0.4340            0.4340        \u001b[94m1.2613\u001b[0m  0.0003  0.5488\n",
      "     22            \u001b[36m0.9560\u001b[0m        \u001b[32m0.5118\u001b[0m       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        1.2643  0.0003  0.4935\n",
      "     23            0.9560        \u001b[32m0.4354\u001b[0m       0.4618            0.4618        1.2663  0.0002  0.5172\n",
      "     24            \u001b[36m0.9600\u001b[0m        0.4707       0.4653            0.4653        1.2635  0.0002  0.5207\n",
      "     25            \u001b[36m0.9640\u001b[0m        \u001b[32m0.4216\u001b[0m       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        \u001b[94m1.2597\u001b[0m  0.0002  0.6810\n",
      "     26            \u001b[36m0.9720\u001b[0m        \u001b[32m0.4159\u001b[0m       0.4722            0.4722        \u001b[94m1.2543\u001b[0m  0.0002  0.5474\n",
      "     27            0.9680        0.4550       0.4757            0.4757        \u001b[94m1.2505\u001b[0m  0.0002  0.5501\n",
      "     28            0.9680        0.4749       0.4722            0.4722        \u001b[94m1.2451\u001b[0m  0.0001  0.4858\n",
      "     29            0.9720        0.4302       0.4722            0.4722        \u001b[94m1.2393\u001b[0m  0.0001  0.5521\n",
      "     30            \u001b[36m0.9760\u001b[0m        \u001b[32m0.4115\u001b[0m       0.4722            0.4722        \u001b[94m1.2351\u001b[0m  0.0001  0.4910\n",
      "     31            0.9760        \u001b[32m0.3958\u001b[0m       0.4722            0.4722        \u001b[94m1.2307\u001b[0m  0.0001  0.4902\n",
      "     32            0.9760        \u001b[32m0.3580\u001b[0m       0.4757            0.4757        \u001b[94m1.2283\u001b[0m  0.0001  0.5670\n",
      "     33            0.9760        0.4024       0.4757            0.4757        \u001b[94m1.2266\u001b[0m  0.0000  0.5499\n",
      "     34            0.9720        0.4272       0.4722            0.4722        \u001b[94m1.2241\u001b[0m  0.0000  0.5507\n",
      "     35            0.9720        0.4044       0.4792            0.4792        \u001b[94m1.2227\u001b[0m  0.0000  0.5983\n",
      "     36            0.9720        0.4256       0.4792            0.4792        \u001b[94m1.2216\u001b[0m  0.0000  0.5970\n",
      "     37            0.9720        0.3787       0.4792            0.4792        \u001b[94m1.2213\u001b[0m  0.0000  0.5894\n",
      "     38            0.9720        0.4091       0.4792            0.4792        \u001b[94m1.2210\u001b[0m  0.0000  0.6432\n",
      "     39            0.9720        0.4012       0.4792            0.4792        \u001b[94m1.2206\u001b[0m  0.0000  0.5358\n",
      "     40            0.9720        0.3759       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        1.2206  0.0000  0.5122\n",
      "Training model for subject 2 with 260 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2615\u001b[0m        \u001b[32m1.6722\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.4435\u001b[0m  0.0006  0.6373\n",
      "      2            \u001b[36m0.3962\u001b[0m        \u001b[32m1.2569\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m2.2638\u001b[0m  0.0006  0.6065\n",
      "      3            0.2885        \u001b[32m1.1651\u001b[0m       0.2396            0.2396        2.6357  0.0006  0.6585\n",
      "      4            0.3654        \u001b[32m1.0855\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m2.1768\u001b[0m  0.0006  0.6174\n",
      "      5            \u001b[36m0.4462\u001b[0m        \u001b[32m0.9599\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m1.9369\u001b[0m  0.0006  0.5925\n",
      "      6            \u001b[36m0.5308\u001b[0m        \u001b[32m0.9250\u001b[0m       0.2708            0.2708        \u001b[94m1.7281\u001b[0m  0.0006  0.6525\n",
      "      7            \u001b[36m0.6423\u001b[0m        \u001b[32m0.8434\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.5415\u001b[0m  0.0006  0.6711\n",
      "      8            \u001b[36m0.7462\u001b[0m        \u001b[32m0.7185\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4105\u001b[0m  0.0006  0.7059\n",
      "      9            \u001b[36m0.7692\u001b[0m        0.7314       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.3696\u001b[0m  0.0006  0.6545\n",
      "     10            \u001b[36m0.8115\u001b[0m        \u001b[32m0.6398\u001b[0m       0.4201            0.4201        \u001b[94m1.3383\u001b[0m  0.0005  0.6337\n",
      "     11            \u001b[36m0.8346\u001b[0m        0.6684       0.4236            0.4236        \u001b[94m1.3053\u001b[0m  0.0005  0.7283\n",
      "     12            \u001b[36m0.8846\u001b[0m        \u001b[32m0.5777\u001b[0m       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.2839\u001b[0m  0.0005  0.6318\n",
      "     13            \u001b[36m0.9077\u001b[0m        \u001b[32m0.5588\u001b[0m       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.2714\u001b[0m  0.0005  0.5959\n",
      "     14            \u001b[36m0.9154\u001b[0m        0.5916       0.4375            0.4375        \u001b[94m1.2633\u001b[0m  0.0005  0.6993\n",
      "     15            \u001b[36m0.9423\u001b[0m        \u001b[32m0.5102\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.2523\u001b[0m  0.0004  0.7478\n",
      "     16            \u001b[36m0.9577\u001b[0m        \u001b[32m0.5001\u001b[0m       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.2418\u001b[0m  0.0004  0.6843\n",
      "     17            0.9577        \u001b[32m0.4816\u001b[0m       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.2377\u001b[0m  0.0004  0.7271\n",
      "     18            \u001b[36m0.9692\u001b[0m        \u001b[32m0.4651\u001b[0m       0.4583            0.4583        \u001b[94m1.2365\u001b[0m  0.0004  0.7217\n",
      "     19            \u001b[36m0.9846\u001b[0m        \u001b[32m0.4549\u001b[0m       0.4618            0.4618        \u001b[94m1.2316\u001b[0m  0.0004  0.6043\n",
      "     20            \u001b[36m0.9885\u001b[0m        \u001b[32m0.4065\u001b[0m       0.4514            0.4514        1.2351  0.0003  0.6020\n",
      "     21            \u001b[36m0.9923\u001b[0m        0.4100       0.4549            0.4549        1.2433  0.0003  0.5973\n",
      "     22            0.9885        \u001b[32m0.3813\u001b[0m       0.4549            0.4549        1.2427  0.0003  0.5977\n",
      "     23            0.9808        \u001b[32m0.3180\u001b[0m       0.4653            0.4653        1.2387  0.0002  0.5986\n",
      "     24            0.9731        0.3404       0.4653            0.4653        \u001b[94m1.2310\u001b[0m  0.0002  0.5905\n",
      "     25            0.9769        0.3217       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2267\u001b[0m  0.0002  0.5886\n",
      "     26            0.9769        0.3569       \u001b[35m0.4722\u001b[0m            \u001b[31m0.4722\u001b[0m        \u001b[94m1.2242\u001b[0m  0.0002  0.5911\n",
      "     27            0.9846        0.3217       0.4688            0.4688        \u001b[94m1.2191\u001b[0m  0.0002  0.5993\n",
      "     28            0.9885        0.3480       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        \u001b[94m1.2146\u001b[0m  0.0001  0.6066\n",
      "     29            0.9923        0.3186       0.4757            0.4757        \u001b[94m1.2142\u001b[0m  0.0001  0.6075\n",
      "     30            0.9923        \u001b[32m0.3073\u001b[0m       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        1.2169  0.0001  0.5926\n",
      "     31            0.9923        \u001b[32m0.2804\u001b[0m       0.4861            0.4861        1.2191  0.0001  0.5876\n",
      "     32            0.9923        0.2874       0.4792            0.4792        1.2220  0.0001  0.6179\n",
      "     33            0.9923        0.3202       0.4757            0.4757        1.2239  0.0000  0.5924\n",
      "     34            0.9923        0.2953       0.4757            0.4757        1.2239  0.0000  0.6762\n",
      "     35            0.9923        0.2880       0.4757            0.4757        1.2239  0.0000  0.7303\n",
      "     36            \u001b[36m0.9962\u001b[0m        0.2945       0.4757            0.4757        1.2238  0.0000  0.6679\n",
      "     37            0.9962        0.2810       0.4757            0.4757        1.2236  0.0000  0.7478\n",
      "     38            0.9962        \u001b[32m0.2682\u001b[0m       0.4757            0.4757        1.2236  0.0000  0.6555\n",
      "     39            0.9962        0.2932       0.4757            0.4757        1.2236  0.0000  0.6050\n",
      "     40            0.9962        0.2782       0.4826            0.4826        1.2237  0.0000  0.6047\n",
      "Training model for subject 2 with 270 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2444\u001b[0m        \u001b[32m1.6478\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.0999\u001b[0m  0.0006  0.6960\n",
      "      2            0.2444        \u001b[32m1.3567\u001b[0m       0.2500            0.2500        \u001b[94m2.9471\u001b[0m  0.0006  0.6061\n",
      "      3            \u001b[36m0.2741\u001b[0m        \u001b[32m1.1590\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m2.1870\u001b[0m  0.0006  0.5867\n",
      "      4            \u001b[36m0.3407\u001b[0m        \u001b[32m1.0783\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m1.9476\u001b[0m  0.0006  0.6111\n",
      "      5            \u001b[36m0.4370\u001b[0m        \u001b[32m0.9937\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m1.7265\u001b[0m  0.0006  0.5846\n",
      "      6            \u001b[36m0.5815\u001b[0m        \u001b[32m0.8738\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.5186\u001b[0m  0.0006  0.5834\n",
      "      7            \u001b[36m0.6926\u001b[0m        \u001b[32m0.7520\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.4218\u001b[0m  0.0006  0.6058\n",
      "      8            \u001b[36m0.7037\u001b[0m        0.8068       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.4092\u001b[0m  0.0006  0.5996\n",
      "      9            \u001b[36m0.8111\u001b[0m        \u001b[32m0.7430\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.3481\u001b[0m  0.0006  0.5918\n",
      "     10            \u001b[36m0.8704\u001b[0m        \u001b[32m0.6492\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.3045\u001b[0m  0.0005  0.5891\n",
      "     11            \u001b[36m0.8926\u001b[0m        \u001b[32m0.6216\u001b[0m       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.2903\u001b[0m  0.0005  0.5909\n",
      "     12            \u001b[36m0.9000\u001b[0m        0.6343       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.2846\u001b[0m  0.0005  0.6044\n",
      "     13            \u001b[36m0.9222\u001b[0m        \u001b[32m0.5906\u001b[0m       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2766\u001b[0m  0.0005  0.5858\n",
      "     14            \u001b[36m0.9296\u001b[0m        \u001b[32m0.5235\u001b[0m       \u001b[35m0.4722\u001b[0m            \u001b[31m0.4722\u001b[0m        \u001b[94m1.2463\u001b[0m  0.0005  0.6714\n",
      "     15            \u001b[36m0.9407\u001b[0m        0.5331       0.4722            0.4722        \u001b[94m1.2251\u001b[0m  0.0004  0.7161\n",
      "     16            \u001b[36m0.9444\u001b[0m        \u001b[32m0.4970\u001b[0m       0.4653            0.4653        \u001b[94m1.2164\u001b[0m  0.0004  0.7236\n",
      "     17            \u001b[36m0.9593\u001b[0m        \u001b[32m0.4640\u001b[0m       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        \u001b[94m1.1962\u001b[0m  0.0004  0.7962\n",
      "     18            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4497\u001b[0m       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        \u001b[94m1.1822\u001b[0m  0.0004  0.5922\n",
      "     19            \u001b[36m0.9704\u001b[0m        \u001b[32m0.4029\u001b[0m       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        \u001b[94m1.1690\u001b[0m  0.0004  0.6038\n",
      "     20            0.9704        \u001b[32m0.3919\u001b[0m       \u001b[35m0.5104\u001b[0m            \u001b[31m0.5104\u001b[0m        1.1723  0.0003  0.6000\n",
      "     21            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3743\u001b[0m       0.5035            0.5035        1.1760  0.0003  0.5931\n",
      "     22            0.9667        0.4393       0.5000            0.5000        1.1760  0.0003  0.6165\n",
      "     23            0.9704        \u001b[32m0.3564\u001b[0m       0.5069            0.5069        1.1737  0.0002  0.5910\n",
      "     24            0.9778        0.4029       0.5069            0.5069        \u001b[94m1.1585\u001b[0m  0.0002  0.5973\n",
      "     25            0.9741        \u001b[32m0.3229\u001b[0m       0.5069            0.5069        \u001b[94m1.1496\u001b[0m  0.0002  0.6257\n",
      "     26            0.9778        \u001b[32m0.3191\u001b[0m       \u001b[35m0.5174\u001b[0m            \u001b[31m0.5174\u001b[0m        \u001b[94m1.1478\u001b[0m  0.0002  0.5894\n",
      "     27            \u001b[36m0.9815\u001b[0m        \u001b[32m0.2977\u001b[0m       \u001b[35m0.5486\u001b[0m            \u001b[31m0.5486\u001b[0m        1.1487  0.0002  0.6093\n",
      "     28            \u001b[36m0.9889\u001b[0m        0.3344       0.5382            0.5382        1.1539  0.0001  0.6037\n",
      "     29            \u001b[36m0.9963\u001b[0m        \u001b[32m0.2866\u001b[0m       0.5312            0.5312        1.1534  0.0001  0.6702\n",
      "     30            0.9963        0.3062       0.5382            0.5382        1.1532  0.0001  0.6350\n",
      "     31            0.9963        0.3287       0.5312            0.5312        1.1533  0.0001  0.6951\n",
      "     32            0.9963        0.3277       0.5382            0.5382        1.1515  0.0001  0.7001\n",
      "     33            0.9963        0.3033       0.5347            0.5347        1.1499  0.0000  0.8595\n",
      "     34            0.9963        \u001b[32m0.2441\u001b[0m       0.5312            0.5312        1.1488  0.0000  0.8597\n",
      "     35            0.9963        0.3123       0.5382            0.5382        \u001b[94m1.1477\u001b[0m  0.0000  0.8389\n",
      "     36            0.9963        0.3219       0.5312            0.5312        \u001b[94m1.1477\u001b[0m  0.0000  0.9459\n",
      "     37            0.9963        0.3111       0.5312            0.5312        \u001b[94m1.1475\u001b[0m  0.0000  0.7612\n",
      "     38            0.9963        0.3027       0.5312            0.5312        1.1476  0.0000  0.7169\n",
      "     39            0.9963        0.2890       0.5312            0.5312        1.1477  0.0000  0.6410\n",
      "     40            0.9963        0.2782       0.5312            0.5312        1.1478  0.0000  0.6425\n",
      "Training model for subject 3 with 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3000\u001b[0m        \u001b[32m1.9378\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.5319\u001b[0m  0.0006  0.4219\n",
      "      2            0.3000        \u001b[32m0.8177\u001b[0m       0.2500            0.2500        4.8665  0.0006  0.3282\n",
      "      3            0.3000        \u001b[32m0.3956\u001b[0m       0.2500            0.2500        4.8402  0.0006  0.3125\n",
      "      4            0.3000        \u001b[32m0.3826\u001b[0m       0.2500            0.2500        4.8283  0.0006  0.3125\n",
      "      5            0.3000        \u001b[32m0.1284\u001b[0m       0.2500            0.2500        4.5827  0.0006  0.3359\n",
      "      6            \u001b[36m0.6000\u001b[0m        0.1329       0.2465            0.2465        \u001b[94m4.1242\u001b[0m  0.0006  0.3974\n",
      "      7            \u001b[36m0.8000\u001b[0m        \u001b[32m0.0892\u001b[0m       0.2361            0.2361        \u001b[94m3.7482\u001b[0m  0.0006  0.3128\n",
      "      8            \u001b[36m0.9000\u001b[0m        0.1229       0.2396            0.2396        \u001b[94m3.3929\u001b[0m  0.0006  0.3438\n",
      "      9            0.9000        \u001b[32m0.0462\u001b[0m       0.2431            0.2431        \u001b[94m3.1232\u001b[0m  0.0006  0.3125\n",
      "     10            \u001b[36m1.0000\u001b[0m        0.0769       0.2500            0.2500        \u001b[94m2.9215\u001b[0m  0.0005  0.3125\n",
      "     11            1.0000        0.0916       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m2.7639\u001b[0m  0.0005  0.3125\n",
      "     12            1.0000        \u001b[32m0.0378\u001b[0m       0.2604            0.2604        \u001b[94m2.6435\u001b[0m  0.0005  0.4225\n",
      "     13            1.0000        0.0489       0.2535            0.2535        \u001b[94m2.5489\u001b[0m  0.0005  0.5207\n",
      "     14            1.0000        \u001b[32m0.0094\u001b[0m       0.2604            0.2604        \u001b[94m2.4681\u001b[0m  0.0005  0.4223\n",
      "     15            1.0000        0.0332       0.2535            0.2535        \u001b[94m2.4001\u001b[0m  0.0004  0.5145\n",
      "     16            1.0000        0.0204       0.2535            0.2535        \u001b[94m2.3396\u001b[0m  0.0004  0.5661\n",
      "     17            1.0000        0.0197       0.2639            0.2639        \u001b[94m2.2893\u001b[0m  0.0004  0.5085\n",
      "     18            1.0000        0.0129       0.2604            0.2604        \u001b[94m2.2501\u001b[0m  0.0004  0.3488\n",
      "     19            1.0000        0.0187       0.2569            0.2569        \u001b[94m2.2158\u001b[0m  0.0004  0.4690\n",
      "     20            1.0000        0.0136       0.2569            0.2569        \u001b[94m2.1870\u001b[0m  0.0003  0.5287\n",
      "     21            1.0000        \u001b[32m0.0068\u001b[0m       0.2604            0.2604        \u001b[94m2.1623\u001b[0m  0.0003  0.5510\n",
      "     22            1.0000        \u001b[32m0.0062\u001b[0m       0.2674            0.2674        \u001b[94m2.1426\u001b[0m  0.0003  0.4502\n",
      "     23            1.0000        0.0120       0.2639            0.2639        \u001b[94m2.1277\u001b[0m  0.0002  0.4744\n",
      "     24            1.0000        0.0093       0.2639            0.2639        \u001b[94m2.1154\u001b[0m  0.0002  0.4375\n",
      "     25            1.0000        0.0082       0.2639            0.2639        \u001b[94m2.1056\u001b[0m  0.0002  0.3594\n",
      "     26            1.0000        0.0085       0.2674            0.2674        \u001b[94m2.0971\u001b[0m  0.0002  0.3259\n",
      "     27            1.0000        \u001b[32m0.0045\u001b[0m       0.2674            0.2674        \u001b[94m2.0896\u001b[0m  0.0002  0.3886\n",
      "     28            1.0000        0.0115       0.2674            0.2674        \u001b[94m2.0843\u001b[0m  0.0001  0.4258\n",
      "     29            1.0000        0.0136       0.2708            0.2708        \u001b[94m2.0767\u001b[0m  0.0001  0.3893\n",
      "     30            1.0000        0.0210       0.2708            0.2708        \u001b[94m2.0715\u001b[0m  0.0001  0.3281\n",
      "     31            1.0000        0.0084       0.2708            0.2708        \u001b[94m2.0675\u001b[0m  0.0001  0.3282\n",
      "     32            1.0000        0.0069       0.2708            0.2708        \u001b[94m2.0654\u001b[0m  0.0001  0.3646\n",
      "     33            1.0000        0.0086       0.2708            0.2708        \u001b[94m2.0623\u001b[0m  0.0000  0.3501\n",
      "     34            1.0000        0.0095       0.2708            0.2708        \u001b[94m2.0581\u001b[0m  0.0000  0.3706\n",
      "     35            1.0000        0.0109       0.2708            0.2708        \u001b[94m2.0549\u001b[0m  0.0000  0.3847\n",
      "     36            1.0000        0.0098       0.2708            0.2708        2.0557  0.0000  0.3208\n",
      "     37            1.0000        0.0136       0.2708            0.2708        2.0555  0.0000  0.3266\n",
      "     38            1.0000        \u001b[32m0.0043\u001b[0m       0.2708            0.2708        2.0557  0.0000  0.3881\n",
      "     39            1.0000        0.0072       0.2708            0.2708        \u001b[94m2.0533\u001b[0m  0.0000  0.3667\n",
      "     40            1.0000        0.0054       0.2708            0.2708        2.0540  0.0000  0.3292\n",
      "Training model for subject 3 with 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.5500\u001b[0m        \u001b[32m1.9576\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m3.6674\u001b[0m  0.0006  0.2686\n",
      "      2            0.5500        \u001b[32m0.9869\u001b[0m       0.3021            0.3021        3.9777  0.0006  0.2662\n",
      "      3            \u001b[36m0.6000\u001b[0m        \u001b[32m0.6098\u001b[0m       0.3056            0.3056        3.8694  0.0006  0.2691\n",
      "      4            0.6000        \u001b[32m0.4883\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        3.6823  0.0006  0.2642\n",
      "      5            0.6000        \u001b[32m0.3825\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m3.4470\u001b[0m  0.0006  0.2621\n",
      "      6            0.6000        \u001b[32m0.2360\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m3.2687\u001b[0m  0.0006  0.2643\n",
      "      7            \u001b[36m0.7000\u001b[0m        \u001b[32m0.2057\u001b[0m       0.3264            0.3264        \u001b[94m2.9704\u001b[0m  0.0006  0.2670\n",
      "      8            \u001b[36m0.8000\u001b[0m        \u001b[32m0.1478\u001b[0m       0.3229            0.3229        \u001b[94m2.7293\u001b[0m  0.0006  0.2623\n",
      "      9            \u001b[36m0.9000\u001b[0m        \u001b[32m0.1211\u001b[0m       0.3229            0.3229        \u001b[94m2.5674\u001b[0m  0.0006  0.2742\n",
      "     10            \u001b[36m1.0000\u001b[0m        0.1371       0.3264            0.3264        \u001b[94m2.4187\u001b[0m  0.0005  0.3354\n",
      "     11            1.0000        \u001b[32m0.0671\u001b[0m       0.3264            0.3264        \u001b[94m2.3070\u001b[0m  0.0005  0.3131\n",
      "     12            1.0000        0.0979       0.3264            0.3264        \u001b[94m2.1897\u001b[0m  0.0005  0.2836\n",
      "     13            1.0000        0.0974       0.3229            0.3229        \u001b[94m2.1231\u001b[0m  0.0005  0.2757\n",
      "     14            1.0000        \u001b[32m0.0499\u001b[0m       0.3194            0.3194        \u001b[94m2.0577\u001b[0m  0.0005  0.2656\n",
      "     15            1.0000        0.0764       0.3194            0.3194        \u001b[94m1.9959\u001b[0m  0.0004  0.2672\n",
      "     16            1.0000        \u001b[32m0.0485\u001b[0m       0.3125            0.3125        \u001b[94m1.9594\u001b[0m  0.0004  0.2786\n",
      "     17            1.0000        0.0516       0.3264            0.3264        \u001b[94m1.9154\u001b[0m  0.0004  0.2848\n",
      "     18            1.0000        \u001b[32m0.0242\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.8749\u001b[0m  0.0004  0.3007\n",
      "     19            1.0000        0.0406       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.8426\u001b[0m  0.0004  0.2833\n",
      "     20            1.0000        0.0312       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.8152\u001b[0m  0.0003  0.5265\n",
      "     21            1.0000        0.0265       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.7971\u001b[0m  0.0003  0.3889\n",
      "     22            1.0000        0.0286       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.7826\u001b[0m  0.0003  0.3539\n",
      "     23            1.0000        \u001b[32m0.0212\u001b[0m       0.3646            0.3646        \u001b[94m1.7713\u001b[0m  0.0002  0.3161\n",
      "     24            1.0000        0.0290       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.7640\u001b[0m  0.0002  0.2657\n",
      "     25            1.0000        \u001b[32m0.0174\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.7578\u001b[0m  0.0002  0.3613\n",
      "     26            1.0000        \u001b[32m0.0108\u001b[0m       0.3750            0.3750        \u001b[94m1.7527\u001b[0m  0.0002  0.3754\n",
      "     27            1.0000        0.0229       0.3681            0.3681        \u001b[94m1.7479\u001b[0m  0.0002  0.4024\n",
      "     28            1.0000        0.0205       0.3681            0.3681        \u001b[94m1.7446\u001b[0m  0.0001  0.5176\n",
      "     29            1.0000        0.0116       0.3681            0.3681        \u001b[94m1.7420\u001b[0m  0.0001  0.6155\n",
      "     30            1.0000        0.0148       0.3681            0.3681        \u001b[94m1.7394\u001b[0m  0.0001  0.4352\n",
      "     31            1.0000        0.0130       0.3715            0.3715        \u001b[94m1.7372\u001b[0m  0.0001  0.3945\n",
      "     32            1.0000        0.0194       0.3750            0.3750        \u001b[94m1.7354\u001b[0m  0.0001  0.3159\n",
      "     33            1.0000        0.0245       0.3785            0.3785        \u001b[94m1.7341\u001b[0m  0.0000  0.3469\n",
      "     34            1.0000        0.0127       0.3785            0.3785        \u001b[94m1.7330\u001b[0m  0.0000  0.3100\n",
      "     35            1.0000        0.0154       0.3750            0.3750        \u001b[94m1.7317\u001b[0m  0.0000  0.3771\n",
      "     36            1.0000        0.0213       0.3750            0.3750        \u001b[94m1.7308\u001b[0m  0.0000  0.4440\n",
      "     37            1.0000        0.0176       0.3750            0.3750        \u001b[94m1.7298\u001b[0m  0.0000  0.4977\n",
      "     38            1.0000        0.0159       0.3750            0.3750        \u001b[94m1.7290\u001b[0m  0.0000  0.5341\n",
      "     39            1.0000        0.0253       0.3750            0.3750        \u001b[94m1.7288\u001b[0m  0.0000  0.4883\n",
      "     40            1.0000        0.0227       0.3750            0.3750        \u001b[94m1.7285\u001b[0m  0.0000  0.4277\n",
      "Training model for subject 3 with 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.7667\u001b[0m        \u001b[32m1.5808\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m1.5100\u001b[0m  0.0006  0.4066\n",
      "      2            0.6667        \u001b[32m1.1570\u001b[0m       0.2743            0.2743        1.7726  0.0006  0.2832\n",
      "      3            0.6333        \u001b[32m0.8313\u001b[0m       0.2569            0.2569        1.9613  0.0006  0.2689\n",
      "      4            0.7000        \u001b[32m0.4987\u001b[0m       0.2465            0.2465        2.0455  0.0006  0.2999\n",
      "      5            0.7333        0.5166       0.2812            0.2812        2.0696  0.0006  0.2500\n",
      "      6            0.7333        \u001b[32m0.3811\u001b[0m       0.2812            0.2812        2.0905  0.0006  0.2503\n",
      "      7            \u001b[36m0.8000\u001b[0m        \u001b[32m0.2077\u001b[0m       0.2986            0.2986        2.0944  0.0006  0.2665\n",
      "      8            \u001b[36m0.9000\u001b[0m        0.2095       0.2743            0.2743        2.0857  0.0006  0.2498\n",
      "      9            \u001b[36m0.9667\u001b[0m        \u001b[32m0.1787\u001b[0m       0.2604            0.2604        2.0946  0.0006  0.2499\n",
      "     10            \u001b[36m1.0000\u001b[0m        0.1850       0.2639            0.2639        2.1025  0.0005  0.2502\n",
      "     11            1.0000        \u001b[32m0.1087\u001b[0m       0.2743            0.2743        2.1011  0.0005  0.2669\n",
      "     12            1.0000        0.1177       0.2847            0.2847        2.1040  0.0005  0.2834\n",
      "     13            1.0000        \u001b[32m0.0790\u001b[0m       0.2847            0.2847        2.0777  0.0005  0.3188\n",
      "     14            1.0000        \u001b[32m0.0651\u001b[0m       0.2951            0.2951        2.0451  0.0005  0.2977\n",
      "     15            1.0000        0.0702       0.3056            0.3056        2.0102  0.0004  0.2681\n",
      "     16            1.0000        \u001b[32m0.0645\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        1.9691  0.0004  0.2832\n",
      "     17            1.0000        \u001b[32m0.0499\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        1.9341  0.0004  0.2831\n",
      "     18            1.0000        0.0597       0.3194            0.3194        1.9067  0.0004  0.3348\n",
      "     19            1.0000        0.0755       0.3264            0.3264        1.8840  0.0004  0.3003\n",
      "     20            1.0000        0.0615       0.3160            0.3160        1.8669  0.0003  0.2826\n",
      "     21            1.0000        0.0516       0.3125            0.3125        1.8594  0.0003  0.3352\n",
      "     22            1.0000        \u001b[32m0.0421\u001b[0m       0.3125            0.3125        1.8561  0.0003  0.2528\n",
      "     23            1.0000        \u001b[32m0.0397\u001b[0m       0.3229            0.3229        1.8570  0.0002  0.3062\n",
      "     24            1.0000        0.0419       0.3264            0.3264        1.8575  0.0002  0.2743\n",
      "     25            1.0000        \u001b[32m0.0349\u001b[0m       0.3229            0.3229        1.8589  0.0002  0.2706\n",
      "     26            1.0000        0.0385       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        1.8605  0.0002  0.2544\n",
      "     27            1.0000        0.0454       0.3264            0.3264        1.8614  0.0002  0.2688\n",
      "     28            1.0000        0.0423       0.3125            0.3125        1.8613  0.0001  0.2820\n",
      "     29            1.0000        \u001b[32m0.0245\u001b[0m       0.3056            0.3056        1.8604  0.0001  0.3177\n",
      "     30            1.0000        0.0342       0.3056            0.3056        1.8595  0.0001  0.3568\n",
      "     31            1.0000        0.0298       0.3056            0.3056        1.8587  0.0001  0.3634\n",
      "     32            1.0000        0.0340       0.3021            0.3021        1.8579  0.0001  0.3336\n",
      "     33            1.0000        0.0303       0.2986            0.2986        1.8566  0.0000  0.3701\n",
      "     34            1.0000        0.0390       0.2986            0.2986        1.8552  0.0000  0.3690\n",
      "     35            1.0000        0.0281       0.2986            0.2986        1.8541  0.0000  0.3995\n",
      "     36            1.0000        0.0277       0.2986            0.2986        1.8530  0.0000  0.4001\n",
      "     37            1.0000        0.0411       0.2986            0.2986        1.8520  0.0000  0.3333\n",
      "     38            1.0000        \u001b[32m0.0226\u001b[0m       0.2986            0.2986        1.8509  0.0000  0.3001\n",
      "     39            1.0000        0.0248       0.2986            0.2986        1.8500  0.0000  0.2831\n",
      "     40            1.0000        0.0511       0.2951            0.2951        1.8490  0.0000  0.2834\n",
      "Training model for subject 3 with 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3000\u001b[0m        \u001b[32m1.7166\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m7.5058\u001b[0m  0.0006  0.2666\n",
      "      2            0.3000        \u001b[32m1.1800\u001b[0m       0.2500            0.2500        \u001b[94m6.4286\u001b[0m  0.0006  0.3000\n",
      "      3            0.3000        \u001b[32m0.8915\u001b[0m       0.2500            0.2500        \u001b[94m5.4822\u001b[0m  0.0006  0.2830\n",
      "      4            0.3000        \u001b[32m0.7270\u001b[0m       0.2500            0.2500        \u001b[94m4.7772\u001b[0m  0.0006  0.3177\n",
      "      5            0.3000        \u001b[32m0.4517\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m4.3248\u001b[0m  0.0006  0.3089\n",
      "      6            0.3000        \u001b[32m0.4136\u001b[0m       0.2535            0.2535        \u001b[94m3.9671\u001b[0m  0.0006  0.3249\n",
      "      7            \u001b[36m0.3500\u001b[0m        \u001b[32m0.3238\u001b[0m       0.2535            0.2535        \u001b[94m3.5191\u001b[0m  0.0006  0.3123\n",
      "      8            \u001b[36m0.3750\u001b[0m        \u001b[32m0.2973\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m3.0768\u001b[0m  0.0006  0.2591\n",
      "      9            \u001b[36m0.5250\u001b[0m        \u001b[32m0.2299\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m2.6389\u001b[0m  0.0006  0.2852\n",
      "     10            \u001b[36m0.7250\u001b[0m        0.2969       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m2.2440\u001b[0m  0.0005  0.2825\n",
      "     11            \u001b[36m0.9250\u001b[0m        \u001b[32m0.1717\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m1.9815\u001b[0m  0.0005  0.2833\n",
      "     12            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1484\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.8030\u001b[0m  0.0005  0.2677\n",
      "     13            1.0000        0.2075       0.3472            0.3472        \u001b[94m1.6910\u001b[0m  0.0005  0.2999\n",
      "     14            1.0000        \u001b[32m0.1249\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.6282\u001b[0m  0.0005  0.2834\n",
      "     15            1.0000        \u001b[32m0.1208\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.6014\u001b[0m  0.0004  0.2668\n",
      "     16            1.0000        \u001b[32m0.1117\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.6013\u001b[0m  0.0004  0.2669\n",
      "     17            1.0000        \u001b[32m0.0765\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        1.6058  0.0004  0.2680\n",
      "     18            1.0000        0.0809       0.3785            0.3785        1.6130  0.0004  0.2669\n",
      "     19            1.0000        \u001b[32m0.0752\u001b[0m       0.3785            0.3785        1.6232  0.0004  0.2500\n",
      "     20            1.0000        \u001b[32m0.0685\u001b[0m       0.3785            0.3785        1.6330  0.0003  0.2666\n",
      "     21            1.0000        \u001b[32m0.0658\u001b[0m       0.3819            0.3819        1.6383  0.0003  0.2667\n",
      "     22            1.0000        0.0916       0.3750            0.3750        1.6436  0.0003  0.2841\n",
      "     23            1.0000        0.0731       0.3750            0.3750        1.6475  0.0002  0.3006\n",
      "     24            1.0000        \u001b[32m0.0629\u001b[0m       0.3785            0.3785        1.6508  0.0002  0.2821\n",
      "     25            1.0000        \u001b[32m0.0541\u001b[0m       0.3819            0.3819        1.6536  0.0002  0.3999\n",
      "     26            1.0000        0.0557       0.3785            0.3785        1.6552  0.0002  0.2838\n",
      "     27            1.0000        \u001b[32m0.0536\u001b[0m       0.3819            0.3819        1.6558  0.0002  0.2842\n",
      "     28            1.0000        0.0542       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        1.6542  0.0001  0.2765\n",
      "     29            1.0000        0.0767       0.3785            0.3785        1.6514  0.0001  0.2727\n",
      "     30            1.0000        0.0723       0.3785            0.3785        1.6486  0.0001  0.2692\n",
      "     31            1.0000        0.0579       0.3750            0.3750        1.6465  0.0001  0.2876\n",
      "     32            1.0000        \u001b[32m0.0464\u001b[0m       0.3785            0.3785        1.6450  0.0001  0.3328\n",
      "     33            1.0000        0.0624       0.3819            0.3819        1.6439  0.0000  0.4277\n",
      "     34            1.0000        0.0569       0.3785            0.3785        1.6426  0.0000  0.3658\n",
      "     35            1.0000        0.0609       0.3819            0.3819        1.6412  0.0000  0.3158\n",
      "     36            1.0000        0.0558       0.3819            0.3819        1.6404  0.0000  0.3156\n",
      "     37            1.0000        \u001b[32m0.0444\u001b[0m       0.3819            0.3819        1.6391  0.0000  0.4043\n",
      "     38            1.0000        0.0504       0.3819            0.3819        1.6381  0.0000  0.3539\n",
      "     39            1.0000        0.0610       0.3819            0.3819        1.6374  0.0000  0.3412\n",
      "     40            1.0000        \u001b[32m0.0318\u001b[0m       0.3785            0.3785        1.6366  0.0000  0.4007\n",
      "Training model for subject 3 with 50 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2800\u001b[0m        \u001b[32m1.6001\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m2.7876\u001b[0m  0.0006  0.4524\n",
      "      2            0.2600        \u001b[32m1.1126\u001b[0m       0.2500            0.2500        4.0843  0.0006  0.4686\n",
      "      3            0.2600        \u001b[32m0.9023\u001b[0m       0.2500            0.2500        4.9837  0.0006  0.4903\n",
      "      4            0.2600        \u001b[32m0.7747\u001b[0m       0.2500            0.2500        5.0082  0.0006  0.3670\n",
      "      5            0.2800        \u001b[32m0.6327\u001b[0m       0.2500            0.2500        5.0111  0.0006  0.2905\n",
      "      6            \u001b[36m0.3000\u001b[0m        \u001b[32m0.5310\u001b[0m       0.2500            0.2500        4.6673  0.0006  0.3633\n",
      "      7            0.3000        0.5391       0.2500            0.2500        4.3751  0.0006  0.4627\n",
      "      8            \u001b[36m0.3400\u001b[0m        \u001b[32m0.5287\u001b[0m       0.2500            0.2500        4.1657  0.0006  0.3134\n",
      "      9            0.3400        \u001b[32m0.4683\u001b[0m       0.2500            0.2500        3.9581  0.0006  0.3128\n",
      "     10            \u001b[36m0.3600\u001b[0m        \u001b[32m0.3592\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        3.5966  0.0005  0.2969\n",
      "     11            \u001b[36m0.4400\u001b[0m        \u001b[32m0.3336\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        3.2239  0.0005  0.2657\n",
      "     12            \u001b[36m0.5000\u001b[0m        \u001b[32m0.2876\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        2.9376  0.0005  0.2872\n",
      "     13            \u001b[36m0.7200\u001b[0m        0.2941       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m2.6839\u001b[0m  0.0005  0.3163\n",
      "     14            \u001b[36m0.8600\u001b[0m        \u001b[32m0.2103\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m2.4639\u001b[0m  0.0005  0.3025\n",
      "     15            \u001b[36m0.9200\u001b[0m        \u001b[32m0.1994\u001b[0m       0.2847            0.2847        \u001b[94m2.2790\u001b[0m  0.0004  0.3015\n",
      "     16            \u001b[36m0.9600\u001b[0m        0.2143       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m2.1742\u001b[0m  0.0004  0.2987\n",
      "     17            0.9600        \u001b[32m0.1717\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m2.0826\u001b[0m  0.0004  0.3185\n",
      "     18            \u001b[36m0.9800\u001b[0m        0.1979       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m2.0115\u001b[0m  0.0004  0.3227\n",
      "     19            0.9800        0.1761       0.3229            0.3229        \u001b[94m1.9431\u001b[0m  0.0004  0.3033\n",
      "     20            \u001b[36m1.0000\u001b[0m        0.1866       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m1.8896\u001b[0m  0.0003  0.2912\n",
      "     21            1.0000        \u001b[32m0.1191\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.8468\u001b[0m  0.0003  0.3307\n",
      "     22            1.0000        \u001b[32m0.1187\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.8153\u001b[0m  0.0003  0.3270\n",
      "     23            1.0000        0.1744       0.3438            0.3438        \u001b[94m1.7867\u001b[0m  0.0002  0.2850\n",
      "     24            1.0000        0.1464       0.3403            0.3403        \u001b[94m1.7666\u001b[0m  0.0002  0.3340\n",
      "     25            1.0000        0.1427       0.3403            0.3403        \u001b[94m1.7499\u001b[0m  0.0002  0.2920\n",
      "     26            1.0000        \u001b[32m0.1065\u001b[0m       0.3438            0.3438        \u001b[94m1.7391\u001b[0m  0.0002  0.3013\n",
      "     27            1.0000        \u001b[32m0.1021\u001b[0m       0.3472            0.3472        \u001b[94m1.7301\u001b[0m  0.0002  0.2820\n",
      "     28            1.0000        0.1375       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.7245\u001b[0m  0.0001  0.3035\n",
      "     29            1.0000        \u001b[32m0.0970\u001b[0m       0.3472            0.3472        \u001b[94m1.7199\u001b[0m  0.0001  0.3015\n",
      "     30            1.0000        0.0974       0.3438            0.3438        \u001b[94m1.7162\u001b[0m  0.0001  0.3365\n",
      "     31            1.0000        \u001b[32m0.0787\u001b[0m       0.3438            0.3438        \u001b[94m1.7129\u001b[0m  0.0001  0.3493\n",
      "     32            1.0000        0.1293       0.3438            0.3438        \u001b[94m1.7103\u001b[0m  0.0001  0.3681\n",
      "     33            1.0000        0.0792       0.3438            0.3438        \u001b[94m1.7085\u001b[0m  0.0000  0.3158\n",
      "     34            1.0000        0.1062       0.3438            0.3438        \u001b[94m1.7066\u001b[0m  0.0000  0.3002\n",
      "     35            1.0000        \u001b[32m0.0706\u001b[0m       0.3438            0.3438        \u001b[94m1.7052\u001b[0m  0.0000  0.3182\n",
      "     36            1.0000        0.1046       0.3403            0.3403        \u001b[94m1.7042\u001b[0m  0.0000  0.2990\n",
      "     37            1.0000        0.1009       0.3438            0.3438        \u001b[94m1.7034\u001b[0m  0.0000  0.3001\n",
      "     38            1.0000        0.1442       0.3438            0.3438        \u001b[94m1.7026\u001b[0m  0.0000  0.2877\n",
      "     39            1.0000        0.0882       0.3438            0.3438        \u001b[94m1.7017\u001b[0m  0.0000  0.3787\n",
      "     40            1.0000        0.0980       0.3438            0.3438        \u001b[94m1.7013\u001b[0m  0.0000  0.3667\n",
      "Training model for subject 3 with 60 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2333\u001b[0m        \u001b[32m1.6406\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.3861\u001b[0m  0.0006  0.4351\n",
      "      2            \u001b[36m0.2500\u001b[0m        \u001b[32m1.1105\u001b[0m       0.2500            0.2500        4.5094  0.0006  0.3989\n",
      "      3            0.2333        \u001b[32m1.0163\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        4.7564  0.0006  0.3890\n",
      "      4            0.2333        \u001b[32m0.8610\u001b[0m       0.2535            0.2535        4.7468  0.0006  0.4090\n",
      "      5            \u001b[36m0.3167\u001b[0m        \u001b[32m0.7389\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        4.6122  0.0006  0.4178\n",
      "      6            \u001b[36m0.3833\u001b[0m        \u001b[32m0.7275\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m4.3819\u001b[0m  0.0006  0.3159\n",
      "      7            \u001b[36m0.4167\u001b[0m        \u001b[32m0.6333\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m4.1097\u001b[0m  0.0006  0.3158\n",
      "      8            \u001b[36m0.4500\u001b[0m        \u001b[32m0.6062\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        \u001b[94m3.8539\u001b[0m  0.0006  0.2999\n",
      "      9            \u001b[36m0.4667\u001b[0m        \u001b[32m0.4847\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m3.5756\u001b[0m  0.0006  0.2999\n",
      "     10            0.4667        \u001b[32m0.4022\u001b[0m       0.3056            0.3056        \u001b[94m3.3014\u001b[0m  0.0005  0.3000\n",
      "     11            \u001b[36m0.5333\u001b[0m        0.4081       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m3.0256\u001b[0m  0.0005  0.3000\n",
      "     12            \u001b[36m0.5667\u001b[0m        \u001b[32m0.3022\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m2.7922\u001b[0m  0.0005  0.3013\n",
      "     13            \u001b[36m0.6000\u001b[0m        \u001b[32m0.2959\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m2.5495\u001b[0m  0.0005  0.2992\n",
      "     14            \u001b[36m0.6833\u001b[0m        0.3370       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m2.3238\u001b[0m  0.0005  0.2850\n",
      "     15            \u001b[36m0.8167\u001b[0m        \u001b[32m0.2225\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m2.1365\u001b[0m  0.0004  0.2987\n",
      "     16            \u001b[36m0.8667\u001b[0m        0.2446       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.9943\u001b[0m  0.0004  0.2836\n",
      "     17            \u001b[36m0.9667\u001b[0m        \u001b[32m0.2167\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.8605\u001b[0m  0.0004  0.2835\n",
      "     18            \u001b[36m0.9833\u001b[0m        \u001b[32m0.2117\u001b[0m       0.3819            0.3819        \u001b[94m1.7590\u001b[0m  0.0004  0.2831\n",
      "     19            0.9833        \u001b[32m0.1850\u001b[0m       0.3681            0.3681        \u001b[94m1.7043\u001b[0m  0.0004  0.2834\n",
      "     20            0.9833        \u001b[32m0.1556\u001b[0m       0.3646            0.3646        \u001b[94m1.6580\u001b[0m  0.0003  0.2835\n",
      "     21            0.9833        \u001b[32m0.1339\u001b[0m       0.3750            0.3750        \u001b[94m1.6234\u001b[0m  0.0003  0.2835\n",
      "     22            \u001b[36m1.0000\u001b[0m        0.1559       0.3819            0.3819        \u001b[94m1.5988\u001b[0m  0.0003  0.2833\n",
      "     23            1.0000        0.1481       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.5842\u001b[0m  0.0002  0.3000\n",
      "     24            1.0000        0.1542       0.3819            0.3819        \u001b[94m1.5758\u001b[0m  0.0002  0.3002\n",
      "     25            1.0000        \u001b[32m0.1229\u001b[0m       0.3785            0.3785        \u001b[94m1.5692\u001b[0m  0.0002  0.2844\n",
      "     26            1.0000        0.1350       0.3819            0.3819        \u001b[94m1.5640\u001b[0m  0.0002  0.3000\n",
      "     27            1.0000        \u001b[32m0.1180\u001b[0m       0.3854            0.3854        \u001b[94m1.5575\u001b[0m  0.0002  0.3004\n",
      "     28            1.0000        \u001b[32m0.1039\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.5521\u001b[0m  0.0001  0.3012\n",
      "     29            1.0000        0.1472       0.3924            0.3924        \u001b[94m1.5478\u001b[0m  0.0001  0.3317\n",
      "     30            1.0000        0.1304       0.3889            0.3889        \u001b[94m1.5449\u001b[0m  0.0001  0.3005\n",
      "     31            1.0000        0.1325       0.3889            0.3889        \u001b[94m1.5429\u001b[0m  0.0001  0.3038\n",
      "     32            1.0000        0.1305       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.5402\u001b[0m  0.0001  0.3122\n",
      "     33            1.0000        0.1205       0.3958            0.3958        \u001b[94m1.5383\u001b[0m  0.0000  0.3001\n",
      "     34            1.0000        0.1150       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.5370\u001b[0m  0.0000  0.2992\n",
      "     35            1.0000        0.1788       0.3993            0.3993        \u001b[94m1.5360\u001b[0m  0.0000  0.3001\n",
      "     36            1.0000        0.1440       0.3993            0.3993        \u001b[94m1.5353\u001b[0m  0.0000  0.3149\n",
      "     37            1.0000        0.1269       0.3993            0.3993        \u001b[94m1.5348\u001b[0m  0.0000  0.3197\n",
      "     38            1.0000        0.1398       0.3993            0.3993        \u001b[94m1.5344\u001b[0m  0.0000  0.2999\n",
      "     39            1.0000        0.1108       0.3993            0.3993        \u001b[94m1.5342\u001b[0m  0.0000  0.2997\n",
      "     40            1.0000        \u001b[32m0.0698\u001b[0m       0.3958            0.3958        \u001b[94m1.5340\u001b[0m  0.0000  0.2822\n",
      "Training model for subject 3 with 70 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2571\u001b[0m        \u001b[32m1.6661\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m2.5587\u001b[0m  0.0006  0.3745\n",
      "      2            \u001b[36m0.5286\u001b[0m        \u001b[32m1.3012\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m1.5557\u001b[0m  0.0006  0.3471\n",
      "      3            0.2429        \u001b[32m0.9863\u001b[0m       0.2500            0.2500        2.3912  0.0006  0.5333\n",
      "      4            0.2429        0.9866       0.2500            0.2500        3.2248  0.0006  0.5454\n",
      "      5            0.2429        \u001b[32m0.8064\u001b[0m       0.2500            0.2500        3.7684  0.0006  0.4249\n",
      "      6            0.2429        \u001b[32m0.6775\u001b[0m       0.2500            0.2500        3.7689  0.0006  0.3904\n",
      "      7            0.2857        \u001b[32m0.6144\u001b[0m       0.2535            0.2535        3.5377  0.0006  0.4342\n",
      "      8            0.3286        \u001b[32m0.5448\u001b[0m       0.2569            0.2569        3.3146  0.0006  0.4155\n",
      "      9            0.3286        \u001b[32m0.4677\u001b[0m       0.2569            0.2569        3.2278  0.0006  0.3728\n",
      "     10            0.4000        \u001b[32m0.4466\u001b[0m       0.2569            0.2569        3.0158  0.0005  0.4701\n",
      "     11            0.5286        \u001b[32m0.4090\u001b[0m       0.2569            0.2569        2.8337  0.0005  0.3494\n",
      "     12            \u001b[36m0.6429\u001b[0m        \u001b[32m0.3913\u001b[0m       0.2778            0.2778        2.5742  0.0005  0.4448\n",
      "     13            \u001b[36m0.7857\u001b[0m        \u001b[32m0.3440\u001b[0m       0.2917            0.2917        2.3686  0.0005  0.3310\n",
      "     14            \u001b[36m0.8429\u001b[0m        \u001b[32m0.3336\u001b[0m       0.3125            0.3125        2.1917  0.0005  0.3501\n",
      "     15            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3014\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        2.0352  0.0004  0.4095\n",
      "     16            \u001b[36m0.9429\u001b[0m        \u001b[32m0.2358\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        1.9128  0.0004  0.4640\n",
      "     17            \u001b[36m0.9571\u001b[0m        0.2493       0.3646            0.3646        1.8199  0.0004  0.4559\n",
      "     18            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2325\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        1.7598  0.0004  0.3238\n",
      "     19            1.0000        \u001b[32m0.1924\u001b[0m       0.3681            0.3681        1.7206  0.0004  0.3404\n",
      "     20            1.0000        \u001b[32m0.1548\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        1.6929  0.0003  0.3981\n",
      "     21            1.0000        0.1803       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        1.6694  0.0003  0.3051\n",
      "     22            1.0000        \u001b[32m0.1426\u001b[0m       0.3715            0.3715        1.6478  0.0003  0.3179\n",
      "     23            1.0000        0.2036       0.3819            0.3819        1.6295  0.0002  0.3507\n",
      "     24            1.0000        \u001b[32m0.1378\u001b[0m       0.3854            0.3854        1.6179  0.0002  0.3807\n",
      "     25            1.0000        \u001b[32m0.1267\u001b[0m       0.3889            0.3889        1.6095  0.0002  0.3152\n",
      "     26            1.0000        \u001b[32m0.1150\u001b[0m       0.3750            0.3750        1.6039  0.0002  0.3531\n",
      "     27            1.0000        \u001b[32m0.1101\u001b[0m       0.3785            0.3785        1.6021  0.0002  0.3902\n",
      "     28            1.0000        0.1815       0.3750            0.3750        1.6014  0.0001  0.3315\n",
      "     29            1.0000        \u001b[32m0.1047\u001b[0m       0.3681            0.3681        1.6015  0.0001  0.2973\n",
      "     30            1.0000        0.1306       0.3646            0.3646        1.6017  0.0001  0.2969\n",
      "     31            1.0000        0.1229       0.3611            0.3611        1.6026  0.0001  0.3125\n",
      "     32            1.0000        0.1292       0.3611            0.3611        1.6033  0.0001  0.2969\n",
      "     33            1.0000        0.1089       0.3611            0.3611        1.6032  0.0000  0.3125\n",
      "     34            1.0000        0.1244       0.3611            0.3611        1.6029  0.0000  0.2969\n",
      "     35            1.0000        0.1491       0.3611            0.3611        1.6025  0.0000  0.3127\n",
      "     36            1.0000        0.1282       0.3646            0.3646        1.6023  0.0000  0.3127\n",
      "     37            1.0000        0.1325       0.3611            0.3611        1.6018  0.0000  0.3126\n",
      "     38            1.0000        0.1179       0.3611            0.3611        1.6014  0.0000  0.3403\n",
      "     39            1.0000        \u001b[32m0.0946\u001b[0m       0.3576            0.3576        1.6009  0.0000  0.3541\n",
      "     40            1.0000        0.1080       0.3576            0.3576        1.6006  0.0000  0.3926\n",
      "Training model for subject 3 with 80 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2875\u001b[0m        \u001b[32m1.7824\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m3.5953\u001b[0m  0.0006  0.4497\n",
      "      2            0.2625        \u001b[32m1.3636\u001b[0m       0.2500            0.2500        4.7353  0.0006  0.3907\n",
      "      3            0.2625        \u001b[32m1.1638\u001b[0m       0.2500            0.2500        5.0778  0.0006  0.3907\n",
      "      4            0.2625        \u001b[32m0.9589\u001b[0m       0.2500            0.2500        4.7108  0.0006  0.3907\n",
      "      5            0.2625        \u001b[32m0.9288\u001b[0m       0.2500            0.2500        4.5301  0.0006  0.3749\n",
      "      6            0.2625        \u001b[32m0.8822\u001b[0m       0.2500            0.2500        4.2328  0.0006  0.4116\n",
      "      7            0.2750        \u001b[32m0.7970\u001b[0m       0.2500            0.2500        3.9236  0.0006  0.3402\n",
      "      8            \u001b[36m0.3000\u001b[0m        \u001b[32m0.6953\u001b[0m       0.2500            0.2500        3.6327  0.0006  0.3295\n",
      "      9            \u001b[36m0.3125\u001b[0m        \u001b[32m0.6414\u001b[0m       0.2500            0.2500        \u001b[94m3.4291\u001b[0m  0.0006  0.3125\n",
      "     10            \u001b[36m0.4000\u001b[0m        \u001b[32m0.5898\u001b[0m       0.2465            0.2465        \u001b[94m3.1941\u001b[0m  0.0005  0.3281\n",
      "     11            \u001b[36m0.4500\u001b[0m        \u001b[32m0.4781\u001b[0m       0.2500            0.2500        \u001b[94m2.9422\u001b[0m  0.0005  0.3283\n",
      "     12            \u001b[36m0.5250\u001b[0m        \u001b[32m0.4355\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m2.7239\u001b[0m  0.0005  0.3281\n",
      "     13            \u001b[36m0.6125\u001b[0m        \u001b[32m0.3904\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m2.5487\u001b[0m  0.0005  0.3281\n",
      "     14            \u001b[36m0.7250\u001b[0m        \u001b[32m0.3363\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m2.3801\u001b[0m  0.0005  0.3299\n",
      "     15            \u001b[36m0.8250\u001b[0m        \u001b[32m0.3128\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m2.2194\u001b[0m  0.0004  0.3125\n",
      "     16            \u001b[36m0.8625\u001b[0m        \u001b[32m0.2838\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m2.0866\u001b[0m  0.0004  0.3149\n",
      "     17            \u001b[36m0.9375\u001b[0m        \u001b[32m0.2759\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m1.9726\u001b[0m  0.0004  0.3129\n",
      "     18            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2596\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.8796\u001b[0m  0.0004  0.3269\n",
      "     19            \u001b[36m0.9875\u001b[0m        0.2934       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.8118\u001b[0m  0.0004  0.3375\n",
      "     20            0.9875        \u001b[32m0.2259\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.7605\u001b[0m  0.0003  0.3281\n",
      "     21            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2087\u001b[0m       0.3542            0.3542        \u001b[94m1.7262\u001b[0m  0.0003  0.3286\n",
      "     22            1.0000        0.2161       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.7014\u001b[0m  0.0003  0.3281\n",
      "     23            1.0000        \u001b[32m0.2034\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.6731\u001b[0m  0.0002  0.3338\n",
      "     24            1.0000        0.2037       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.6576\u001b[0m  0.0002  0.3240\n",
      "     25            1.0000        \u001b[32m0.1769\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.6448\u001b[0m  0.0002  0.3300\n",
      "     26            1.0000        0.1917       0.3854            0.3854        \u001b[94m1.6395\u001b[0m  0.0002  0.3282\n",
      "     27            1.0000        0.2178       0.3854            0.3854        \u001b[94m1.6366\u001b[0m  0.0002  0.3578\n",
      "     28            1.0000        0.1888       0.3785            0.3785        \u001b[94m1.6340\u001b[0m  0.0001  0.3395\n",
      "     29            1.0000        0.1841       0.3715            0.3715        \u001b[94m1.6335\u001b[0m  0.0001  0.3257\n",
      "     30            1.0000        \u001b[32m0.1552\u001b[0m       0.3785            0.3785        \u001b[94m1.6324\u001b[0m  0.0001  0.3455\n",
      "     31            1.0000        0.1767       0.3785            0.3785        \u001b[94m1.6318\u001b[0m  0.0001  0.3594\n",
      "     32            1.0000        0.1630       0.3785            0.3785        \u001b[94m1.6311\u001b[0m  0.0001  0.3594\n",
      "     33            1.0000        0.1867       0.3819            0.3819        \u001b[94m1.6309\u001b[0m  0.0000  0.3281\n",
      "     34            1.0000        0.1687       0.3819            0.3819        \u001b[94m1.6304\u001b[0m  0.0000  0.3125\n",
      "     35            1.0000        \u001b[32m0.1536\u001b[0m       0.3819            0.3819        \u001b[94m1.6300\u001b[0m  0.0000  0.3281\n",
      "     36            1.0000        0.1613       0.3819            0.3819        \u001b[94m1.6294\u001b[0m  0.0000  0.4219\n",
      "     37            1.0000        0.1778       0.3819            0.3819        \u001b[94m1.6288\u001b[0m  0.0000  0.3438\n",
      "     38            1.0000        \u001b[32m0.1506\u001b[0m       0.3819            0.3819        \u001b[94m1.6282\u001b[0m  0.0000  0.3480\n",
      "     39            1.0000        \u001b[32m0.1314\u001b[0m       0.3785            0.3785        \u001b[94m1.6276\u001b[0m  0.0000  0.3281\n",
      "     40            1.0000        0.1502       0.3750            0.3750        \u001b[94m1.6270\u001b[0m  0.0000  0.3750\n",
      "Training model for subject 3 with 90 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2444\u001b[0m        \u001b[32m1.5495\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.3575\u001b[0m  0.0006  0.3907\n",
      "      2            0.2444        \u001b[32m1.2756\u001b[0m       0.2500            0.2500        \u001b[94m2.9660\u001b[0m  0.0006  0.4219\n",
      "      3            0.2444        \u001b[32m1.1555\u001b[0m       0.2500            0.2500        \u001b[94m2.7391\u001b[0m  0.0006  0.3906\n",
      "      4            \u001b[36m0.2889\u001b[0m        \u001b[32m0.9462\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m2.1106\u001b[0m  0.0006  0.3907\n",
      "      5            \u001b[36m0.5667\u001b[0m        \u001b[32m0.8552\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m1.5792\u001b[0m  0.0006  0.4531\n",
      "      6            \u001b[36m0.7111\u001b[0m        \u001b[32m0.7799\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.5303\u001b[0m  0.0006  0.4219\n",
      "      7            \u001b[36m0.7222\u001b[0m        \u001b[32m0.7128\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        1.6767  0.0006  0.3750\n",
      "      8            0.7000        0.7234       0.3368            0.3368        1.8787  0.0006  0.3440\n",
      "      9            \u001b[36m0.7333\u001b[0m        \u001b[32m0.5814\u001b[0m       0.3403            0.3403        1.9167  0.0006  0.3442\n",
      "     10            0.7333        \u001b[32m0.5328\u001b[0m       0.3403            0.3403        1.8601  0.0005  0.3438\n",
      "     11            \u001b[36m0.8000\u001b[0m        \u001b[32m0.4609\u001b[0m       0.3611            0.3611        1.7517  0.0005  0.3438\n",
      "     12            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4355\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        1.6165  0.0005  0.4201\n",
      "     13            \u001b[36m0.9444\u001b[0m        \u001b[32m0.3507\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.5264\u001b[0m  0.0005  0.3428\n",
      "     14            \u001b[36m0.9667\u001b[0m        0.3778       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.4794\u001b[0m  0.0005  0.3441\n",
      "     15            \u001b[36m0.9889\u001b[0m        0.3561       0.4201            0.4201        \u001b[94m1.4558\u001b[0m  0.0004  0.4812\n",
      "     16            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3504\u001b[0m       0.4201            0.4201        \u001b[94m1.4464\u001b[0m  0.0004  0.3303\n",
      "     17            1.0000        \u001b[32m0.2726\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        1.4562  0.0004  0.3445\n",
      "     18            1.0000        0.2851       0.4201            0.4201        1.4631  0.0004  0.3281\n",
      "     19            1.0000        \u001b[32m0.2702\u001b[0m       0.4097            0.4097        1.4706  0.0004  0.3283\n",
      "     20            1.0000        0.2906       0.4097            0.4097        1.4829  0.0003  0.3831\n",
      "     21            1.0000        \u001b[32m0.2570\u001b[0m       0.4132            0.4132        1.4876  0.0003  0.3623\n",
      "     22            1.0000        \u001b[32m0.2390\u001b[0m       0.3993            0.3993        1.4890  0.0003  0.3438\n",
      "     23            1.0000        \u001b[32m0.2083\u001b[0m       0.3958            0.3958        1.4933  0.0002  0.3438\n",
      "     24            1.0000        0.2350       0.3958            0.3958        1.4959  0.0002  0.3281\n",
      "     25            1.0000        0.2442       0.3958            0.3958        1.4923  0.0002  0.3285\n",
      "     26            1.0000        0.2201       0.3924            0.3924        1.4856  0.0002  0.3594\n",
      "     27            1.0000        0.2173       0.3889            0.3889        1.4813  0.0002  0.3445\n",
      "     28            1.0000        \u001b[32m0.2075\u001b[0m       0.3924            0.3924        1.4757  0.0001  0.3282\n",
      "     29            1.0000        \u001b[32m0.1823\u001b[0m       0.3889            0.3889        1.4708  0.0001  0.3281\n",
      "     30            1.0000        \u001b[32m0.1712\u001b[0m       0.3889            0.3889        1.4682  0.0001  0.3281\n",
      "     31            1.0000        0.1946       0.3958            0.3958        1.4661  0.0001  0.4573\n",
      "     32            1.0000        0.2198       0.3958            0.3958        1.4636  0.0001  0.3266\n",
      "     33            1.0000        \u001b[32m0.1492\u001b[0m       0.3958            0.3958        1.4612  0.0000  0.3438\n",
      "     34            1.0000        0.1681       0.3958            0.3958        1.4597  0.0000  0.4484\n",
      "     35            1.0000        \u001b[32m0.1439\u001b[0m       0.3958            0.3958        1.4585  0.0000  0.3423\n",
      "     36            1.0000        \u001b[32m0.1368\u001b[0m       0.3958            0.3958        1.4573  0.0000  0.3594\n",
      "     37            1.0000        0.1711       0.3993            0.3993        1.4564  0.0000  0.3906\n",
      "     38            1.0000        0.1778       0.3993            0.3993        1.4556  0.0000  0.4063\n",
      "     39            1.0000        0.1770       0.3993            0.3993        1.4550  0.0000  0.4375\n",
      "     40            1.0000        0.1639       0.3993            0.3993        1.4546  0.0000  0.3906\n",
      "Training model for subject 3 with 100 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2600\u001b[0m        \u001b[32m1.7078\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m2.6326\u001b[0m  0.0006  0.4067\n",
      "      2            \u001b[36m0.3400\u001b[0m        \u001b[32m1.2955\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        2.7689  0.0006  0.4063\n",
      "      3            \u001b[36m0.4000\u001b[0m        \u001b[32m1.0929\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m2.5836\u001b[0m  0.0006  0.4688\n",
      "      4            \u001b[36m0.4500\u001b[0m        \u001b[32m1.0187\u001b[0m       0.2917            0.2917        \u001b[94m2.3928\u001b[0m  0.0006  0.3594\n",
      "      5            \u001b[36m0.6000\u001b[0m        \u001b[32m0.9016\u001b[0m       0.3021            0.3021        \u001b[94m2.3080\u001b[0m  0.0006  0.3438\n",
      "      6            \u001b[36m0.6200\u001b[0m        \u001b[32m0.8026\u001b[0m       0.3229            0.3229        \u001b[94m2.2915\u001b[0m  0.0006  0.3594\n",
      "      7            0.6000        \u001b[32m0.7513\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m2.2372\u001b[0m  0.0006  0.3438\n",
      "      8            \u001b[36m0.6500\u001b[0m        0.7931       0.3472            0.3472        \u001b[94m2.1048\u001b[0m  0.0006  0.3594\n",
      "      9            \u001b[36m0.6900\u001b[0m        \u001b[32m0.6395\u001b[0m       0.3507            0.3507        \u001b[94m1.9936\u001b[0m  0.0006  0.3437\n",
      "     10            \u001b[36m0.7100\u001b[0m        \u001b[32m0.6143\u001b[0m       0.3472            0.3472        \u001b[94m1.9125\u001b[0m  0.0005  0.3438\n",
      "     11            \u001b[36m0.7600\u001b[0m        \u001b[32m0.6053\u001b[0m       0.3368            0.3368        \u001b[94m1.8651\u001b[0m  0.0005  0.3438\n",
      "     12            \u001b[36m0.8400\u001b[0m        \u001b[32m0.4989\u001b[0m       0.3368            0.3368        \u001b[94m1.8203\u001b[0m  0.0005  0.3438\n",
      "     13            \u001b[36m0.8800\u001b[0m        0.5236       0.3472            0.3472        \u001b[94m1.7749\u001b[0m  0.0005  0.3594\n",
      "     14            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4037\u001b[0m       0.3542            0.3542        \u001b[94m1.7448\u001b[0m  0.0005  0.3594\n",
      "     15            \u001b[36m0.9400\u001b[0m        0.4518       0.3542            0.3542        \u001b[94m1.7292\u001b[0m  0.0004  0.4698\n",
      "     16            \u001b[36m0.9600\u001b[0m        \u001b[32m0.3544\u001b[0m       0.3507            0.3507        \u001b[94m1.7074\u001b[0m  0.0004  0.3723\n",
      "     17            \u001b[36m0.9700\u001b[0m        \u001b[32m0.3528\u001b[0m       0.3438            0.3438        \u001b[94m1.6746\u001b[0m  0.0004  0.3438\n",
      "     18            \u001b[36m0.9900\u001b[0m        \u001b[32m0.3477\u001b[0m       0.3403            0.3403        \u001b[94m1.6313\u001b[0m  0.0004  0.3438\n",
      "     19            \u001b[36m1.0000\u001b[0m        0.4053       0.3576            0.3576        \u001b[94m1.5934\u001b[0m  0.0004  0.3438\n",
      "     20            1.0000        \u001b[32m0.3313\u001b[0m       0.3576            0.3576        \u001b[94m1.5729\u001b[0m  0.0003  0.3440\n",
      "     21            1.0000        \u001b[32m0.3004\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.5647\u001b[0m  0.0003  0.3594\n",
      "     22            1.0000        \u001b[32m0.2969\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.5579\u001b[0m  0.0003  0.3438\n",
      "     23            1.0000        \u001b[32m0.2440\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.5536\u001b[0m  0.0002  0.3438\n",
      "     24            1.0000        \u001b[32m0.2222\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.5500\u001b[0m  0.0002  0.3448\n",
      "     25            1.0000        0.2459       0.3750            0.3750        \u001b[94m1.5452\u001b[0m  0.0002  0.3438\n",
      "     26            1.0000        \u001b[32m0.1961\u001b[0m       0.3785            0.3785        \u001b[94m1.5422\u001b[0m  0.0002  0.3438\n",
      "     27            1.0000        0.2228       0.3785            0.3785        \u001b[94m1.5379\u001b[0m  0.0002  0.3438\n",
      "     28            1.0000        0.2634       0.3715            0.3715        \u001b[94m1.5354\u001b[0m  0.0001  0.3442\n",
      "     29            1.0000        0.2090       0.3750            0.3750        \u001b[94m1.5315\u001b[0m  0.0001  0.3594\n",
      "     30            1.0000        0.2206       0.3750            0.3750        \u001b[94m1.5289\u001b[0m  0.0001  0.3437\n",
      "     31            1.0000        0.2001       0.3785            0.3785        \u001b[94m1.5256\u001b[0m  0.0001  0.3442\n",
      "     32            1.0000        \u001b[32m0.1858\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.5233\u001b[0m  0.0001  0.3438\n",
      "     33            1.0000        0.1896       0.3854            0.3854        \u001b[94m1.5222\u001b[0m  0.0000  0.3438\n",
      "     34            1.0000        0.1896       0.3819            0.3819        \u001b[94m1.5210\u001b[0m  0.0000  0.4063\n",
      "     35            1.0000        0.2266       0.3785            0.3785        1.5212  0.0000  0.4532\n",
      "     36            1.0000        0.2150       0.3785            0.3785        1.5212  0.0000  0.4375\n",
      "     37            1.0000        \u001b[32m0.1651\u001b[0m       0.3819            0.3819        1.5211  0.0000  0.5068\n",
      "     38            1.0000        0.2059       0.3854            0.3854        1.5211  0.0000  0.6441\n",
      "     39            1.0000        0.1948       0.3854            0.3854        \u001b[94m1.5208\u001b[0m  0.0000  0.4699\n",
      "     40            1.0000        0.2398       0.3854            0.3854        \u001b[94m1.5206\u001b[0m  0.0000  0.4063\n",
      "Training model for subject 3 with 110 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3000\u001b[0m        \u001b[32m1.7624\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m2.8243\u001b[0m  0.0006  0.3750\n",
      "      2            \u001b[36m0.3636\u001b[0m        \u001b[32m1.3776\u001b[0m       0.2743            0.2743        \u001b[94m2.7730\u001b[0m  0.0006  0.3594\n",
      "      3            0.3455        \u001b[32m1.2117\u001b[0m       0.2708            0.2708        3.2332  0.0006  0.3594\n",
      "      4            \u001b[36m0.3818\u001b[0m        \u001b[32m1.0305\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        3.3430  0.0006  0.3594\n",
      "      5            0.3545        \u001b[32m0.9441\u001b[0m       0.2847            0.2847        3.2915  0.0006  0.3438\n",
      "      6            0.3364        \u001b[32m0.8037\u001b[0m       0.2882            0.2882        3.1178  0.0006  0.3750\n",
      "      7            \u001b[36m0.4091\u001b[0m        \u001b[32m0.6992\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        2.9025  0.0006  0.3676\n",
      "      8            \u001b[36m0.4364\u001b[0m        0.7497       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m2.7246\u001b[0m  0.0006  0.4299\n",
      "      9            \u001b[36m0.5545\u001b[0m        \u001b[32m0.6696\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m2.5017\u001b[0m  0.0006  0.4333\n",
      "     10            \u001b[36m0.5909\u001b[0m        \u001b[32m0.6181\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m2.3437\u001b[0m  0.0005  0.4453\n",
      "     11            \u001b[36m0.6182\u001b[0m        \u001b[32m0.5088\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m2.2147\u001b[0m  0.0005  0.4132\n",
      "     12            \u001b[36m0.7000\u001b[0m        \u001b[32m0.4698\u001b[0m       0.3750            0.3750        \u001b[94m2.0747\u001b[0m  0.0005  0.3979\n",
      "     13            \u001b[36m0.7727\u001b[0m        0.4989       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.9862\u001b[0m  0.0005  0.4184\n",
      "     14            \u001b[36m0.8273\u001b[0m        \u001b[32m0.4409\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.8777\u001b[0m  0.0005  0.4146\n",
      "     15            \u001b[36m0.8455\u001b[0m        0.5236       0.3958            0.3958        \u001b[94m1.7804\u001b[0m  0.0004  0.4117\n",
      "     16            \u001b[36m0.9182\u001b[0m        0.4440       0.3958            0.3958        \u001b[94m1.6858\u001b[0m  0.0004  0.3950\n",
      "     17            \u001b[36m0.9545\u001b[0m        \u001b[32m0.4307\u001b[0m       0.3958            0.3958        \u001b[94m1.6180\u001b[0m  0.0004  0.3660\n",
      "     18            \u001b[36m0.9727\u001b[0m        \u001b[32m0.3754\u001b[0m       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.5665\u001b[0m  0.0004  0.4031\n",
      "     19            \u001b[36m0.9909\u001b[0m        \u001b[32m0.2924\u001b[0m       0.4028            0.4028        \u001b[94m1.5380\u001b[0m  0.0004  0.3962\n",
      "     20            0.9909        0.3774       0.4167            0.4167        \u001b[94m1.5134\u001b[0m  0.0003  0.4281\n",
      "     21            0.9909        \u001b[32m0.2626\u001b[0m       0.4097            0.4097        \u001b[94m1.5011\u001b[0m  0.0003  0.4012\n",
      "     22            \u001b[36m1.0000\u001b[0m        0.3103       0.4132            0.4132        \u001b[94m1.4888\u001b[0m  0.0003  0.4372\n",
      "     23            1.0000        0.2652       0.4167            0.4167        \u001b[94m1.4837\u001b[0m  0.0002  0.3837\n",
      "     24            1.0000        0.3169       0.4028            0.4028        \u001b[94m1.4832\u001b[0m  0.0002  0.3916\n",
      "     25            1.0000        \u001b[32m0.2394\u001b[0m       0.4028            0.4028        \u001b[94m1.4819\u001b[0m  0.0002  0.4508\n",
      "     26            1.0000        0.2987       0.3993            0.3993        1.4828  0.0002  0.5643\n",
      "     27            1.0000        0.2560       0.4097            0.4097        \u001b[94m1.4812\u001b[0m  0.0002  0.4757\n",
      "     28            1.0000        0.2645       0.4062            0.4062        1.4813  0.0001  0.5078\n",
      "     29            1.0000        \u001b[32m0.2133\u001b[0m       0.3993            0.3993        \u001b[94m1.4806\u001b[0m  0.0001  0.5855\n",
      "     30            1.0000        0.2208       0.3993            0.3993        \u001b[94m1.4795\u001b[0m  0.0001  0.5057\n",
      "     31            1.0000        0.2559       0.4097            0.4097        \u001b[94m1.4754\u001b[0m  0.0001  0.4417\n",
      "     32            1.0000        0.2586       0.4167            0.4167        \u001b[94m1.4731\u001b[0m  0.0001  0.3651\n",
      "     33            1.0000        0.2135       0.4167            0.4167        \u001b[94m1.4709\u001b[0m  0.0000  0.3908\n",
      "     34            1.0000        0.2397       0.4201            0.4201        \u001b[94m1.4704\u001b[0m  0.0000  0.4064\n",
      "     35            1.0000        0.2413       0.4201            0.4201        \u001b[94m1.4698\u001b[0m  0.0000  0.5000\n",
      "     36            1.0000        \u001b[32m0.2043\u001b[0m       0.4201            0.4201        \u001b[94m1.4698\u001b[0m  0.0000  0.3609\n",
      "     37            1.0000        0.2406       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.4695\u001b[0m  0.0000  0.3594\n",
      "     38            1.0000        0.2126       0.4236            0.4236        \u001b[94m1.4691\u001b[0m  0.0000  0.3594\n",
      "     39            1.0000        0.2462       0.4236            0.4236        \u001b[94m1.4686\u001b[0m  0.0000  0.3774\n",
      "     40            1.0000        0.2236       0.4236            0.4236        \u001b[94m1.4684\u001b[0m  0.0000  0.3438\n",
      "Training model for subject 3 with 120 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2583\u001b[0m        \u001b[32m1.5958\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.2057\u001b[0m  0.0006  0.3750\n",
      "      2            \u001b[36m0.3417\u001b[0m        \u001b[32m1.3654\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m2.8787\u001b[0m  0.0006  0.3907\n",
      "      3            \u001b[36m0.4750\u001b[0m        \u001b[32m1.1894\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m2.3377\u001b[0m  0.0006  0.3754\n",
      "      4            \u001b[36m0.5583\u001b[0m        \u001b[32m0.9853\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m2.0586\u001b[0m  0.0006  0.3594\n",
      "      5            \u001b[36m0.5917\u001b[0m        \u001b[32m0.9395\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.8377\u001b[0m  0.0006  0.3750\n",
      "      6            \u001b[36m0.6583\u001b[0m        \u001b[32m0.7854\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.7073\u001b[0m  0.0006  0.4063\n",
      "      7            \u001b[36m0.7500\u001b[0m        \u001b[32m0.6875\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.5755\u001b[0m  0.0006  0.3906\n",
      "      8            \u001b[36m0.8500\u001b[0m        \u001b[32m0.6663\u001b[0m       0.3576            0.3576        \u001b[94m1.5100\u001b[0m  0.0006  0.3983\n",
      "      9            \u001b[36m0.8833\u001b[0m        \u001b[32m0.5876\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.4634\u001b[0m  0.0006  0.3750\n",
      "     10            \u001b[36m0.9250\u001b[0m        \u001b[32m0.5842\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        1.4673  0.0005  0.3750\n",
      "     11            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5263\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        1.4984  0.0005  0.3750\n",
      "     12            0.9250        \u001b[32m0.4923\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        1.5289  0.0005  0.3755\n",
      "     13            0.9167        \u001b[32m0.4370\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        1.5440  0.0005  0.3750\n",
      "     14            0.9333        \u001b[32m0.4242\u001b[0m       0.3924            0.3924        1.5282  0.0005  0.3751\n",
      "     15            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3632\u001b[0m       0.3785            0.3785        1.5095  0.0004  0.3594\n",
      "     16            \u001b[36m0.9833\u001b[0m        0.3636       0.3993            0.3993        1.5072  0.0004  0.3594\n",
      "     17            \u001b[36m0.9917\u001b[0m        0.3786       0.3889            0.3889        1.5162  0.0004  0.3907\n",
      "     18            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3044\u001b[0m       0.3924            0.3924        1.5285  0.0004  0.4067\n",
      "     19            1.0000        0.3130       0.3889            0.3889        1.5237  0.0004  0.4531\n",
      "     20            1.0000        0.3493       0.3993            0.3993        1.5102  0.0003  0.4688\n",
      "     21            1.0000        \u001b[32m0.2540\u001b[0m       0.3924            0.3924        1.5034  0.0003  0.4688\n",
      "     22            1.0000        0.3015       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        1.4975  0.0003  0.4375\n",
      "     23            1.0000        \u001b[32m0.2537\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        1.4935  0.0002  0.4844\n",
      "     24            1.0000        \u001b[32m0.2481\u001b[0m       0.4028            0.4028        1.4846  0.0002  0.4848\n",
      "     25            1.0000        \u001b[32m0.2400\u001b[0m       0.4062            0.4062        1.4766  0.0002  0.3750\n",
      "     26            1.0000        0.2550       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        1.4697  0.0002  0.3753\n",
      "     27            1.0000        0.2429       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        1.4649  0.0002  0.3594\n",
      "     28            1.0000        0.2719       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.4595\u001b[0m  0.0001  0.3754\n",
      "     29            1.0000        0.2653       0.4167            0.4167        \u001b[94m1.4577\u001b[0m  0.0001  0.3906\n",
      "     30            1.0000        0.2693       0.4132            0.4132        \u001b[94m1.4573\u001b[0m  0.0001  0.3594\n",
      "     31            1.0000        \u001b[32m0.2196\u001b[0m       0.4201            0.4201        \u001b[94m1.4551\u001b[0m  0.0001  0.3907\n",
      "     32            1.0000        0.2289       0.4167            0.4167        \u001b[94m1.4533\u001b[0m  0.0001  0.3594\n",
      "     33            1.0000        0.2222       0.4167            0.4167        \u001b[94m1.4511\u001b[0m  0.0000  0.3907\n",
      "     34            1.0000        \u001b[32m0.2153\u001b[0m       0.4132            0.4132        \u001b[94m1.4500\u001b[0m  0.0000  0.3750\n",
      "     35            1.0000        \u001b[32m0.2084\u001b[0m       0.4132            0.4132        \u001b[94m1.4498\u001b[0m  0.0000  0.3756\n",
      "     36            1.0000        0.2121       0.4167            0.4167        1.4498  0.0000  0.3750\n",
      "     37            1.0000        0.2592       0.4132            0.4132        \u001b[94m1.4497\u001b[0m  0.0000  0.3755\n",
      "     38            1.0000        0.2372       0.4132            0.4132        \u001b[94m1.4497\u001b[0m  0.0000  0.3750\n",
      "     39            1.0000        0.2094       0.4132            0.4132        1.4499  0.0000  0.3750\n",
      "     40            1.0000        \u001b[32m0.1902\u001b[0m       0.4167            0.4167        1.4498  0.0000  0.3750\n",
      "Training model for subject 3 with 130 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3308\u001b[0m        \u001b[32m1.6621\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m1.7572\u001b[0m  0.0006  0.3759\n",
      "      2            \u001b[36m0.4231\u001b[0m        \u001b[32m1.3325\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m1.6064\u001b[0m  0.0006  0.3907\n",
      "      3            \u001b[36m0.4692\u001b[0m        \u001b[32m1.1731\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        1.7987  0.0006  0.3911\n",
      "      4            0.4154        \u001b[32m1.0734\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        1.9369  0.0006  0.4063\n",
      "      5            0.3769        \u001b[32m0.9051\u001b[0m       0.3264            0.3264        2.2146  0.0006  0.3906\n",
      "      6            0.3538        \u001b[32m0.8236\u001b[0m       0.3125            0.3125        2.3360  0.0006  0.3911\n",
      "      7            0.3462        \u001b[32m0.7756\u001b[0m       0.2986            0.2986        2.4003  0.0006  0.3750\n",
      "      8            0.3615        \u001b[32m0.7435\u001b[0m       0.3056            0.3056        2.3247  0.0006  0.3906\n",
      "      9            0.4077        0.7640       0.3229            0.3229        2.2231  0.0006  0.3906\n",
      "     10            0.4692        \u001b[32m0.6449\u001b[0m       0.3472            0.3472        2.0760  0.0005  0.3750\n",
      "     11            \u001b[36m0.5385\u001b[0m        \u001b[32m0.6397\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        1.9764  0.0005  0.3907\n",
      "     12            \u001b[36m0.6000\u001b[0m        \u001b[32m0.5701\u001b[0m       0.3681            0.3681        1.8776  0.0005  0.4693\n",
      "     13            \u001b[36m0.6846\u001b[0m        \u001b[32m0.5137\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        1.7976  0.0005  0.4688\n",
      "     14            \u001b[36m0.7538\u001b[0m        0.5675       0.3750            0.3750        1.7190  0.0005  0.4379\n",
      "     15            \u001b[36m0.7846\u001b[0m        \u001b[32m0.5082\u001b[0m       0.3819            0.3819        1.6679  0.0004  0.4844\n",
      "     16            \u001b[36m0.8154\u001b[0m        \u001b[32m0.4875\u001b[0m       0.3785            0.3785        1.6234  0.0004  0.4844\n",
      "     17            \u001b[36m0.8769\u001b[0m        \u001b[32m0.4421\u001b[0m       0.3785            0.3785        \u001b[94m1.5791\u001b[0m  0.0004  0.4844\n",
      "     18            \u001b[36m0.9231\u001b[0m        \u001b[32m0.4205\u001b[0m       0.3576            0.3576        \u001b[94m1.5388\u001b[0m  0.0004  0.3916\n",
      "     19            \u001b[36m0.9308\u001b[0m        \u001b[32m0.3925\u001b[0m       0.3646            0.3646        \u001b[94m1.5091\u001b[0m  0.0004  0.3907\n",
      "     20            \u001b[36m0.9462\u001b[0m        0.4345       0.3681            0.3681        \u001b[94m1.4858\u001b[0m  0.0003  0.3906\n",
      "     21            \u001b[36m0.9692\u001b[0m        \u001b[32m0.3598\u001b[0m       0.3646            0.3646        \u001b[94m1.4706\u001b[0m  0.0003  0.3750\n",
      "     22            \u001b[36m0.9769\u001b[0m        \u001b[32m0.3482\u001b[0m       0.3750            0.3750        \u001b[94m1.4616\u001b[0m  0.0003  0.3906\n",
      "     23            \u001b[36m0.9846\u001b[0m        \u001b[32m0.3362\u001b[0m       0.3681            0.3681        \u001b[94m1.4559\u001b[0m  0.0002  0.3907\n",
      "     24            \u001b[36m1.0000\u001b[0m        0.3507       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.4495\u001b[0m  0.0002  0.3906\n",
      "     25            1.0000        \u001b[32m0.2713\u001b[0m       0.3924            0.3924        \u001b[94m1.4482\u001b[0m  0.0002  0.3907\n",
      "     26            1.0000        0.3245       0.3924            0.3924        1.4511  0.0002  0.3907\n",
      "     27            1.0000        0.2837       0.3854            0.3854        1.4523  0.0002  0.3913\n",
      "     28            1.0000        0.3027       0.3854            0.3854        1.4524  0.0001  0.3906\n",
      "     29            1.0000        \u001b[32m0.2672\u001b[0m       0.3819            0.3819        1.4531  0.0001  0.3750\n",
      "     30            1.0000        0.3078       0.3785            0.3785        1.4535  0.0001  0.3907\n",
      "     31            1.0000        0.2884       0.3785            0.3785        1.4535  0.0001  0.3911\n",
      "     32            1.0000        0.2936       0.3819            0.3819        1.4537  0.0001  0.3906\n",
      "     33            1.0000        0.2876       0.3785            0.3785        1.4541  0.0000  0.3914\n",
      "     34            1.0000        0.2770       0.3750            0.3750        1.4546  0.0000  0.3906\n",
      "     35            1.0000        0.3290       0.3750            0.3750        1.4546  0.0000  0.3750\n",
      "     36            1.0000        0.2754       0.3750            0.3750        1.4545  0.0000  0.3911\n",
      "     37            1.0000        0.2900       0.3750            0.3750        1.4543  0.0000  0.3750\n",
      "     38            1.0000        0.3146       0.3750            0.3750        1.4543  0.0000  0.3907\n",
      "     39            1.0000        0.2774       0.3750            0.3750        1.4541  0.0000  0.3907\n",
      "     40            1.0000        0.2991       0.3750            0.3750        1.4541  0.0000  0.3906\n",
      "Training model for subject 3 with 140 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3714\u001b[0m        \u001b[32m1.6333\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m2.3903\u001b[0m  0.0006  0.3915\n",
      "      2            \u001b[36m0.4071\u001b[0m        \u001b[32m1.3706\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        \u001b[94m2.3195\u001b[0m  0.0006  0.3906\n",
      "      3            \u001b[36m0.4643\u001b[0m        \u001b[32m1.1554\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        \u001b[94m2.1526\u001b[0m  0.0006  0.3750\n",
      "      4            0.4571        \u001b[32m1.0466\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        2.1830  0.0006  0.4375\n",
      "      5            0.4357        \u001b[32m1.0459\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        2.3183  0.0006  0.4698\n",
      "      6            \u001b[36m0.4714\u001b[0m        \u001b[32m0.8725\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        2.3100  0.0006  0.5000\n",
      "      7            \u001b[36m0.4786\u001b[0m        \u001b[32m0.8388\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        2.2914  0.0006  0.4375\n",
      "      8            \u001b[36m0.4929\u001b[0m        \u001b[32m0.7494\u001b[0m       0.3611            0.3611        2.2732  0.0006  0.5157\n",
      "      9            \u001b[36m0.5214\u001b[0m        \u001b[32m0.7262\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        2.1542  0.0006  0.4688\n",
      "     10            \u001b[36m0.5714\u001b[0m        0.7691       0.3646            0.3646        \u001b[94m2.0590\u001b[0m  0.0005  0.4219\n",
      "     11            \u001b[36m0.6143\u001b[0m        \u001b[32m0.6070\u001b[0m       0.3646            0.3646        \u001b[94m1.9829\u001b[0m  0.0005  0.3750\n",
      "     12            \u001b[36m0.6429\u001b[0m        \u001b[32m0.5832\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.9016\u001b[0m  0.0005  0.3907\n",
      "     13            \u001b[36m0.6571\u001b[0m        \u001b[32m0.5066\u001b[0m       0.3715            0.3715        \u001b[94m1.8500\u001b[0m  0.0005  0.3750\n",
      "     14            \u001b[36m0.7000\u001b[0m        0.5173       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.8226\u001b[0m  0.0005  0.3907\n",
      "     15            \u001b[36m0.7357\u001b[0m        0.5129       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.7732\u001b[0m  0.0004  0.3753\n",
      "     16            \u001b[36m0.7786\u001b[0m        \u001b[32m0.4803\u001b[0m       0.3854            0.3854        \u001b[94m1.7032\u001b[0m  0.0004  0.3755\n",
      "     17            \u001b[36m0.8357\u001b[0m        \u001b[32m0.4354\u001b[0m       0.3889            0.3889        \u001b[94m1.6423\u001b[0m  0.0004  0.3907\n",
      "     18            \u001b[36m0.8857\u001b[0m        \u001b[32m0.4291\u001b[0m       0.3889            0.3889        \u001b[94m1.5851\u001b[0m  0.0004  0.3907\n",
      "     19            \u001b[36m0.9429\u001b[0m        \u001b[32m0.4115\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.5439\u001b[0m  0.0004  0.3908\n",
      "     20            \u001b[36m0.9714\u001b[0m        \u001b[32m0.3980\u001b[0m       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.5153\u001b[0m  0.0003  0.3750\n",
      "     21            \u001b[36m0.9786\u001b[0m        \u001b[32m0.3736\u001b[0m       0.3993            0.3993        \u001b[94m1.4991\u001b[0m  0.0003  0.3907\n",
      "     22            \u001b[36m0.9857\u001b[0m        0.4288       0.3854            0.3854        \u001b[94m1.4842\u001b[0m  0.0003  0.3751\n",
      "     23            0.9857        \u001b[32m0.3623\u001b[0m       0.3785            0.3785        \u001b[94m1.4808\u001b[0m  0.0002  0.3754\n",
      "     24            \u001b[36m0.9929\u001b[0m        0.3798       0.3958            0.3958        \u001b[94m1.4784\u001b[0m  0.0002  0.3907\n",
      "     25            0.9929        \u001b[32m0.3224\u001b[0m       0.3958            0.3958        \u001b[94m1.4747\u001b[0m  0.0002  0.3906\n",
      "     26            0.9929        0.3370       0.3958            0.3958        \u001b[94m1.4703\u001b[0m  0.0002  0.3910\n",
      "     27            0.9929        \u001b[32m0.3037\u001b[0m       0.4028            0.4028        \u001b[94m1.4667\u001b[0m  0.0002  0.3907\n",
      "     28            0.9929        \u001b[32m0.2771\u001b[0m       0.4062            0.4062        \u001b[94m1.4626\u001b[0m  0.0001  0.3750\n",
      "     29            0.9929        0.2934       0.4028            0.4028        \u001b[94m1.4575\u001b[0m  0.0001  0.3907\n",
      "     30            0.9929        0.2973       0.3993            0.3993        \u001b[94m1.4514\u001b[0m  0.0001  0.3752\n",
      "     31            0.9929        0.3124       0.3993            0.3993        \u001b[94m1.4466\u001b[0m  0.0001  0.3911\n",
      "     32            0.9929        0.3198       0.3958            0.3958        \u001b[94m1.4420\u001b[0m  0.0001  0.3909\n",
      "     33            0.9929        0.3034       0.3993            0.3993        \u001b[94m1.4387\u001b[0m  0.0000  0.3755\n",
      "     34            0.9929        0.2781       0.4028            0.4028        \u001b[94m1.4363\u001b[0m  0.0000  0.3750\n",
      "     35            0.9929        0.3186       0.4028            0.4028        \u001b[94m1.4347\u001b[0m  0.0000  0.4063\n",
      "     36            0.9929        0.3100       0.3993            0.3993        \u001b[94m1.4337\u001b[0m  0.0000  0.4219\n",
      "     37            0.9929        0.2964       0.3993            0.3993        \u001b[94m1.4324\u001b[0m  0.0000  0.4375\n",
      "     38            0.9929        0.2880       0.4028            0.4028        \u001b[94m1.4314\u001b[0m  0.0000  0.4692\n",
      "     39            0.9929        0.2786       0.4028            0.4028        \u001b[94m1.4301\u001b[0m  0.0000  0.4375\n",
      "     40            0.9929        0.2978       0.4028            0.4028        \u001b[94m1.4290\u001b[0m  0.0000  0.4375\n",
      "Training model for subject 3 with 150 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2667\u001b[0m        \u001b[32m1.8578\u001b[0m       \u001b[35m0.2396\u001b[0m            \u001b[31m0.2396\u001b[0m        \u001b[94m3.5816\u001b[0m  0.0006  0.4375\n",
      "      2            \u001b[36m0.3067\u001b[0m        \u001b[32m1.5825\u001b[0m       \u001b[35m0.2465\u001b[0m            \u001b[31m0.2465\u001b[0m        \u001b[94m2.0150\u001b[0m  0.0006  0.4860\n",
      "      3            \u001b[36m0.5800\u001b[0m        \u001b[32m1.2514\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        \u001b[94m1.4439\u001b[0m  0.0006  0.3907\n",
      "      4            0.4667        \u001b[32m1.0741\u001b[0m       0.2882            0.2882        1.6562  0.0006  0.3907\n",
      "      5            0.3733        \u001b[32m0.9902\u001b[0m       0.2743            0.2743        1.8484  0.0006  0.4065\n",
      "      6            0.3600        0.9989       0.2743            0.2743        1.9419  0.0006  0.3907\n",
      "      7            0.4133        \u001b[32m0.9292\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        1.8435  0.0006  0.3907\n",
      "      8            0.4267        0.9622       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        1.8015  0.0006  0.3907\n",
      "      9            0.4600        \u001b[32m0.8047\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        1.7862  0.0006  0.3907\n",
      "     10            0.4600        \u001b[32m0.7240\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        1.8242  0.0005  0.3980\n",
      "     11            0.4600        0.7589       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        1.8292  0.0005  0.3907\n",
      "     12            0.4933        \u001b[32m0.6658\u001b[0m       0.3194            0.3194        1.8049  0.0005  0.3907\n",
      "     13            0.5267        \u001b[32m0.6350\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        1.8075  0.0005  0.4844\n",
      "     14            0.5800        \u001b[32m0.5602\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        1.7758  0.0005  0.3911\n",
      "     15            \u001b[36m0.6867\u001b[0m        0.5620       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        1.7098  0.0004  0.3910\n",
      "     16            \u001b[36m0.7800\u001b[0m        \u001b[32m0.5427\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        1.6416  0.0004  0.3907\n",
      "     17            \u001b[36m0.8267\u001b[0m        \u001b[32m0.5089\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        1.5904  0.0004  0.3750\n",
      "     18            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4991\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        1.5343  0.0004  0.3750\n",
      "     19            \u001b[36m0.9067\u001b[0m        \u001b[32m0.4570\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        1.5007  0.0004  0.3755\n",
      "     20            \u001b[36m0.9533\u001b[0m        0.4919       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        1.4723  0.0003  0.3909\n",
      "     21            \u001b[36m0.9667\u001b[0m        0.4633       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        1.4571  0.0003  0.3907\n",
      "     22            \u001b[36m0.9800\u001b[0m        \u001b[32m0.3601\u001b[0m       0.3854            0.3854        \u001b[94m1.4427\u001b[0m  0.0003  0.3909\n",
      "     23            \u001b[36m0.9867\u001b[0m        0.4216       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4330\u001b[0m  0.0002  0.3750\n",
      "     24            0.9867        0.3753       0.3924            0.3924        \u001b[94m1.4285\u001b[0m  0.0002  0.3750\n",
      "     25            0.9867        0.4218       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.4276\u001b[0m  0.0002  0.3750\n",
      "     26            0.9867        0.4074       0.3924            0.3924        \u001b[94m1.4245\u001b[0m  0.0002  0.3751\n",
      "     27            0.9867        0.3790       0.3889            0.3889        \u001b[94m1.4211\u001b[0m  0.0002  0.3758\n",
      "     28            \u001b[36m0.9933\u001b[0m        0.3824       0.3924            0.3924        \u001b[94m1.4174\u001b[0m  0.0001  0.4375\n",
      "     29            0.9933        0.3668       0.3958            0.3958        \u001b[94m1.4142\u001b[0m  0.0001  0.4532\n",
      "     30            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3443\u001b[0m       0.3924            0.3924        \u001b[94m1.4132\u001b[0m  0.0001  0.4844\n",
      "     31            1.0000        0.3521       0.3958            0.3958        \u001b[94m1.4125\u001b[0m  0.0001  0.4844\n",
      "     32            1.0000        0.3566       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.4117\u001b[0m  0.0001  0.4532\n",
      "     33            1.0000        \u001b[32m0.3417\u001b[0m       0.4028            0.4028        \u001b[94m1.4117\u001b[0m  0.0000  0.4688\n",
      "     34            1.0000        0.3437       0.4028            0.4028        \u001b[94m1.4104\u001b[0m  0.0000  0.4532\n",
      "     35            1.0000        0.3457       0.4028            0.4028        \u001b[94m1.4088\u001b[0m  0.0000  0.3907\n",
      "     36            1.0000        0.3544       0.4028            0.4028        \u001b[94m1.4081\u001b[0m  0.0000  0.3911\n",
      "     37            1.0000        0.3907       0.3993            0.3993        \u001b[94m1.4072\u001b[0m  0.0000  0.3910\n",
      "     38            1.0000        0.3586       0.3993            0.3993        \u001b[94m1.4065\u001b[0m  0.0000  0.3759\n",
      "     39            1.0000        \u001b[32m0.3379\u001b[0m       0.3993            0.3993        1.4065  0.0000  0.3907\n",
      "     40            1.0000        \u001b[32m0.3145\u001b[0m       0.3993            0.3993        \u001b[94m1.4064\u001b[0m  0.0000  0.3907\n",
      "Training model for subject 3 with 160 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.6352\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.0776\u001b[0m  0.0006  0.3753\n",
      "      2            0.2500        \u001b[32m1.4261\u001b[0m       0.2500            0.2500        \u001b[94m2.8430\u001b[0m  0.0006  0.3907\n",
      "      3            \u001b[36m0.2750\u001b[0m        \u001b[32m1.1505\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m2.5695\u001b[0m  0.0006  0.3907\n",
      "      4            0.2625        \u001b[32m1.0103\u001b[0m       0.2500            0.2500        3.0535  0.0006  0.3750\n",
      "      5            0.2625        1.0427       0.2535            0.2535        3.3210  0.0006  0.3912\n",
      "      6            0.2625        \u001b[32m0.9447\u001b[0m       0.2535            0.2535        3.1609  0.0006  0.3750\n",
      "      7            \u001b[36m0.2875\u001b[0m        \u001b[32m0.8865\u001b[0m       0.2535            0.2535        3.0147  0.0006  0.3916\n",
      "      8            0.2875        \u001b[32m0.8201\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        2.9641  0.0006  0.3916\n",
      "      9            \u001b[36m0.3312\u001b[0m        \u001b[32m0.7682\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        2.6835  0.0006  0.3752\n",
      "     10            \u001b[36m0.4000\u001b[0m        0.7911       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m2.3102\u001b[0m  0.0005  0.3906\n",
      "     11            \u001b[36m0.5437\u001b[0m        \u001b[32m0.6893\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        \u001b[94m1.9860\u001b[0m  0.0005  0.3750\n",
      "     12            \u001b[36m0.6438\u001b[0m        \u001b[32m0.6001\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        \u001b[94m1.8089\u001b[0m  0.0005  0.3750\n",
      "     13            \u001b[36m0.7000\u001b[0m        0.6570       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m1.7150\u001b[0m  0.0005  0.3911\n",
      "     14            \u001b[36m0.7625\u001b[0m        0.6118       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.6554\u001b[0m  0.0005  0.3908\n",
      "     15            \u001b[36m0.7937\u001b[0m        \u001b[32m0.5707\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.5922\u001b[0m  0.0004  0.3750\n",
      "     16            \u001b[36m0.8562\u001b[0m        \u001b[32m0.5067\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.5357\u001b[0m  0.0004  0.3757\n",
      "     17            \u001b[36m0.8938\u001b[0m        0.5171       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.4890\u001b[0m  0.0004  0.3754\n",
      "     18            \u001b[36m0.9375\u001b[0m        \u001b[32m0.4234\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4688\u001b[0m  0.0004  0.3907\n",
      "     19            \u001b[36m0.9437\u001b[0m        0.5101       0.3889            0.3889        \u001b[94m1.4669\u001b[0m  0.0004  0.4074\n",
      "     20            \u001b[36m0.9563\u001b[0m        0.4358       0.3958            0.3958        \u001b[94m1.4669\u001b[0m  0.0003  0.4537\n",
      "     21            \u001b[36m0.9750\u001b[0m        \u001b[32m0.4133\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        1.4734  0.0003  0.4844\n",
      "     22            \u001b[36m0.9812\u001b[0m        \u001b[32m0.4089\u001b[0m       0.3854            0.3854        1.4824  0.0003  0.4532\n",
      "     23            0.9812        0.4277       0.3854            0.3854        1.4860  0.0002  0.4375\n",
      "     24            \u001b[36m0.9875\u001b[0m        0.4329       0.3924            0.3924        1.4853  0.0002  0.4844\n",
      "     25            \u001b[36m0.9938\u001b[0m        \u001b[32m0.3788\u001b[0m       0.3924            0.3924        1.4781  0.0002  0.4844\n",
      "     26            0.9938        \u001b[32m0.3376\u001b[0m       0.3924            0.3924        1.4731  0.0002  0.3912\n",
      "     27            0.9938        0.3600       0.3889            0.3889        1.4706  0.0002  0.4063\n",
      "     28            0.9938        0.4026       0.3854            0.3854        1.4682  0.0001  0.3750\n",
      "     29            \u001b[36m1.0000\u001b[0m        0.3723       0.3854            0.3854        1.4677  0.0001  0.3906\n",
      "     30            1.0000        \u001b[32m0.3251\u001b[0m       0.3889            0.3889        1.4677  0.0001  0.3907\n",
      "     31            1.0000        0.3386       0.3958            0.3958        \u001b[94m1.4664\u001b[0m  0.0001  0.3907\n",
      "     32            1.0000        0.3376       0.3958            0.3958        \u001b[94m1.4655\u001b[0m  0.0001  0.3754\n",
      "     33            1.0000        0.3354       0.3958            0.3958        \u001b[94m1.4635\u001b[0m  0.0000  0.3752\n",
      "     34            1.0000        0.3376       0.3958            0.3958        \u001b[94m1.4614\u001b[0m  0.0000  0.3907\n",
      "     35            1.0000        \u001b[32m0.2774\u001b[0m       0.3958            0.3958        \u001b[94m1.4593\u001b[0m  0.0000  0.3907\n",
      "     36            1.0000        0.3524       0.3924            0.3924        \u001b[94m1.4580\u001b[0m  0.0000  0.3755\n",
      "     37            1.0000        0.3471       0.3924            0.3924        \u001b[94m1.4573\u001b[0m  0.0000  0.3906\n",
      "     38            1.0000        0.3683       0.3924            0.3924        \u001b[94m1.4565\u001b[0m  0.0000  0.3907\n",
      "     39            1.0000        0.3585       0.3924            0.3924        \u001b[94m1.4553\u001b[0m  0.0000  0.3907\n",
      "     40            1.0000        0.2984       0.3924            0.3924        \u001b[94m1.4542\u001b[0m  0.0000  0.3750\n",
      "Training model for subject 3 with 170 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2471\u001b[0m        \u001b[32m1.7285\u001b[0m       \u001b[35m0.2465\u001b[0m            \u001b[31m0.2465\u001b[0m        \u001b[94m2.5159\u001b[0m  0.0006  0.3907\n",
      "      2            \u001b[36m0.2588\u001b[0m        \u001b[32m1.4723\u001b[0m       0.2431            0.2431        2.6859  0.0006  0.3907\n",
      "      3            0.2471        \u001b[32m1.3417\u001b[0m       0.2431            0.2431        3.0780  0.0006  0.3906\n",
      "      4            0.2471        \u001b[32m1.1437\u001b[0m       0.2465            0.2465        3.5565  0.0006  0.4063\n",
      "      5            0.2471        \u001b[32m0.9629\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        3.8089  0.0006  0.4063\n",
      "      6            0.2471        1.0054       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        3.5158  0.0006  0.3751\n",
      "      7            0.2588        \u001b[32m0.9232\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        3.1777  0.0006  0.3907\n",
      "      8            \u001b[36m0.2706\u001b[0m        0.9693       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        2.8455  0.0006  0.3906\n",
      "      9            \u001b[36m0.3000\u001b[0m        \u001b[32m0.8124\u001b[0m       0.2604            0.2604        2.6233  0.0006  0.3750\n",
      "     10            \u001b[36m0.3235\u001b[0m        \u001b[32m0.7387\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m2.4763\u001b[0m  0.0005  0.4223\n",
      "     11            \u001b[36m0.3882\u001b[0m        \u001b[32m0.5855\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m2.2337\u001b[0m  0.0005  0.4688\n",
      "     12            \u001b[36m0.4529\u001b[0m        0.6839       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m1.9972\u001b[0m  0.0005  0.4532\n",
      "     13            \u001b[36m0.6000\u001b[0m        0.6604       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        \u001b[94m1.7629\u001b[0m  0.0005  0.5000\n",
      "     14            \u001b[36m0.6941\u001b[0m        0.6108       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m1.6011\u001b[0m  0.0005  0.4375\n",
      "     15            \u001b[36m0.8059\u001b[0m        0.6140       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.4919\u001b[0m  0.0004  0.5000\n",
      "     16            \u001b[36m0.8706\u001b[0m        \u001b[32m0.5085\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.4253\u001b[0m  0.0004  0.4532\n",
      "     17            \u001b[36m0.9118\u001b[0m        \u001b[32m0.4933\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.3923\u001b[0m  0.0004  0.4063\n",
      "     18            \u001b[36m0.9235\u001b[0m        \u001b[32m0.4468\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.3845\u001b[0m  0.0004  0.3907\n",
      "     19            \u001b[36m0.9412\u001b[0m        \u001b[32m0.4274\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        1.3858  0.0004  0.3750\n",
      "     20            \u001b[36m0.9529\u001b[0m        0.4620       0.4062            0.4062        \u001b[94m1.3796\u001b[0m  0.0003  0.3750\n",
      "     21            0.9529        0.4683       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.3721\u001b[0m  0.0003  0.3750\n",
      "     22            \u001b[36m0.9588\u001b[0m        \u001b[32m0.4153\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.3687\u001b[0m  0.0003  0.3911\n",
      "     23            0.9588        0.4371       0.4271            0.4271        \u001b[94m1.3587\u001b[0m  0.0002  0.3911\n",
      "     24            0.9588        0.4290       0.4236            0.4236        \u001b[94m1.3488\u001b[0m  0.0002  0.3755\n",
      "     25            0.9588        0.4584       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.3413\u001b[0m  0.0002  0.3911\n",
      "     26            0.9588        \u001b[32m0.3757\u001b[0m       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.3378\u001b[0m  0.0002  0.3907\n",
      "     27            0.9588        0.4005       0.4375            0.4375        \u001b[94m1.3345\u001b[0m  0.0002  0.3750\n",
      "     28            \u001b[36m0.9647\u001b[0m        0.4103       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.3305\u001b[0m  0.0001  0.3911\n",
      "     29            0.9647        0.3936       0.4410            0.4410        \u001b[94m1.3285\u001b[0m  0.0001  0.3754\n",
      "     30            0.9647        \u001b[32m0.3417\u001b[0m       0.4410            0.4410        \u001b[94m1.3251\u001b[0m  0.0001  0.3907\n",
      "     31            0.9647        0.3610       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.3222\u001b[0m  0.0001  0.3750\n",
      "     32            0.9647        0.3683       0.4444            0.4444        \u001b[94m1.3214\u001b[0m  0.0001  0.3754\n",
      "     33            0.9647        0.3418       0.4410            0.4410        \u001b[94m1.3196\u001b[0m  0.0000  0.3750\n",
      "     34            0.9647        \u001b[32m0.3348\u001b[0m       0.4410            0.4410        \u001b[94m1.3177\u001b[0m  0.0000  0.3911\n",
      "     35            \u001b[36m0.9706\u001b[0m        0.3801       0.4410            0.4410        \u001b[94m1.3167\u001b[0m  0.0000  0.3911\n",
      "     36            0.9706        0.3417       0.4444            0.4444        \u001b[94m1.3154\u001b[0m  0.0000  0.3911\n",
      "     37            0.9706        0.3777       0.4444            0.4444        \u001b[94m1.3152\u001b[0m  0.0000  0.3752\n",
      "     38            0.9706        0.3512       0.4410            0.4410        \u001b[94m1.3134\u001b[0m  0.0000  0.3907\n",
      "     39            0.9706        0.4834       0.4444            0.4444        \u001b[94m1.3126\u001b[0m  0.0000  0.3754\n",
      "     40            0.9706        0.3999       0.4444            0.4444        1.3135  0.0000  0.3906\n",
      "Training model for subject 3 with 180 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2389\u001b[0m        \u001b[32m1.5782\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m2.8552\u001b[0m  0.0006  0.4223\n",
      "      2            \u001b[36m0.2500\u001b[0m        \u001b[32m1.3087\u001b[0m       0.2500            0.2500        4.3684  0.0006  0.4844\n",
      "      3            0.2500        \u001b[32m1.1308\u001b[0m       0.2500            0.2500        5.9974  0.0006  0.5313\n",
      "      4            0.2500        \u001b[32m1.1180\u001b[0m       0.2500            0.2500        6.4655  0.0006  0.4532\n",
      "      5            0.2500        \u001b[32m1.0239\u001b[0m       0.2500            0.2500        6.3291  0.0006  0.4532\n",
      "      6            0.2500        \u001b[32m0.9525\u001b[0m       0.2500            0.2500        5.5701  0.0006  0.5000\n",
      "      7            0.2500        \u001b[32m0.8703\u001b[0m       0.2500            0.2500        4.7549  0.0006  0.4376\n",
      "      8            0.2500        0.8951       0.2535            0.2535        4.0080  0.0006  0.3907\n",
      "      9            0.2500        0.8905       0.2535            0.2535        3.5506  0.0006  0.3910\n",
      "     10            0.2500        \u001b[32m0.8105\u001b[0m       0.2535            0.2535        3.3434  0.0005  0.3751\n",
      "     11            \u001b[36m0.2556\u001b[0m        \u001b[32m0.7674\u001b[0m       0.2535            0.2535        3.1180  0.0005  0.3755\n",
      "     12            \u001b[36m0.2667\u001b[0m        \u001b[32m0.7217\u001b[0m       0.2535            0.2535        2.9107  0.0005  0.3750\n",
      "     13            \u001b[36m0.2833\u001b[0m        0.7274       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m2.7258\u001b[0m  0.0005  0.3753\n",
      "     14            \u001b[36m0.3111\u001b[0m        \u001b[32m0.6494\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m2.4775\u001b[0m  0.0005  0.3755\n",
      "     15            \u001b[36m0.3556\u001b[0m        \u001b[32m0.6374\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m2.2307\u001b[0m  0.0004  0.3906\n",
      "     16            \u001b[36m0.4222\u001b[0m        \u001b[32m0.5946\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        \u001b[94m2.0774\u001b[0m  0.0004  0.3906\n",
      "     17            \u001b[36m0.5889\u001b[0m        0.6495       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m1.8630\u001b[0m  0.0004  0.3910\n",
      "     18            \u001b[36m0.7000\u001b[0m        \u001b[32m0.4886\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.6539\u001b[0m  0.0004  0.3750\n",
      "     19            \u001b[36m0.7833\u001b[0m        0.5043       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.5262\u001b[0m  0.0004  0.3907\n",
      "     20            \u001b[36m0.8389\u001b[0m        0.5408       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.4268\u001b[0m  0.0003  0.3906\n",
      "     21            \u001b[36m0.8889\u001b[0m        0.5209       0.4201            0.4201        \u001b[94m1.3647\u001b[0m  0.0003  0.3906\n",
      "     22            \u001b[36m0.9167\u001b[0m        0.4973       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.3268\u001b[0m  0.0003  0.3912\n",
      "     23            \u001b[36m0.9278\u001b[0m        \u001b[32m0.4677\u001b[0m       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.3021\u001b[0m  0.0002  0.3751\n",
      "     24            \u001b[36m0.9444\u001b[0m        \u001b[32m0.4447\u001b[0m       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.2790\u001b[0m  0.0002  0.3750\n",
      "     25            \u001b[36m0.9556\u001b[0m        0.4820       0.4618            0.4618        \u001b[94m1.2605\u001b[0m  0.0002  0.3906\n",
      "     26            \u001b[36m0.9722\u001b[0m        0.4552       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        \u001b[94m1.2469\u001b[0m  0.0002  0.3907\n",
      "     27            \u001b[36m0.9778\u001b[0m        0.4782       0.4653            0.4653        \u001b[94m1.2362\u001b[0m  0.0002  0.3915\n",
      "     28            0.9778        \u001b[32m0.4115\u001b[0m       0.4618            0.4618        \u001b[94m1.2269\u001b[0m  0.0001  0.3906\n",
      "     29            0.9722        \u001b[32m0.3856\u001b[0m       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        \u001b[94m1.2174\u001b[0m  0.0001  0.3750\n",
      "     30            0.9722        0.4281       \u001b[35m0.5000\u001b[0m            \u001b[31m0.5000\u001b[0m        \u001b[94m1.2098\u001b[0m  0.0001  0.5157\n",
      "     31            0.9722        \u001b[32m0.3786\u001b[0m       \u001b[35m0.5069\u001b[0m            \u001b[31m0.5069\u001b[0m        \u001b[94m1.2041\u001b[0m  0.0001  0.4018\n",
      "     32            0.9722        0.4796       \u001b[35m0.5104\u001b[0m            \u001b[31m0.5104\u001b[0m        \u001b[94m1.1992\u001b[0m  0.0001  0.4531\n",
      "     33            0.9722        0.3973       0.5104            0.5104        \u001b[94m1.1972\u001b[0m  0.0000  0.4844\n",
      "     34            0.9722        \u001b[32m0.3696\u001b[0m       0.5069            0.5069        \u001b[94m1.1956\u001b[0m  0.0000  0.5001\n",
      "     35            0.9722        0.4266       \u001b[35m0.5139\u001b[0m            \u001b[31m0.5139\u001b[0m        \u001b[94m1.1940\u001b[0m  0.0000  0.5314\n",
      "     36            0.9722        0.4386       \u001b[35m0.5174\u001b[0m            \u001b[31m0.5174\u001b[0m        \u001b[94m1.1929\u001b[0m  0.0000  0.5004\n",
      "     37            0.9722        0.4410       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        \u001b[94m1.1922\u001b[0m  0.0000  0.4380\n",
      "     38            0.9722        \u001b[32m0.3653\u001b[0m       0.5174            0.5174        \u001b[94m1.1919\u001b[0m  0.0000  0.3753\n",
      "     39            0.9722        0.4349       0.5139            0.5139        \u001b[94m1.1917\u001b[0m  0.0000  0.3915\n",
      "     40            0.9722        0.3782       0.5139            0.5139        1.1917  0.0000  0.3907\n",
      "Training model for subject 3 with 190 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2474\u001b[0m        \u001b[32m1.7339\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m5.9324\u001b[0m  0.0006  0.3907\n",
      "      2            \u001b[36m0.3053\u001b[0m        \u001b[32m1.4012\u001b[0m       0.2500            0.2500        \u001b[94m4.7384\u001b[0m  0.0006  0.3906\n",
      "      3            0.2474        \u001b[32m1.3303\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        5.1482  0.0006  0.3907\n",
      "      4            0.2474        \u001b[32m1.1050\u001b[0m       0.2535            0.2535        4.7436  0.0006  0.3760\n",
      "      5            0.2526        \u001b[32m1.0794\u001b[0m       0.2535            0.2535        \u001b[94m4.1726\u001b[0m  0.0006  0.3907\n",
      "      6            0.2789        \u001b[32m1.0023\u001b[0m       0.2535            0.2535        \u001b[94m3.5243\u001b[0m  0.0006  0.3750\n",
      "      7            \u001b[36m0.3316\u001b[0m        1.0437       0.2535            0.2535        \u001b[94m2.8427\u001b[0m  0.0006  0.3906\n",
      "      8            \u001b[36m0.4053\u001b[0m        1.0167       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m2.2481\u001b[0m  0.0006  0.3750\n",
      "      9            \u001b[36m0.4895\u001b[0m        \u001b[32m0.8899\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m1.8925\u001b[0m  0.0006  0.3906\n",
      "     10            \u001b[36m0.5368\u001b[0m        \u001b[32m0.7624\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        \u001b[94m1.7668\u001b[0m  0.0005  0.3750\n",
      "     11            \u001b[36m0.5526\u001b[0m        0.8477       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m1.7188\u001b[0m  0.0005  0.3750\n",
      "     12            \u001b[36m0.6053\u001b[0m        \u001b[32m0.6790\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.6835\u001b[0m  0.0005  0.3909\n",
      "     13            \u001b[36m0.6368\u001b[0m        0.7509       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.6418\u001b[0m  0.0005  0.3750\n",
      "     14            \u001b[36m0.6895\u001b[0m        0.7706       0.3576            0.3576        \u001b[94m1.5889\u001b[0m  0.0005  0.3906\n",
      "     15            \u001b[36m0.7474\u001b[0m        0.7312       0.3576            0.3576        \u001b[94m1.5306\u001b[0m  0.0004  0.3750\n",
      "     16            \u001b[36m0.7684\u001b[0m        \u001b[32m0.6498\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.4867\u001b[0m  0.0004  0.3750\n",
      "     17            \u001b[36m0.8000\u001b[0m        0.6524       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.4454\u001b[0m  0.0004  0.3911\n",
      "     18            \u001b[36m0.8158\u001b[0m        \u001b[32m0.6304\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.4245\u001b[0m  0.0004  0.3750\n",
      "     19            \u001b[36m0.8368\u001b[0m        0.6382       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.3965\u001b[0m  0.0004  0.3911\n",
      "     20            \u001b[36m0.8579\u001b[0m        \u001b[32m0.6059\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.3797\u001b[0m  0.0003  0.3907\n",
      "     21            \u001b[36m0.8684\u001b[0m        \u001b[32m0.5718\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.3665\u001b[0m  0.0003  0.3752\n",
      "     22            \u001b[36m0.8789\u001b[0m        \u001b[32m0.5139\u001b[0m       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.3578\u001b[0m  0.0003  0.4375\n",
      "     23            \u001b[36m0.8947\u001b[0m        0.5717       0.4167            0.4167        \u001b[94m1.3452\u001b[0m  0.0002  0.5157\n",
      "     24            \u001b[36m0.9105\u001b[0m        0.5273       0.4167            0.4167        \u001b[94m1.3321\u001b[0m  0.0002  0.5000\n",
      "     25            \u001b[36m0.9211\u001b[0m        0.5719       0.4167            0.4167        \u001b[94m1.3168\u001b[0m  0.0002  0.5000\n",
      "     26            \u001b[36m0.9316\u001b[0m        0.5206       0.4167            0.4167        \u001b[94m1.3017\u001b[0m  0.0002  0.4844\n",
      "     27            \u001b[36m0.9421\u001b[0m        \u001b[32m0.4908\u001b[0m       0.4167            0.4167        \u001b[94m1.2916\u001b[0m  0.0002  0.3907\n",
      "     28            0.9421        0.5099       0.4201            0.4201        \u001b[94m1.2828\u001b[0m  0.0001  0.3906\n",
      "     29            \u001b[36m0.9474\u001b[0m        \u001b[32m0.4757\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.2757\u001b[0m  0.0001  0.3907\n",
      "     30            \u001b[36m0.9579\u001b[0m        \u001b[32m0.4282\u001b[0m       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.2701\u001b[0m  0.0001  0.3907\n",
      "     31            0.9579        0.4917       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.2671\u001b[0m  0.0001  0.3906\n",
      "     32            0.9579        \u001b[32m0.4121\u001b[0m       0.4340            0.4340        \u001b[94m1.2641\u001b[0m  0.0001  0.3906\n",
      "     33            \u001b[36m0.9632\u001b[0m        0.4290       0.4306            0.4306        \u001b[94m1.2613\u001b[0m  0.0000  0.3750\n",
      "     34            \u001b[36m0.9684\u001b[0m        0.4703       0.4306            0.4306        \u001b[94m1.2595\u001b[0m  0.0000  0.3911\n",
      "     35            0.9684        0.4344       0.4271            0.4271        \u001b[94m1.2567\u001b[0m  0.0000  0.3750\n",
      "     36            0.9684        0.4472       0.4236            0.4236        \u001b[94m1.2554\u001b[0m  0.0000  0.3907\n",
      "     37            0.9684        0.4894       0.4236            0.4236        \u001b[94m1.2532\u001b[0m  0.0000  0.3756\n",
      "     38            0.9684        0.4722       0.4236            0.4236        \u001b[94m1.2520\u001b[0m  0.0000  0.3909\n",
      "     39            0.9684        0.4428       0.4271            0.4271        \u001b[94m1.2503\u001b[0m  0.0000  0.3907\n",
      "     40            0.9684        0.4917       0.4306            0.4306        \u001b[94m1.2497\u001b[0m  0.0000  0.3759\n",
      "Training model for subject 3 with 200 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2850\u001b[0m        \u001b[32m1.6344\u001b[0m       \u001b[35m0.2431\u001b[0m            \u001b[31m0.2431\u001b[0m        \u001b[94m1.9874\u001b[0m  0.0006  0.4844\n",
      "      2            \u001b[36m0.2900\u001b[0m        \u001b[32m1.4380\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        2.0577  0.0006  0.4851\n",
      "      3            0.2500        \u001b[32m1.2304\u001b[0m       0.2535            0.2535        2.6010  0.0006  0.4844\n",
      "      4            0.2500        \u001b[32m1.0766\u001b[0m       0.2535            0.2535        2.9929  0.0006  0.4848\n",
      "      5            0.2550        \u001b[32m0.9350\u001b[0m       0.2569            0.2569        3.0926  0.0006  0.4844\n",
      "      6            0.2700        \u001b[32m0.9189\u001b[0m       0.2639            0.2639        2.7724  0.0006  0.4695\n",
      "      7            \u001b[36m0.3050\u001b[0m        \u001b[32m0.8566\u001b[0m       0.2674            0.2674        2.4373  0.0006  0.4846\n",
      "      8            \u001b[36m0.3850\u001b[0m        \u001b[32m0.7698\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        2.1276  0.0006  0.4847\n",
      "      9            \u001b[36m0.5050\u001b[0m        0.7932       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m1.8555\u001b[0m  0.0006  0.4844\n",
      "     10            \u001b[36m0.6300\u001b[0m        \u001b[32m0.6901\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.6095\u001b[0m  0.0005  0.5625\n",
      "     11            \u001b[36m0.7650\u001b[0m        \u001b[32m0.6471\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.4675\u001b[0m  0.0005  0.5938\n",
      "     12            \u001b[36m0.8250\u001b[0m        \u001b[32m0.6202\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4067\u001b[0m  0.0005  0.5625\n",
      "     13            \u001b[36m0.8700\u001b[0m        \u001b[32m0.5745\u001b[0m       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.3603\u001b[0m  0.0005  0.6094\n",
      "     14            \u001b[36m0.8950\u001b[0m        \u001b[32m0.5372\u001b[0m       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.3436\u001b[0m  0.0005  0.6407\n",
      "     15            \u001b[36m0.9200\u001b[0m        \u001b[32m0.5273\u001b[0m       0.4236            0.4236        \u001b[94m1.3308\u001b[0m  0.0004  0.5000\n",
      "     16            \u001b[36m0.9300\u001b[0m        \u001b[32m0.4744\u001b[0m       0.4306            0.4306        \u001b[94m1.3164\u001b[0m  0.0004  0.4847\n",
      "     17            \u001b[36m0.9400\u001b[0m        0.4830       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.3034\u001b[0m  0.0004  0.5000\n",
      "     18            \u001b[36m0.9650\u001b[0m        0.4763       0.4444            0.4444        \u001b[94m1.2883\u001b[0m  0.0004  0.4849\n",
      "     19            \u001b[36m0.9800\u001b[0m        \u001b[32m0.4123\u001b[0m       0.4514            0.4514        \u001b[94m1.2858\u001b[0m  0.0004  0.5001\n",
      "     20            \u001b[36m0.9850\u001b[0m        \u001b[32m0.3903\u001b[0m       0.4479            0.4479        \u001b[94m1.2814\u001b[0m  0.0003  0.4851\n",
      "     21            0.9850        0.4262       0.4549            0.4549        1.2820  0.0003  0.4844\n",
      "     22            \u001b[36m0.9900\u001b[0m        \u001b[32m0.3637\u001b[0m       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2765\u001b[0m  0.0003  0.4851\n",
      "     23            0.9850        \u001b[32m0.3533\u001b[0m       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.2693\u001b[0m  0.0002  0.4846\n",
      "     24            0.9850        \u001b[32m0.3474\u001b[0m       0.4618            0.4618        \u001b[94m1.2631\u001b[0m  0.0002  0.5006\n",
      "     25            0.9900        \u001b[32m0.3293\u001b[0m       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2616\u001b[0m  0.0002  0.4848\n",
      "     26            0.9900        \u001b[32m0.3201\u001b[0m       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        1.2619  0.0002  0.4688\n",
      "     27            0.9900        \u001b[32m0.3172\u001b[0m       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        \u001b[94m1.2615\u001b[0m  0.0002  0.4996\n",
      "     28            0.9900        \u001b[32m0.3105\u001b[0m       0.4826            0.4826        \u001b[94m1.2607\u001b[0m  0.0001  0.5004\n",
      "     29            \u001b[36m0.9950\u001b[0m        \u001b[32m0.3069\u001b[0m       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        \u001b[94m1.2606\u001b[0m  0.0001  0.4849\n",
      "     30            0.9950        0.3503       0.4826            0.4826        \u001b[94m1.2601\u001b[0m  0.0001  0.5004\n",
      "     31            0.9950        \u001b[32m0.2991\u001b[0m       \u001b[35m0.4931\u001b[0m            \u001b[31m0.4931\u001b[0m        1.2607  0.0001  0.4844\n",
      "     32            0.9950        0.3188       0.4931            0.4931        1.2609  0.0001  0.5003\n",
      "     33            0.9950        0.3533       0.4896            0.4896        \u001b[94m1.2596\u001b[0m  0.0000  0.5011\n",
      "     34            0.9950        0.3116       0.4896            0.4896        \u001b[94m1.2575\u001b[0m  0.0000  0.5006\n",
      "     35            0.9950        \u001b[32m0.2950\u001b[0m       0.4896            0.4896        \u001b[94m1.2564\u001b[0m  0.0000  0.5625\n",
      "     36            0.9950        0.3080       0.4861            0.4861        \u001b[94m1.2562\u001b[0m  0.0000  0.6251\n",
      "     37            0.9950        0.2958       0.4861            0.4861        \u001b[94m1.2561\u001b[0m  0.0000  0.5625\n",
      "     38            0.9950        0.3092       0.4792            0.4792        1.2561  0.0000  0.6094\n",
      "     39            0.9950        0.3287       0.4826            0.4826        1.2563  0.0000  0.5942\n",
      "     40            0.9950        0.3317       0.4826            0.4826        1.2564  0.0000  0.4844\n",
      "Training model for subject 3 with 210 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2667\u001b[0m        \u001b[32m1.6741\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m3.3948\u001b[0m  0.0006  0.4845\n",
      "      2            \u001b[36m0.3524\u001b[0m        \u001b[32m1.3194\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        \u001b[94m2.3833\u001b[0m  0.0006  0.4844\n",
      "      3            \u001b[36m0.4286\u001b[0m        \u001b[32m1.1157\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.6165\u001b[0m  0.0006  0.4848\n",
      "      4            0.4286        \u001b[32m1.0034\u001b[0m       0.3229            0.3229        \u001b[94m1.6147\u001b[0m  0.0006  0.4844\n",
      "      5            \u001b[36m0.6190\u001b[0m        \u001b[32m0.9334\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.4310\u001b[0m  0.0006  0.5006\n",
      "      6            \u001b[36m0.6857\u001b[0m        \u001b[32m0.8681\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.3423\u001b[0m  0.0006  0.4692\n",
      "      7            \u001b[36m0.7429\u001b[0m        \u001b[32m0.7760\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.3285\u001b[0m  0.0006  0.4849\n",
      "      8            \u001b[36m0.7524\u001b[0m        0.8033       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.3197\u001b[0m  0.0006  0.4849\n",
      "      9            \u001b[36m0.8333\u001b[0m        \u001b[32m0.7584\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.2873\u001b[0m  0.0006  0.4844\n",
      "     10            \u001b[36m0.8857\u001b[0m        \u001b[32m0.6394\u001b[0m       0.4375            0.4375        \u001b[94m1.2523\u001b[0m  0.0005  0.4844\n",
      "     11            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5884\u001b[0m       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.2229\u001b[0m  0.0005  0.4844\n",
      "     12            \u001b[36m0.9190\u001b[0m        \u001b[32m0.5653\u001b[0m       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.2178\u001b[0m  0.0005  0.5000\n",
      "     13            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5511\u001b[0m       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        \u001b[94m1.2165\u001b[0m  0.0005  0.4844\n",
      "     14            \u001b[36m0.9429\u001b[0m        \u001b[32m0.5236\u001b[0m       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        \u001b[94m1.2077\u001b[0m  0.0005  0.4844\n",
      "     15            \u001b[36m0.9476\u001b[0m        \u001b[32m0.5065\u001b[0m       0.4861            0.4861        \u001b[94m1.1984\u001b[0m  0.0004  0.4844\n",
      "     16            \u001b[36m0.9524\u001b[0m        0.5146       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        \u001b[94m1.1917\u001b[0m  0.0004  0.4844\n",
      "     17            \u001b[36m0.9571\u001b[0m        \u001b[32m0.4262\u001b[0m       0.4931            0.4931        \u001b[94m1.1853\u001b[0m  0.0004  0.4849\n",
      "     18            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4036\u001b[0m       \u001b[35m0.5000\u001b[0m            \u001b[31m0.5000\u001b[0m        \u001b[94m1.1779\u001b[0m  0.0004  0.4848\n",
      "     19            0.9667        0.4126       \u001b[35m0.5035\u001b[0m            \u001b[31m0.5035\u001b[0m        \u001b[94m1.1738\u001b[0m  0.0004  0.5157\n",
      "     20            \u001b[36m0.9762\u001b[0m        0.4077       \u001b[35m0.5104\u001b[0m            \u001b[31m0.5104\u001b[0m        \u001b[94m1.1637\u001b[0m  0.0003  0.5625\n",
      "     21            \u001b[36m0.9905\u001b[0m        \u001b[32m0.3396\u001b[0m       0.5069            0.5069        \u001b[94m1.1561\u001b[0m  0.0003  0.5782\n",
      "     22            0.9905        0.3561       0.5069            0.5069        \u001b[94m1.1517\u001b[0m  0.0003  0.5625\n",
      "     23            0.9905        \u001b[32m0.3309\u001b[0m       0.5104            0.5104        1.1533  0.0002  0.6407\n",
      "     24            0.9905        \u001b[32m0.3220\u001b[0m       \u001b[35m0.5174\u001b[0m            \u001b[31m0.5174\u001b[0m        1.1562  0.0002  0.5938\n",
      "     25            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3181\u001b[0m       0.5174            0.5174        1.1576  0.0002  0.4848\n",
      "     26            0.9952        0.3446       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        1.1567  0.0002  0.5001\n",
      "     27            0.9952        \u001b[32m0.3102\u001b[0m       0.5208            0.5208        1.1545  0.0002  0.5000\n",
      "     28            0.9952        \u001b[32m0.2978\u001b[0m       0.5208            0.5208        1.1547  0.0001  0.4778\n",
      "     29            0.9952        0.3255       0.5208            0.5208        1.1548  0.0001  0.4844\n",
      "     30            0.9952        0.3070       0.5208            0.5208        1.1524  0.0001  0.4844\n",
      "     31            0.9952        0.3582       \u001b[35m0.5243\u001b[0m            \u001b[31m0.5243\u001b[0m        \u001b[94m1.1491\u001b[0m  0.0001  0.4844\n",
      "     32            1.0000        0.3193       \u001b[35m0.5278\u001b[0m            \u001b[31m0.5278\u001b[0m        \u001b[94m1.1464\u001b[0m  0.0001  0.4844\n",
      "     33            1.0000        \u001b[32m0.2683\u001b[0m       0.5243            0.5243        \u001b[94m1.1452\u001b[0m  0.0000  0.5940\n",
      "     34            1.0000        \u001b[32m0.2613\u001b[0m       0.5243            0.5243        \u001b[94m1.1442\u001b[0m  0.0000  0.4848\n",
      "     35            1.0000        0.2785       0.5278            0.5278        \u001b[94m1.1434\u001b[0m  0.0000  0.4848\n",
      "     36            1.0000        0.2756       0.5278            0.5278        \u001b[94m1.1426\u001b[0m  0.0000  0.4844\n",
      "     37            1.0000        0.3111       0.5278            0.5278        \u001b[94m1.1425\u001b[0m  0.0000  0.4848\n",
      "     38            1.0000        0.2889       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        1.1426  0.0000  0.4854\n",
      "     39            1.0000        0.2860       0.5243            0.5243        1.1428  0.0000  0.4844\n",
      "     40            1.0000        \u001b[32m0.2598\u001b[0m       0.5208            0.5208        1.1431  0.0000  0.4844\n",
      "Training model for subject 3 with 220 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2455\u001b[0m        \u001b[32m1.5442\u001b[0m       \u001b[35m0.2465\u001b[0m            \u001b[31m0.2465\u001b[0m        \u001b[94m3.2234\u001b[0m  0.0006  0.5006\n",
      "      2            \u001b[36m0.3773\u001b[0m        \u001b[32m1.3548\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m1.7136\u001b[0m  0.0006  0.4688\n",
      "      3            \u001b[36m0.4000\u001b[0m        \u001b[32m1.2579\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        1.8526  0.0006  0.5000\n",
      "      4            \u001b[36m0.4364\u001b[0m        \u001b[32m1.0655\u001b[0m       0.2778            0.2778        1.7697  0.0006  0.5782\n",
      "      5            \u001b[36m0.5091\u001b[0m        1.1064       0.2743            0.2743        \u001b[94m1.7100\u001b[0m  0.0006  0.6095\n",
      "      6            0.5045        \u001b[32m0.9612\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        1.7277  0.0006  0.5628\n",
      "      7            \u001b[36m0.5364\u001b[0m        \u001b[32m0.9193\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m1.6607\u001b[0m  0.0006  0.5938\n",
      "      8            \u001b[36m0.5591\u001b[0m        \u001b[32m0.8817\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m1.5795\u001b[0m  0.0006  0.6094\n",
      "      9            \u001b[36m0.5682\u001b[0m        \u001b[32m0.7892\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.5278\u001b[0m  0.0006  0.4844\n",
      "     10            \u001b[36m0.6000\u001b[0m        \u001b[32m0.7839\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.4935\u001b[0m  0.0005  0.4848\n",
      "     11            \u001b[36m0.6727\u001b[0m        \u001b[32m0.6993\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.4127\u001b[0m  0.0005  0.5469\n",
      "     12            \u001b[36m0.7727\u001b[0m        \u001b[32m0.6830\u001b[0m       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.3358\u001b[0m  0.0005  0.6407\n",
      "     13            \u001b[36m0.8318\u001b[0m        \u001b[32m0.6541\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.2894\u001b[0m  0.0005  0.5469\n",
      "     14            \u001b[36m0.8773\u001b[0m        \u001b[32m0.6091\u001b[0m       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2654\u001b[0m  0.0005  0.4844\n",
      "     15            \u001b[36m0.9136\u001b[0m        \u001b[32m0.5608\u001b[0m       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        \u001b[94m1.2531\u001b[0m  0.0004  0.4848\n",
      "     16            \u001b[36m0.9227\u001b[0m        0.5704       0.4722            0.4722        \u001b[94m1.2493\u001b[0m  0.0004  0.4848\n",
      "     17            \u001b[36m0.9364\u001b[0m        0.5671       0.4792            0.4792        \u001b[94m1.2465\u001b[0m  0.0004  0.5003\n",
      "     18            \u001b[36m0.9409\u001b[0m        \u001b[32m0.4594\u001b[0m       0.4792            0.4792        \u001b[94m1.2436\u001b[0m  0.0004  0.4844\n",
      "     19            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4285\u001b[0m       0.4792            0.4792        \u001b[94m1.2380\u001b[0m  0.0004  0.5001\n",
      "     20            \u001b[36m0.9727\u001b[0m        0.4909       0.4792            0.4792        \u001b[94m1.2323\u001b[0m  0.0003  0.4854\n",
      "     21            0.9682        0.4821       \u001b[35m0.4931\u001b[0m            \u001b[31m0.4931\u001b[0m        \u001b[94m1.2281\u001b[0m  0.0003  0.5009\n",
      "     22            0.9727        0.4332       \u001b[35m0.5000\u001b[0m            \u001b[31m0.5000\u001b[0m        \u001b[94m1.2243\u001b[0m  0.0003  0.5004\n",
      "     23            0.9682        \u001b[32m0.4144\u001b[0m       0.4965            0.4965        \u001b[94m1.2240\u001b[0m  0.0002  0.4847\n",
      "     24            \u001b[36m0.9773\u001b[0m        \u001b[32m0.4136\u001b[0m       \u001b[35m0.5104\u001b[0m            \u001b[31m0.5104\u001b[0m        1.2249  0.0002  0.4844\n",
      "     25            \u001b[36m0.9818\u001b[0m        \u001b[32m0.3713\u001b[0m       \u001b[35m0.5174\u001b[0m            \u001b[31m0.5174\u001b[0m        1.2241  0.0002  0.4844\n",
      "     26            \u001b[36m0.9909\u001b[0m        0.4036       0.5174            0.5174        1.2240  0.0002  0.5000\n",
      "     27            0.9909        0.3782       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        \u001b[94m1.2188\u001b[0m  0.0002  0.5157\n",
      "     28            \u001b[36m0.9955\u001b[0m        0.4072       0.5208            0.5208        \u001b[94m1.2135\u001b[0m  0.0001  0.5938\n",
      "     29            0.9909        0.3823       0.5035            0.5035        \u001b[94m1.2083\u001b[0m  0.0001  0.5786\n",
      "     30            0.9955        \u001b[32m0.3583\u001b[0m       0.5035            0.5035        \u001b[94m1.2054\u001b[0m  0.0001  0.5629\n",
      "     31            0.9955        0.3636       0.5000            0.5000        \u001b[94m1.2034\u001b[0m  0.0001  0.5938\n",
      "     32            0.9955        0.3774       0.5069            0.5069        \u001b[94m1.2013\u001b[0m  0.0001  0.6876\n",
      "     33            0.9955        0.3596       0.5069            0.5069        \u001b[94m1.1997\u001b[0m  0.0000  0.5786\n",
      "     34            0.9955        0.3627       0.5069            0.5069        \u001b[94m1.1988\u001b[0m  0.0000  0.5316\n",
      "     35            0.9955        \u001b[32m0.3523\u001b[0m       0.5139            0.5139        \u001b[94m1.1986\u001b[0m  0.0000  0.5001\n",
      "     36            0.9955        \u001b[32m0.2891\u001b[0m       0.5139            0.5139        \u001b[94m1.1984\u001b[0m  0.0000  0.4849\n",
      "     37            \u001b[36m1.0000\u001b[0m        0.3495       0.5139            0.5139        1.1984  0.0000  0.4862\n",
      "     38            1.0000        0.3319       0.5139            0.5139        \u001b[94m1.1983\u001b[0m  0.0000  0.4847\n",
      "     39            1.0000        0.3476       0.5139            0.5139        \u001b[94m1.1982\u001b[0m  0.0000  0.4848\n",
      "     40            1.0000        0.3832       0.5174            0.5174        \u001b[94m1.1981\u001b[0m  0.0000  0.5000\n",
      "Training model for subject 3 with 230 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2522\u001b[0m        \u001b[32m1.6489\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.3222\u001b[0m  0.0006  0.5004\n",
      "      2            0.2522        \u001b[32m1.3854\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m2.7537\u001b[0m  0.0006  0.4844\n",
      "      3            \u001b[36m0.3043\u001b[0m        \u001b[32m1.1449\u001b[0m       0.2535            0.2535        \u001b[94m1.8097\u001b[0m  0.0006  0.5053\n",
      "      4            \u001b[36m0.4391\u001b[0m        \u001b[32m1.0689\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m1.5465\u001b[0m  0.0006  0.5000\n",
      "      5            \u001b[36m0.6261\u001b[0m        \u001b[32m1.0274\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m1.4792\u001b[0m  0.0006  0.4844\n",
      "      6            0.5000        \u001b[32m0.9617\u001b[0m       0.2951            0.2951        1.6354  0.0006  0.4847\n",
      "      7            0.4565        \u001b[32m0.8767\u001b[0m       0.3021            0.3021        1.6834  0.0006  0.4850\n",
      "      8            0.5000        \u001b[32m0.8748\u001b[0m       0.2986            0.2986        1.6662  0.0006  0.4849\n",
      "      9            0.5261        \u001b[32m0.8191\u001b[0m       0.3056            0.3056        1.6425  0.0006  0.4848\n",
      "     10            \u001b[36m0.6304\u001b[0m        \u001b[32m0.7734\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        1.4960  0.0005  0.4844\n",
      "     11            \u001b[36m0.7348\u001b[0m        \u001b[32m0.6920\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.3779\u001b[0m  0.0005  0.5625\n",
      "     12            \u001b[36m0.7696\u001b[0m        \u001b[32m0.6831\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.3391\u001b[0m  0.0005  0.5782\n",
      "     13            \u001b[36m0.7870\u001b[0m        \u001b[32m0.6491\u001b[0m       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.3092\u001b[0m  0.0005  0.5782\n",
      "     14            \u001b[36m0.8478\u001b[0m        0.6510       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.2707\u001b[0m  0.0005  0.6098\n",
      "     15            \u001b[36m0.8783\u001b[0m        \u001b[32m0.5792\u001b[0m       0.4549            0.4549        \u001b[94m1.2486\u001b[0m  0.0004  0.6094\n",
      "     16            \u001b[36m0.8913\u001b[0m        \u001b[32m0.5388\u001b[0m       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        \u001b[94m1.2408\u001b[0m  0.0004  0.4844\n",
      "     17            \u001b[36m0.9130\u001b[0m        0.5438       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        \u001b[94m1.2299\u001b[0m  0.0004  0.4848\n",
      "     18            \u001b[36m0.9261\u001b[0m        \u001b[32m0.4770\u001b[0m       \u001b[35m0.5035\u001b[0m            \u001b[31m0.5035\u001b[0m        \u001b[94m1.2080\u001b[0m  0.0004  0.4844\n",
      "     19            \u001b[36m0.9565\u001b[0m        \u001b[32m0.4464\u001b[0m       0.4861            0.4861        \u001b[94m1.1903\u001b[0m  0.0004  0.4844\n",
      "     20            \u001b[36m0.9652\u001b[0m        0.4805       0.4688            0.4688        \u001b[94m1.1844\u001b[0m  0.0003  0.4853\n",
      "     21            \u001b[36m0.9696\u001b[0m        \u001b[32m0.4217\u001b[0m       0.4757            0.4757        \u001b[94m1.1820\u001b[0m  0.0003  0.4844\n",
      "     22            \u001b[36m0.9739\u001b[0m        0.4375       0.4757            0.4757        \u001b[94m1.1795\u001b[0m  0.0003  0.4848\n",
      "     23            \u001b[36m0.9783\u001b[0m        \u001b[32m0.4119\u001b[0m       0.4861            0.4861        \u001b[94m1.1747\u001b[0m  0.0002  0.4844\n",
      "     24            \u001b[36m0.9870\u001b[0m        0.4205       0.4826            0.4826        \u001b[94m1.1695\u001b[0m  0.0002  0.4844\n",
      "     25            0.9870        \u001b[32m0.4038\u001b[0m       0.4931            0.4931        \u001b[94m1.1648\u001b[0m  0.0002  0.4848\n",
      "     26            \u001b[36m0.9913\u001b[0m        \u001b[32m0.3792\u001b[0m       0.4965            0.4965        \u001b[94m1.1631\u001b[0m  0.0002  0.5001\n",
      "     27            0.9913        \u001b[32m0.3455\u001b[0m       0.4931            0.4931        \u001b[94m1.1601\u001b[0m  0.0002  0.4848\n",
      "     28            0.9913        \u001b[32m0.3251\u001b[0m       0.4931            0.4931        \u001b[94m1.1579\u001b[0m  0.0001  0.4844\n",
      "     29            0.9870        0.3270       0.4965            0.4965        \u001b[94m1.1554\u001b[0m  0.0001  0.4846\n",
      "     30            0.9870        0.3776       0.4965            0.4965        \u001b[94m1.1533\u001b[0m  0.0001  0.4846\n",
      "     31            0.9870        0.3624       0.4896            0.4896        \u001b[94m1.1523\u001b[0m  0.0001  0.4849\n",
      "     32            0.9870        0.3613       0.4861            0.4861        \u001b[94m1.1512\u001b[0m  0.0001  0.4844\n",
      "     33            0.9913        0.3545       0.4861            0.4861        \u001b[94m1.1508\u001b[0m  0.0000  0.4848\n",
      "     34            0.9913        0.3353       0.4826            0.4826        1.1509  0.0000  0.4854\n",
      "     35            0.9913        0.3451       0.4861            0.4861        \u001b[94m1.1508\u001b[0m  0.0000  0.5469\n",
      "     36            0.9913        \u001b[32m0.3071\u001b[0m       0.4861            0.4861        \u001b[94m1.1508\u001b[0m  0.0000  0.5938\n",
      "     37            0.9913        0.3509       0.4861            0.4861        \u001b[94m1.1508\u001b[0m  0.0000  0.5782\n",
      "     38            0.9913        0.3255       0.4861            0.4861        \u001b[94m1.1507\u001b[0m  0.0000  0.6094\n",
      "     39            0.9913        0.3363       0.4861            0.4861        \u001b[94m1.1507\u001b[0m  0.0000  0.6096\n",
      "     40            0.9913        0.3330       0.4896            0.4896        1.1507  0.0000  0.5001\n",
      "Training model for subject 3 with 240 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2833\u001b[0m        \u001b[32m1.6210\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m2.4915\u001b[0m  0.0006  0.4846\n",
      "      2            0.2500        \u001b[32m1.4041\u001b[0m       0.2500            0.2500        3.8644  0.0006  0.4844\n",
      "      3            0.2500        \u001b[32m1.2678\u001b[0m       0.2500            0.2500        4.3051  0.0006  0.4844\n",
      "      4            0.2500        \u001b[32m1.0743\u001b[0m       0.2500            0.2500        4.1635  0.0006  0.4844\n",
      "      5            0.2542        \u001b[32m1.0272\u001b[0m       0.2500            0.2500        3.2185  0.0006  0.4844\n",
      "      6            0.2583        \u001b[32m0.9572\u001b[0m       0.2535            0.2535        2.8582  0.0006  0.4844\n",
      "      7            0.2750        \u001b[32m0.9217\u001b[0m       0.2569            0.2569        2.5178  0.0006  0.4851\n",
      "      8            \u001b[36m0.3458\u001b[0m        \u001b[32m0.8372\u001b[0m       0.2674            0.2674        \u001b[94m2.1675\u001b[0m  0.0006  0.4853\n",
      "      9            \u001b[36m0.4333\u001b[0m        \u001b[32m0.8143\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m1.9020\u001b[0m  0.0006  0.4857\n",
      "     10            \u001b[36m0.5250\u001b[0m        \u001b[32m0.7318\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m1.6861\u001b[0m  0.0005  0.4844\n",
      "     11            \u001b[36m0.6667\u001b[0m        \u001b[32m0.6657\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.5270\u001b[0m  0.0005  0.4848\n",
      "     12            \u001b[36m0.7375\u001b[0m        \u001b[32m0.6121\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.4268\u001b[0m  0.0005  0.5000\n",
      "     13            \u001b[36m0.8125\u001b[0m        0.6731       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.3551\u001b[0m  0.0005  0.5007\n",
      "     14            \u001b[36m0.8625\u001b[0m        \u001b[32m0.5945\u001b[0m       0.3819            0.3819        \u001b[94m1.3184\u001b[0m  0.0005  0.4848\n",
      "     15            \u001b[36m0.9000\u001b[0m        0.6007       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.2986\u001b[0m  0.0004  0.4845\n",
      "     16            \u001b[36m0.9125\u001b[0m        \u001b[32m0.5761\u001b[0m       0.4062            0.4062        \u001b[94m1.2973\u001b[0m  0.0004  0.4848\n",
      "     17            \u001b[36m0.9167\u001b[0m        0.5899       0.4028            0.4028        1.3062  0.0004  0.4844\n",
      "     18            0.9042        \u001b[32m0.5353\u001b[0m       0.4062            0.4062        1.3083  0.0004  0.4848\n",
      "     19            0.9125        \u001b[32m0.4814\u001b[0m       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.2966\u001b[0m  0.0004  0.5469\n",
      "     20            \u001b[36m0.9375\u001b[0m        0.5295       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.2767\u001b[0m  0.0003  0.5938\n",
      "     21            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4694\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.2663\u001b[0m  0.0003  0.5625\n",
      "     22            0.9667        \u001b[32m0.4265\u001b[0m       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.2598\u001b[0m  0.0003  0.5781\n",
      "     23            \u001b[36m0.9708\u001b[0m        0.4381       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        \u001b[94m1.2584\u001b[0m  0.0002  0.6094\n",
      "     24            0.9708        \u001b[32m0.3858\u001b[0m       0.4653            0.4653        \u001b[94m1.2547\u001b[0m  0.0002  0.5000\n",
      "     25            0.9667        0.4174       0.4653            0.4653        \u001b[94m1.2506\u001b[0m  0.0002  0.4855\n",
      "     26            0.9708        0.4464       0.4618            0.4618        \u001b[94m1.2494\u001b[0m  0.0002  0.5000\n",
      "     27            0.9708        0.4219       0.4549            0.4549        \u001b[94m1.2459\u001b[0m  0.0002  0.4849\n",
      "     28            0.9708        0.3992       0.4653            0.4653        \u001b[94m1.2433\u001b[0m  0.0001  0.4844\n",
      "     29            0.9708        \u001b[32m0.3517\u001b[0m       0.4722            0.4722        \u001b[94m1.2421\u001b[0m  0.0001  0.4846\n",
      "     30            \u001b[36m0.9750\u001b[0m        0.3836       0.4722            0.4722        \u001b[94m1.2414\u001b[0m  0.0001  0.4850\n",
      "     31            0.9750        0.4073       0.4792            0.4792        \u001b[94m1.2407\u001b[0m  0.0001  0.4844\n",
      "     32            0.9750        0.3869       0.4792            0.4792        \u001b[94m1.2395\u001b[0m  0.0001  0.4846\n",
      "     33            0.9750        0.4263       0.4722            0.4722        \u001b[94m1.2381\u001b[0m  0.0000  0.4846\n",
      "     34            0.9750        0.4134       0.4722            0.4722        \u001b[94m1.2371\u001b[0m  0.0000  0.4844\n",
      "     35            0.9750        \u001b[32m0.3445\u001b[0m       0.4722            0.4722        \u001b[94m1.2370\u001b[0m  0.0000  0.4848\n",
      "     36            0.9750        \u001b[32m0.3236\u001b[0m       0.4688            0.4688        1.2370  0.0000  0.5000\n",
      "     37            0.9750        0.4005       0.4722            0.4722        \u001b[94m1.2362\u001b[0m  0.0000  0.5000\n",
      "     38            0.9750        0.3756       0.4722            0.4722        1.2362  0.0000  0.4844\n",
      "     39            0.9750        0.3895       0.4688            0.4688        1.2367  0.0000  0.4844\n",
      "     40            0.9750        0.4098       0.4722            0.4722        \u001b[94m1.2361\u001b[0m  0.0000  0.4848\n",
      "Training model for subject 3 with 250 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2520\u001b[0m        \u001b[32m1.5784\u001b[0m       \u001b[35m0.2465\u001b[0m            \u001b[31m0.2465\u001b[0m        \u001b[94m2.6447\u001b[0m  0.0006  0.4844\n",
      "      2            \u001b[36m0.2560\u001b[0m        \u001b[32m1.3264\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m2.3382\u001b[0m  0.0006  0.4847\n",
      "      3            \u001b[36m0.2600\u001b[0m        \u001b[32m1.2021\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        2.3786  0.0006  0.5469\n",
      "      4            \u001b[36m0.2960\u001b[0m        \u001b[32m1.1426\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m2.0237\u001b[0m  0.0006  0.5938\n",
      "      5            \u001b[36m0.4880\u001b[0m        \u001b[32m0.9819\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m1.8592\u001b[0m  0.0006  0.5782\n",
      "      6            \u001b[36m0.5840\u001b[0m        \u001b[32m0.9764\u001b[0m       0.2917            0.2917        \u001b[94m1.7755\u001b[0m  0.0006  0.6094\n",
      "      7            0.5840        \u001b[32m0.8118\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m1.7216\u001b[0m  0.0006  0.6267\n",
      "      8            0.5400        0.8947       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m1.6995\u001b[0m  0.0006  0.4850\n",
      "      9            \u001b[36m0.6160\u001b[0m        \u001b[32m0.7794\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.5637\u001b[0m  0.0006  0.4849\n",
      "     10            \u001b[36m0.6960\u001b[0m        \u001b[32m0.7707\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.4424\u001b[0m  0.0005  0.5625\n",
      "     11            \u001b[36m0.7760\u001b[0m        \u001b[32m0.6974\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.3632\u001b[0m  0.0005  0.5000\n",
      "     12            \u001b[36m0.8080\u001b[0m        \u001b[32m0.6552\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.3249\u001b[0m  0.0005  0.5000\n",
      "     13            \u001b[36m0.8320\u001b[0m        0.6927       0.3854            0.3854        \u001b[94m1.2969\u001b[0m  0.0005  0.5017\n",
      "     14            \u001b[36m0.8480\u001b[0m        \u001b[32m0.6055\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.2780\u001b[0m  0.0005  0.4848\n",
      "     15            \u001b[36m0.8720\u001b[0m        0.6477       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.2620\u001b[0m  0.0004  0.4845\n",
      "     16            \u001b[36m0.9280\u001b[0m        \u001b[32m0.5357\u001b[0m       0.4271            0.4271        \u001b[94m1.2411\u001b[0m  0.0004  0.5002\n",
      "     17            \u001b[36m0.9400\u001b[0m        0.5602       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.2332\u001b[0m  0.0004  0.4844\n",
      "     18            \u001b[36m0.9440\u001b[0m        \u001b[32m0.5153\u001b[0m       0.4410            0.4410        \u001b[94m1.2299\u001b[0m  0.0004  0.4849\n",
      "     19            0.9440        0.5309       0.4410            0.4410        \u001b[94m1.2292\u001b[0m  0.0004  0.5000\n",
      "     20            \u001b[36m0.9480\u001b[0m        \u001b[32m0.4980\u001b[0m       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.2233\u001b[0m  0.0003  0.4844\n",
      "     21            0.9480        0.5088       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2164\u001b[0m  0.0003  0.4844\n",
      "     22            0.9440        0.5083       0.4549            0.4549        1.2182  0.0003  0.4851\n",
      "     23            \u001b[36m0.9520\u001b[0m        0.5032       0.4549            0.4549        1.2286  0.0002  0.4848\n",
      "     24            \u001b[36m0.9560\u001b[0m        \u001b[32m0.4567\u001b[0m       0.4514            0.4514        1.2349  0.0002  0.4844\n",
      "     25            0.9560        \u001b[32m0.4283\u001b[0m       0.4583            0.4583        1.2293  0.0002  0.6407\n",
      "     26            0.9560        0.4502       0.4583            0.4583        1.2221  0.0002  0.5468\n",
      "     27            \u001b[36m0.9600\u001b[0m        \u001b[32m0.3822\u001b[0m       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2158\u001b[0m  0.0002  0.5782\n",
      "     28            \u001b[36m0.9640\u001b[0m        0.3932       0.4688            0.4688        \u001b[94m1.2101\u001b[0m  0.0001  0.5938\n",
      "     29            \u001b[36m0.9680\u001b[0m        0.4660       0.4653            0.4653        \u001b[94m1.2056\u001b[0m  0.0001  0.5782\n",
      "     30            0.9680        \u001b[32m0.3442\u001b[0m       0.4618            0.4618        \u001b[94m1.2035\u001b[0m  0.0001  0.6407\n",
      "     31            \u001b[36m0.9720\u001b[0m        0.3832       0.4618            0.4618        \u001b[94m1.2026\u001b[0m  0.0001  0.5000\n",
      "     32            0.9720        \u001b[32m0.3367\u001b[0m       0.4653            0.4653        \u001b[94m1.2026\u001b[0m  0.0001  0.4847\n",
      "     33            0.9720        0.3728       0.4688            0.4688        1.2036  0.0000  0.4844\n",
      "     34            0.9720        0.3731       0.4688            0.4688        1.2038  0.0000  0.4853\n",
      "     35            0.9720        \u001b[32m0.3231\u001b[0m       0.4688            0.4688        1.2047  0.0000  0.4844\n",
      "     36            0.9720        0.3566       0.4688            0.4688        1.2049  0.0000  0.4844\n",
      "     37            0.9720        0.3879       0.4688            0.4688        1.2049  0.0000  0.4844\n",
      "     38            0.9720        0.3862       0.4688            0.4688        1.2044  0.0000  0.4847\n",
      "     39            0.9720        0.3348       0.4653            0.4653        1.2044  0.0000  0.5004\n",
      "     40            0.9720        0.3596       0.4688            0.4688        1.2044  0.0000  0.4844\n",
      "Training model for subject 3 with 260 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2462\u001b[0m        \u001b[32m1.6554\u001b[0m       \u001b[35m0.2465\u001b[0m            \u001b[31m0.2465\u001b[0m        \u001b[94m2.6218\u001b[0m  0.0006  0.5950\n",
      "      2            0.2462        \u001b[32m1.2948\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        3.5657  0.0006  0.5782\n",
      "      3            \u001b[36m0.2923\u001b[0m        \u001b[32m1.0993\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        2.8009  0.0006  0.5786\n",
      "      4            \u001b[36m0.4923\u001b[0m        \u001b[32m0.9787\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m1.9557\u001b[0m  0.0006  0.5786\n",
      "      5            \u001b[36m0.5692\u001b[0m        \u001b[32m0.8759\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m1.6842\u001b[0m  0.0006  0.5943\n",
      "      6            0.5577        \u001b[32m0.7791\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        1.7357  0.0006  0.5785\n",
      "      7            \u001b[36m0.6500\u001b[0m        \u001b[32m0.7585\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m1.6317\u001b[0m  0.0006  0.5938\n",
      "      8            \u001b[36m0.7269\u001b[0m        0.7748       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.4323\u001b[0m  0.0006  0.5942\n",
      "      9            \u001b[36m0.8577\u001b[0m        \u001b[32m0.6586\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.3227\u001b[0m  0.0006  0.6719\n",
      "     10            \u001b[36m0.8962\u001b[0m        0.6933       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.2953\u001b[0m  0.0005  0.6880\n",
      "     11            0.8808        \u001b[32m0.6111\u001b[0m       0.4236            0.4236        1.3033  0.0005  0.6876\n",
      "     12            \u001b[36m0.9154\u001b[0m        \u001b[32m0.5589\u001b[0m       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.2818\u001b[0m  0.0005  0.7501\n",
      "     13            \u001b[36m0.9308\u001b[0m        \u001b[32m0.5032\u001b[0m       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        \u001b[94m1.2533\u001b[0m  0.0005  0.6103\n",
      "     14            \u001b[36m0.9385\u001b[0m        0.5250       0.4792            0.4792        \u001b[94m1.2502\u001b[0m  0.0005  0.5938\n",
      "     15            \u001b[36m0.9538\u001b[0m        \u001b[32m0.4840\u001b[0m       0.4757            0.4757        \u001b[94m1.2412\u001b[0m  0.0004  0.5938\n",
      "     16            \u001b[36m0.9731\u001b[0m        \u001b[32m0.4605\u001b[0m       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        \u001b[94m1.2238\u001b[0m  0.0004  0.5786\n",
      "     17            0.9731        0.4879       0.4965            0.4965        \u001b[94m1.2095\u001b[0m  0.0004  0.5782\n",
      "     18            0.9731        \u001b[32m0.3739\u001b[0m       0.4792            0.4792        \u001b[94m1.2020\u001b[0m  0.0004  0.5785\n",
      "     19            \u001b[36m0.9769\u001b[0m        0.3932       0.4965            0.4965        \u001b[94m1.1996\u001b[0m  0.0004  0.5792\n",
      "     20            0.9769        \u001b[32m0.3609\u001b[0m       \u001b[35m0.5035\u001b[0m            \u001b[31m0.5035\u001b[0m        \u001b[94m1.1969\u001b[0m  0.0003  0.5942\n",
      "     21            \u001b[36m0.9846\u001b[0m        \u001b[32m0.3385\u001b[0m       \u001b[35m0.5069\u001b[0m            \u001b[31m0.5069\u001b[0m        1.1996  0.0003  0.5784\n",
      "     22            0.9846        \u001b[32m0.2992\u001b[0m       \u001b[35m0.5139\u001b[0m            \u001b[31m0.5139\u001b[0m        \u001b[94m1.1963\u001b[0m  0.0003  0.5788\n",
      "     23            0.9808        0.3720       0.5000            0.5000        \u001b[94m1.1908\u001b[0m  0.0002  0.5782\n",
      "     24            \u001b[36m0.9923\u001b[0m        \u001b[32m0.2918\u001b[0m       0.4861            0.4861        \u001b[94m1.1881\u001b[0m  0.0002  0.5938\n",
      "     25            0.9923        \u001b[32m0.2877\u001b[0m       0.5035            0.5035        \u001b[94m1.1847\u001b[0m  0.0002  0.5943\n",
      "     26            \u001b[36m0.9962\u001b[0m        \u001b[32m0.2847\u001b[0m       0.5035            0.5035        \u001b[94m1.1762\u001b[0m  0.0002  0.5782\n",
      "     27            0.9962        \u001b[32m0.2750\u001b[0m       \u001b[35m0.5174\u001b[0m            \u001b[31m0.5174\u001b[0m        \u001b[94m1.1737\u001b[0m  0.0002  0.6407\n",
      "     28            0.9962        0.2916       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        \u001b[94m1.1728\u001b[0m  0.0001  0.5938\n",
      "     29            0.9962        \u001b[32m0.2713\u001b[0m       \u001b[35m0.5278\u001b[0m            \u001b[31m0.5278\u001b[0m        \u001b[94m1.1707\u001b[0m  0.0001  0.6719\n",
      "     30            0.9962        \u001b[32m0.2485\u001b[0m       0.5278            0.5278        \u001b[94m1.1667\u001b[0m  0.0001  0.7505\n",
      "     31            0.9923        0.2577       \u001b[35m0.5347\u001b[0m            \u001b[31m0.5347\u001b[0m        \u001b[94m1.1644\u001b[0m  0.0001  0.6719\n",
      "     32            0.9923        0.2514       \u001b[35m0.5382\u001b[0m            \u001b[31m0.5382\u001b[0m        \u001b[94m1.1620\u001b[0m  0.0001  0.7344\n",
      "     33            0.9923        0.2578       0.5382            0.5382        \u001b[94m1.1613\u001b[0m  0.0000  0.6094\n",
      "     34            0.9923        0.2614       0.5347            0.5347        \u001b[94m1.1608\u001b[0m  0.0000  0.5938\n",
      "     35            0.9962        \u001b[32m0.2449\u001b[0m       0.5382            0.5382        \u001b[94m1.1596\u001b[0m  0.0000  0.5952\n",
      "     36            0.9962        \u001b[32m0.2405\u001b[0m       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        1.1597  0.0000  0.5938\n",
      "     37            0.9962        \u001b[32m0.2281\u001b[0m       \u001b[35m0.5451\u001b[0m            \u001b[31m0.5451\u001b[0m        1.1599  0.0000  0.5942\n",
      "     38            0.9962        0.2580       0.5451            0.5451        1.1599  0.0000  0.5782\n",
      "     39            0.9962        0.2819       0.5451            0.5451        1.1601  0.0000  0.5938\n",
      "     40            0.9962        0.2752       0.5451            0.5451        1.1603  0.0000  0.5938\n",
      "Training model for subject 3 with 270 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.4111\u001b[0m        \u001b[32m1.6122\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m1.5707\u001b[0m  0.0006  0.5942\n",
      "      2            \u001b[36m0.5074\u001b[0m        \u001b[32m1.3240\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m1.5694\u001b[0m  0.0006  0.5782\n",
      "      3            \u001b[36m0.5407\u001b[0m        \u001b[32m1.1594\u001b[0m       0.2951            0.2951        \u001b[94m1.4856\u001b[0m  0.0006  0.5938\n",
      "      4            0.4926        \u001b[32m1.1079\u001b[0m       0.3229            0.3229        1.5557  0.0006  0.5786\n",
      "      5            0.4815        \u001b[32m0.9211\u001b[0m       0.3264            0.3264        1.5869  0.0006  0.5938\n",
      "      6            0.5370        0.9565       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        1.5170  0.0006  0.5938\n",
      "      7            \u001b[36m0.6444\u001b[0m        \u001b[32m0.9191\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.4094\u001b[0m  0.0006  0.5948\n",
      "      8            \u001b[36m0.7111\u001b[0m        \u001b[32m0.7599\u001b[0m       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.3513\u001b[0m  0.0006  0.5801\n",
      "      9            \u001b[36m0.7963\u001b[0m        \u001b[32m0.7262\u001b[0m       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.2931\u001b[0m  0.0006  0.6719\n",
      "     10            \u001b[36m0.8259\u001b[0m        \u001b[32m0.6466\u001b[0m       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        \u001b[94m1.2785\u001b[0m  0.0005  0.6875\n",
      "     11            \u001b[36m0.8815\u001b[0m        0.6808       0.4514            0.4514        \u001b[94m1.2680\u001b[0m  0.0005  0.7032\n",
      "     12            \u001b[36m0.8926\u001b[0m        0.6529       0.4722            0.4722        \u001b[94m1.2587\u001b[0m  0.0005  0.7210\n",
      "     13            0.8926        \u001b[32m0.5471\u001b[0m       0.4896            0.4896        \u001b[94m1.2511\u001b[0m  0.0005  0.6567\n",
      "     14            \u001b[36m0.9185\u001b[0m        \u001b[32m0.5356\u001b[0m       \u001b[35m0.4931\u001b[0m            \u001b[31m0.4931\u001b[0m        1.2538  0.0005  0.5943\n",
      "     15            \u001b[36m0.9370\u001b[0m        \u001b[32m0.5153\u001b[0m       0.4861            0.4861        1.2532  0.0004  0.6571\n",
      "     16            \u001b[36m0.9481\u001b[0m        \u001b[32m0.4367\u001b[0m       0.4861            0.4861        \u001b[94m1.2439\u001b[0m  0.0004  0.6407\n",
      "     17            \u001b[36m0.9556\u001b[0m        0.4423       \u001b[35m0.5069\u001b[0m            \u001b[31m0.5069\u001b[0m        \u001b[94m1.2400\u001b[0m  0.0004  0.5938\n",
      "     18            0.9556        0.4518       \u001b[35m0.5174\u001b[0m            \u001b[31m0.5174\u001b[0m        1.2460  0.0004  0.5938\n",
      "     19            0.9370        \u001b[32m0.4081\u001b[0m       0.5174            0.5174        1.2480  0.0004  0.6094\n",
      "     20            0.9519        0.4210       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        \u001b[94m1.2361\u001b[0m  0.0003  0.6251\n",
      "     21            \u001b[36m0.9704\u001b[0m        0.4134       \u001b[35m0.5243\u001b[0m            \u001b[31m0.5243\u001b[0m        \u001b[94m1.2135\u001b[0m  0.0003  0.5785\n",
      "     22            \u001b[36m0.9815\u001b[0m        \u001b[32m0.3417\u001b[0m       0.5174            0.5174        \u001b[94m1.2089\u001b[0m  0.0003  0.6098\n",
      "     23            0.9778        \u001b[32m0.3141\u001b[0m       \u001b[35m0.5382\u001b[0m            \u001b[31m0.5382\u001b[0m        \u001b[94m1.2017\u001b[0m  0.0002  0.6407\n",
      "     24            0.9815        0.3453       \u001b[35m0.5451\u001b[0m            \u001b[31m0.5451\u001b[0m        \u001b[94m1.1975\u001b[0m  0.0002  0.6407\n",
      "     25            0.9815        0.3512       0.5417            0.5417        \u001b[94m1.1971\u001b[0m  0.0002  0.6094\n",
      "     26            \u001b[36m0.9852\u001b[0m        0.3196       0.5451            0.5451        1.1984  0.0002  0.5938\n",
      "     27            \u001b[36m0.9889\u001b[0m        \u001b[32m0.3009\u001b[0m       0.5451            0.5451        1.2005  0.0002  0.5938\n",
      "     28            \u001b[36m0.9926\u001b[0m        0.3285       0.5417            0.5417        1.2010  0.0001  0.6098\n",
      "     29            0.9926        \u001b[32m0.2894\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.2022  0.0001  0.7813\n",
      "     30            0.9926        0.3182       0.5521            0.5521        1.2027  0.0001  0.7813\n",
      "     31            0.9926        0.3033       0.5451            0.5451        1.2010  0.0001  0.7813\n",
      "     32            0.9926        0.3185       0.5347            0.5347        1.1984  0.0001  0.6563\n",
      "     33            \u001b[36m0.9963\u001b[0m        \u001b[32m0.2829\u001b[0m       0.5451            0.5451        \u001b[94m1.1962\u001b[0m  0.0000  0.6100\n",
      "     34            0.9963        0.3326       0.5417            0.5417        \u001b[94m1.1947\u001b[0m  0.0000  0.5947\n",
      "     35            0.9963        \u001b[32m0.2815\u001b[0m       0.5417            0.5417        \u001b[94m1.1936\u001b[0m  0.0000  0.5942\n",
      "     36            0.9963        \u001b[32m0.2520\u001b[0m       0.5451            0.5451        \u001b[94m1.1925\u001b[0m  0.0000  0.6098\n",
      "     37            0.9963        0.3097       0.5417            0.5417        \u001b[94m1.1911\u001b[0m  0.0000  0.5938\n",
      "     38            0.9963        0.2655       0.5417            0.5417        \u001b[94m1.1909\u001b[0m  0.0000  0.5938\n",
      "     39            0.9963        0.3017       0.5417            0.5417        1.1913  0.0000  0.5938\n",
      "     40            0.9963        0.3039       0.5417            0.5417        1.1909  0.0000  0.5942\n",
      "Training model for subject 4 with 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.5000\u001b[0m        \u001b[32m2.3849\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.2884\u001b[0m  0.0006  0.3125\n",
      "      2            0.5000        \u001b[32m0.7366\u001b[0m       0.2500            0.2500        4.5279  0.0006  0.3282\n",
      "      3            \u001b[36m0.8000\u001b[0m        \u001b[32m0.3765\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        5.1357  0.0006  0.3282\n",
      "      4            0.8000        \u001b[32m0.2387\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        5.5758  0.0006  0.3281\n",
      "      5            0.8000        0.2572       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        5.7647  0.0006  0.3125\n",
      "      6            0.8000        \u001b[32m0.1241\u001b[0m       0.2882            0.2882        5.7657  0.0006  0.3125\n",
      "      7            0.8000        0.1958       0.2812            0.2812        5.6918  0.0006  0.3125\n",
      "      8            0.8000        \u001b[32m0.0740\u001b[0m       0.2812            0.2812        5.5067  0.0006  0.3125\n",
      "      9            0.8000        \u001b[32m0.0713\u001b[0m       0.2882            0.2882        5.2350  0.0006  0.3125\n",
      "     10            0.8000        0.0844       0.2743            0.2743        4.9266  0.0005  0.3125\n",
      "     11            0.8000        \u001b[32m0.0321\u001b[0m       0.2847            0.2847        4.6141  0.0005  0.3125\n",
      "     12            0.8000        0.0986       0.2882            0.2882        4.3204  0.0005  0.3125\n",
      "     13            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0301\u001b[0m       0.2708            0.2708        4.0502  0.0005  0.3125\n",
      "     14            1.0000        0.0307       0.2708            0.2708        3.7949  0.0005  0.3438\n",
      "     15            1.0000        0.0316       0.2847            0.2847        3.5551  0.0004  0.3289\n",
      "     16            1.0000        0.0305       0.2847            0.2847        3.3380  0.0004  0.3282\n",
      "     17            1.0000        \u001b[32m0.0083\u001b[0m       0.2812            0.2812        \u001b[94m3.1486\u001b[0m  0.0004  0.3282\n",
      "     18            1.0000        0.0475       0.2778            0.2778        \u001b[94m2.9735\u001b[0m  0.0004  0.4375\n",
      "     19            1.0000        \u001b[32m0.0081\u001b[0m       0.2778            0.2778        \u001b[94m2.8305\u001b[0m  0.0004  0.4375\n",
      "     20            1.0000        0.0103       0.2778            0.2778        \u001b[94m2.7183\u001b[0m  0.0003  0.4219\n",
      "     21            1.0000        0.0151       0.2812            0.2812        \u001b[94m2.6274\u001b[0m  0.0003  0.4375\n",
      "     22            1.0000        0.0156       0.2812            0.2812        \u001b[94m2.5472\u001b[0m  0.0003  0.4375\n",
      "     23            1.0000        0.0155       0.2812            0.2812        \u001b[94m2.4843\u001b[0m  0.0002  0.4532\n",
      "     24            1.0000        \u001b[32m0.0078\u001b[0m       0.2882            0.2882        \u001b[94m2.4352\u001b[0m  0.0002  0.3438\n",
      "     25            1.0000        0.0103       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m2.3951\u001b[0m  0.0002  0.3907\n",
      "     26            1.0000        0.0115       0.2917            0.2917        \u001b[94m2.3653\u001b[0m  0.0002  0.3589\n",
      "     27            1.0000        \u001b[32m0.0041\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m2.3362\u001b[0m  0.0002  0.3125\n",
      "     28            1.0000        0.0171       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m2.3151\u001b[0m  0.0001  0.4063\n",
      "     29            1.0000        0.0076       0.2986            0.2986        \u001b[94m2.3006\u001b[0m  0.0001  0.3438\n",
      "     30            1.0000        0.0109       0.2986            0.2986        \u001b[94m2.2851\u001b[0m  0.0001  0.3281\n",
      "     31            1.0000        0.0062       0.2986            0.2986        \u001b[94m2.2735\u001b[0m  0.0001  0.3594\n",
      "     32            1.0000        0.0085       0.2986            0.2986        \u001b[94m2.2643\u001b[0m  0.0001  0.3438\n",
      "     33            1.0000        0.0083       0.2986            0.2986        \u001b[94m2.2582\u001b[0m  0.0000  0.2969\n",
      "     34            1.0000        0.0103       0.2986            0.2986        \u001b[94m2.2481\u001b[0m  0.0000  0.3438\n",
      "     35            1.0000        0.0090       0.2986            0.2986        \u001b[94m2.2443\u001b[0m  0.0000  0.3910\n",
      "     36            1.0000        0.0091       0.2986            0.2986        \u001b[94m2.2418\u001b[0m  0.0000  0.3281\n",
      "     37            1.0000        0.0061       0.2986            0.2986        \u001b[94m2.2370\u001b[0m  0.0000  0.3750\n",
      "     38            1.0000        0.0075       0.2986            0.2986        \u001b[94m2.2363\u001b[0m  0.0000  0.3589\n",
      "     39            1.0000        0.0106       0.2986            0.2986        \u001b[94m2.2308\u001b[0m  0.0000  0.3281\n",
      "     40            1.0000        0.0164       0.2986            0.2986        \u001b[94m2.2254\u001b[0m  0.0000  0.3750\n",
      "Training model for subject 4 with 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.4500\u001b[0m        \u001b[32m1.6940\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m2.0345\u001b[0m  0.0006  0.3281\n",
      "      2            \u001b[36m0.5000\u001b[0m        \u001b[32m0.7670\u001b[0m       0.2812            0.2812        2.3638  0.0006  0.2970\n",
      "      3            \u001b[36m0.5500\u001b[0m        \u001b[32m0.7626\u001b[0m       0.2847            0.2847        2.5443  0.0006  0.3594\n",
      "      4            \u001b[36m0.6000\u001b[0m        \u001b[32m0.6330\u001b[0m       0.2951            0.2951        2.7630  0.0006  0.2969\n",
      "      5            0.6000        \u001b[32m0.6048\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        2.7226  0.0006  0.2969\n",
      "      6            0.6000        \u001b[32m0.2137\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        2.6417  0.0006  0.3438\n",
      "      7            0.6000        \u001b[32m0.1613\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        2.6267  0.0006  0.2656\n",
      "      8            \u001b[36m0.7000\u001b[0m        \u001b[32m0.1355\u001b[0m       0.3403            0.3403        2.5607  0.0006  0.2813\n",
      "      9            \u001b[36m0.7500\u001b[0m        \u001b[32m0.1313\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        2.4672  0.0006  0.3461\n",
      "     10            \u001b[36m0.8000\u001b[0m        \u001b[32m0.1140\u001b[0m       0.3542            0.3542        2.3685  0.0005  0.2732\n",
      "     11            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1097\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        2.2497  0.0005  0.3281\n",
      "     12            1.0000        \u001b[32m0.0781\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        2.1852  0.0005  0.2813\n",
      "     13            1.0000        0.0869       0.3542            0.3542        2.1538  0.0005  0.3281\n",
      "     14            1.0000        \u001b[32m0.0689\u001b[0m       0.3472            0.3472        2.1335  0.0005  0.3437\n",
      "     15            1.0000        \u001b[32m0.0455\u001b[0m       0.3507            0.3507        2.0936  0.0004  0.2813\n",
      "     16            1.0000        0.0484       0.3576            0.3576        2.0481  0.0004  0.3125\n",
      "     17            1.0000        \u001b[32m0.0433\u001b[0m       0.3576            0.3576        \u001b[94m2.0138\u001b[0m  0.0004  0.3754\n",
      "     18            1.0000        0.0461       0.3576            0.3576        \u001b[94m1.9797\u001b[0m  0.0004  0.2813\n",
      "     19            1.0000        \u001b[32m0.0244\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.9469\u001b[0m  0.0004  0.2656\n",
      "     20            1.0000        0.0267       0.3715            0.3715        \u001b[94m1.9204\u001b[0m  0.0003  0.3129\n",
      "     21            1.0000        0.0291       0.3715            0.3715        \u001b[94m1.9018\u001b[0m  0.0003  0.2968\n",
      "     22            1.0000        0.0476       0.3646            0.3646        \u001b[94m1.8887\u001b[0m  0.0003  0.3594\n",
      "     23            1.0000        0.0391       0.3611            0.3611        \u001b[94m1.8757\u001b[0m  0.0002  0.3750\n",
      "     24            1.0000        0.0256       0.3611            0.3611        \u001b[94m1.8649\u001b[0m  0.0002  0.3750\n",
      "     25            1.0000        0.0297       0.3611            0.3611        \u001b[94m1.8578\u001b[0m  0.0002  0.3281\n",
      "     26            1.0000        0.0270       0.3542            0.3542        \u001b[94m1.8496\u001b[0m  0.0002  0.3594\n",
      "     27            1.0000        \u001b[32m0.0224\u001b[0m       0.3576            0.3576        \u001b[94m1.8440\u001b[0m  0.0002  0.4532\n",
      "     28            1.0000        \u001b[32m0.0162\u001b[0m       0.3611            0.3611        \u001b[94m1.8381\u001b[0m  0.0001  0.3750\n",
      "     29            1.0000        0.0190       0.3611            0.3611        \u001b[94m1.8327\u001b[0m  0.0001  0.3598\n",
      "     30            1.0000        \u001b[32m0.0138\u001b[0m       0.3611            0.3611        \u001b[94m1.8276\u001b[0m  0.0001  0.3125\n",
      "     31            1.0000        \u001b[32m0.0109\u001b[0m       0.3646            0.3646        \u001b[94m1.8231\u001b[0m  0.0001  0.3125\n",
      "     32            1.0000        0.0180       0.3611            0.3611        \u001b[94m1.8186\u001b[0m  0.0001  0.3121\n",
      "     33            1.0000        0.0199       0.3646            0.3646        \u001b[94m1.8145\u001b[0m  0.0000  0.2813\n",
      "     34            1.0000        0.0276       0.3646            0.3646        \u001b[94m1.8123\u001b[0m  0.0000  0.3438\n",
      "     35            1.0000        0.0120       0.3646            0.3646        \u001b[94m1.8100\u001b[0m  0.0000  0.2969\n",
      "     36            1.0000        \u001b[32m0.0097\u001b[0m       0.3646            0.3646        \u001b[94m1.8077\u001b[0m  0.0000  0.3441\n",
      "     37            1.0000        0.0126       0.3646            0.3646        \u001b[94m1.8060\u001b[0m  0.0000  0.3438\n",
      "     38            1.0000        0.0204       0.3646            0.3646        \u001b[94m1.8040\u001b[0m  0.0000  0.3281\n",
      "     39            1.0000        0.0173       0.3646            0.3646        \u001b[94m1.8024\u001b[0m  0.0000  0.2972\n",
      "     40            1.0000        0.0207       0.3646            0.3646        \u001b[94m1.8012\u001b[0m  0.0000  0.3134\n",
      "Training model for subject 4 with 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3333\u001b[0m        \u001b[32m1.6597\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m2.2079\u001b[0m  0.0006  0.2661\n",
      "      2            0.3000        \u001b[32m1.1489\u001b[0m       0.2535            0.2535        2.9872  0.0006  0.3282\n",
      "      3            0.3000        \u001b[32m0.8441\u001b[0m       0.2535            0.2535        3.6119  0.0006  0.2500\n",
      "      4            0.3000        \u001b[32m0.4666\u001b[0m       0.2535            0.2535        3.7874  0.0006  0.3481\n",
      "      5            \u001b[36m0.4000\u001b[0m        0.4754       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        3.5727  0.0006  0.2675\n",
      "      6            \u001b[36m0.5000\u001b[0m        \u001b[32m0.3567\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        3.1918  0.0006  0.2660\n",
      "      7            \u001b[36m0.6333\u001b[0m        \u001b[32m0.2183\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        2.8741  0.0006  0.2656\n",
      "      8            \u001b[36m0.7333\u001b[0m        0.2780       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        2.6501  0.0006  0.2500\n",
      "      9            \u001b[36m0.7667\u001b[0m        0.2288       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        2.4562  0.0006  0.2505\n",
      "     10            \u001b[36m0.8000\u001b[0m        \u001b[32m0.1782\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        2.3041  0.0005  0.2504\n",
      "     11            \u001b[36m0.8333\u001b[0m        0.1991       0.3299            0.3299        \u001b[94m2.1937\u001b[0m  0.0005  0.2501\n",
      "     12            \u001b[36m0.9000\u001b[0m        \u001b[32m0.1158\u001b[0m       0.3299            0.3299        \u001b[94m2.1158\u001b[0m  0.0005  0.2659\n",
      "     13            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1017\u001b[0m       0.3368            0.3368        \u001b[94m2.0436\u001b[0m  0.0005  0.2500\n",
      "     14            1.0000        \u001b[32m0.0732\u001b[0m       0.3299            0.3299        \u001b[94m1.9827\u001b[0m  0.0005  0.2505\n",
      "     15            1.0000        0.0800       0.3229            0.3229        \u001b[94m1.9280\u001b[0m  0.0004  0.2516\n",
      "     16            1.0000        0.0984       0.3264            0.3264        \u001b[94m1.8806\u001b[0m  0.0004  0.2504\n",
      "     17            1.0000        0.0753       0.3368            0.3368        \u001b[94m1.8392\u001b[0m  0.0004  0.2505\n",
      "     18            1.0000        0.0832       0.3333            0.3333        \u001b[94m1.8038\u001b[0m  0.0004  0.2656\n",
      "     19            1.0000        \u001b[32m0.0567\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.7782\u001b[0m  0.0004  0.2661\n",
      "     20            1.0000        \u001b[32m0.0413\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.7610\u001b[0m  0.0003  0.2656\n",
      "     21            1.0000        0.0641       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.7492\u001b[0m  0.0003  0.2500\n",
      "     22            1.0000        \u001b[32m0.0402\u001b[0m       0.3507            0.3507        \u001b[94m1.7374\u001b[0m  0.0003  0.2500\n",
      "     23            1.0000        0.0442       0.3438            0.3438        \u001b[94m1.7303\u001b[0m  0.0002  0.2503\n",
      "     24            1.0000        \u001b[32m0.0313\u001b[0m       0.3438            0.3438        \u001b[94m1.7233\u001b[0m  0.0002  0.2500\n",
      "     25            1.0000        0.0396       0.3438            0.3438        \u001b[94m1.7183\u001b[0m  0.0002  0.2501\n",
      "     26            1.0000        0.0429       0.3438            0.3438        \u001b[94m1.7140\u001b[0m  0.0002  0.2667\n",
      "     27            1.0000        0.0401       0.3438            0.3438        \u001b[94m1.7112\u001b[0m  0.0002  0.2813\n",
      "     28            1.0000        0.0442       0.3438            0.3438        \u001b[94m1.7106\u001b[0m  0.0001  0.2660\n",
      "     29            1.0000        0.0423       0.3438            0.3438        1.7111  0.0001  0.2602\n",
      "     30            1.0000        0.0410       0.3438            0.3438        1.7115  0.0001  0.2500\n",
      "     31            1.0000        0.0379       0.3438            0.3438        1.7125  0.0001  0.2500\n",
      "     32            1.0000        0.0380       0.3438            0.3438        1.7118  0.0001  0.2969\n",
      "     33            1.0000        0.0320       0.3438            0.3438        1.7106  0.0000  0.3441\n",
      "     34            1.0000        0.0565       0.3438            0.3438        \u001b[94m1.7096\u001b[0m  0.0000  0.3438\n",
      "     35            1.0000        0.0680       0.3438            0.3438        \u001b[94m1.7080\u001b[0m  0.0000  0.3281\n",
      "     36            1.0000        0.0423       0.3403            0.3403        \u001b[94m1.7069\u001b[0m  0.0000  0.3438\n",
      "     37            1.0000        0.0423       0.3403            0.3403        \u001b[94m1.7056\u001b[0m  0.0000  0.3443\n",
      "     38            1.0000        0.0473       0.3403            0.3403        \u001b[94m1.7044\u001b[0m  0.0000  0.3594\n",
      "     39            1.0000        0.0407       0.3403            0.3403        \u001b[94m1.7037\u001b[0m  0.0000  0.3594\n",
      "     40            1.0000        0.0332       0.3403            0.3403        \u001b[94m1.7031\u001b[0m  0.0000  0.2503\n",
      "Training model for subject 4 with 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3000\u001b[0m        \u001b[32m1.9900\u001b[0m       \u001b[35m0.2465\u001b[0m            \u001b[31m0.2465\u001b[0m        \u001b[94m3.4706\u001b[0m  0.0006  0.2656\n",
      "      2            0.3000        \u001b[32m1.4042\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.2467\u001b[0m  0.0006  0.2657\n",
      "      3            0.3000        \u001b[32m1.0355\u001b[0m       0.2500            0.2500        3.6121  0.0006  0.2657\n",
      "      4            \u001b[36m0.3250\u001b[0m        \u001b[32m0.7078\u001b[0m       0.2500            0.2500        3.5956  0.0006  0.2661\n",
      "      5            \u001b[36m0.3500\u001b[0m        \u001b[32m0.6586\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m3.2451\u001b[0m  0.0006  0.2500\n",
      "      6            0.3500        \u001b[32m0.5494\u001b[0m       0.2535            0.2535        \u001b[94m2.8652\u001b[0m  0.0006  0.2969\n",
      "      7            \u001b[36m0.5500\u001b[0m        \u001b[32m0.3732\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m2.5021\u001b[0m  0.0006  0.3750\n",
      "      8            \u001b[36m0.6750\u001b[0m        0.4273       0.2535            0.2535        \u001b[94m2.2662\u001b[0m  0.0006  0.2813\n",
      "      9            \u001b[36m0.7750\u001b[0m        \u001b[32m0.3418\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m2.1303\u001b[0m  0.0006  0.2656\n",
      "     10            \u001b[36m0.8750\u001b[0m        \u001b[32m0.2745\u001b[0m       0.2743            0.2743        \u001b[94m2.0284\u001b[0m  0.0005  0.2656\n",
      "     11            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2503\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        \u001b[94m1.9918\u001b[0m  0.0005  0.2656\n",
      "     12            0.9500        \u001b[32m0.2224\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m1.9603\u001b[0m  0.0005  0.2657\n",
      "     13            0.9500        \u001b[32m0.1874\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m1.9203\u001b[0m  0.0005  0.2503\n",
      "     14            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1445\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m1.8711\u001b[0m  0.0005  0.2656\n",
      "     15            1.0000        \u001b[32m0.1244\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m1.8317\u001b[0m  0.0004  0.2656\n",
      "     16            1.0000        0.1296       0.3264            0.3264        \u001b[94m1.7937\u001b[0m  0.0004  0.2500\n",
      "     17            1.0000        \u001b[32m0.1170\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.7607\u001b[0m  0.0004  0.2656\n",
      "     18            1.0000        \u001b[32m0.1028\u001b[0m       0.3368            0.3368        \u001b[94m1.7416\u001b[0m  0.0004  0.2657\n",
      "     19            1.0000        0.1097       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.7320\u001b[0m  0.0004  0.2656\n",
      "     20            1.0000        \u001b[32m0.0872\u001b[0m       0.3472            0.3472        \u001b[94m1.7247\u001b[0m  0.0003  0.2658\n",
      "     21            1.0000        0.1167       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.7195\u001b[0m  0.0003  0.2656\n",
      "     22            1.0000        0.0895       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.7154\u001b[0m  0.0003  0.2656\n",
      "     23            1.0000        0.1311       0.3785            0.3785        \u001b[94m1.7139\u001b[0m  0.0002  0.2656\n",
      "     24            1.0000        0.1256       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        1.7152  0.0002  0.2500\n",
      "     25            1.0000        \u001b[32m0.0755\u001b[0m       0.3785            0.3785        1.7169  0.0002  0.2656\n",
      "     26            1.0000        0.0782       0.3611            0.3611        1.7191  0.0002  0.2500\n",
      "     27            1.0000        \u001b[32m0.0535\u001b[0m       0.3576            0.3576        1.7215  0.0002  0.2656\n",
      "     28            1.0000        0.0789       0.3542            0.3542        1.7230  0.0001  0.2659\n",
      "     29            1.0000        0.0785       0.3542            0.3542        1.7248  0.0001  0.2965\n",
      "     30            1.0000        0.0763       0.3507            0.3507        1.7264  0.0001  0.2656\n",
      "     31            1.0000        0.0613       0.3542            0.3542        1.7281  0.0001  0.2500\n",
      "     32            1.0000        0.0594       0.3542            0.3542        1.7292  0.0001  0.2656\n",
      "     33            1.0000        0.0559       0.3542            0.3542        1.7302  0.0000  0.2656\n",
      "     34            1.0000        0.0578       0.3542            0.3542        1.7304  0.0000  0.2500\n",
      "     35            1.0000        0.0962       0.3542            0.3542        1.7309  0.0000  0.2656\n",
      "     36            1.0000        0.0652       0.3542            0.3542        1.7310  0.0000  0.2656\n",
      "     37            1.0000        0.0579       0.3542            0.3542        1.7311  0.0000  0.2656\n",
      "     38            1.0000        0.0724       0.3542            0.3542        1.7308  0.0000  0.2656\n",
      "     39            1.0000        \u001b[32m0.0523\u001b[0m       0.3542            0.3542        1.7306  0.0000  0.2656\n",
      "     40            1.0000        \u001b[32m0.0509\u001b[0m       0.3542            0.3542        1.7303  0.0000  0.2500\n",
      "Training model for subject 4 with 50 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2600\u001b[0m        \u001b[32m1.5668\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.9812\u001b[0m  0.0006  0.2813\n",
      "      2            0.2600        \u001b[32m1.0265\u001b[0m       0.2500            0.2500        4.6367  0.0006  0.2816\n",
      "      3            0.2600        \u001b[32m1.0050\u001b[0m       0.2500            0.2500        4.7292  0.0006  0.2818\n",
      "      4            0.2600        \u001b[32m0.8451\u001b[0m       0.2500            0.2500        4.3991  0.0006  0.3438\n",
      "      5            0.2600        \u001b[32m0.5734\u001b[0m       0.2500            0.2500        4.2296  0.0006  0.3438\n",
      "      6            \u001b[36m0.3600\u001b[0m        0.5876       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m3.8588\u001b[0m  0.0006  0.3907\n",
      "      7            \u001b[36m0.4000\u001b[0m        \u001b[32m0.4633\u001b[0m       0.2535            0.2535        \u001b[94m3.7747\u001b[0m  0.0006  0.3282\n",
      "      8            \u001b[36m0.4200\u001b[0m        \u001b[32m0.3888\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m3.4549\u001b[0m  0.0006  0.3438\n",
      "      9            \u001b[36m0.5200\u001b[0m        0.4043       0.2569            0.2569        \u001b[94m3.1093\u001b[0m  0.0006  0.3750\n",
      "     10            \u001b[36m0.6000\u001b[0m        \u001b[32m0.3508\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m2.8419\u001b[0m  0.0005  0.3750\n",
      "     11            \u001b[36m0.6800\u001b[0m        \u001b[32m0.3022\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m2.6160\u001b[0m  0.0005  0.2969\n",
      "     12            \u001b[36m0.7400\u001b[0m        \u001b[32m0.2567\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m2.4563\u001b[0m  0.0005  0.2656\n",
      "     13            \u001b[36m0.8400\u001b[0m        0.2745       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m2.3270\u001b[0m  0.0005  0.2981\n",
      "     14            \u001b[36m0.8600\u001b[0m        0.2762       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m2.2357\u001b[0m  0.0005  0.2970\n",
      "     15            \u001b[36m0.9000\u001b[0m        \u001b[32m0.1947\u001b[0m       0.2986            0.2986        \u001b[94m2.1533\u001b[0m  0.0004  0.2815\n",
      "     16            \u001b[36m0.9200\u001b[0m        0.2188       0.2917            0.2917        \u001b[94m2.0824\u001b[0m  0.0004  0.2813\n",
      "     17            \u001b[36m0.9400\u001b[0m        \u001b[32m0.1357\u001b[0m       0.2986            0.2986        \u001b[94m2.0122\u001b[0m  0.0004  0.2813\n",
      "     18            \u001b[36m0.9800\u001b[0m        0.1685       0.2986            0.2986        \u001b[94m1.9531\u001b[0m  0.0004  0.2822\n",
      "     19            \u001b[36m1.0000\u001b[0m        0.2451       0.2986            0.2986        \u001b[94m1.8957\u001b[0m  0.0004  0.2813\n",
      "     20            1.0000        0.1395       0.2847            0.2847        \u001b[94m1.8489\u001b[0m  0.0003  0.2813\n",
      "     21            1.0000        0.1395       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        \u001b[94m1.8114\u001b[0m  0.0003  0.2813\n",
      "     22            1.0000        0.1363       0.2986            0.2986        \u001b[94m1.7884\u001b[0m  0.0003  0.2813\n",
      "     23            1.0000        \u001b[32m0.1144\u001b[0m       0.2986            0.2986        \u001b[94m1.7753\u001b[0m  0.0002  0.2813\n",
      "     24            1.0000        \u001b[32m0.0914\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m1.7671\u001b[0m  0.0002  0.2656\n",
      "     25            1.0000        0.1352       0.3056            0.3056        \u001b[94m1.7611\u001b[0m  0.0002  0.2656\n",
      "     26            1.0000        0.1175       0.3021            0.3021        \u001b[94m1.7585\u001b[0m  0.0002  0.2813\n",
      "     27            1.0000        0.1027       0.3056            0.3056        \u001b[94m1.7573\u001b[0m  0.0002  0.2813\n",
      "     28            1.0000        0.0935       0.3090            0.3090        \u001b[94m1.7556\u001b[0m  0.0001  0.2814\n",
      "     29            1.0000        \u001b[32m0.0833\u001b[0m       0.3090            0.3090        \u001b[94m1.7544\u001b[0m  0.0001  0.2813\n",
      "     30            1.0000        0.1701       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        \u001b[94m1.7520\u001b[0m  0.0001  0.2813\n",
      "     31            1.0000        0.1064       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m1.7503\u001b[0m  0.0001  0.2813\n",
      "     32            1.0000        0.1200       0.3229            0.3229        \u001b[94m1.7484\u001b[0m  0.0001  0.2817\n",
      "     33            1.0000        0.1134       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m1.7470\u001b[0m  0.0000  0.2908\n",
      "     34            1.0000        0.1022       0.3264            0.3264        \u001b[94m1.7463\u001b[0m  0.0000  0.2656\n",
      "     35            1.0000        0.1206       0.3264            0.3264        \u001b[94m1.7454\u001b[0m  0.0000  0.2817\n",
      "     36            1.0000        0.0986       0.3264            0.3264        \u001b[94m1.7448\u001b[0m  0.0000  0.2813\n",
      "     37            1.0000        0.1334       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m1.7444\u001b[0m  0.0000  0.2656\n",
      "     38            1.0000        0.1173       0.3299            0.3299        \u001b[94m1.7442\u001b[0m  0.0000  0.2818\n",
      "     39            1.0000        0.1243       0.3299            0.3299        \u001b[94m1.7441\u001b[0m  0.0000  0.2661\n",
      "     40            1.0000        0.0988       0.3299            0.3299        \u001b[94m1.7438\u001b[0m  0.0000  0.2656\n",
      "Training model for subject 4 with 60 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2667\u001b[0m        \u001b[32m1.7232\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m3.5103\u001b[0m  0.0006  0.2970\n",
      "      2            \u001b[36m0.4167\u001b[0m        \u001b[32m1.2653\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m2.4201\u001b[0m  0.0006  0.2969\n",
      "      3            \u001b[36m0.5000\u001b[0m        \u001b[32m0.9943\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m2.3387\u001b[0m  0.0006  0.2969\n",
      "      4            0.3833        \u001b[32m0.8804\u001b[0m       0.2569            0.2569        2.5381  0.0006  0.2813\n",
      "      5            0.4333        \u001b[32m0.7281\u001b[0m       0.2743            0.2743        2.4452  0.0006  0.2975\n",
      "      6            \u001b[36m0.6167\u001b[0m        \u001b[32m0.5989\u001b[0m       0.2812            0.2812        \u001b[94m2.1870\u001b[0m  0.0006  0.3594\n",
      "      7            \u001b[36m0.7000\u001b[0m        \u001b[32m0.4923\u001b[0m       0.3056            0.3056        \u001b[94m2.0596\u001b[0m  0.0006  0.3125\n",
      "      8            0.7000        0.4936       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m2.0593\u001b[0m  0.0006  0.2813\n",
      "      9            \u001b[36m0.7333\u001b[0m        \u001b[32m0.4241\u001b[0m       0.3299            0.3299        2.0787  0.0006  0.2813\n",
      "     10            \u001b[36m0.7500\u001b[0m        \u001b[32m0.4152\u001b[0m       0.3333            0.3333        \u001b[94m2.0532\u001b[0m  0.0005  0.2969\n",
      "     11            0.7500        \u001b[32m0.3110\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        2.0637  0.0005  0.3125\n",
      "     12            \u001b[36m0.8167\u001b[0m        0.3483       0.3368            0.3368        2.0709  0.0005  0.3594\n",
      "     13            \u001b[36m0.8333\u001b[0m        \u001b[32m0.2710\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        2.0554  0.0005  0.3594\n",
      "     14            \u001b[36m0.8500\u001b[0m        0.2990       0.3368            0.3368        \u001b[94m2.0391\u001b[0m  0.0005  0.3438\n",
      "     15            \u001b[36m0.9167\u001b[0m        \u001b[32m0.1868\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m2.0101\u001b[0m  0.0004  0.3594\n",
      "     16            \u001b[36m0.9667\u001b[0m        0.2010       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.9671\u001b[0m  0.0004  0.3438\n",
      "     17            \u001b[36m0.9833\u001b[0m        0.1958       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.9044\u001b[0m  0.0004  0.3750\n",
      "     18            \u001b[36m1.0000\u001b[0m        0.2102       0.3646            0.3646        \u001b[94m1.8320\u001b[0m  0.0004  0.4063\n",
      "     19            1.0000        \u001b[32m0.1495\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.7744\u001b[0m  0.0004  0.2972\n",
      "     20            1.0000        0.1682       0.3750            0.3750        \u001b[94m1.7322\u001b[0m  0.0003  0.2970\n",
      "     21            1.0000        0.2165       0.3646            0.3646        \u001b[94m1.7029\u001b[0m  0.0003  0.2969\n",
      "     22            1.0000        0.1519       0.3681            0.3681        \u001b[94m1.6836\u001b[0m  0.0003  0.2982\n",
      "     23            1.0000        \u001b[32m0.1379\u001b[0m       0.3646            0.3646        \u001b[94m1.6728\u001b[0m  0.0002  0.2972\n",
      "     24            1.0000        0.1635       0.3681            0.3681        \u001b[94m1.6686\u001b[0m  0.0002  0.2972\n",
      "     25            1.0000        0.1483       0.3646            0.3646        \u001b[94m1.6676\u001b[0m  0.0002  0.2813\n",
      "     26            1.0000        \u001b[32m0.1290\u001b[0m       0.3646            0.3646        1.6677  0.0002  0.2826\n",
      "     27            1.0000        0.1439       0.3646            0.3646        1.6687  0.0002  0.2813\n",
      "     28            1.0000        0.1388       0.3646            0.3646        1.6690  0.0001  0.2813\n",
      "     29            1.0000        \u001b[32m0.0925\u001b[0m       0.3611            0.3611        1.6683  0.0001  0.2975\n",
      "     30            1.0000        0.1027       0.3576            0.3576        \u001b[94m1.6661\u001b[0m  0.0001  0.2821\n",
      "     31            1.0000        0.1051       0.3611            0.3611        \u001b[94m1.6640\u001b[0m  0.0001  0.2969\n",
      "     32            1.0000        0.1198       0.3611            0.3611        \u001b[94m1.6617\u001b[0m  0.0001  0.3281\n",
      "     33            1.0000        \u001b[32m0.0856\u001b[0m       0.3611            0.3611        \u001b[94m1.6595\u001b[0m  0.0000  0.3125\n",
      "     34            1.0000        0.1028       0.3646            0.3646        \u001b[94m1.6573\u001b[0m  0.0000  0.3137\n",
      "     35            1.0000        \u001b[32m0.0752\u001b[0m       0.3646            0.3646        \u001b[94m1.6555\u001b[0m  0.0000  0.2813\n",
      "     36            1.0000        0.1168       0.3646            0.3646        \u001b[94m1.6538\u001b[0m  0.0000  0.2969\n",
      "     37            1.0000        0.1553       0.3611            0.3611        \u001b[94m1.6523\u001b[0m  0.0000  0.2969\n",
      "     38            1.0000        0.1285       0.3611            0.3611        \u001b[94m1.6511\u001b[0m  0.0000  0.2975\n",
      "     39            1.0000        0.1077       0.3681            0.3681        \u001b[94m1.6498\u001b[0m  0.0000  0.2969\n",
      "     40            1.0000        0.1107       0.3681            0.3681        \u001b[94m1.6491\u001b[0m  0.0000  0.2813\n",
      "Training model for subject 4 with 70 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2286\u001b[0m        \u001b[32m1.6932\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m5.1298\u001b[0m  0.0006  0.2969\n",
      "      2            0.2286        \u001b[32m1.2739\u001b[0m       0.2500            0.2500        \u001b[94m4.3005\u001b[0m  0.0006  0.2969\n",
      "      3            \u001b[36m0.2714\u001b[0m        \u001b[32m1.0929\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m3.6432\u001b[0m  0.0006  0.2974\n",
      "      4            \u001b[36m0.3714\u001b[0m        \u001b[32m0.9548\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m3.0889\u001b[0m  0.0006  0.3125\n",
      "      5            \u001b[36m0.4286\u001b[0m        \u001b[32m0.7177\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m2.7753\u001b[0m  0.0006  0.3125\n",
      "      6            \u001b[36m0.4429\u001b[0m        \u001b[32m0.5747\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m2.5405\u001b[0m  0.0006  0.2969\n",
      "      7            \u001b[36m0.4571\u001b[0m        \u001b[32m0.5309\u001b[0m       0.3056            0.3056        \u001b[94m2.3420\u001b[0m  0.0006  0.3282\n",
      "      8            \u001b[36m0.5000\u001b[0m        0.6415       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m2.2909\u001b[0m  0.0006  0.3130\n",
      "      9            \u001b[36m0.5429\u001b[0m        \u001b[32m0.4913\u001b[0m       0.3333            0.3333        \u001b[94m2.2480\u001b[0m  0.0006  0.3125\n",
      "     10            \u001b[36m0.5857\u001b[0m        \u001b[32m0.4403\u001b[0m       0.3299            0.3299        \u001b[94m2.1945\u001b[0m  0.0005  0.3125\n",
      "     11            \u001b[36m0.7000\u001b[0m        \u001b[32m0.4400\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m2.1230\u001b[0m  0.0005  0.3125\n",
      "     12            \u001b[36m0.7286\u001b[0m        \u001b[32m0.3864\u001b[0m       0.3333            0.3333        \u001b[94m2.0240\u001b[0m  0.0005  0.3125\n",
      "     13            \u001b[36m0.8571\u001b[0m        \u001b[32m0.3701\u001b[0m       0.3368            0.3368        \u001b[94m1.9438\u001b[0m  0.0005  0.3125\n",
      "     14            \u001b[36m0.9143\u001b[0m        \u001b[32m0.3011\u001b[0m       0.3333            0.3333        \u001b[94m1.8800\u001b[0m  0.0005  0.4375\n",
      "     15            \u001b[36m0.9429\u001b[0m        \u001b[32m0.2840\u001b[0m       0.3229            0.3229        \u001b[94m1.8157\u001b[0m  0.0004  0.3125\n",
      "     16            \u001b[36m0.9857\u001b[0m        \u001b[32m0.2432\u001b[0m       0.3299            0.3299        \u001b[94m1.7708\u001b[0m  0.0004  0.3438\n",
      "     17            0.9857        0.2589       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.7306\u001b[0m  0.0004  0.3594\n",
      "     18            0.9857        \u001b[32m0.2121\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.6970\u001b[0m  0.0004  0.3750\n",
      "     19            \u001b[36m1.0000\u001b[0m        0.2220       0.3681            0.3681        \u001b[94m1.6733\u001b[0m  0.0004  0.4063\n",
      "     20            1.0000        \u001b[32m0.1853\u001b[0m       0.3681            0.3681        \u001b[94m1.6571\u001b[0m  0.0003  0.3750\n",
      "     21            1.0000        0.1866       0.3611            0.3611        \u001b[94m1.6437\u001b[0m  0.0003  0.3906\n",
      "     22            1.0000        \u001b[32m0.1651\u001b[0m       0.3576            0.3576        \u001b[94m1.6280\u001b[0m  0.0003  0.4063\n",
      "     23            1.0000        0.1688       0.3542            0.3542        \u001b[94m1.6203\u001b[0m  0.0002  0.3438\n",
      "     24            1.0000        \u001b[32m0.1355\u001b[0m       0.3646            0.3646        \u001b[94m1.6148\u001b[0m  0.0002  0.3743\n",
      "     25            1.0000        0.1519       0.3646            0.3646        \u001b[94m1.6133\u001b[0m  0.0002  0.3288\n",
      "     26            1.0000        0.1638       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.6112\u001b[0m  0.0002  0.3125\n",
      "     27            1.0000        0.1356       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.6092\u001b[0m  0.0002  0.3129\n",
      "     28            1.0000        0.1678       0.3715            0.3715        \u001b[94m1.6084\u001b[0m  0.0001  0.3438\n",
      "     29            1.0000        0.1664       0.3750            0.3750        1.6104  0.0001  0.3125\n",
      "     30            1.0000        0.1724       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        1.6111  0.0001  0.3125\n",
      "     31            1.0000        0.1794       0.3715            0.3715        1.6113  0.0001  0.2969\n",
      "     32            1.0000        \u001b[32m0.1184\u001b[0m       0.3715            0.3715        1.6125  0.0001  0.3129\n",
      "     33            1.0000        0.1242       0.3681            0.3681        1.6135  0.0000  0.3125\n",
      "     34            1.0000        0.1208       0.3681            0.3681        1.6143  0.0000  0.3141\n",
      "     35            1.0000        \u001b[32m0.1134\u001b[0m       0.3681            0.3681        1.6150  0.0000  0.3125\n",
      "     36            1.0000        0.1542       0.3681            0.3681        1.6155  0.0000  0.2969\n",
      "     37            1.0000        0.1441       0.3681            0.3681        1.6157  0.0000  0.2973\n",
      "     38            1.0000        0.1339       0.3681            0.3681        1.6159  0.0000  0.2969\n",
      "     39            1.0000        0.1478       0.3646            0.3646        1.6162  0.0000  0.3125\n",
      "     40            1.0000        0.1136       0.3646            0.3646        1.6164  0.0000  0.3131\n",
      "Training model for subject 4 with 80 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.5493\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m5.3034\u001b[0m  0.0006  0.3281\n",
      "      2            \u001b[36m0.3500\u001b[0m        \u001b[32m1.2849\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m3.9668\u001b[0m  0.0006  0.3252\n",
      "      3            \u001b[36m0.4625\u001b[0m        \u001b[32m1.1061\u001b[0m       0.2986            0.2986        \u001b[94m3.9323\u001b[0m  0.0006  0.3290\n",
      "      4            0.3875        \u001b[32m0.9325\u001b[0m       0.2708            0.2708        4.0359  0.0006  0.3285\n",
      "      5            0.3500        \u001b[32m0.8857\u001b[0m       0.2569            0.2569        \u001b[94m3.9156\u001b[0m  0.0006  0.3495\n",
      "      6            0.3375        \u001b[32m0.7667\u001b[0m       0.2535            0.2535        \u001b[94m3.8548\u001b[0m  0.0006  0.3281\n",
      "      7            0.3500        \u001b[32m0.6824\u001b[0m       0.2569            0.2569        \u001b[94m3.7632\u001b[0m  0.0006  0.3283\n",
      "      8            0.3625        \u001b[32m0.5825\u001b[0m       0.2674            0.2674        \u001b[94m3.6200\u001b[0m  0.0006  0.3281\n",
      "      9            0.4250        0.6804       0.2778            0.2778        \u001b[94m3.3754\u001b[0m  0.0006  0.3125\n",
      "     10            0.4625        \u001b[32m0.5580\u001b[0m       0.2882            0.2882        \u001b[94m3.1610\u001b[0m  0.0005  0.3281\n",
      "     11            \u001b[36m0.5375\u001b[0m        \u001b[32m0.5060\u001b[0m       0.2951            0.2951        \u001b[94m2.9487\u001b[0m  0.0005  0.3442\n",
      "     12            \u001b[36m0.6000\u001b[0m        \u001b[32m0.4286\u001b[0m       0.2917            0.2917        \u001b[94m2.8090\u001b[0m  0.0005  0.3281\n",
      "     13            \u001b[36m0.7125\u001b[0m        \u001b[32m0.3748\u001b[0m       0.3125            0.3125        \u001b[94m2.6030\u001b[0m  0.0005  0.3286\n",
      "     14            \u001b[36m0.7375\u001b[0m        \u001b[32m0.3371\u001b[0m       0.3125            0.3125        \u001b[94m2.4476\u001b[0m  0.0005  0.3281\n",
      "     15            \u001b[36m0.8000\u001b[0m        \u001b[32m0.3338\u001b[0m       0.3125            0.3125        \u001b[94m2.3047\u001b[0m  0.0004  0.3282\n",
      "     16            \u001b[36m0.8500\u001b[0m        0.3425       0.3125            0.3125        \u001b[94m2.1935\u001b[0m  0.0004  0.3281\n",
      "     17            \u001b[36m0.8750\u001b[0m        \u001b[32m0.2847\u001b[0m       0.3333            0.3333        \u001b[94m2.0901\u001b[0m  0.0004  0.3594\n",
      "     18            \u001b[36m0.9625\u001b[0m        \u001b[32m0.2491\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.9990\u001b[0m  0.0004  0.4063\n",
      "     19            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2458\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.9104\u001b[0m  0.0004  0.3911\n",
      "     20            0.9750        \u001b[32m0.1879\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.8256\u001b[0m  0.0003  0.3907\n",
      "     21            \u001b[36m1.0000\u001b[0m        0.2175       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.7584\u001b[0m  0.0003  0.3907\n",
      "     22            1.0000        0.2386       0.3750            0.3750        \u001b[94m1.7083\u001b[0m  0.0003  0.4219\n",
      "     23            1.0000        0.2044       0.3785            0.3785        \u001b[94m1.6585\u001b[0m  0.0002  0.4375\n",
      "     24            1.0000        \u001b[32m0.1863\u001b[0m       0.3750            0.3750        \u001b[94m1.6214\u001b[0m  0.0002  0.3907\n",
      "     25            1.0000        \u001b[32m0.1579\u001b[0m       0.3785            0.3785        \u001b[94m1.5947\u001b[0m  0.0002  0.3125\n",
      "     26            1.0000        0.2059       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.5738\u001b[0m  0.0002  0.3284\n",
      "     27            1.0000        0.1729       0.3958            0.3958        \u001b[94m1.5601\u001b[0m  0.0002  0.3129\n",
      "     28            1.0000        \u001b[32m0.1493\u001b[0m       0.3958            0.3958        \u001b[94m1.5536\u001b[0m  0.0001  0.3133\n",
      "     29            1.0000        0.1835       0.3924            0.3924        \u001b[94m1.5450\u001b[0m  0.0001  0.3126\n",
      "     30            1.0000        0.1618       0.3958            0.3958        \u001b[94m1.5389\u001b[0m  0.0001  0.3281\n",
      "     31            1.0000        \u001b[32m0.1431\u001b[0m       0.3924            0.3924        \u001b[94m1.5353\u001b[0m  0.0001  0.3125\n",
      "     32            1.0000        0.1674       0.3924            0.3924        \u001b[94m1.5321\u001b[0m  0.0001  0.3281\n",
      "     33            1.0000        0.1760       0.3958            0.3958        \u001b[94m1.5295\u001b[0m  0.0000  0.3291\n",
      "     34            1.0000        0.1508       0.3958            0.3958        \u001b[94m1.5265\u001b[0m  0.0000  0.3285\n",
      "     35            1.0000        \u001b[32m0.1406\u001b[0m       0.3958            0.3958        \u001b[94m1.5230\u001b[0m  0.0000  0.3282\n",
      "     36            1.0000        0.1443       0.3958            0.3958        \u001b[94m1.5203\u001b[0m  0.0000  0.3287\n",
      "     37            1.0000        0.1987       0.3958            0.3958        \u001b[94m1.5174\u001b[0m  0.0000  0.3125\n",
      "     38            1.0000        0.1598       0.3958            0.3958        \u001b[94m1.5149\u001b[0m  0.0000  0.3281\n",
      "     39            1.0000        0.1646       0.3958            0.3958        \u001b[94m1.5128\u001b[0m  0.0000  0.3282\n",
      "     40            1.0000        \u001b[32m0.1334\u001b[0m       0.3924            0.3924        \u001b[94m1.5108\u001b[0m  0.0000  0.3281\n",
      "Training model for subject 4 with 90 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2889\u001b[0m        \u001b[32m1.6571\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m3.1589\u001b[0m  0.0006  0.3286\n",
      "      2            \u001b[36m0.3667\u001b[0m        \u001b[32m1.4154\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m2.4285\u001b[0m  0.0006  0.3442\n",
      "      3            0.3000        \u001b[32m1.0030\u001b[0m       0.2535            0.2535        \u001b[94m2.3752\u001b[0m  0.0006  0.3284\n",
      "      4            0.3000        1.0123       0.2500            0.2500        2.4773  0.0006  0.3289\n",
      "      5            0.2889        \u001b[32m0.8137\u001b[0m       0.2500            0.2500        2.6250  0.0006  0.3292\n",
      "      6            0.3222        \u001b[32m0.7814\u001b[0m       0.2500            0.2500        2.5786  0.0006  0.3438\n",
      "      7            \u001b[36m0.4222\u001b[0m        \u001b[32m0.6283\u001b[0m       0.2535            0.2535        2.4131  0.0006  0.3286\n",
      "      8            \u001b[36m0.4778\u001b[0m        0.6527       0.2639            0.2639        \u001b[94m2.2399\u001b[0m  0.0006  0.3281\n",
      "      9            \u001b[36m0.6111\u001b[0m        0.6449       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m2.0059\u001b[0m  0.0006  0.3285\n",
      "     10            \u001b[36m0.7000\u001b[0m        \u001b[32m0.5147\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m1.8938\u001b[0m  0.0005  0.3282\n",
      "     11            \u001b[36m0.8333\u001b[0m        \u001b[32m0.4648\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m1.8121\u001b[0m  0.0005  0.3438\n",
      "     12            \u001b[36m0.8889\u001b[0m        \u001b[32m0.4449\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m1.7393\u001b[0m  0.0005  0.3281\n",
      "     13            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4066\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.6900\u001b[0m  0.0005  0.3438\n",
      "     14            \u001b[36m0.9667\u001b[0m        0.4073       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.6612\u001b[0m  0.0005  0.3281\n",
      "     15            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3941\u001b[0m       0.3472            0.3472        \u001b[94m1.6360\u001b[0m  0.0004  0.3281\n",
      "     16            0.9778        \u001b[32m0.3725\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.6108\u001b[0m  0.0004  0.3438\n",
      "     17            0.9778        0.3817       0.3507            0.3507        \u001b[94m1.6005\u001b[0m  0.0004  0.3594\n",
      "     18            \u001b[36m0.9889\u001b[0m        \u001b[32m0.2772\u001b[0m       0.3472            0.3472        \u001b[94m1.5922\u001b[0m  0.0004  0.3924\n",
      "     19            \u001b[36m1.0000\u001b[0m        0.2986       0.3507            0.3507        \u001b[94m1.5862\u001b[0m  0.0004  0.4068\n",
      "     20            1.0000        0.2781       0.3542            0.3542        \u001b[94m1.5754\u001b[0m  0.0003  0.3909\n",
      "     21            1.0000        \u001b[32m0.2534\u001b[0m       0.3472            0.3472        \u001b[94m1.5713\u001b[0m  0.0003  0.3750\n",
      "     22            1.0000        0.3047       0.3576            0.3576        \u001b[94m1.5590\u001b[0m  0.0003  0.3906\n",
      "     23            1.0000        \u001b[32m0.2502\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.5447\u001b[0m  0.0002  0.4532\n",
      "     24            1.0000        \u001b[32m0.2165\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.5338\u001b[0m  0.0002  0.4375\n",
      "     25            1.0000        0.2465       0.3854            0.3854        \u001b[94m1.5228\u001b[0m  0.0002  0.3594\n",
      "     26            1.0000        0.2550       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.5168\u001b[0m  0.0002  0.3282\n",
      "     27            1.0000        \u001b[32m0.2064\u001b[0m       0.3819            0.3819        \u001b[94m1.5120\u001b[0m  0.0002  0.3287\n",
      "     28            1.0000        0.2128       0.3889            0.3889        \u001b[94m1.5110\u001b[0m  0.0001  0.3283\n",
      "     29            1.0000        0.2127       0.3924            0.3924        1.5113  0.0001  0.3438\n",
      "     30            1.0000        \u001b[32m0.2033\u001b[0m       0.3889            0.3889        \u001b[94m1.5104\u001b[0m  0.0001  0.3440\n",
      "     31            1.0000        \u001b[32m0.1600\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.5099\u001b[0m  0.0001  0.3286\n",
      "     32            1.0000        0.1726       0.3993            0.3993        \u001b[94m1.5097\u001b[0m  0.0001  0.3281\n",
      "     33            1.0000        0.2323       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        1.5097  0.0000  0.3446\n",
      "     34            1.0000        0.1863       0.4028            0.4028        1.5101  0.0000  0.3438\n",
      "     35            1.0000        0.2009       0.3993            0.3993        1.5102  0.0000  0.3438\n",
      "     36            1.0000        0.2078       0.3993            0.3993        1.5099  0.0000  0.3281\n",
      "     37            1.0000        0.1978       0.3993            0.3993        1.5098  0.0000  0.3441\n",
      "     38            1.0000        0.1979       0.3993            0.3993        \u001b[94m1.5095\u001b[0m  0.0000  0.3438\n",
      "     39            1.0000        0.2439       0.4028            0.4028        \u001b[94m1.5092\u001b[0m  0.0000  0.3282\n",
      "     40            1.0000        0.2338       0.4028            0.4028        \u001b[94m1.5090\u001b[0m  0.0000  0.3441\n",
      "Training model for subject 4 with 100 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.4200\u001b[0m        \u001b[32m1.6240\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m2.6995\u001b[0m  0.0006  0.3594\n",
      "      2            \u001b[36m0.4700\u001b[0m        \u001b[32m1.2743\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m2.2858\u001b[0m  0.0006  0.3594\n",
      "      3            0.4700        \u001b[32m1.0905\u001b[0m       0.2743            0.2743        \u001b[94m2.0928\u001b[0m  0.0006  0.3442\n",
      "      4            \u001b[36m0.5600\u001b[0m        \u001b[32m0.9020\u001b[0m       0.2778            0.2778        \u001b[94m1.9157\u001b[0m  0.0006  0.3991\n",
      "      5            \u001b[36m0.5800\u001b[0m        \u001b[32m0.8981\u001b[0m       0.2812            0.2812        \u001b[94m1.8892\u001b[0m  0.0006  0.3442\n",
      "      6            0.5800        \u001b[32m0.7466\u001b[0m       0.2882            0.2882        2.0146  0.0006  0.3594\n",
      "      7            \u001b[36m0.6600\u001b[0m        \u001b[32m0.7096\u001b[0m       0.3229            0.3229        1.9739  0.0006  0.3438\n",
      "      8            \u001b[36m0.6900\u001b[0m        \u001b[32m0.6041\u001b[0m       0.3229            0.3229        2.0221  0.0006  0.3438\n",
      "      9            \u001b[36m0.7700\u001b[0m        \u001b[32m0.5604\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        1.9607  0.0006  0.3438\n",
      "     10            \u001b[36m0.8400\u001b[0m        \u001b[32m0.5228\u001b[0m       0.3194            0.3194        \u001b[94m1.8694\u001b[0m  0.0005  0.3594\n",
      "     11            \u001b[36m0.9200\u001b[0m        \u001b[32m0.4454\u001b[0m       0.3194            0.3194        \u001b[94m1.7948\u001b[0m  0.0005  0.3439\n",
      "     12            0.9200        \u001b[32m0.4275\u001b[0m       0.3333            0.3333        \u001b[94m1.7386\u001b[0m  0.0005  0.3438\n",
      "     13            \u001b[36m0.9600\u001b[0m        0.4334       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.6965\u001b[0m  0.0005  0.3597\n",
      "     14            \u001b[36m0.9700\u001b[0m        \u001b[32m0.4154\u001b[0m       0.3368            0.3368        \u001b[94m1.6773\u001b[0m  0.0005  0.3594\n",
      "     15            \u001b[36m0.9900\u001b[0m        \u001b[32m0.3724\u001b[0m       0.3264            0.3264        1.6941  0.0004  0.3907\n",
      "     16            0.9900        \u001b[32m0.3191\u001b[0m       0.3194            0.3194        1.7021  0.0004  0.4064\n",
      "     17            0.9900        \u001b[32m0.3166\u001b[0m       0.3299            0.3299        1.7150  0.0004  0.4688\n",
      "     18            0.9900        \u001b[32m0.3006\u001b[0m       0.3299            0.3299        1.7235  0.0004  0.4219\n",
      "     19            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2545\u001b[0m       0.3333            0.3333        1.7309  0.0004  0.4219\n",
      "     20            1.0000        \u001b[32m0.2301\u001b[0m       0.3403            0.3403        1.7351  0.0003  0.4531\n",
      "     21            1.0000        0.2421       0.3403            0.3403        1.7313  0.0003  0.4844\n",
      "     22            1.0000        0.2471       0.3403            0.3403        1.7237  0.0003  0.3750\n",
      "     23            1.0000        \u001b[32m0.2189\u001b[0m       0.3403            0.3403        1.7120  0.0002  0.3594\n",
      "     24            1.0000        0.2300       0.3403            0.3403        1.6998  0.0002  0.3594\n",
      "     25            1.0000        \u001b[32m0.2169\u001b[0m       0.3403            0.3403        1.6900  0.0002  0.3438\n",
      "     26            1.0000        \u001b[32m0.1754\u001b[0m       0.3438            0.3438        1.6787  0.0002  0.3438\n",
      "     27            1.0000        0.2278       0.3472            0.3472        \u001b[94m1.6683\u001b[0m  0.0002  0.3594\n",
      "     28            1.0000        \u001b[32m0.1715\u001b[0m       0.3507            0.3507        \u001b[94m1.6610\u001b[0m  0.0001  0.3594\n",
      "     29            1.0000        0.1879       0.3542            0.3542        \u001b[94m1.6527\u001b[0m  0.0001  0.3438\n",
      "     30            1.0000        0.1835       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.6460\u001b[0m  0.0001  0.3438\n",
      "     31            1.0000        0.1825       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.6386\u001b[0m  0.0001  0.3597\n",
      "     32            1.0000        0.1828       0.3611            0.3611        \u001b[94m1.6336\u001b[0m  0.0001  0.3442\n",
      "     33            1.0000        \u001b[32m0.1711\u001b[0m       0.3611            0.3611        \u001b[94m1.6307\u001b[0m  0.0000  0.3438\n",
      "     34            1.0000        0.1970       0.3611            0.3611        \u001b[94m1.6274\u001b[0m  0.0000  0.3438\n",
      "     35            1.0000        0.2188       0.3611            0.3611        \u001b[94m1.6251\u001b[0m  0.0000  0.3438\n",
      "     36            1.0000        0.2141       0.3611            0.3611        \u001b[94m1.6227\u001b[0m  0.0000  0.4279\n",
      "     37            1.0000        0.1845       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.6212\u001b[0m  0.0000  0.4579\n",
      "     38            1.0000        0.1917       0.3646            0.3646        \u001b[94m1.6198\u001b[0m  0.0000  0.4242\n",
      "     39            1.0000        0.2065       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.6188\u001b[0m  0.0000  0.6666\n",
      "     40            1.0000        0.1744       0.3681            0.3681        \u001b[94m1.6179\u001b[0m  0.0000  0.4009\n",
      "Training model for subject 4 with 110 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2545\u001b[0m        \u001b[32m1.5866\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.8772\u001b[0m  0.0006  0.4496\n",
      "      2            0.2545        \u001b[32m1.2085\u001b[0m       0.2500            0.2500        \u001b[94m3.8998\u001b[0m  0.0006  0.4325\n",
      "      3            \u001b[36m0.4000\u001b[0m        \u001b[32m1.0488\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        \u001b[94m2.7475\u001b[0m  0.0006  0.3990\n",
      "      4            \u001b[36m0.6818\u001b[0m        \u001b[32m0.8921\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m2.3755\u001b[0m  0.0006  0.4081\n",
      "      5            0.6273        \u001b[32m0.7314\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m2.3220\u001b[0m  0.0006  0.4440\n",
      "      6            0.6000        0.8091       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m2.2454\u001b[0m  0.0006  0.3691\n",
      "      7            0.6000        \u001b[32m0.6394\u001b[0m       0.3750            0.3750        \u001b[94m2.1630\u001b[0m  0.0006  0.3860\n",
      "      8            0.6636        \u001b[32m0.6299\u001b[0m       0.3750            0.3750        \u001b[94m1.9931\u001b[0m  0.0006  0.4003\n",
      "      9            \u001b[36m0.7091\u001b[0m        0.6497       0.3785            0.3785        \u001b[94m1.8979\u001b[0m  0.0006  0.4499\n",
      "     10            0.7091        \u001b[32m0.5926\u001b[0m       0.3785            0.3785        \u001b[94m1.8940\u001b[0m  0.0005  0.4702\n",
      "     11            \u001b[36m0.7727\u001b[0m        \u001b[32m0.5015\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.8443\u001b[0m  0.0005  0.4209\n",
      "     12            \u001b[36m0.8364\u001b[0m        \u001b[32m0.4153\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.8022\u001b[0m  0.0005  0.4143\n",
      "     13            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3841\u001b[0m       0.3854            0.3854        \u001b[94m1.7662\u001b[0m  0.0005  0.4453\n",
      "     14            \u001b[36m0.9273\u001b[0m        0.4305       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.7204\u001b[0m  0.0005  0.4472\n",
      "     15            \u001b[36m0.9455\u001b[0m        \u001b[32m0.3654\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.6841\u001b[0m  0.0004  0.4250\n",
      "     16            \u001b[36m0.9636\u001b[0m        \u001b[32m0.3350\u001b[0m       0.3993            0.3993        \u001b[94m1.6498\u001b[0m  0.0004  0.3700\n",
      "     17            \u001b[36m0.9909\u001b[0m        0.3495       0.3958            0.3958        \u001b[94m1.6123\u001b[0m  0.0004  0.3951\n",
      "     18            0.9909        \u001b[32m0.3159\u001b[0m       0.3958            0.3958        \u001b[94m1.5857\u001b[0m  0.0004  0.4005\n",
      "     19            0.9909        \u001b[32m0.3068\u001b[0m       0.3958            0.3958        \u001b[94m1.5774\u001b[0m  0.0004  0.3813\n",
      "     20            0.9909        \u001b[32m0.2945\u001b[0m       0.3924            0.3924        1.5806  0.0003  0.4079\n",
      "     21            0.9909        \u001b[32m0.2525\u001b[0m       0.3993            0.3993        1.5814  0.0003  0.4389\n",
      "     22            0.9909        \u001b[32m0.2520\u001b[0m       0.3993            0.3993        \u001b[94m1.5768\u001b[0m  0.0003  0.4169\n",
      "     23            \u001b[36m1.0000\u001b[0m        0.2968       0.3889            0.3889        \u001b[94m1.5694\u001b[0m  0.0002  0.3987\n",
      "     24            1.0000        \u001b[32m0.2178\u001b[0m       0.3889            0.3889        \u001b[94m1.5615\u001b[0m  0.0002  0.4098\n",
      "     25            1.0000        0.2434       0.3819            0.3819        \u001b[94m1.5547\u001b[0m  0.0002  0.3855\n",
      "     26            1.0000        0.2365       0.3819            0.3819        \u001b[94m1.5457\u001b[0m  0.0002  0.3713\n",
      "     27            1.0000        0.2802       0.3958            0.3958        \u001b[94m1.5349\u001b[0m  0.0002  0.3707\n",
      "     28            1.0000        \u001b[32m0.2042\u001b[0m       0.3924            0.3924        \u001b[94m1.5287\u001b[0m  0.0001  0.3640\n",
      "     29            1.0000        0.2344       0.3924            0.3924        \u001b[94m1.5218\u001b[0m  0.0001  0.3812\n",
      "     30            1.0000        \u001b[32m0.1927\u001b[0m       0.3924            0.3924        \u001b[94m1.5164\u001b[0m  0.0001  0.3730\n",
      "     31            1.0000        \u001b[32m0.1823\u001b[0m       0.3924            0.3924        \u001b[94m1.5132\u001b[0m  0.0001  0.3799\n",
      "     32            1.0000        0.2176       0.3958            0.3958        \u001b[94m1.5108\u001b[0m  0.0001  0.3812\n",
      "     33            1.0000        0.2065       0.3924            0.3924        \u001b[94m1.5091\u001b[0m  0.0000  0.3840\n",
      "     34            1.0000        0.1904       0.3889            0.3889        \u001b[94m1.5078\u001b[0m  0.0000  0.3677\n",
      "     35            1.0000        0.2233       0.3889            0.3889        \u001b[94m1.5069\u001b[0m  0.0000  0.3718\n",
      "     36            1.0000        0.1904       0.3889            0.3889        \u001b[94m1.5060\u001b[0m  0.0000  0.3731\n",
      "     37            1.0000        0.1991       0.3889            0.3889        \u001b[94m1.5052\u001b[0m  0.0000  0.3826\n",
      "     38            1.0000        0.1969       0.3924            0.3924        \u001b[94m1.5045\u001b[0m  0.0000  0.3815\n",
      "     39            1.0000        0.1844       0.3924            0.3924        \u001b[94m1.5038\u001b[0m  0.0000  0.3774\n",
      "     40            1.0000        0.2068       0.3924            0.3924        \u001b[94m1.5034\u001b[0m  0.0000  0.3629\n",
      "Training model for subject 4 with 120 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.6040\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.1324\u001b[0m  0.0006  0.4155\n",
      "      2            0.2500        \u001b[32m1.3658\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m2.9022\u001b[0m  0.0006  0.4369\n",
      "      3            \u001b[36m0.3083\u001b[0m        \u001b[32m1.0921\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m2.4929\u001b[0m  0.0006  0.4565\n",
      "      4            \u001b[36m0.4000\u001b[0m        \u001b[32m0.9609\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m2.3696\u001b[0m  0.0006  0.4986\n",
      "      5            \u001b[36m0.4750\u001b[0m        \u001b[32m0.8604\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        \u001b[94m2.1505\u001b[0m  0.0006  0.4802\n",
      "      6            \u001b[36m0.6333\u001b[0m        \u001b[32m0.7683\u001b[0m       0.3125            0.3125        \u001b[94m2.0084\u001b[0m  0.0006  0.4684\n",
      "      7            \u001b[36m0.6917\u001b[0m        \u001b[32m0.6433\u001b[0m       0.3125            0.3125        2.0240  0.0006  0.4604\n",
      "      8            \u001b[36m0.7167\u001b[0m        \u001b[32m0.6381\u001b[0m       0.3160            0.3160        2.0191  0.0006  0.4150\n",
      "      9            \u001b[36m0.7500\u001b[0m        \u001b[32m0.6063\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.9315\u001b[0m  0.0006  0.3828\n",
      "     10            \u001b[36m0.7750\u001b[0m        \u001b[32m0.5543\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.8530\u001b[0m  0.0005  0.3980\n",
      "     11            0.7750        \u001b[32m0.4746\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.7998\u001b[0m  0.0005  0.3852\n",
      "     12            \u001b[36m0.8250\u001b[0m        0.4819       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.7474\u001b[0m  0.0005  0.3804\n",
      "     13            \u001b[36m0.8417\u001b[0m        \u001b[32m0.4392\u001b[0m       0.3611            0.3611        \u001b[94m1.7172\u001b[0m  0.0005  0.3946\n",
      "     14            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4272\u001b[0m       0.3611            0.3611        \u001b[94m1.6880\u001b[0m  0.0005  0.3875\n",
      "     15            \u001b[36m0.9750\u001b[0m        \u001b[32m0.4033\u001b[0m       0.3576            0.3576        \u001b[94m1.6401\u001b[0m  0.0004  0.3828\n",
      "     16            \u001b[36m0.9833\u001b[0m        \u001b[32m0.3851\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.6167\u001b[0m  0.0004  0.5129\n",
      "     17            0.9833        \u001b[32m0.3503\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.5903\u001b[0m  0.0004  0.3931\n",
      "     18            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3184\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.5697\u001b[0m  0.0004  0.3851\n",
      "     19            1.0000        \u001b[32m0.2903\u001b[0m       0.3819            0.3819        \u001b[94m1.5548\u001b[0m  0.0004  0.3799\n",
      "     20            1.0000        \u001b[32m0.2644\u001b[0m       0.3750            0.3750        \u001b[94m1.5429\u001b[0m  0.0003  0.3925\n",
      "     21            1.0000        \u001b[32m0.2386\u001b[0m       0.3785            0.3785        \u001b[94m1.5342\u001b[0m  0.0003  0.3847\n",
      "     22            1.0000        \u001b[32m0.2296\u001b[0m       0.3819            0.3819        \u001b[94m1.5321\u001b[0m  0.0003  0.3907\n",
      "     23            1.0000        0.2497       0.3819            0.3819        1.5359  0.0002  0.3987\n",
      "     24            1.0000        0.2479       0.3889            0.3889        1.5356  0.0002  0.3777\n",
      "     25            1.0000        0.2384       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        1.5371  0.0002  0.3983\n",
      "     26            1.0000        \u001b[32m0.2169\u001b[0m       0.3924            0.3924        1.5378  0.0002  0.3907\n",
      "     27            1.0000        0.2678       0.3958            0.3958        1.5380  0.0002  0.3827\n",
      "     28            1.0000        \u001b[32m0.1937\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        1.5356  0.0001  0.4015\n",
      "     29            1.0000        0.2036       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        1.5355  0.0001  0.3925\n",
      "     30            1.0000        0.2144       0.4062            0.4062        1.5324  0.0001  0.4002\n",
      "     31            1.0000        0.2316       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.5293\u001b[0m  0.0001  0.3981\n",
      "     32            1.0000        0.2010       0.4132            0.4132        \u001b[94m1.5261\u001b[0m  0.0001  0.3877\n",
      "     33            1.0000        \u001b[32m0.1884\u001b[0m       0.4132            0.4132        \u001b[94m1.5242\u001b[0m  0.0000  0.3877\n",
      "     34            1.0000        0.2037       0.4132            0.4132        \u001b[94m1.5227\u001b[0m  0.0000  0.4166\n",
      "     35            1.0000        0.2031       0.4132            0.4132        \u001b[94m1.5211\u001b[0m  0.0000  0.4584\n",
      "     36            1.0000        0.2041       0.4132            0.4132        \u001b[94m1.5194\u001b[0m  0.0000  0.4727\n",
      "     37            1.0000        \u001b[32m0.1842\u001b[0m       0.4132            0.4132        \u001b[94m1.5181\u001b[0m  0.0000  0.4711\n",
      "     38            1.0000        0.1962       0.4132            0.4132        \u001b[94m1.5169\u001b[0m  0.0000  0.4256\n",
      "     39            1.0000        0.2123       0.4132            0.4132        \u001b[94m1.5158\u001b[0m  0.0000  0.4911\n",
      "     40            1.0000        0.1844       0.4132            0.4132        \u001b[94m1.5150\u001b[0m  0.0000  0.5002\n",
      "Training model for subject 4 with 130 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2615\u001b[0m        \u001b[32m1.6701\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.7465\u001b[0m  0.0006  0.4212\n",
      "      2            0.2615        \u001b[32m1.3561\u001b[0m       0.2500            0.2500        5.9080  0.0006  0.4059\n",
      "      3            0.2615        \u001b[32m1.1684\u001b[0m       0.2500            0.2500        5.3018  0.0006  0.3792\n",
      "      4            0.2615        \u001b[32m1.0839\u001b[0m       0.2500            0.2500        \u001b[94m4.3169\u001b[0m  0.0006  0.3881\n",
      "      5            \u001b[36m0.3231\u001b[0m        \u001b[32m0.9727\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        \u001b[94m3.2995\u001b[0m  0.0006  0.4110\n",
      "      6            \u001b[36m0.4231\u001b[0m        0.9727       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m2.6801\u001b[0m  0.0006  0.3911\n",
      "      7            \u001b[36m0.4692\u001b[0m        \u001b[32m0.8500\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m2.5222\u001b[0m  0.0006  0.3991\n",
      "      8            \u001b[36m0.4769\u001b[0m        \u001b[32m0.7797\u001b[0m       0.3438            0.3438        \u001b[94m2.4344\u001b[0m  0.0006  0.3836\n",
      "      9            \u001b[36m0.4846\u001b[0m        \u001b[32m0.7412\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m2.3491\u001b[0m  0.0006  0.3953\n",
      "     10            \u001b[36m0.5000\u001b[0m        \u001b[32m0.6201\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m2.1724\u001b[0m  0.0005  0.4076\n",
      "     11            \u001b[36m0.5308\u001b[0m        0.6249       0.3715            0.3715        \u001b[94m1.9543\u001b[0m  0.0005  0.4003\n",
      "     12            \u001b[36m0.5462\u001b[0m        \u001b[32m0.5308\u001b[0m       0.3611            0.3611        \u001b[94m1.8143\u001b[0m  0.0005  0.3934\n",
      "     13            \u001b[36m0.7231\u001b[0m        0.5893       0.3715            0.3715        \u001b[94m1.6871\u001b[0m  0.0005  0.4007\n",
      "     14            \u001b[36m0.8000\u001b[0m        0.5864       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.6104\u001b[0m  0.0005  0.3975\n",
      "     15            \u001b[36m0.8846\u001b[0m        \u001b[32m0.5166\u001b[0m       0.3681            0.3681        \u001b[94m1.5636\u001b[0m  0.0004  0.3853\n",
      "     16            \u001b[36m0.9385\u001b[0m        0.5286       0.3715            0.3715        \u001b[94m1.5291\u001b[0m  0.0004  0.3957\n",
      "     17            \u001b[36m0.9462\u001b[0m        \u001b[32m0.4094\u001b[0m       0.3750            0.3750        \u001b[94m1.5011\u001b[0m  0.0004  0.4132\n",
      "     18            \u001b[36m0.9538\u001b[0m        0.4302       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.4759\u001b[0m  0.0004  0.4100\n",
      "     19            \u001b[36m0.9769\u001b[0m        \u001b[32m0.3785\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4553\u001b[0m  0.0004  0.4052\n",
      "     20            0.9769        \u001b[32m0.3662\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.4365\u001b[0m  0.0003  0.4934\n",
      "     21            \u001b[36m0.9846\u001b[0m        \u001b[32m0.3339\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.4265\u001b[0m  0.0003  0.4156\n",
      "     22            0.9846        0.3523       0.4132            0.4132        \u001b[94m1.4106\u001b[0m  0.0003  0.3980\n",
      "     23            \u001b[36m0.9923\u001b[0m        \u001b[32m0.3336\u001b[0m       0.4062            0.4062        \u001b[94m1.4019\u001b[0m  0.0002  0.4173\n",
      "     24            0.9923        \u001b[32m0.3066\u001b[0m       0.4062            0.4062        \u001b[94m1.3951\u001b[0m  0.0002  0.4001\n",
      "     25            0.9923        \u001b[32m0.2930\u001b[0m       0.4062            0.4062        \u001b[94m1.3899\u001b[0m  0.0002  0.4114\n",
      "     26            0.9923        0.3482       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.3869\u001b[0m  0.0002  0.4703\n",
      "     27            0.9846        \u001b[32m0.2755\u001b[0m       0.4167            0.4167        \u001b[94m1.3835\u001b[0m  0.0002  0.5053\n",
      "     28            0.9923        \u001b[32m0.2724\u001b[0m       0.4132            0.4132        \u001b[94m1.3821\u001b[0m  0.0001  0.4658\n",
      "     29            0.9923        0.2803       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.3817\u001b[0m  0.0001  0.4757\n",
      "     30            0.9923        \u001b[32m0.2531\u001b[0m       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.3812\u001b[0m  0.0001  0.4721\n",
      "     31            0.9923        0.2714       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        1.3817  0.0001  0.4948\n",
      "     32            0.9923        \u001b[32m0.2517\u001b[0m       0.4236            0.4236        \u001b[94m1.3807\u001b[0m  0.0001  0.4364\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.2990       0.4271            0.4271        \u001b[94m1.3798\u001b[0m  0.0000  0.4005\n",
      "     34            1.0000        0.2560       0.4271            0.4271        \u001b[94m1.3790\u001b[0m  0.0000  0.4382\n",
      "     35            1.0000        0.2518       0.4236            0.4236        \u001b[94m1.3780\u001b[0m  0.0000  0.4059\n",
      "     36            1.0000        0.2523       0.4271            0.4271        \u001b[94m1.3769\u001b[0m  0.0000  0.4032\n",
      "     37            1.0000        0.3071       0.4236            0.4236        \u001b[94m1.3762\u001b[0m  0.0000  0.4143\n",
      "     38            1.0000        0.2952       0.4236            0.4236        \u001b[94m1.3756\u001b[0m  0.0000  0.4113\n",
      "     39            1.0000        0.2635       0.4236            0.4236        \u001b[94m1.3752\u001b[0m  0.0000  0.4046\n",
      "     40            1.0000        \u001b[32m0.2417\u001b[0m       0.4236            0.4236        \u001b[94m1.3750\u001b[0m  0.0000  0.4367\n",
      "Training model for subject 4 with 140 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2429\u001b[0m        \u001b[32m1.7364\u001b[0m       \u001b[35m0.2431\u001b[0m            \u001b[31m0.2431\u001b[0m        \u001b[94m2.5227\u001b[0m  0.0006  0.4212\n",
      "      2            \u001b[36m0.2643\u001b[0m        \u001b[32m1.3561\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m1.8880\u001b[0m  0.0006  0.3946\n",
      "      3            \u001b[36m0.4571\u001b[0m        \u001b[32m1.2650\u001b[0m       0.2535            0.2535        \u001b[94m1.6233\u001b[0m  0.0006  0.3818\n",
      "      4            0.3071        \u001b[32m1.1511\u001b[0m       0.2569            0.2569        2.0378  0.0006  0.4128\n",
      "      5            0.2857        \u001b[32m0.9320\u001b[0m       0.2569            0.2569        2.3835  0.0006  0.4118\n",
      "      6            0.2857        \u001b[32m0.9097\u001b[0m       0.2569            0.2569        2.5558  0.0006  0.4187\n",
      "      7            0.2929        \u001b[32m0.8488\u001b[0m       0.2569            0.2569        2.5394  0.0006  0.4000\n",
      "      8            0.3071        0.8514       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        2.3495  0.0006  0.3882\n",
      "      9            0.3786        \u001b[32m0.8317\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        2.0722  0.0006  0.3945\n",
      "     10            \u001b[36m0.5571\u001b[0m        \u001b[32m0.7285\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        1.8339  0.0005  0.3844\n",
      "     11            \u001b[36m0.7429\u001b[0m        \u001b[32m0.6229\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        1.6789  0.0005  0.3997\n",
      "     12            \u001b[36m0.7857\u001b[0m        0.6695       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.6070\u001b[0m  0.0005  0.3871\n",
      "     13            \u001b[36m0.8429\u001b[0m        0.6319       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.5665\u001b[0m  0.0005  0.3880\n",
      "     14            \u001b[36m0.8643\u001b[0m        \u001b[32m0.5527\u001b[0m       0.3542            0.3542        \u001b[94m1.5322\u001b[0m  0.0005  0.3985\n",
      "     15            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5063\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.5095\u001b[0m  0.0004  0.3825\n",
      "     16            \u001b[36m0.9143\u001b[0m        0.5499       0.3819            0.3819        \u001b[94m1.4965\u001b[0m  0.0004  0.3965\n",
      "     17            0.9143        \u001b[32m0.5001\u001b[0m       0.3819            0.3819        \u001b[94m1.4869\u001b[0m  0.0004  0.4578\n",
      "     18            \u001b[36m0.9571\u001b[0m        \u001b[32m0.4424\u001b[0m       0.3819            0.3819        \u001b[94m1.4794\u001b[0m  0.0004  0.4560\n",
      "     19            \u001b[36m0.9643\u001b[0m        \u001b[32m0.4188\u001b[0m       0.3715            0.3715        \u001b[94m1.4751\u001b[0m  0.0004  0.4775\n",
      "     20            0.9643        \u001b[32m0.3816\u001b[0m       0.3819            0.3819        1.4763  0.0003  0.4578\n",
      "     21            \u001b[36m0.9786\u001b[0m        0.4291       0.3785            0.3785        1.4779  0.0003  0.5198\n",
      "     22            0.9786        0.4266       0.3715            0.3715        1.4813  0.0003  0.4980\n",
      "     23            0.9786        \u001b[32m0.3608\u001b[0m       0.3750            0.3750        1.4838  0.0002  0.5841\n",
      "     24            0.9786        0.3750       0.3715            0.3715        1.4810  0.0002  0.4014\n",
      "     25            0.9786        \u001b[32m0.3339\u001b[0m       0.3819            0.3819        1.4809  0.0002  0.3960\n",
      "     26            0.9786        \u001b[32m0.3023\u001b[0m       0.3819            0.3819        1.4807  0.0002  0.3961\n",
      "     27            0.9786        0.3539       0.3785            0.3785        1.4816  0.0002  0.4006\n",
      "     28            \u001b[36m0.9857\u001b[0m        0.3852       0.3750            0.3750        1.4819  0.0001  0.3900\n",
      "     29            0.9857        0.3323       0.3785            0.3785        1.4806  0.0001  0.3994\n",
      "     30            \u001b[36m0.9929\u001b[0m        0.3187       0.3785            0.3785        1.4804  0.0001  0.3904\n",
      "     31            0.9929        0.3260       0.3785            0.3785        1.4809  0.0001  0.3863\n",
      "     32            0.9929        0.3152       0.3785            0.3785        1.4805  0.0001  0.3951\n",
      "     33            0.9929        0.3265       0.3785            0.3785        1.4789  0.0000  0.3909\n",
      "     34            0.9929        0.3276       0.3785            0.3785        1.4783  0.0000  0.3968\n",
      "     35            0.9929        0.3424       0.3750            0.3750        1.4775  0.0000  0.3934\n",
      "     36            0.9929        0.3125       0.3750            0.3750        1.4762  0.0000  0.3913\n",
      "     37            0.9929        \u001b[32m0.2850\u001b[0m       0.3750            0.3750        1.4759  0.0000  0.4155\n",
      "     38            0.9929        0.3407       0.3750            0.3750        1.4757  0.0000  0.4009\n",
      "     39            0.9929        0.3521       0.3750            0.3750        \u001b[94m1.4749\u001b[0m  0.0000  0.3989\n",
      "     40            0.9929        0.3338       0.3785            0.3785        \u001b[94m1.4746\u001b[0m  0.0000  0.3996\n",
      "Training model for subject 4 with 150 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2467\u001b[0m        \u001b[32m1.7293\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m2.5901\u001b[0m  0.0006  0.4067\n",
      "      2            0.2467        \u001b[32m1.3104\u001b[0m       0.2500            0.2500        2.8664  0.0006  0.4024\n",
      "      3            0.2467        \u001b[32m1.1003\u001b[0m       0.2500            0.2500        3.5094  0.0006  0.3925\n",
      "      4            0.2467        \u001b[32m1.0686\u001b[0m       0.2500            0.2500        3.6331  0.0006  0.4092\n",
      "      5            0.2467        \u001b[32m0.9729\u001b[0m       0.2500            0.2500        3.6208  0.0006  0.3915\n",
      "      6            \u001b[36m0.2533\u001b[0m        \u001b[32m0.8740\u001b[0m       0.2500            0.2500        3.3365  0.0006  0.3991\n",
      "      7            \u001b[36m0.2667\u001b[0m        \u001b[32m0.8433\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        2.9514  0.0006  0.3779\n",
      "      8            \u001b[36m0.3000\u001b[0m        \u001b[32m0.7733\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        2.6483  0.0006  0.4813\n",
      "      9            \u001b[36m0.3933\u001b[0m        \u001b[32m0.6843\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        \u001b[94m2.3513\u001b[0m  0.0006  0.5206\n",
      "     10            \u001b[36m0.4733\u001b[0m        0.7294       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m2.2037\u001b[0m  0.0005  0.5071\n",
      "     11            \u001b[36m0.5400\u001b[0m        \u001b[32m0.6754\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m2.0716\u001b[0m  0.0005  0.4657\n",
      "     12            \u001b[36m0.5667\u001b[0m        \u001b[32m0.5891\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.9293\u001b[0m  0.0005  0.4905\n",
      "     13            \u001b[36m0.6400\u001b[0m        \u001b[32m0.5441\u001b[0m       0.3611            0.3611        \u001b[94m1.7842\u001b[0m  0.0005  0.4102\n",
      "     14            \u001b[36m0.7200\u001b[0m        \u001b[32m0.5284\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.6880\u001b[0m  0.0005  0.4094\n",
      "     15            \u001b[36m0.8067\u001b[0m        \u001b[32m0.4765\u001b[0m       0.3542            0.3542        \u001b[94m1.6229\u001b[0m  0.0004  0.3848\n",
      "     16            \u001b[36m0.8600\u001b[0m        0.4810       0.3715            0.3715        \u001b[94m1.5790\u001b[0m  0.0004  0.4119\n",
      "     17            \u001b[36m0.9200\u001b[0m        \u001b[32m0.4622\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.5182\u001b[0m  0.0004  0.3977\n",
      "     18            \u001b[36m0.9400\u001b[0m        \u001b[32m0.4198\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.4855\u001b[0m  0.0004  0.4042\n",
      "     19            0.9333        0.4589       0.3785            0.3785        \u001b[94m1.4735\u001b[0m  0.0004  0.4121\n",
      "     20            \u001b[36m0.9533\u001b[0m        0.4233       0.3715            0.3715        \u001b[94m1.4623\u001b[0m  0.0003  0.4051\n",
      "     21            0.9533        \u001b[32m0.3811\u001b[0m       0.3785            0.3785        \u001b[94m1.4522\u001b[0m  0.0003  0.4015\n",
      "     22            \u001b[36m0.9800\u001b[0m        \u001b[32m0.3464\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.4422\u001b[0m  0.0003  0.3997\n",
      "     23            \u001b[36m0.9867\u001b[0m        0.3823       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4309\u001b[0m  0.0002  0.4117\n",
      "     24            \u001b[36m0.9933\u001b[0m        \u001b[32m0.3286\u001b[0m       0.3958            0.3958        \u001b[94m1.4206\u001b[0m  0.0002  0.4054\n",
      "     25            0.9933        0.3537       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.4108\u001b[0m  0.0002  0.4145\n",
      "     26            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3048\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.4044\u001b[0m  0.0002  0.3898\n",
      "     27            1.0000        0.3488       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.3979\u001b[0m  0.0002  0.3884\n",
      "     28            1.0000        0.3547       0.4028            0.4028        \u001b[94m1.3948\u001b[0m  0.0001  0.4000\n",
      "     29            1.0000        \u001b[32m0.2947\u001b[0m       0.3993            0.3993        \u001b[94m1.3921\u001b[0m  0.0001  0.3966\n",
      "     30            1.0000        \u001b[32m0.2886\u001b[0m       0.3993            0.3993        \u001b[94m1.3901\u001b[0m  0.0001  0.3831\n",
      "     31            1.0000        0.3033       0.3993            0.3993        \u001b[94m1.3886\u001b[0m  0.0001  0.4004\n",
      "     32            1.0000        \u001b[32m0.2820\u001b[0m       0.3958            0.3958        \u001b[94m1.3878\u001b[0m  0.0001  0.3972\n",
      "     33            1.0000        0.2831       0.3958            0.3958        1.3879  0.0000  0.4953\n",
      "     34            1.0000        \u001b[32m0.2628\u001b[0m       0.3958            0.3958        \u001b[94m1.3872\u001b[0m  0.0000  0.3923\n",
      "     35            1.0000        0.3387       0.3958            0.3958        \u001b[94m1.3866\u001b[0m  0.0000  0.4121\n",
      "     36            1.0000        0.3277       0.3958            0.3958        \u001b[94m1.3863\u001b[0m  0.0000  0.3999\n",
      "     37            1.0000        0.3296       0.3958            0.3958        1.3865  0.0000  0.4049\n",
      "     38            1.0000        0.3355       0.3993            0.3993        \u001b[94m1.3861\u001b[0m  0.0000  0.4491\n",
      "     39            1.0000        0.3156       0.4028            0.4028        \u001b[94m1.3860\u001b[0m  0.0000  0.4906\n",
      "     40            1.0000        0.2980       0.3993            0.3993        \u001b[94m1.3860\u001b[0m  0.0000  0.4907\n",
      "Training model for subject 4 with 160 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3625\u001b[0m        \u001b[32m1.5964\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        \u001b[94m1.5691\u001b[0m  0.0006  0.4829\n",
      "      2            0.2562        \u001b[32m1.4523\u001b[0m       0.2500            0.2500        2.5465  0.0006  0.4560\n",
      "      3            0.2500        \u001b[32m1.3433\u001b[0m       0.2500            0.2500        3.1982  0.0006  0.4963\n",
      "      4            0.2500        \u001b[32m1.0898\u001b[0m       0.2500            0.2500        3.5016  0.0006  0.4126\n",
      "      5            0.2500        \u001b[32m0.9581\u001b[0m       0.2535            0.2535        3.3375  0.0006  0.4102\n",
      "      6            0.2562        \u001b[32m0.9096\u001b[0m       0.2535            0.2535        3.1273  0.0006  0.3845\n",
      "      7            0.3000        \u001b[32m0.8630\u001b[0m       0.2639            0.2639        2.7416  0.0006  0.4058\n",
      "      8            \u001b[36m0.3812\u001b[0m        \u001b[32m0.8584\u001b[0m       0.2882            0.2882        2.5248  0.0006  0.3987\n",
      "      9            \u001b[36m0.4250\u001b[0m        \u001b[32m0.8316\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        2.3846  0.0006  0.3925\n",
      "     10            \u001b[36m0.5125\u001b[0m        \u001b[32m0.6844\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        2.2175  0.0005  0.3912\n",
      "     11            \u001b[36m0.5687\u001b[0m        0.7864       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        2.0099  0.0005  0.3947\n",
      "     12            \u001b[36m0.6750\u001b[0m        \u001b[32m0.6614\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        1.8040  0.0005  0.4022\n",
      "     13            \u001b[36m0.7500\u001b[0m        \u001b[32m0.6393\u001b[0m       0.3542            0.3542        1.7273  0.0005  0.3984\n",
      "     14            \u001b[36m0.7812\u001b[0m        0.6821       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        1.6809  0.0005  0.3974\n",
      "     15            \u001b[36m0.7937\u001b[0m        \u001b[32m0.5809\u001b[0m       0.3507            0.3507        1.6615  0.0004  0.3894\n",
      "     16            \u001b[36m0.8125\u001b[0m        \u001b[32m0.4756\u001b[0m       0.3472            0.3472        1.6508  0.0004  0.3904\n",
      "     17            \u001b[36m0.8187\u001b[0m        0.5982       0.3507            0.3507        1.6419  0.0004  0.4165\n",
      "     18            \u001b[36m0.8500\u001b[0m        0.6199       0.3542            0.3542        1.6405  0.0004  0.3871\n",
      "     19            \u001b[36m0.8750\u001b[0m        0.5346       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        1.6123  0.0004  0.3968\n",
      "     20            \u001b[36m0.9000\u001b[0m        0.4996       0.3576            0.3576        1.5790  0.0003  0.4070\n",
      "     21            \u001b[36m0.9313\u001b[0m        0.4795       0.3542            0.3542        \u001b[94m1.5448\u001b[0m  0.0003  0.4121\n",
      "     22            \u001b[36m0.9375\u001b[0m        \u001b[32m0.4594\u001b[0m       0.3472            0.3472        \u001b[94m1.5296\u001b[0m  0.0003  0.3845\n",
      "     23            0.9375        0.4758       0.3438            0.3438        \u001b[94m1.5180\u001b[0m  0.0002  0.4095\n",
      "     24            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4126\u001b[0m       0.3507            0.3507        \u001b[94m1.5050\u001b[0m  0.0002  0.4082\n",
      "     25            \u001b[36m0.9563\u001b[0m        0.4682       0.3542            0.3542        \u001b[94m1.4973\u001b[0m  0.0002  0.3936\n",
      "     26            \u001b[36m0.9625\u001b[0m        \u001b[32m0.4075\u001b[0m       0.3611            0.3611        \u001b[94m1.4891\u001b[0m  0.0002  0.3963\n",
      "     27            \u001b[36m0.9688\u001b[0m        0.4117       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.4796\u001b[0m  0.0002  0.4073\n",
      "     28            0.9688        \u001b[32m0.3872\u001b[0m       0.3646            0.3646        \u001b[94m1.4696\u001b[0m  0.0001  0.3906\n",
      "     29            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3862\u001b[0m       0.3681            0.3681        \u001b[94m1.4595\u001b[0m  0.0001  0.5006\n",
      "     30            0.9750        0.4361       0.3681            0.3681        \u001b[94m1.4507\u001b[0m  0.0001  0.4923\n",
      "     31            0.9750        \u001b[32m0.3839\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.4437\u001b[0m  0.0001  0.4851\n",
      "     32            0.9750        \u001b[32m0.3497\u001b[0m       0.3681            0.3681        \u001b[94m1.4374\u001b[0m  0.0001  0.4957\n",
      "     33            \u001b[36m0.9812\u001b[0m        0.4006       0.3750            0.3750        \u001b[94m1.4325\u001b[0m  0.0000  0.4980\n",
      "     34            0.9812        0.3557       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.4284\u001b[0m  0.0000  0.4246\n",
      "     35            0.9812        0.3963       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.4265\u001b[0m  0.0000  0.3973\n",
      "     36            0.9812        0.3850       0.3819            0.3819        \u001b[94m1.4241\u001b[0m  0.0000  0.4083\n",
      "     37            0.9812        0.3558       0.3819            0.3819        \u001b[94m1.4222\u001b[0m  0.0000  0.4077\n",
      "     38            0.9812        0.3852       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.4208\u001b[0m  0.0000  0.4101\n",
      "     39            0.9812        0.3501       0.3854            0.3854        \u001b[94m1.4197\u001b[0m  0.0000  0.4011\n",
      "     40            0.9812        0.3512       0.3854            0.3854        \u001b[94m1.4192\u001b[0m  0.0000  0.4171\n",
      "Training model for subject 4 with 170 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2471\u001b[0m        \u001b[32m1.6188\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.8742\u001b[0m  0.0006  0.4880\n",
      "      2            \u001b[36m0.2941\u001b[0m        \u001b[32m1.4064\u001b[0m       0.2500            0.2500        \u001b[94m2.7814\u001b[0m  0.0006  0.3989\n",
      "      3            0.2471        \u001b[32m1.2735\u001b[0m       0.2500            0.2500        3.3903  0.0006  0.3904\n",
      "      4            0.2471        \u001b[32m1.0654\u001b[0m       0.2500            0.2500        3.4624  0.0006  0.4057\n",
      "      5            0.2529        \u001b[32m1.0381\u001b[0m       0.2500            0.2500        3.3861  0.0006  0.3903\n",
      "      6            0.2588        1.0476       0.2500            0.2500        3.0258  0.0006  0.3983\n",
      "      7            0.2941        \u001b[32m0.9484\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m2.6817\u001b[0m  0.0006  0.3860\n",
      "      8            \u001b[36m0.3059\u001b[0m        \u001b[32m0.8737\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m2.5687\u001b[0m  0.0006  0.3881\n",
      "      9            \u001b[36m0.3353\u001b[0m        \u001b[32m0.8261\u001b[0m       0.2708            0.2708        \u001b[94m2.3602\u001b[0m  0.0006  0.3843\n",
      "     10            \u001b[36m0.3588\u001b[0m        \u001b[32m0.7975\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m2.1908\u001b[0m  0.0005  0.3960\n",
      "     11            \u001b[36m0.4235\u001b[0m        \u001b[32m0.7449\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m2.0748\u001b[0m  0.0005  0.4134\n",
      "     12            \u001b[36m0.5353\u001b[0m        \u001b[32m0.6948\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m1.9170\u001b[0m  0.0005  0.3862\n",
      "     13            \u001b[36m0.6353\u001b[0m        0.7469       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.7735\u001b[0m  0.0005  0.3859\n",
      "     14            \u001b[36m0.7118\u001b[0m        \u001b[32m0.6842\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.6660\u001b[0m  0.0005  0.4172\n",
      "     15            \u001b[36m0.7941\u001b[0m        \u001b[32m0.5791\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.5663\u001b[0m  0.0004  0.4086\n",
      "     16            \u001b[36m0.8353\u001b[0m        0.6593       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.4937\u001b[0m  0.0004  0.3904\n",
      "     17            0.8353        \u001b[32m0.5057\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.4622\u001b[0m  0.0004  0.4067\n",
      "     18            0.8353        0.5724       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4555\u001b[0m  0.0004  0.4152\n",
      "     19            \u001b[36m0.8471\u001b[0m        0.5659       0.3924            0.3924        \u001b[94m1.4437\u001b[0m  0.0004  0.4691\n",
      "     20            \u001b[36m0.8941\u001b[0m        \u001b[32m0.4778\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.4391\u001b[0m  0.0003  0.4558\n",
      "     21            0.8882        0.4815       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.4350\u001b[0m  0.0003  0.4765\n",
      "     22            \u001b[36m0.9059\u001b[0m        \u001b[32m0.4696\u001b[0m       0.3854            0.3854        \u001b[94m1.4323\u001b[0m  0.0003  0.4634\n",
      "     23            \u001b[36m0.9118\u001b[0m        0.4862       0.3854            0.3854        \u001b[94m1.4231\u001b[0m  0.0002  0.5196\n",
      "     24            \u001b[36m0.9176\u001b[0m        \u001b[32m0.4625\u001b[0m       0.3854            0.3854        \u001b[94m1.4138\u001b[0m  0.0002  0.5082\n",
      "     25            \u001b[36m0.9353\u001b[0m        \u001b[32m0.4583\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.3989\u001b[0m  0.0002  0.4035\n",
      "     26            \u001b[36m0.9412\u001b[0m        0.5179       0.3993            0.3993        \u001b[94m1.3881\u001b[0m  0.0002  0.4015\n",
      "     27            \u001b[36m0.9588\u001b[0m        \u001b[32m0.4488\u001b[0m       0.4062            0.4062        \u001b[94m1.3806\u001b[0m  0.0002  0.3948\n",
      "     28            \u001b[36m0.9647\u001b[0m        0.4919       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.3778\u001b[0m  0.0001  0.4151\n",
      "     29            0.9588        0.4542       0.4097            0.4097        \u001b[94m1.3753\u001b[0m  0.0001  0.4094\n",
      "     30            0.9588        \u001b[32m0.4180\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.3720\u001b[0m  0.0001  0.3938\n",
      "     31            0.9588        0.4577       0.4097            0.4097        \u001b[94m1.3685\u001b[0m  0.0001  0.4015\n",
      "     32            0.9588        0.4199       0.4062            0.4062        \u001b[94m1.3646\u001b[0m  0.0001  0.4178\n",
      "     33            0.9588        0.4331       0.4062            0.4062        \u001b[94m1.3613\u001b[0m  0.0000  0.4041\n",
      "     34            0.9529        \u001b[32m0.3832\u001b[0m       0.4028            0.4028        \u001b[94m1.3587\u001b[0m  0.0000  0.3998\n",
      "     35            0.9529        0.4325       0.4097            0.4097        \u001b[94m1.3573\u001b[0m  0.0000  0.4089\n",
      "     36            0.9588        0.4048       0.4097            0.4097        \u001b[94m1.3563\u001b[0m  0.0000  0.4079\n",
      "     37            0.9588        0.4171       0.4132            0.4132        \u001b[94m1.3556\u001b[0m  0.0000  0.4011\n",
      "     38            0.9588        0.3907       0.4132            0.4132        \u001b[94m1.3551\u001b[0m  0.0000  0.4249\n",
      "     39            0.9588        0.4102       0.4132            0.4132        1.3553  0.0000  0.3893\n",
      "     40            0.9588        0.4369       0.4132            0.4132        1.3555  0.0000  0.4059\n",
      "Training model for subject 4 with 180 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2611\u001b[0m        \u001b[32m1.7926\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m5.3442\u001b[0m  0.0006  0.3975\n",
      "      2            \u001b[36m0.3944\u001b[0m        \u001b[32m1.4510\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m2.9759\u001b[0m  0.0006  0.3781\n",
      "      3            0.3444        \u001b[32m1.2340\u001b[0m       0.2743            0.2743        \u001b[94m2.1095\u001b[0m  0.0006  0.3904\n",
      "      4            0.3167        \u001b[32m1.0738\u001b[0m       0.2535            0.2535        \u001b[94m2.0027\u001b[0m  0.0006  0.3937\n",
      "      5            0.2778        \u001b[32m1.0011\u001b[0m       0.2604            0.2604        2.3745  0.0006  0.5325\n",
      "      6            0.2778        \u001b[32m0.9674\u001b[0m       0.2569            0.2569        2.5609  0.0006  0.3978\n",
      "      7            0.2722        \u001b[32m0.9121\u001b[0m       0.2569            0.2569        2.5965  0.0006  0.4046\n",
      "      8            0.2833        \u001b[32m0.8451\u001b[0m       0.2569            0.2569        2.5699  0.0006  0.4340\n",
      "      9            0.2778        \u001b[32m0.7953\u001b[0m       0.2569            0.2569        2.6491  0.0006  0.5052\n",
      "     10            0.2833        \u001b[32m0.7878\u001b[0m       0.2569            0.2569        2.7186  0.0005  0.5062\n",
      "     11            0.2944        \u001b[32m0.7276\u001b[0m       0.2604            0.2604        2.7000  0.0005  0.4451\n",
      "     12            0.3000        \u001b[32m0.6937\u001b[0m       0.2604            0.2604        2.6064  0.0005  0.4641\n",
      "     13            0.3167        \u001b[32m0.6430\u001b[0m       0.2674            0.2674        2.4129  0.0005  0.5106\n",
      "     14            0.3833        \u001b[32m0.6240\u001b[0m       0.2743            0.2743        2.1621  0.0005  0.4701\n",
      "     15            \u001b[36m0.4889\u001b[0m        \u001b[32m0.5997\u001b[0m       0.3125            0.3125        \u001b[94m1.9129\u001b[0m  0.0004  0.3943\n",
      "     16            \u001b[36m0.6056\u001b[0m        \u001b[32m0.5638\u001b[0m       0.3368            0.3368        \u001b[94m1.7447\u001b[0m  0.0004  0.4066\n",
      "     17            \u001b[36m0.7000\u001b[0m        0.6587       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.5874\u001b[0m  0.0004  0.3908\n",
      "     18            \u001b[36m0.8000\u001b[0m        \u001b[32m0.5039\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.4686\u001b[0m  0.0004  0.3979\n",
      "     19            \u001b[36m0.8778\u001b[0m        0.5184       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.3845\u001b[0m  0.0004  0.3764\n",
      "     20            \u001b[36m0.9278\u001b[0m        \u001b[32m0.4784\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.3259\u001b[0m  0.0003  0.3949\n",
      "     21            \u001b[36m0.9444\u001b[0m        0.4811       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.2808\u001b[0m  0.0003  0.3935\n",
      "     22            \u001b[36m0.9611\u001b[0m        0.5701       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.2517\u001b[0m  0.0003  0.4007\n",
      "     23            0.9611        \u001b[32m0.4556\u001b[0m       0.4410            0.4410        \u001b[94m1.2344\u001b[0m  0.0002  0.4081\n",
      "     24            0.9611        0.4907       0.4444            0.4444        \u001b[94m1.2249\u001b[0m  0.0002  0.3928\n",
      "     25            0.9611        \u001b[32m0.4326\u001b[0m       0.4479            0.4479        \u001b[94m1.2207\u001b[0m  0.0002  0.3953\n",
      "     26            0.9611        \u001b[32m0.4210\u001b[0m       0.4514            0.4514        \u001b[94m1.2200\u001b[0m  0.0002  0.3850\n",
      "     27            0.9611        \u001b[32m0.4168\u001b[0m       0.4514            0.4514        \u001b[94m1.2190\u001b[0m  0.0002  0.4067\n",
      "     28            0.9611        0.4265       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.2155\u001b[0m  0.0001  0.3972\n",
      "     29            0.9611        \u001b[32m0.3521\u001b[0m       0.4583            0.4583        \u001b[94m1.2130\u001b[0m  0.0001  0.3992\n",
      "     30            0.9611        0.3672       0.4583            0.4583        \u001b[94m1.2101\u001b[0m  0.0001  0.4087\n",
      "     31            0.9611        0.3593       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.2073\u001b[0m  0.0001  0.4079\n",
      "     32            0.9611        0.4149       0.4653            0.4653        \u001b[94m1.2053\u001b[0m  0.0001  0.4063\n",
      "     33            0.9611        0.3971       0.4653            0.4653        \u001b[94m1.2034\u001b[0m  0.0000  0.4036\n",
      "     34            0.9611        0.3995       0.4653            0.4653        \u001b[94m1.2017\u001b[0m  0.0000  0.4000\n",
      "     35            0.9611        0.3856       0.4653            0.4653        \u001b[94m1.2011\u001b[0m  0.0000  0.4136\n",
      "     36            0.9611        0.3778       0.4653            0.4653        \u001b[94m1.2002\u001b[0m  0.0000  0.4016\n",
      "     37            0.9611        0.4088       0.4653            0.4653        \u001b[94m1.1998\u001b[0m  0.0000  0.3961\n",
      "     38            0.9611        \u001b[32m0.3402\u001b[0m       0.4653            0.4653        \u001b[94m1.1997\u001b[0m  0.0000  0.4753\n",
      "     39            0.9611        0.3582       0.4653            0.4653        1.1998  0.0000  0.4748\n",
      "     40            0.9611        0.3858       0.4653            0.4653        \u001b[94m1.1995\u001b[0m  0.0000  0.4354\n",
      "Training model for subject 4 with 190 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2526\u001b[0m        \u001b[32m1.9002\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.3984\u001b[0m  0.0006  0.4928\n",
      "      2            0.2526        \u001b[32m1.4108\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m4.0652\u001b[0m  0.0006  0.4886\n",
      "      3            \u001b[36m0.3684\u001b[0m        \u001b[32m1.3178\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m3.3721\u001b[0m  0.0006  0.5042\n",
      "      4            0.2474        \u001b[32m1.2783\u001b[0m       0.2500            0.2500        3.4536  0.0006  0.4144\n",
      "      5            0.2947        \u001b[32m1.0221\u001b[0m       0.2569            0.2569        \u001b[94m3.0419\u001b[0m  0.0006  0.4128\n",
      "      6            \u001b[36m0.3789\u001b[0m        1.0308       0.2674            0.2674        \u001b[94m2.6799\u001b[0m  0.0006  0.4014\n",
      "      7            \u001b[36m0.4474\u001b[0m        \u001b[32m0.9679\u001b[0m       0.2778            0.2778        \u001b[94m2.3563\u001b[0m  0.0006  0.4036\n",
      "      8            \u001b[36m0.4842\u001b[0m        \u001b[32m0.8553\u001b[0m       0.2917            0.2917        \u001b[94m2.0509\u001b[0m  0.0006  0.3912\n",
      "      9            \u001b[36m0.5105\u001b[0m        0.9285       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m1.8502\u001b[0m  0.0006  0.4102\n",
      "     10            \u001b[36m0.5474\u001b[0m        \u001b[32m0.7718\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        \u001b[94m1.7822\u001b[0m  0.0005  0.3971\n",
      "     11            \u001b[36m0.5789\u001b[0m        0.7883       0.2986            0.2986        \u001b[94m1.7180\u001b[0m  0.0005  0.4019\n",
      "     12            \u001b[36m0.6211\u001b[0m        \u001b[32m0.6989\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m1.6541\u001b[0m  0.0005  0.4413\n",
      "     13            \u001b[36m0.6526\u001b[0m        0.7008       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.5694\u001b[0m  0.0005  0.4076\n",
      "     14            \u001b[36m0.7105\u001b[0m        \u001b[32m0.6722\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.4872\u001b[0m  0.0005  0.4072\n",
      "     15            \u001b[36m0.7316\u001b[0m        0.7382       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.4171\u001b[0m  0.0004  0.4026\n",
      "     16            \u001b[36m0.7789\u001b[0m        \u001b[32m0.5713\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.3568\u001b[0m  0.0004  0.3839\n",
      "     17            \u001b[36m0.8263\u001b[0m        0.7268       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.3168\u001b[0m  0.0004  0.3920\n",
      "     18            \u001b[36m0.8474\u001b[0m        0.6150       0.4132            0.4132        \u001b[94m1.2939\u001b[0m  0.0004  0.4033\n",
      "     19            \u001b[36m0.8684\u001b[0m        0.6708       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.2725\u001b[0m  0.0004  0.3958\n",
      "     20            \u001b[36m0.9053\u001b[0m        \u001b[32m0.5240\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.2564\u001b[0m  0.0003  0.4041\n",
      "     21            \u001b[36m0.9158\u001b[0m        0.5935       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.2449\u001b[0m  0.0003  0.3969\n",
      "     22            \u001b[36m0.9368\u001b[0m        0.5950       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.2325\u001b[0m  0.0003  0.5490\n",
      "     23            \u001b[36m0.9526\u001b[0m        \u001b[32m0.5046\u001b[0m       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        \u001b[94m1.2228\u001b[0m  0.0002  0.4093\n",
      "     24            \u001b[36m0.9579\u001b[0m        0.5055       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        \u001b[94m1.2155\u001b[0m  0.0002  0.4126\n",
      "     25            \u001b[36m0.9684\u001b[0m        \u001b[32m0.4542\u001b[0m       \u001b[35m0.4931\u001b[0m            \u001b[31m0.4931\u001b[0m        \u001b[94m1.2106\u001b[0m  0.0002  0.4341\n",
      "     26            0.9684        0.5741       \u001b[35m0.5000\u001b[0m            \u001b[31m0.5000\u001b[0m        \u001b[94m1.2077\u001b[0m  0.0002  0.4201\n",
      "     27            0.9684        0.4596       0.5000            0.5000        \u001b[94m1.2039\u001b[0m  0.0002  0.4700\n",
      "     28            0.9684        0.5155       0.5000            0.5000        \u001b[94m1.2025\u001b[0m  0.0001  0.4710\n",
      "     29            0.9684        \u001b[32m0.4449\u001b[0m       \u001b[35m0.5035\u001b[0m            \u001b[31m0.5035\u001b[0m        \u001b[94m1.2009\u001b[0m  0.0001  0.4572\n",
      "     30            0.9684        0.4652       0.5035            0.5035        \u001b[94m1.1988\u001b[0m  0.0001  0.5034\n",
      "     31            0.9526        \u001b[32m0.4205\u001b[0m       \u001b[35m0.5069\u001b[0m            \u001b[31m0.5069\u001b[0m        \u001b[94m1.1988\u001b[0m  0.0001  0.4832\n",
      "     32            0.9526        0.4808       0.5069            0.5069        1.1989  0.0001  0.5106\n",
      "     33            0.9474        0.4335       \u001b[35m0.5104\u001b[0m            \u001b[31m0.5104\u001b[0m        1.1990  0.0000  0.4014\n",
      "     34            0.9526        0.4922       \u001b[35m0.5139\u001b[0m            \u001b[31m0.5139\u001b[0m        1.1990  0.0000  0.4106\n",
      "     35            0.9474        0.4924       0.5139            0.5139        1.1988  0.0000  0.4155\n",
      "     36            0.9474        0.4679       0.5139            0.5139        1.1989  0.0000  0.4134\n",
      "     37            0.9474        0.4435       0.5139            0.5139        \u001b[94m1.1987\u001b[0m  0.0000  0.4030\n",
      "     38            0.9474        0.4961       0.5139            0.5139        1.1988  0.0000  0.4243\n",
      "     39            0.9421        0.4430       0.5139            0.5139        1.1990  0.0000  0.4241\n",
      "     40            0.9474        0.4402       0.5139            0.5139        1.1990  0.0000  0.4186\n",
      "Training model for subject 4 with 200 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2850\u001b[0m        \u001b[32m1.6788\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m2.1137\u001b[0m  0.0006  0.5009\n",
      "      2            \u001b[36m0.3550\u001b[0m        \u001b[32m1.3011\u001b[0m       0.2569            0.2569        2.7902  0.0006  0.5066\n",
      "      3            0.3400        \u001b[32m1.1738\u001b[0m       0.2708            0.2708        2.8903  0.0006  0.5160\n",
      "      4            0.2900        \u001b[32m1.1390\u001b[0m       0.2639            0.2639        2.6120  0.0006  0.5105\n",
      "      5            \u001b[36m0.3750\u001b[0m        \u001b[32m0.9304\u001b[0m       0.2674            0.2674        2.2284  0.0006  0.5327\n",
      "      6            \u001b[36m0.5100\u001b[0m        \u001b[32m0.9116\u001b[0m       0.2604            0.2604        \u001b[94m1.9367\u001b[0m  0.0006  0.5136\n",
      "      7            \u001b[36m0.5350\u001b[0m        \u001b[32m0.8317\u001b[0m       0.2847            0.2847        \u001b[94m1.7636\u001b[0m  0.0006  0.4845\n",
      "      8            \u001b[36m0.6200\u001b[0m        0.8398       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m1.6179\u001b[0m  0.0006  0.5048\n",
      "      9            \u001b[36m0.7300\u001b[0m        \u001b[32m0.7438\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.4787\u001b[0m  0.0006  0.4968\n",
      "     10            \u001b[36m0.7850\u001b[0m        \u001b[32m0.7264\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.4180\u001b[0m  0.0005  0.5040\n",
      "     11            \u001b[36m0.8150\u001b[0m        \u001b[32m0.7021\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.3743\u001b[0m  0.0005  0.5082\n",
      "     12            \u001b[36m0.8300\u001b[0m        \u001b[32m0.6115\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.3485\u001b[0m  0.0005  0.5166\n",
      "     13            \u001b[36m0.8800\u001b[0m        \u001b[32m0.5754\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.3097\u001b[0m  0.0005  0.5343\n",
      "     14            \u001b[36m0.9050\u001b[0m        0.5920       0.4410            0.4410        \u001b[94m1.2949\u001b[0m  0.0005  0.5791\n",
      "     15            \u001b[36m0.9350\u001b[0m        \u001b[32m0.5090\u001b[0m       0.4340            0.4340        \u001b[94m1.2770\u001b[0m  0.0004  0.5667\n",
      "     16            0.9350        \u001b[32m0.4897\u001b[0m       0.4271            0.4271        \u001b[94m1.2704\u001b[0m  0.0004  0.6670\n",
      "     17            \u001b[36m0.9600\u001b[0m        \u001b[32m0.4763\u001b[0m       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.2563\u001b[0m  0.0004  0.6203\n",
      "     18            \u001b[36m0.9700\u001b[0m        \u001b[32m0.4700\u001b[0m       0.4514            0.4514        \u001b[94m1.2448\u001b[0m  0.0004  0.5297\n",
      "     19            \u001b[36m0.9800\u001b[0m        \u001b[32m0.4205\u001b[0m       0.4514            0.4514        \u001b[94m1.2382\u001b[0m  0.0004  0.5166\n",
      "     20            0.9750        0.4344       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2311\u001b[0m  0.0003  0.5056\n",
      "     21            0.9750        \u001b[32m0.4150\u001b[0m       0.4618            0.4618        \u001b[94m1.2257\u001b[0m  0.0003  0.5061\n",
      "     22            0.9800        0.4197       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        \u001b[94m1.2253\u001b[0m  0.0003  0.5018\n",
      "     23            \u001b[36m0.9850\u001b[0m        0.4270       0.4757            0.4757        \u001b[94m1.2205\u001b[0m  0.0002  0.5009\n",
      "     24            \u001b[36m0.9900\u001b[0m        \u001b[32m0.3986\u001b[0m       0.4653            0.4653        \u001b[94m1.2157\u001b[0m  0.0002  0.6313\n",
      "     25            0.9900        \u001b[32m0.3946\u001b[0m       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        \u001b[94m1.2141\u001b[0m  0.0002  0.5079\n",
      "     26            0.9900        \u001b[32m0.3289\u001b[0m       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        1.2148  0.0002  0.4953\n",
      "     27            0.9900        0.3610       0.4861            0.4861        1.2171  0.0002  0.5167\n",
      "     28            \u001b[36m0.9950\u001b[0m        0.3380       0.4861            0.4861        1.2142  0.0001  0.5079\n",
      "     29            0.9950        \u001b[32m0.3183\u001b[0m       0.4861            0.4861        \u001b[94m1.2126\u001b[0m  0.0001  0.5016\n",
      "     30            0.9950        0.3689       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        \u001b[94m1.2121\u001b[0m  0.0001  0.5154\n",
      "     31            0.9950        \u001b[32m0.3040\u001b[0m       \u001b[35m0.4931\u001b[0m            \u001b[31m0.4931\u001b[0m        \u001b[94m1.2111\u001b[0m  0.0001  0.4962\n",
      "     32            0.9950        0.3616       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        \u001b[94m1.2101\u001b[0m  0.0001  0.5110\n",
      "     33            0.9950        0.3111       0.4965            0.4965        \u001b[94m1.2095\u001b[0m  0.0000  0.5076\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2999\u001b[0m       0.4965            0.4965        1.2096  0.0000  0.5031\n",
      "     35            1.0000        0.3009       0.4896            0.4896        \u001b[94m1.2092\u001b[0m  0.0000  0.5061\n",
      "     36            1.0000        0.3177       0.4931            0.4931        \u001b[94m1.2088\u001b[0m  0.0000  0.5110\n",
      "     37            1.0000        \u001b[32m0.2854\u001b[0m       0.4931            0.4931        \u001b[94m1.2084\u001b[0m  0.0000  0.5833\n",
      "     38            1.0000        0.3228       0.4965            0.4965        \u001b[94m1.2083\u001b[0m  0.0000  0.5915\n",
      "     39            1.0000        0.3210       0.4965            0.4965        1.2083  0.0000  0.5842\n",
      "     40            1.0000        0.3451       0.4965            0.4965        \u001b[94m1.2083\u001b[0m  0.0000  0.5908\n",
      "Training model for subject 4 with 210 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2476\u001b[0m        \u001b[32m1.6064\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.2275\u001b[0m  0.0006  0.6611\n",
      "      2            \u001b[36m0.2667\u001b[0m        \u001b[32m1.3221\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m2.9655\u001b[0m  0.0006  0.5118\n",
      "      3            \u001b[36m0.3095\u001b[0m        \u001b[32m1.2078\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m2.3080\u001b[0m  0.0006  0.4985\n",
      "      4            \u001b[36m0.3571\u001b[0m        \u001b[32m1.0654\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m1.9835\u001b[0m  0.0006  0.4954\n",
      "      5            \u001b[36m0.4143\u001b[0m        \u001b[32m1.0067\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m1.8522\u001b[0m  0.0006  0.4977\n",
      "      6            \u001b[36m0.5095\u001b[0m        \u001b[32m0.8599\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        \u001b[94m1.7091\u001b[0m  0.0006  0.5113\n",
      "      7            \u001b[36m0.5952\u001b[0m        \u001b[32m0.8563\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m1.5949\u001b[0m  0.0006  0.4968\n",
      "      8            \u001b[36m0.6000\u001b[0m        \u001b[32m0.8294\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.5464\u001b[0m  0.0006  0.5047\n",
      "      9            0.5952        \u001b[32m0.7362\u001b[0m       0.3403            0.3403        1.5787  0.0006  0.5017\n",
      "     10            \u001b[36m0.6714\u001b[0m        \u001b[32m0.6963\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.5113\u001b[0m  0.0005  0.5302\n",
      "     11            \u001b[36m0.7714\u001b[0m        0.7658       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.4183\u001b[0m  0.0005  0.5002\n",
      "     12            \u001b[36m0.8143\u001b[0m        0.7333       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.3606\u001b[0m  0.0005  0.5098\n",
      "     13            \u001b[36m0.8857\u001b[0m        \u001b[32m0.6832\u001b[0m       0.4236            0.4236        \u001b[94m1.3282\u001b[0m  0.0005  0.5169\n",
      "     14            \u001b[36m0.8952\u001b[0m        \u001b[32m0.5993\u001b[0m       0.4167            0.4167        \u001b[94m1.3183\u001b[0m  0.0005  0.5025\n",
      "     15            \u001b[36m0.9048\u001b[0m        \u001b[32m0.5891\u001b[0m       0.4271            0.4271        1.3297  0.0004  0.4950\n",
      "     16            0.9048        \u001b[32m0.5192\u001b[0m       0.4271            0.4271        1.3364  0.0004  0.4969\n",
      "     17            \u001b[36m0.9143\u001b[0m        0.5284       0.4167            0.4167        1.3333  0.0004  0.5039\n",
      "     18            \u001b[36m0.9476\u001b[0m        \u001b[32m0.4681\u001b[0m       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.3119\u001b[0m  0.0004  0.4957\n",
      "     19            \u001b[36m0.9524\u001b[0m        \u001b[32m0.4679\u001b[0m       0.4410            0.4410        \u001b[94m1.3009\u001b[0m  0.0004  0.5119\n",
      "     20            \u001b[36m0.9571\u001b[0m        \u001b[32m0.4351\u001b[0m       0.4375            0.4375        \u001b[94m1.2892\u001b[0m  0.0003  0.5165\n",
      "     21            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4327\u001b[0m       0.4479            0.4479        \u001b[94m1.2837\u001b[0m  0.0003  0.5610\n",
      "     22            \u001b[36m0.9762\u001b[0m        0.4470       0.4444            0.4444        \u001b[94m1.2778\u001b[0m  0.0003  0.5704\n",
      "     23            0.9762        \u001b[32m0.4060\u001b[0m       0.4479            0.4479        \u001b[94m1.2752\u001b[0m  0.0002  0.5799\n",
      "     24            \u001b[36m0.9857\u001b[0m        \u001b[32m0.4043\u001b[0m       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2656\u001b[0m  0.0002  0.5979\n",
      "     25            \u001b[36m0.9905\u001b[0m        \u001b[32m0.3761\u001b[0m       0.4549            0.4549        \u001b[94m1.2624\u001b[0m  0.0002  0.7473\n",
      "     26            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3337\u001b[0m       0.4479            0.4479        \u001b[94m1.2619\u001b[0m  0.0002  0.5193\n",
      "     27            1.0000        \u001b[32m0.3235\u001b[0m       0.4479            0.4479        \u001b[94m1.2584\u001b[0m  0.0002  0.5063\n",
      "     28            1.0000        0.3243       0.4479            0.4479        \u001b[94m1.2547\u001b[0m  0.0001  0.5216\n",
      "     29            1.0000        0.3374       0.4618            0.4618        \u001b[94m1.2475\u001b[0m  0.0001  0.5131\n",
      "     30            1.0000        0.3491       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        \u001b[94m1.2435\u001b[0m  0.0001  0.4962\n",
      "     31            1.0000        0.3513       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        \u001b[94m1.2393\u001b[0m  0.0001  0.5145\n",
      "     32            1.0000        0.3303       0.4826            0.4826        \u001b[94m1.2368\u001b[0m  0.0001  0.5054\n",
      "     33            1.0000        \u001b[32m0.3205\u001b[0m       0.4826            0.4826        \u001b[94m1.2357\u001b[0m  0.0000  0.5008\n",
      "     34            1.0000        0.3210       0.4792            0.4792        \u001b[94m1.2339\u001b[0m  0.0000  0.4987\n",
      "     35            1.0000        0.3213       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        \u001b[94m1.2324\u001b[0m  0.0000  0.5075\n",
      "     36            1.0000        \u001b[32m0.3018\u001b[0m       0.4861            0.4861        \u001b[94m1.2323\u001b[0m  0.0000  0.5140\n",
      "     37            1.0000        0.3229       0.4861            0.4861        \u001b[94m1.2322\u001b[0m  0.0000  0.4939\n",
      "     38            1.0000        \u001b[32m0.3004\u001b[0m       0.4861            0.4861        \u001b[94m1.2318\u001b[0m  0.0000  0.5235\n",
      "     39            1.0000        0.3262       0.4861            0.4861        1.2319  0.0000  0.5071\n",
      "     40            1.0000        0.3144       0.4861            0.4861        \u001b[94m1.2311\u001b[0m  0.0000  0.5120\n",
      "Training model for subject 4 with 220 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2909\u001b[0m        \u001b[32m1.6863\u001b[0m       \u001b[35m0.2431\u001b[0m            \u001b[31m0.2431\u001b[0m        \u001b[94m2.3798\u001b[0m  0.0006  0.5218\n",
      "      2            \u001b[36m0.3273\u001b[0m        \u001b[32m1.3606\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        2.6453  0.0006  0.5021\n",
      "      3            0.2682        \u001b[32m1.1876\u001b[0m       0.2535            0.2535        2.4650  0.0006  0.4983\n",
      "      4            0.2864        \u001b[32m1.1036\u001b[0m       0.2569            0.2569        \u001b[94m2.0540\u001b[0m  0.0006  0.5103\n",
      "      5            \u001b[36m0.3409\u001b[0m        \u001b[32m1.0163\u001b[0m       0.2639            0.2639        \u001b[94m1.9193\u001b[0m  0.0006  0.5678\n",
      "      6            \u001b[36m0.4864\u001b[0m        \u001b[32m0.7909\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m1.5796\u001b[0m  0.0006  0.5687\n",
      "      7            \u001b[36m0.6227\u001b[0m        0.8189       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.4947\u001b[0m  0.0006  0.6157\n",
      "      8            \u001b[36m0.6409\u001b[0m        \u001b[32m0.7325\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.4719\u001b[0m  0.0006  0.5802\n",
      "      9            \u001b[36m0.7318\u001b[0m        0.7862       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.4076\u001b[0m  0.0006  0.6495\n",
      "     10            \u001b[36m0.7727\u001b[0m        \u001b[32m0.6888\u001b[0m       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.3585\u001b[0m  0.0005  0.4958\n",
      "     11            \u001b[36m0.8091\u001b[0m        \u001b[32m0.6506\u001b[0m       0.4410            0.4410        \u001b[94m1.3228\u001b[0m  0.0005  0.5062\n",
      "     12            \u001b[36m0.8591\u001b[0m        \u001b[32m0.6360\u001b[0m       0.4410            0.4410        \u001b[94m1.2893\u001b[0m  0.0005  0.5122\n",
      "     13            \u001b[36m0.8727\u001b[0m        \u001b[32m0.5780\u001b[0m       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        \u001b[94m1.2729\u001b[0m  0.0005  0.5101\n",
      "     14            \u001b[36m0.9045\u001b[0m        0.5782       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        \u001b[94m1.2478\u001b[0m  0.0005  0.5054\n",
      "     15            \u001b[36m0.9182\u001b[0m        \u001b[32m0.4729\u001b[0m       0.4653            0.4653        \u001b[94m1.2287\u001b[0m  0.0004  0.5203\n",
      "     16            \u001b[36m0.9364\u001b[0m        0.5601       0.4479            0.4479        \u001b[94m1.2245\u001b[0m  0.0004  0.5059\n",
      "     17            \u001b[36m0.9545\u001b[0m        0.5058       0.4688            0.4688        \u001b[94m1.2111\u001b[0m  0.0004  0.6747\n",
      "     18            \u001b[36m0.9591\u001b[0m        0.5063       0.4757            0.4757        \u001b[94m1.1966\u001b[0m  0.0004  0.5150\n",
      "     19            \u001b[36m0.9682\u001b[0m        0.4982       0.4792            0.4792        \u001b[94m1.1822\u001b[0m  0.0004  0.5063\n",
      "     20            0.9682        \u001b[32m0.4446\u001b[0m       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        \u001b[94m1.1821\u001b[0m  0.0003  0.5062\n",
      "     21            \u001b[36m0.9773\u001b[0m        \u001b[32m0.3657\u001b[0m       0.4826            0.4826        1.1839  0.0003  0.5093\n",
      "     22            0.9727        0.3767       0.4826            0.4826        1.1889  0.0003  0.5019\n",
      "     23            0.9773        0.3727       \u001b[35m0.4931\u001b[0m            \u001b[31m0.4931\u001b[0m        1.1863  0.0002  0.5037\n",
      "     24            \u001b[36m0.9864\u001b[0m        0.3773       \u001b[35m0.5035\u001b[0m            \u001b[31m0.5035\u001b[0m        \u001b[94m1.1771\u001b[0m  0.0002  0.4992\n",
      "     25            0.9818        \u001b[32m0.3435\u001b[0m       0.5035            0.5035        \u001b[94m1.1716\u001b[0m  0.0002  0.5113\n",
      "     26            0.9864        0.3623       \u001b[35m0.5139\u001b[0m            \u001b[31m0.5139\u001b[0m        \u001b[94m1.1710\u001b[0m  0.0002  0.5042\n",
      "     27            0.9864        \u001b[32m0.3042\u001b[0m       0.5104            0.5104        \u001b[94m1.1701\u001b[0m  0.0002  0.5087\n",
      "     28            0.9818        0.3855       0.5139            0.5139        \u001b[94m1.1661\u001b[0m  0.0001  0.6732\n",
      "     29            0.9818        0.3758       0.5069            0.5069        \u001b[94m1.1636\u001b[0m  0.0001  0.6090\n",
      "     30            0.9818        0.3318       0.5139            0.5139        \u001b[94m1.1629\u001b[0m  0.0001  0.5845\n",
      "     31            0.9818        \u001b[32m0.3033\u001b[0m       0.5139            0.5139        1.1630  0.0001  0.5892\n",
      "     32            0.9818        \u001b[32m0.2911\u001b[0m       0.5069            0.5069        \u001b[94m1.1627\u001b[0m  0.0001  0.6005\n",
      "     33            0.9818        0.3284       0.5139            0.5139        \u001b[94m1.1616\u001b[0m  0.0000  0.5075\n",
      "     34            0.9864        0.3266       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        \u001b[94m1.1611\u001b[0m  0.0000  0.5167\n",
      "     35            0.9864        0.3271       0.5208            0.5208        \u001b[94m1.1609\u001b[0m  0.0000  0.5177\n",
      "     36            0.9864        0.3613       0.5208            0.5208        \u001b[94m1.1606\u001b[0m  0.0000  0.4919\n",
      "     37            0.9864        0.3522       0.5174            0.5174        \u001b[94m1.1605\u001b[0m  0.0000  0.5169\n",
      "     38            0.9864        0.3307       0.5174            0.5174        1.1606  0.0000  0.5117\n",
      "     39            0.9864        0.3094       0.5174            0.5174        \u001b[94m1.1603\u001b[0m  0.0000  0.4941\n",
      "     40            0.9864        0.3259       0.5174            0.5174        1.1605  0.0000  0.5037\n",
      "Training model for subject 4 with 230 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2565\u001b[0m        \u001b[32m1.6425\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m2.8890\u001b[0m  0.0006  0.5218\n",
      "      2            0.2522        \u001b[32m1.2443\u001b[0m       0.2500            0.2500        4.6474  0.0006  0.5043\n",
      "      3            0.2522        \u001b[32m1.2087\u001b[0m       0.2500            0.2500        4.7783  0.0006  0.5000\n",
      "      4            0.2522        \u001b[32m1.0905\u001b[0m       0.2500            0.2500        3.9591  0.0006  0.5074\n",
      "      5            \u001b[36m0.2696\u001b[0m        \u001b[32m0.9745\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        2.9180  0.0006  0.5001\n",
      "      6            \u001b[36m0.3391\u001b[0m        0.9880       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        \u001b[94m2.0682\u001b[0m  0.0006  0.4861\n",
      "      7            \u001b[36m0.5435\u001b[0m        \u001b[32m0.9006\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.6407\u001b[0m  0.0006  0.5015\n",
      "      8            \u001b[36m0.7000\u001b[0m        \u001b[32m0.7842\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.4443\u001b[0m  0.0006  0.5026\n",
      "      9            \u001b[36m0.7739\u001b[0m        0.7948       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.3842\u001b[0m  0.0006  0.4989\n",
      "     10            0.7652        0.7864       0.4410            0.4410        \u001b[94m1.3758\u001b[0m  0.0005  0.4911\n",
      "     11            0.7522        \u001b[32m0.7145\u001b[0m       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        \u001b[94m1.3689\u001b[0m  0.0005  0.5067\n",
      "     12            0.7739        \u001b[32m0.6508\u001b[0m       0.4792            0.4792        \u001b[94m1.3337\u001b[0m  0.0005  0.5694\n",
      "     13            \u001b[36m0.8087\u001b[0m        \u001b[32m0.6110\u001b[0m       \u001b[35m0.5104\u001b[0m            \u001b[31m0.5104\u001b[0m        \u001b[94m1.2813\u001b[0m  0.0005  0.5900\n",
      "     14            \u001b[36m0.8391\u001b[0m        0.6308       0.5035            0.5035        \u001b[94m1.2270\u001b[0m  0.0005  0.5655\n",
      "     15            \u001b[36m0.8652\u001b[0m        \u001b[32m0.5140\u001b[0m       0.5035            0.5035        \u001b[94m1.1894\u001b[0m  0.0004  0.5725\n",
      "     16            \u001b[36m0.9043\u001b[0m        \u001b[32m0.5074\u001b[0m       \u001b[35m0.5139\u001b[0m            \u001b[31m0.5139\u001b[0m        \u001b[94m1.1647\u001b[0m  0.0004  0.6356\n",
      "     17            \u001b[36m0.9261\u001b[0m        0.5177       0.5139            0.5139        \u001b[94m1.1460\u001b[0m  0.0004  0.5127\n",
      "     18            \u001b[36m0.9391\u001b[0m        \u001b[32m0.4649\u001b[0m       0.4931            0.4931        \u001b[94m1.1325\u001b[0m  0.0004  0.5040\n",
      "     19            \u001b[36m0.9478\u001b[0m        0.4901       0.5104            0.5104        \u001b[94m1.1221\u001b[0m  0.0004  0.5191\n",
      "     20            \u001b[36m0.9565\u001b[0m        \u001b[32m0.4460\u001b[0m       0.5139            0.5139        \u001b[94m1.1176\u001b[0m  0.0003  0.5083\n",
      "     21            \u001b[36m0.9696\u001b[0m        0.4508       \u001b[35m0.5174\u001b[0m            \u001b[31m0.5174\u001b[0m        \u001b[94m1.1144\u001b[0m  0.0003  0.5052\n",
      "     22            \u001b[36m0.9826\u001b[0m        0.4801       0.5104            0.5104        1.1184  0.0003  0.5345\n",
      "     23            \u001b[36m0.9870\u001b[0m        \u001b[32m0.3634\u001b[0m       0.5174            0.5174        1.1158  0.0002  0.5010\n",
      "     24            0.9870        0.3804       0.5035            0.5035        1.1190  0.0002  0.5139\n",
      "     25            0.9870        \u001b[32m0.3568\u001b[0m       0.5035            0.5035        1.1178  0.0002  0.5091\n",
      "     26            \u001b[36m0.9913\u001b[0m        \u001b[32m0.3486\u001b[0m       0.5000            0.5000        1.1154  0.0002  0.4892\n",
      "     27            0.9913        \u001b[32m0.3263\u001b[0m       0.5139            0.5139        \u001b[94m1.1141\u001b[0m  0.0002  0.5156\n",
      "     28            0.9913        0.3433       0.5174            0.5174        \u001b[94m1.1122\u001b[0m  0.0001  0.5017\n",
      "     29            \u001b[36m0.9957\u001b[0m        \u001b[32m0.3237\u001b[0m       0.5174            0.5174        \u001b[94m1.1104\u001b[0m  0.0001  0.5083\n",
      "     30            0.9957        0.3291       0.5174            0.5174        \u001b[94m1.1092\u001b[0m  0.0001  0.5012\n",
      "     31            0.9957        \u001b[32m0.3185\u001b[0m       0.5174            0.5174        \u001b[94m1.1065\u001b[0m  0.0001  0.4878\n",
      "     32            0.9957        0.3242       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        \u001b[94m1.1031\u001b[0m  0.0001  0.5189\n",
      "     33            0.9957        0.3376       0.5208            0.5208        \u001b[94m1.1010\u001b[0m  0.0000  0.5845\n",
      "     34            0.9957        0.3508       \u001b[35m0.5278\u001b[0m            \u001b[31m0.5278\u001b[0m        \u001b[94m1.0991\u001b[0m  0.0000  0.5334\n",
      "     35            0.9957        \u001b[32m0.3030\u001b[0m       \u001b[35m0.5347\u001b[0m            \u001b[31m0.5347\u001b[0m        \u001b[94m1.0979\u001b[0m  0.0000  0.5860\n",
      "     36            0.9957        0.3528       0.5347            0.5347        \u001b[94m1.0974\u001b[0m  0.0000  0.6307\n",
      "     37            0.9957        0.3102       0.5347            0.5347        \u001b[94m1.0972\u001b[0m  0.0000  0.5823\n",
      "     38            0.9957        0.3487       0.5347            0.5347        \u001b[94m1.0971\u001b[0m  0.0000  0.5800\n",
      "     39            0.9957        0.3291       0.5347            0.5347        \u001b[94m1.0971\u001b[0m  0.0000  0.5986\n",
      "     40            0.9957        0.3138       0.5347            0.5347        1.0971  0.0000  0.5017\n",
      "Training model for subject 4 with 240 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.5287\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.2029\u001b[0m  0.0006  0.5394\n",
      "      2            \u001b[36m0.2875\u001b[0m        \u001b[32m1.3388\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m3.4908\u001b[0m  0.0006  0.5010\n",
      "      3            0.2875        \u001b[32m1.1747\u001b[0m       0.2569            0.2569        \u001b[94m3.1318\u001b[0m  0.0006  0.5115\n",
      "      4            \u001b[36m0.3542\u001b[0m        \u001b[32m1.0953\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m2.7075\u001b[0m  0.0006  0.5119\n",
      "      5            \u001b[36m0.4042\u001b[0m        \u001b[32m0.9298\u001b[0m       0.2569            0.2569        \u001b[94m2.4814\u001b[0m  0.0006  0.5149\n",
      "      6            \u001b[36m0.4292\u001b[0m        \u001b[32m0.9287\u001b[0m       0.2569            0.2569        \u001b[94m2.3009\u001b[0m  0.0006  0.5018\n",
      "      7            \u001b[36m0.4917\u001b[0m        \u001b[32m0.9280\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        \u001b[94m2.0209\u001b[0m  0.0006  0.5037\n",
      "      8            \u001b[36m0.5917\u001b[0m        \u001b[32m0.7919\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m1.6905\u001b[0m  0.0006  0.5008\n",
      "      9            \u001b[36m0.7208\u001b[0m        \u001b[32m0.7765\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.4696\u001b[0m  0.0006  0.4876\n",
      "     10            \u001b[36m0.7708\u001b[0m        \u001b[32m0.7527\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.3815\u001b[0m  0.0005  0.5042\n",
      "     11            \u001b[36m0.8083\u001b[0m        \u001b[32m0.6633\u001b[0m       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.3157\u001b[0m  0.0005  0.5251\n",
      "     12            \u001b[36m0.8542\u001b[0m        0.6786       0.4410            0.4410        \u001b[94m1.2775\u001b[0m  0.0005  0.4948\n",
      "     13            \u001b[36m0.8833\u001b[0m        \u001b[32m0.5463\u001b[0m       0.4410            0.4410        \u001b[94m1.2653\u001b[0m  0.0005  0.5070\n",
      "     14            \u001b[36m0.9000\u001b[0m        0.5851       0.4167            0.4167        1.2656  0.0005  0.5019\n",
      "     15            0.8917        0.5978       0.4271            0.4271        1.2658  0.0004  0.4894\n",
      "     16            \u001b[36m0.9208\u001b[0m        \u001b[32m0.5167\u001b[0m       0.4340            0.4340        \u001b[94m1.2430\u001b[0m  0.0004  0.5121\n",
      "     17            \u001b[36m0.9375\u001b[0m        \u001b[32m0.5126\u001b[0m       0.4444            0.4444        \u001b[94m1.2210\u001b[0m  0.0004  0.4953\n",
      "     18            \u001b[36m0.9417\u001b[0m        0.5308       0.4444            0.4444        1.2218  0.0004  0.5653\n",
      "     19            0.9417        \u001b[32m0.3968\u001b[0m       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.2071\u001b[0m  0.0004  0.6001\n",
      "     20            \u001b[36m0.9458\u001b[0m        0.4566       \u001b[35m0.4931\u001b[0m            \u001b[31m0.4931\u001b[0m        \u001b[94m1.1898\u001b[0m  0.0003  0.5828\n",
      "     21            0.9458        0.4361       0.4931            0.4931        \u001b[94m1.1867\u001b[0m  0.0003  0.5917\n",
      "     22            \u001b[36m0.9542\u001b[0m        0.4251       0.4931            0.4931        1.1879  0.0003  0.6422\n",
      "     23            \u001b[36m0.9583\u001b[0m        0.4191       0.4826            0.4826        \u001b[94m1.1771\u001b[0m  0.0002  0.5215\n",
      "     24            \u001b[36m0.9667\u001b[0m        0.4029       0.4896            0.4896        \u001b[94m1.1726\u001b[0m  0.0002  0.5088\n",
      "     25            \u001b[36m0.9708\u001b[0m        \u001b[32m0.3718\u001b[0m       \u001b[35m0.5035\u001b[0m            \u001b[31m0.5035\u001b[0m        \u001b[94m1.1662\u001b[0m  0.0002  0.5143\n",
      "     26            0.9708        0.3914       0.5035            0.5035        1.1672  0.0002  0.4993\n",
      "     27            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3317\u001b[0m       \u001b[35m0.5104\u001b[0m            \u001b[31m0.5104\u001b[0m        1.1727  0.0002  0.5171\n",
      "     28            \u001b[36m0.9792\u001b[0m        0.4327       0.5035            0.5035        1.1747  0.0001  0.5148\n",
      "     29            0.9792        0.3514       0.4965            0.4965        1.1749  0.0001  0.5041\n",
      "     30            0.9792        \u001b[32m0.3297\u001b[0m       0.4896            0.4896        1.1758  0.0001  0.5042\n",
      "     31            0.9792        0.3335       0.5000            0.5000        1.1780  0.0001  0.5118\n",
      "     32            0.9792        0.3822       0.5000            0.5000        1.1750  0.0001  0.5069\n",
      "     33            \u001b[36m0.9833\u001b[0m        \u001b[32m0.3278\u001b[0m       0.5000            0.5000        1.1757  0.0000  0.5108\n",
      "     34            \u001b[36m0.9875\u001b[0m        0.3294       0.4931            0.4931        1.1742  0.0000  0.4963\n",
      "     35            0.9875        0.3845       0.4896            0.4896        1.1741  0.0000  0.6124\n",
      "     36            0.9875        0.3554       0.4931            0.4931        1.1733  0.0000  0.5010\n",
      "     37            0.9875        0.3629       0.4896            0.4896        1.1721  0.0000  0.5191\n",
      "     38            0.9875        0.3397       0.4861            0.4861        1.1722  0.0000  0.5119\n",
      "     39            0.9875        \u001b[32m0.3047\u001b[0m       0.4896            0.4896        1.1724  0.0000  0.5079\n",
      "     40            0.9875        0.3868       0.4896            0.4896        1.1725  0.0000  0.5162\n",
      "Training model for subject 4 with 250 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2520\u001b[0m        \u001b[32m1.6248\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.5901\u001b[0m  0.0006  0.5712\n",
      "      2            0.2520        \u001b[32m1.3434\u001b[0m       0.2500            0.2500        4.6032  0.0006  0.5965\n",
      "      3            0.2520        \u001b[32m1.2741\u001b[0m       0.2500            0.2500        \u001b[94m3.3234\u001b[0m  0.0006  0.5823\n",
      "      4            \u001b[36m0.3680\u001b[0m        \u001b[32m1.1144\u001b[0m       0.2500            0.2500        \u001b[94m2.1544\u001b[0m  0.0006  0.5844\n",
      "      5            \u001b[36m0.5480\u001b[0m        \u001b[32m1.0206\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.7599\u001b[0m  0.0006  0.6354\n",
      "      6            0.5320        \u001b[32m0.9991\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.6271\u001b[0m  0.0006  0.5183\n",
      "      7            \u001b[36m0.5600\u001b[0m        \u001b[32m0.9297\u001b[0m       0.3889            0.3889        \u001b[94m1.5491\u001b[0m  0.0006  0.5103\n",
      "      8            \u001b[36m0.6040\u001b[0m        \u001b[32m0.8986\u001b[0m       0.3993            0.3993        \u001b[94m1.5204\u001b[0m  0.0006  0.5106\n",
      "      9            \u001b[36m0.6680\u001b[0m        0.9247       0.3715            0.3715        \u001b[94m1.5035\u001b[0m  0.0006  0.4985\n",
      "     10            \u001b[36m0.6960\u001b[0m        \u001b[32m0.7183\u001b[0m       0.3715            0.3715        \u001b[94m1.4798\u001b[0m  0.0005  0.5025\n",
      "     11            \u001b[36m0.7160\u001b[0m        0.7215       0.3542            0.3542        \u001b[94m1.4614\u001b[0m  0.0005  0.5133\n",
      "     12            \u001b[36m0.7480\u001b[0m        0.7474       0.3576            0.3576        \u001b[94m1.4360\u001b[0m  0.0005  0.4894\n",
      "     13            \u001b[36m0.8040\u001b[0m        \u001b[32m0.6703\u001b[0m       0.3611            0.3611        \u001b[94m1.4126\u001b[0m  0.0005  0.5105\n",
      "     14            \u001b[36m0.8240\u001b[0m        0.6993       0.3715            0.3715        \u001b[94m1.3930\u001b[0m  0.0005  0.5243\n",
      "     15            \u001b[36m0.9040\u001b[0m        \u001b[32m0.6302\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.3527\u001b[0m  0.0004  0.4988\n",
      "     16            \u001b[36m0.9080\u001b[0m        \u001b[32m0.6096\u001b[0m       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.3202\u001b[0m  0.0004  0.5054\n",
      "     17            \u001b[36m0.9360\u001b[0m        \u001b[32m0.5482\u001b[0m       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.3063\u001b[0m  0.0004  0.5184\n",
      "     18            \u001b[36m0.9480\u001b[0m        0.5504       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.3032\u001b[0m  0.0004  0.4941\n",
      "     19            \u001b[36m0.9560\u001b[0m        \u001b[32m0.5100\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        1.3046  0.0004  0.5018\n",
      "     20            0.9520        0.5584       0.4479            0.4479        \u001b[94m1.3007\u001b[0m  0.0003  0.5167\n",
      "     21            0.9560        0.5196       0.4479            0.4479        \u001b[94m1.2926\u001b[0m  0.0003  0.4910\n",
      "     22            0.9560        \u001b[32m0.3887\u001b[0m       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.2775\u001b[0m  0.0003  0.4917\n",
      "     23            0.9520        0.4848       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2680\u001b[0m  0.0002  0.5128\n",
      "     24            0.9560        0.4316       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        \u001b[94m1.2673\u001b[0m  0.0002  0.5433\n",
      "     25            \u001b[36m0.9600\u001b[0m        0.4425       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        \u001b[94m1.2665\u001b[0m  0.0002  0.5743\n",
      "     26            0.9600        0.3946       0.4792            0.4792        1.2674  0.0002  0.5892\n",
      "     27            0.9600        \u001b[32m0.3791\u001b[0m       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        1.2697  0.0002  0.5848\n",
      "     28            \u001b[36m0.9640\u001b[0m        0.4066       0.4722            0.4722        1.2700  0.0001  0.6199\n",
      "     29            0.9640        0.4072       0.4792            0.4792        1.2678  0.0001  0.5207\n",
      "     30            0.9640        0.3819       0.4826            0.4826        \u001b[94m1.2644\u001b[0m  0.0001  0.4977\n",
      "     31            \u001b[36m0.9680\u001b[0m        0.4209       0.4792            0.4792        \u001b[94m1.2619\u001b[0m  0.0001  0.5194\n",
      "     32            \u001b[36m0.9720\u001b[0m        0.4156       0.4792            0.4792        \u001b[94m1.2598\u001b[0m  0.0001  0.5180\n",
      "     33            \u001b[36m0.9760\u001b[0m        0.3937       0.4757            0.4757        \u001b[94m1.2592\u001b[0m  0.0000  0.5063\n",
      "     34            \u001b[36m0.9800\u001b[0m        \u001b[32m0.3210\u001b[0m       0.4757            0.4757        \u001b[94m1.2589\u001b[0m  0.0000  0.5176\n",
      "     35            0.9800        0.3439       0.4792            0.4792        \u001b[94m1.2587\u001b[0m  0.0000  0.5093\n",
      "     36            0.9800        0.3532       0.4826            0.4826        \u001b[94m1.2587\u001b[0m  0.0000  0.4997\n",
      "     37            0.9800        0.4015       0.4826            0.4826        \u001b[94m1.2586\u001b[0m  0.0000  0.5144\n",
      "     38            0.9800        0.3885       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        \u001b[94m1.2584\u001b[0m  0.0000  0.5009\n",
      "     39            0.9800        0.3549       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        1.2585  0.0000  0.5085\n",
      "     40            0.9800        0.3665       0.4896            0.4896        \u001b[94m1.2584\u001b[0m  0.0000  0.6498\n",
      "Training model for subject 4 with 260 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3731\u001b[0m        \u001b[32m1.5647\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m2.9369\u001b[0m  0.0006  0.6204\n",
      "      2            \u001b[36m0.3846\u001b[0m        \u001b[32m1.1908\u001b[0m       0.2917            0.2917        \u001b[94m2.0220\u001b[0m  0.0006  0.6057\n",
      "      3            0.3577        \u001b[32m1.0692\u001b[0m       0.2639            0.2639        2.2902  0.0006  0.6000\n",
      "      4            \u001b[36m0.4846\u001b[0m        \u001b[32m0.9714\u001b[0m       0.2847            0.2847        \u001b[94m1.8981\u001b[0m  0.0006  0.5953\n",
      "      5            \u001b[36m0.5231\u001b[0m        \u001b[32m0.8762\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.7144\u001b[0m  0.0006  0.6075\n",
      "      6            \u001b[36m0.5962\u001b[0m        \u001b[32m0.8076\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.5806\u001b[0m  0.0006  0.6507\n",
      "      7            \u001b[36m0.7115\u001b[0m        \u001b[32m0.7878\u001b[0m       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.4046\u001b[0m  0.0006  0.7286\n",
      "      8            \u001b[36m0.8308\u001b[0m        \u001b[32m0.7163\u001b[0m       0.4271            0.4271        \u001b[94m1.3043\u001b[0m  0.0006  0.7184\n",
      "      9            \u001b[36m0.8615\u001b[0m        \u001b[32m0.6291\u001b[0m       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.2727\u001b[0m  0.0006  0.7317\n",
      "     10            \u001b[36m0.8923\u001b[0m        \u001b[32m0.6019\u001b[0m       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.2492\u001b[0m  0.0005  0.6156\n",
      "     11            \u001b[36m0.9038\u001b[0m        0.6052       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        \u001b[94m1.2274\u001b[0m  0.0005  0.6131\n",
      "     12            0.9038        \u001b[32m0.5620\u001b[0m       0.4722            0.4722        \u001b[94m1.2210\u001b[0m  0.0005  0.6003\n",
      "     13            \u001b[36m0.9308\u001b[0m        \u001b[32m0.5187\u001b[0m       \u001b[35m0.4931\u001b[0m            \u001b[31m0.4931\u001b[0m        \u001b[94m1.2010\u001b[0m  0.0005  0.6156\n",
      "     14            0.9269        \u001b[32m0.4566\u001b[0m       \u001b[35m0.5035\u001b[0m            \u001b[31m0.5035\u001b[0m        1.2109  0.0005  0.6006\n",
      "     15            \u001b[36m0.9385\u001b[0m        0.4900       0.4792            0.4792        1.2177  0.0004  0.6036\n",
      "     16            \u001b[36m0.9577\u001b[0m        \u001b[32m0.4227\u001b[0m       0.4826            0.4826        \u001b[94m1.1996\u001b[0m  0.0004  0.5981\n",
      "     17            0.9500        \u001b[32m0.4040\u001b[0m       \u001b[35m0.5139\u001b[0m            \u001b[31m0.5139\u001b[0m        \u001b[94m1.1809\u001b[0m  0.0004  0.6090\n",
      "     18            \u001b[36m0.9731\u001b[0m        \u001b[32m0.3980\u001b[0m       \u001b[35m0.5278\u001b[0m            \u001b[31m0.5278\u001b[0m        \u001b[94m1.1771\u001b[0m  0.0004  0.6040\n",
      "     19            0.9731        \u001b[32m0.3842\u001b[0m       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        1.1861  0.0004  0.6057\n",
      "     20            \u001b[36m0.9769\u001b[0m        \u001b[32m0.3435\u001b[0m       0.5347            0.5347        1.1817  0.0003  0.6036\n",
      "     21            0.9769        0.3483       0.5243            0.5243        1.1854  0.0003  0.6107\n",
      "     22            \u001b[36m0.9808\u001b[0m        \u001b[32m0.3116\u001b[0m       0.5243            0.5243        1.1782  0.0003  0.6225\n",
      "     23            0.9808        0.3157       0.5278            0.5278        \u001b[94m1.1654\u001b[0m  0.0002  0.6719\n",
      "     24            \u001b[36m0.9923\u001b[0m        0.3171       0.5382            0.5382        \u001b[94m1.1503\u001b[0m  0.0002  0.6312\n",
      "     25            0.9923        \u001b[32m0.2858\u001b[0m       0.5382            0.5382        \u001b[94m1.1430\u001b[0m  0.0002  0.6044\n",
      "     26            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2771\u001b[0m       0.5382            0.5382        \u001b[94m1.1345\u001b[0m  0.0002  0.7168\n",
      "     27            1.0000        \u001b[32m0.2463\u001b[0m       \u001b[35m0.5451\u001b[0m            \u001b[31m0.5451\u001b[0m        \u001b[94m1.1299\u001b[0m  0.0002  0.6938\n",
      "     28            0.9923        0.2518       \u001b[35m0.5486\u001b[0m            \u001b[31m0.5486\u001b[0m        \u001b[94m1.1299\u001b[0m  0.0001  0.7110\n",
      "     29            0.9962        0.2638       0.5451            0.5451        1.1304  0.0001  0.7409\n",
      "     30            0.9962        0.2963       0.5451            0.5451        \u001b[94m1.1299\u001b[0m  0.0001  0.6212\n",
      "     31            1.0000        0.2710       0.5451            0.5451        \u001b[94m1.1296\u001b[0m  0.0001  0.5961\n",
      "     32            1.0000        \u001b[32m0.2287\u001b[0m       0.5417            0.5417        1.1308  0.0001  0.6057\n",
      "     33            1.0000        0.2428       0.5417            0.5417        1.1310  0.0000  0.6058\n",
      "     34            1.0000        0.2418       0.5486            0.5486        1.1302  0.0000  0.5949\n",
      "     35            1.0000        0.2298       0.5486            0.5486        1.1302  0.0000  0.6232\n",
      "     36            1.0000        0.2439       0.5486            0.5486        1.1303  0.0000  0.6156\n",
      "     37            1.0000        0.2461       0.5486            0.5486        1.1304  0.0000  0.6111\n",
      "     38            1.0000        0.2431       0.5486            0.5486        1.1304  0.0000  0.6146\n",
      "     39            1.0000        0.2348       0.5486            0.5486        1.1306  0.0000  0.6163\n",
      "     40            1.0000        0.2446       0.5451            0.5451        1.1307  0.0000  0.6721\n",
      "Training model for subject 4 with 270 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3852\u001b[0m        \u001b[32m1.6394\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        \u001b[94m2.3309\u001b[0m  0.0006  0.6093\n",
      "      2            \u001b[36m0.4407\u001b[0m        \u001b[32m1.2468\u001b[0m       0.2847            0.2847        \u001b[94m2.2340\u001b[0m  0.0006  0.6004\n",
      "      3            \u001b[36m0.4852\u001b[0m        \u001b[32m1.1009\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m2.0423\u001b[0m  0.0006  0.6814\n",
      "      4            0.4704        \u001b[32m0.9699\u001b[0m       0.2951            0.2951        \u001b[94m1.9066\u001b[0m  0.0006  0.6245\n",
      "      5            \u001b[36m0.5370\u001b[0m        \u001b[32m0.9609\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m1.6495\u001b[0m  0.0006  0.6284\n",
      "      6            \u001b[36m0.6259\u001b[0m        \u001b[32m0.8859\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.4330\u001b[0m  0.0006  0.7269\n",
      "      7            \u001b[36m0.7741\u001b[0m        \u001b[32m0.7911\u001b[0m       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.2962\u001b[0m  0.0006  0.7224\n",
      "      8            \u001b[36m0.8333\u001b[0m        \u001b[32m0.7270\u001b[0m       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.2509\u001b[0m  0.0006  0.6970\n",
      "      9            \u001b[36m0.8481\u001b[0m        \u001b[32m0.6735\u001b[0m       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.2302\u001b[0m  0.0006  0.7229\n",
      "     10            \u001b[36m0.8630\u001b[0m        \u001b[32m0.6123\u001b[0m       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        \u001b[94m1.2075\u001b[0m  0.0005  0.5969\n",
      "     11            \u001b[36m0.8926\u001b[0m        \u001b[32m0.5747\u001b[0m       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        \u001b[94m1.2001\u001b[0m  0.0005  0.6007\n",
      "     12            \u001b[36m0.9185\u001b[0m        \u001b[32m0.5657\u001b[0m       \u001b[35m0.5000\u001b[0m            \u001b[31m0.5000\u001b[0m        \u001b[94m1.1937\u001b[0m  0.0005  0.6103\n",
      "     13            \u001b[36m0.9481\u001b[0m        \u001b[32m0.4944\u001b[0m       0.4896            0.4896        \u001b[94m1.1848\u001b[0m  0.0005  0.6053\n",
      "     14            0.9296        0.5344       \u001b[35m0.5104\u001b[0m            \u001b[31m0.5104\u001b[0m        1.2097  0.0005  0.5932\n",
      "     15            \u001b[36m0.9519\u001b[0m        \u001b[32m0.4720\u001b[0m       0.4965            0.4965        1.1958  0.0004  0.6167\n",
      "     16            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3837\u001b[0m       0.5035            0.5035        \u001b[94m1.1777\u001b[0m  0.0004  0.5957\n",
      "     17            \u001b[36m0.9778\u001b[0m        0.4331       0.5035            0.5035        \u001b[94m1.1638\u001b[0m  0.0004  0.5979\n",
      "     18            0.9704        \u001b[32m0.3835\u001b[0m       0.5035            0.5035        1.1676  0.0004  0.5997\n",
      "     19            \u001b[36m0.9889\u001b[0m        \u001b[32m0.3331\u001b[0m       \u001b[35m0.5139\u001b[0m            \u001b[31m0.5139\u001b[0m        1.1678  0.0004  0.5959\n",
      "     20            \u001b[36m0.9926\u001b[0m        0.3859       0.4965            0.4965        1.1694  0.0003  0.6205\n",
      "     21            \u001b[36m0.9963\u001b[0m        0.3599       0.5035            0.5035        \u001b[94m1.1583\u001b[0m  0.0003  0.5999\n",
      "     22            0.9963        \u001b[32m0.3158\u001b[0m       \u001b[35m0.5243\u001b[0m            \u001b[31m0.5243\u001b[0m        \u001b[94m1.1428\u001b[0m  0.0003  0.6260\n",
      "     23            0.9926        \u001b[32m0.2806\u001b[0m       0.5174            0.5174        \u001b[94m1.1369\u001b[0m  0.0002  0.6028\n",
      "     24            \u001b[36m1.0000\u001b[0m        0.3074       \u001b[35m0.5278\u001b[0m            \u001b[31m0.5278\u001b[0m        \u001b[94m1.1295\u001b[0m  0.0002  0.6022\n",
      "     25            1.0000        0.2841       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        1.1311  0.0002  0.7248\n",
      "     26            1.0000        \u001b[32m0.2728\u001b[0m       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        1.1321  0.0002  0.7443\n",
      "     27            1.0000        0.2774       0.5382            0.5382        \u001b[94m1.1291\u001b[0m  0.0002  0.6969\n",
      "     28            1.0000        \u001b[32m0.2638\u001b[0m       0.5347            0.5347        \u001b[94m1.1281\u001b[0m  0.0001  0.7443\n",
      "     29            1.0000        \u001b[32m0.2246\u001b[0m       0.5278            0.5278        1.1288  0.0001  0.6175\n",
      "     30            1.0000        0.2616       0.5278            0.5278        \u001b[94m1.1270\u001b[0m  0.0001  0.5960\n",
      "     31            1.0000        0.2921       0.5278            0.5278        \u001b[94m1.1255\u001b[0m  0.0001  0.6000\n",
      "     32            1.0000        0.2388       0.5278            0.5278        1.1267  0.0001  0.6001\n",
      "     33            1.0000        0.2251       0.5312            0.5312        1.1267  0.0000  0.5987\n",
      "     34            1.0000        0.2314       0.5382            0.5382        \u001b[94m1.1247\u001b[0m  0.0000  0.6050\n",
      "     35            1.0000        0.2479       0.5382            0.5382        \u001b[94m1.1231\u001b[0m  0.0000  0.6479\n",
      "     36            1.0000        0.2640       0.5417            0.5417        \u001b[94m1.1224\u001b[0m  0.0000  0.6163\n",
      "     37            1.0000        0.2423       0.5417            0.5417        \u001b[94m1.1220\u001b[0m  0.0000  0.6173\n",
      "     38            1.0000        0.2478       0.5417            0.5417        \u001b[94m1.1215\u001b[0m  0.0000  0.6061\n",
      "     39            1.0000        0.2602       0.5417            0.5417        1.1216  0.0000  0.6117\n",
      "     40            1.0000        0.2666       0.5417            0.5417        1.1219  0.0000  0.6115\n",
      "Training model for subject 5 with 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.7000\u001b[0m        \u001b[32m1.6258\u001b[0m       \u001b[35m0.2396\u001b[0m            \u001b[31m0.2396\u001b[0m        \u001b[94m2.9704\u001b[0m  0.0006  0.3256\n",
      "      2            0.7000        \u001b[32m0.7009\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        4.5551  0.0006  0.3250\n",
      "      3            0.6000        \u001b[32m0.4150\u001b[0m       0.2500            0.2500        5.5676  0.0006  0.3328\n",
      "      4            0.6000        \u001b[32m0.2367\u001b[0m       0.2500            0.2500        6.1087  0.0006  0.4345\n",
      "      5            0.7000        \u001b[32m0.1795\u001b[0m       0.2500            0.2500        6.2138  0.0006  0.3156\n",
      "      6            0.7000        \u001b[32m0.1272\u001b[0m       0.2500            0.2500        6.0099  0.0006  0.3241\n",
      "      7            0.7000        0.1624       0.2500            0.2500        5.6833  0.0006  0.3238\n",
      "      8            \u001b[36m0.8000\u001b[0m        \u001b[32m0.0552\u001b[0m       0.2500            0.2500        5.3003  0.0006  0.3515\n",
      "      9            0.8000        0.0847       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        4.8867  0.0006  0.4339\n",
      "     10            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0411\u001b[0m       0.2500            0.2500        4.4930  0.0005  0.4293\n",
      "     11            1.0000        \u001b[32m0.0313\u001b[0m       0.2431            0.2431        4.1586  0.0005  0.3952\n",
      "     12            1.0000        \u001b[32m0.0146\u001b[0m       0.2465            0.2465        3.8727  0.0005  0.4124\n",
      "     13            1.0000        0.0168       0.2396            0.2396        3.6276  0.0005  0.4334\n",
      "     14            1.0000        0.0383       0.2396            0.2396        3.4023  0.0005  0.3703\n",
      "     15            1.0000        0.0255       0.2431            0.2431        3.2202  0.0004  0.3500\n",
      "     16            1.0000        \u001b[32m0.0122\u001b[0m       0.2431            0.2431        3.0702  0.0004  0.3145\n",
      "     17            1.0000        \u001b[32m0.0090\u001b[0m       0.2465            0.2465        \u001b[94m2.9559\u001b[0m  0.0004  0.3511\n",
      "     18            1.0000        0.0138       0.2500            0.2500        \u001b[94m2.8629\u001b[0m  0.0004  0.3152\n",
      "     19            1.0000        0.0145       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m2.7926\u001b[0m  0.0004  0.3379\n",
      "     20            1.0000        0.0237       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m2.7276\u001b[0m  0.0003  0.3386\n",
      "     21            1.0000        0.0148       0.2639            0.2639        \u001b[94m2.6767\u001b[0m  0.0003  0.3200\n",
      "     22            1.0000        \u001b[32m0.0053\u001b[0m       0.2639            0.2639        \u001b[94m2.6430\u001b[0m  0.0003  0.3227\n",
      "     23            1.0000        0.0083       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m2.6178\u001b[0m  0.0002  0.3137\n",
      "     24            1.0000        0.0076       0.2708            0.2708        \u001b[94m2.5969\u001b[0m  0.0002  0.3338\n",
      "     25            1.0000        0.0167       0.2674            0.2674        \u001b[94m2.5774\u001b[0m  0.0002  0.3303\n",
      "     26            1.0000        0.0105       0.2674            0.2674        \u001b[94m2.5679\u001b[0m  0.0002  0.3264\n",
      "     27            1.0000        \u001b[32m0.0050\u001b[0m       0.2639            0.2639        \u001b[94m2.5602\u001b[0m  0.0002  0.3151\n",
      "     28            1.0000        0.0116       0.2639            0.2639        \u001b[94m2.5565\u001b[0m  0.0001  0.3248\n",
      "     29            1.0000        0.0157       0.2639            0.2639        \u001b[94m2.5504\u001b[0m  0.0001  0.3315\n",
      "     30            1.0000        0.0078       0.2639            0.2639        \u001b[94m2.5437\u001b[0m  0.0001  0.3135\n",
      "     31            1.0000        0.0165       0.2639            0.2639        \u001b[94m2.5414\u001b[0m  0.0001  0.3333\n",
      "     32            1.0000        0.0064       0.2639            0.2639        \u001b[94m2.5388\u001b[0m  0.0001  0.3341\n",
      "     33            1.0000        0.0091       0.2639            0.2639        \u001b[94m2.5309\u001b[0m  0.0000  0.3201\n",
      "     34            1.0000        0.0054       0.2639            0.2639        \u001b[94m2.5293\u001b[0m  0.0000  0.3225\n",
      "     35            1.0000        0.0141       0.2639            0.2639        2.5300  0.0000  0.3260\n",
      "     36            1.0000        0.0123       0.2639            0.2639        \u001b[94m2.5237\u001b[0m  0.0000  0.3244\n",
      "     37            1.0000        0.0069       0.2639            0.2639        \u001b[94m2.5193\u001b[0m  0.0000  0.3267\n",
      "     38            1.0000        0.0081       0.2639            0.2639        \u001b[94m2.5170\u001b[0m  0.0000  0.3358\n",
      "     39            1.0000        0.0081       0.2639            0.2639        2.5184  0.0000  0.3240\n",
      "     40            1.0000        0.0128       0.2639            0.2639        \u001b[94m2.5139\u001b[0m  0.0000  0.3380\n",
      "Training model for subject 5 with 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.4000\u001b[0m        \u001b[32m1.5499\u001b[0m       \u001b[35m0.2431\u001b[0m            \u001b[31m0.2431\u001b[0m        \u001b[94m2.7856\u001b[0m  0.0006  0.2655\n",
      "      2            \u001b[36m0.5500\u001b[0m        \u001b[32m0.8850\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m2.0151\u001b[0m  0.0006  0.2642\n",
      "      3            \u001b[36m0.7000\u001b[0m        \u001b[32m0.6358\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m1.8588\u001b[0m  0.0006  0.2832\n",
      "      4            \u001b[36m0.7500\u001b[0m        \u001b[32m0.4218\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m1.8187\u001b[0m  0.0006  0.2899\n",
      "      5            \u001b[36m0.9000\u001b[0m        \u001b[32m0.2434\u001b[0m       0.3125            0.3125        1.9028  0.0006  0.2625\n",
      "      6            \u001b[36m0.9500\u001b[0m        0.2871       0.3333            0.3333        1.9841  0.0006  0.2998\n",
      "      7            0.9500        \u001b[32m0.1495\u001b[0m       0.3299            0.3299        2.0366  0.0006  0.2983\n",
      "      8            \u001b[36m1.0000\u001b[0m        0.1697       0.3299            0.3299        2.0422  0.0006  0.2690\n",
      "      9            1.0000        \u001b[32m0.0974\u001b[0m       0.3229            0.3229        2.0231  0.0006  0.2834\n",
      "     10            1.0000        \u001b[32m0.0911\u001b[0m       0.3264            0.3264        1.9827  0.0005  0.2784\n",
      "     11            1.0000        \u001b[32m0.0638\u001b[0m       0.3333            0.3333        1.9466  0.0005  0.3129\n",
      "     12            1.0000        \u001b[32m0.0393\u001b[0m       0.3299            0.3299        1.9158  0.0005  0.2650\n",
      "     13            1.0000        \u001b[32m0.0346\u001b[0m       0.3264            0.3264        1.8837  0.0005  0.2682\n",
      "     14            1.0000        0.0450       0.3194            0.3194        1.8549  0.0005  0.2780\n",
      "     15            1.0000        0.0492       0.3194            0.3194        1.8387  0.0004  0.3012\n",
      "     16            1.0000        \u001b[32m0.0195\u001b[0m       0.3229            0.3229        1.8253  0.0004  0.3555\n",
      "     17            1.0000        0.0237       0.3368            0.3368        \u001b[94m1.8128\u001b[0m  0.0004  0.3507\n",
      "     18            1.0000        0.0200       0.3368            0.3368        \u001b[94m1.8017\u001b[0m  0.0004  0.3573\n",
      "     19            1.0000        0.0197       0.3333            0.3333        \u001b[94m1.7916\u001b[0m  0.0004  0.3603\n",
      "     20            1.0000        0.0209       0.3299            0.3299        \u001b[94m1.7824\u001b[0m  0.0003  0.3515\n",
      "     21            1.0000        0.0213       0.3333            0.3333        \u001b[94m1.7757\u001b[0m  0.0003  0.4060\n",
      "     22            1.0000        \u001b[32m0.0155\u001b[0m       0.3368            0.3368        \u001b[94m1.7692\u001b[0m  0.0003  0.3653\n",
      "     23            1.0000        \u001b[32m0.0155\u001b[0m       0.3333            0.3333        \u001b[94m1.7645\u001b[0m  0.0002  0.3013\n",
      "     24            1.0000        \u001b[32m0.0136\u001b[0m       0.3333            0.3333        \u001b[94m1.7623\u001b[0m  0.0002  0.2592\n",
      "     25            1.0000        \u001b[32m0.0091\u001b[0m       0.3333            0.3333        \u001b[94m1.7604\u001b[0m  0.0002  0.2868\n",
      "     26            1.0000        0.0199       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.7595\u001b[0m  0.0002  0.2820\n",
      "     27            1.0000        0.0198       0.3368            0.3368        \u001b[94m1.7593\u001b[0m  0.0002  0.2716\n",
      "     28            1.0000        0.0134       0.3368            0.3368        \u001b[94m1.7588\u001b[0m  0.0001  0.2853\n",
      "     29            1.0000        0.0169       0.3368            0.3368        \u001b[94m1.7582\u001b[0m  0.0001  0.2782\n",
      "     30            1.0000        0.0106       0.3368            0.3368        \u001b[94m1.7575\u001b[0m  0.0001  0.2621\n",
      "     31            1.0000        0.0409       0.3368            0.3368        \u001b[94m1.7564\u001b[0m  0.0001  0.2837\n",
      "     32            1.0000        0.0194       0.3368            0.3368        \u001b[94m1.7552\u001b[0m  0.0001  0.3823\n",
      "     33            1.0000        0.0162       0.3368            0.3368        \u001b[94m1.7538\u001b[0m  0.0000  0.2824\n",
      "     34            1.0000        0.0106       0.3368            0.3368        \u001b[94m1.7526\u001b[0m  0.0000  0.2818\n",
      "     35            1.0000        0.0094       0.3368            0.3368        \u001b[94m1.7515\u001b[0m  0.0000  0.2787\n",
      "     36            1.0000        0.0149       0.3368            0.3368        \u001b[94m1.7503\u001b[0m  0.0000  0.2819\n",
      "     37            1.0000        0.0136       0.3333            0.3333        \u001b[94m1.7497\u001b[0m  0.0000  0.2871\n",
      "     38            1.0000        0.0194       0.3333            0.3333        \u001b[94m1.7490\u001b[0m  0.0000  0.2755\n",
      "     39            1.0000        0.0309       0.3333            0.3333        \u001b[94m1.7482\u001b[0m  0.0000  0.2792\n",
      "     40            1.0000        0.0136       0.3333            0.3333        \u001b[94m1.7481\u001b[0m  0.0000  0.2665\n",
      "Training model for subject 5 with 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3000\u001b[0m        \u001b[32m1.8581\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m6.2147\u001b[0m  0.0006  0.2785\n",
      "      2            0.3000        \u001b[32m1.2134\u001b[0m       0.2500            0.2500        \u001b[94m5.1170\u001b[0m  0.0006  0.2502\n",
      "      3            \u001b[36m0.4333\u001b[0m        \u001b[32m0.9453\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m4.3868\u001b[0m  0.0006  0.2672\n",
      "      4            \u001b[36m0.5000\u001b[0m        \u001b[32m0.7339\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        \u001b[94m4.0320\u001b[0m  0.0006  0.2484\n",
      "      5            \u001b[36m0.5333\u001b[0m        \u001b[32m0.4717\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m3.8335\u001b[0m  0.0006  0.2520\n",
      "      6            0.5333        \u001b[32m0.4476\u001b[0m       0.2951            0.2951        \u001b[94m3.5518\u001b[0m  0.0006  0.2707\n",
      "      7            0.5333        \u001b[32m0.3891\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m3.2496\u001b[0m  0.0006  0.2548\n",
      "      8            0.5333        \u001b[32m0.3079\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m3.0078\u001b[0m  0.0006  0.2737\n",
      "      9            \u001b[36m0.6000\u001b[0m        \u001b[32m0.2235\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m2.8726\u001b[0m  0.0006  0.2608\n",
      "     10            \u001b[36m0.7667\u001b[0m        0.2675       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m2.6825\u001b[0m  0.0005  0.2712\n",
      "     11            \u001b[36m0.9000\u001b[0m        \u001b[32m0.1987\u001b[0m       0.3368            0.3368        \u001b[94m2.4651\u001b[0m  0.0005  0.2511\n",
      "     12            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1365\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m2.2863\u001b[0m  0.0005  0.2687\n",
      "     13            1.0000        \u001b[32m0.1190\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m2.1485\u001b[0m  0.0005  0.2698\n",
      "     14            1.0000        \u001b[32m0.1001\u001b[0m       0.3472            0.3472        \u001b[94m2.0525\u001b[0m  0.0005  0.2499\n",
      "     15            1.0000        \u001b[32m0.0798\u001b[0m       0.3507            0.3507        \u001b[94m1.9840\u001b[0m  0.0004  0.2722\n",
      "     16            1.0000        0.0843       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.9382\u001b[0m  0.0004  0.2760\n",
      "     17            1.0000        \u001b[32m0.0664\u001b[0m       0.3542            0.3542        \u001b[94m1.9051\u001b[0m  0.0004  0.2667\n",
      "     18            1.0000        \u001b[32m0.0474\u001b[0m       0.3438            0.3438        \u001b[94m1.8823\u001b[0m  0.0004  0.2619\n",
      "     19            1.0000        0.0596       0.3507            0.3507        \u001b[94m1.8683\u001b[0m  0.0004  0.2712\n",
      "     20            1.0000        0.0806       0.3507            0.3507        \u001b[94m1.8604\u001b[0m  0.0003  0.2678\n",
      "     21            1.0000        0.0476       0.3472            0.3472        \u001b[94m1.8522\u001b[0m  0.0003  0.2818\n",
      "     22            1.0000        0.0889       0.3472            0.3472        \u001b[94m1.8446\u001b[0m  0.0003  0.2685\n",
      "     23            1.0000        \u001b[32m0.0441\u001b[0m       0.3507            0.3507        \u001b[94m1.8396\u001b[0m  0.0002  0.2716\n",
      "     24            1.0000        0.0572       0.3507            0.3507        \u001b[94m1.8369\u001b[0m  0.0002  0.2800\n",
      "     25            1.0000        \u001b[32m0.0383\u001b[0m       0.3507            0.3507        \u001b[94m1.8342\u001b[0m  0.0002  0.2689\n",
      "     26            1.0000        \u001b[32m0.0374\u001b[0m       0.3438            0.3438        \u001b[94m1.8310\u001b[0m  0.0002  0.2530\n",
      "     27            1.0000        \u001b[32m0.0268\u001b[0m       0.3403            0.3403        \u001b[94m1.8283\u001b[0m  0.0002  0.3067\n",
      "     28            1.0000        0.0344       0.3403            0.3403        \u001b[94m1.8255\u001b[0m  0.0001  0.3550\n",
      "     29            1.0000        0.0633       0.3403            0.3403        \u001b[94m1.8228\u001b[0m  0.0001  0.3490\n",
      "     30            1.0000        0.0847       0.3368            0.3368        \u001b[94m1.8213\u001b[0m  0.0001  0.3280\n",
      "     31            1.0000        0.0675       0.3368            0.3368        \u001b[94m1.8210\u001b[0m  0.0001  0.3556\n",
      "     32            1.0000        0.0365       0.3368            0.3368        \u001b[94m1.8201\u001b[0m  0.0001  0.3354\n",
      "     33            1.0000        0.0314       0.3333            0.3333        \u001b[94m1.8199\u001b[0m  0.0000  0.3631\n",
      "     34            1.0000        0.0401       0.3333            0.3333        \u001b[94m1.8195\u001b[0m  0.0000  0.3625\n",
      "     35            1.0000        0.0371       0.3333            0.3333        \u001b[94m1.8193\u001b[0m  0.0000  0.2797\n",
      "     36            1.0000        0.0411       0.3333            0.3333        \u001b[94m1.8189\u001b[0m  0.0000  0.2676\n",
      "     37            1.0000        0.0621       0.3333            0.3333        \u001b[94m1.8185\u001b[0m  0.0000  0.3728\n",
      "     38            1.0000        0.0473       0.3333            0.3333        \u001b[94m1.8177\u001b[0m  0.0000  0.2524\n",
      "     39            1.0000        0.0451       0.3333            0.3333        \u001b[94m1.8172\u001b[0m  0.0000  0.3589\n",
      "     40            1.0000        0.0524       0.3333            0.3333        \u001b[94m1.8170\u001b[0m  0.0000  0.2674\n",
      "Training model for subject 5 with 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.4750\u001b[0m        \u001b[32m1.6929\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m5.1551\u001b[0m  0.0006  0.2930\n",
      "      2            0.4000        \u001b[32m1.2426\u001b[0m       0.2743            0.2743        5.6563  0.0006  0.2654\n",
      "      3            0.3500        \u001b[32m0.8014\u001b[0m       0.2569            0.2569        5.9633  0.0006  0.2843\n",
      "      4            0.3250        \u001b[32m0.6970\u001b[0m       0.2535            0.2535        6.1742  0.0006  0.2799\n",
      "      5            0.3250        \u001b[32m0.6336\u001b[0m       0.2535            0.2535        6.0219  0.0006  0.2736\n",
      "      6            0.3250        \u001b[32m0.4593\u001b[0m       0.2535            0.2535        5.5512  0.0006  0.2798\n",
      "      7            0.3250        \u001b[32m0.4290\u001b[0m       0.2535            0.2535        \u001b[94m5.0370\u001b[0m  0.0006  0.2725\n",
      "      8            0.3500        \u001b[32m0.3562\u001b[0m       0.2569            0.2569        \u001b[94m4.4413\u001b[0m  0.0006  0.2840\n",
      "      9            0.4000        \u001b[32m0.2895\u001b[0m       0.2674            0.2674        \u001b[94m3.8559\u001b[0m  0.0006  0.2682\n",
      "     10            0.4500        \u001b[32m0.1841\u001b[0m       0.2708            0.2708        \u001b[94m3.4608\u001b[0m  0.0005  0.2646\n",
      "     11            \u001b[36m0.5000\u001b[0m        0.2022       0.2778            0.2778        \u001b[94m3.1086\u001b[0m  0.0005  0.2713\n",
      "     12            \u001b[36m0.6500\u001b[0m        0.2045       0.2917            0.2917        \u001b[94m2.8342\u001b[0m  0.0005  0.2775\n",
      "     13            \u001b[36m0.7750\u001b[0m        0.1946       0.2986            0.2986        \u001b[94m2.5974\u001b[0m  0.0005  0.2810\n",
      "     14            \u001b[36m0.8750\u001b[0m        \u001b[32m0.1229\u001b[0m       0.2986            0.2986        \u001b[94m2.3897\u001b[0m  0.0005  0.2663\n",
      "     15            \u001b[36m0.9750\u001b[0m        0.1257       0.3125            0.3125        \u001b[94m2.2225\u001b[0m  0.0004  0.2794\n",
      "     16            \u001b[36m1.0000\u001b[0m        0.1680       0.3125            0.3125        \u001b[94m2.0845\u001b[0m  0.0004  0.2683\n",
      "     17            1.0000        0.1421       0.3229            0.3229        \u001b[94m1.9791\u001b[0m  0.0004  0.2736\n",
      "     18            1.0000        \u001b[32m0.1198\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.8883\u001b[0m  0.0004  0.2639\n",
      "     19            1.0000        0.1219       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.8274\u001b[0m  0.0004  0.2909\n",
      "     20            1.0000        0.1210       0.3472            0.3472        \u001b[94m1.7906\u001b[0m  0.0003  0.2605\n",
      "     21            1.0000        \u001b[32m0.0885\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.7613\u001b[0m  0.0003  0.2665\n",
      "     22            1.0000        0.1135       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.7442\u001b[0m  0.0003  0.2708\n",
      "     23            1.0000        \u001b[32m0.0884\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.7324\u001b[0m  0.0002  0.2583\n",
      "     24            1.0000        \u001b[32m0.0809\u001b[0m       0.3750            0.3750        \u001b[94m1.7233\u001b[0m  0.0002  0.2682\n",
      "     25            1.0000        \u001b[32m0.0530\u001b[0m       0.3715            0.3715        \u001b[94m1.7182\u001b[0m  0.0002  0.2850\n",
      "     26            1.0000        0.0908       0.3681            0.3681        \u001b[94m1.7134\u001b[0m  0.0002  0.2659\n",
      "     27            1.0000        0.0690       0.3681            0.3681        \u001b[94m1.7089\u001b[0m  0.0002  0.2712\n",
      "     28            1.0000        0.0740       0.3681            0.3681        \u001b[94m1.7057\u001b[0m  0.0001  0.2841\n",
      "     29            1.0000        0.0616       0.3750            0.3750        \u001b[94m1.7037\u001b[0m  0.0001  0.2875\n",
      "     30            1.0000        0.0985       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.7020\u001b[0m  0.0001  0.2674\n",
      "     31            1.0000        0.0540       0.3785            0.3785        \u001b[94m1.7004\u001b[0m  0.0001  0.2814\n",
      "     32            1.0000        0.0656       0.3819            0.3819        \u001b[94m1.6982\u001b[0m  0.0001  0.2675\n",
      "     33            1.0000        0.0762       0.3785            0.3785        \u001b[94m1.6965\u001b[0m  0.0000  0.2846\n",
      "     34            1.0000        \u001b[32m0.0525\u001b[0m       0.3715            0.3715        \u001b[94m1.6954\u001b[0m  0.0000  0.2715\n",
      "     35            1.0000        0.0790       0.3681            0.3681        \u001b[94m1.6947\u001b[0m  0.0000  0.2663\n",
      "     36            1.0000        0.0639       0.3681            0.3681        \u001b[94m1.6943\u001b[0m  0.0000  0.2839\n",
      "     37            1.0000        0.0655       0.3681            0.3681        \u001b[94m1.6941\u001b[0m  0.0000  0.2963\n",
      "     38            1.0000        0.0721       0.3681            0.3681        \u001b[94m1.6936\u001b[0m  0.0000  0.3456\n",
      "     39            1.0000        0.0961       0.3646            0.3646        \u001b[94m1.6929\u001b[0m  0.0000  0.3464\n",
      "     40            1.0000        \u001b[32m0.0476\u001b[0m       0.3646            0.3646        \u001b[94m1.6927\u001b[0m  0.0000  0.3318\n",
      "Training model for subject 5 with 50 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2800\u001b[0m        \u001b[32m1.7265\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.6603\u001b[0m  0.0006  0.3480\n",
      "      2            0.2800        \u001b[32m1.1946\u001b[0m       0.2500            0.2500        4.0821  0.0006  0.3325\n",
      "      3            \u001b[36m0.3200\u001b[0m        \u001b[32m1.0769\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        4.4694  0.0006  0.3667\n",
      "      4            \u001b[36m0.3800\u001b[0m        \u001b[32m0.9286\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        4.6999  0.0006  0.3530\n",
      "      5            0.3800        \u001b[32m0.7152\u001b[0m       0.2569            0.2569        4.7082  0.0006  0.3358\n",
      "      6            0.3200        \u001b[32m0.4971\u001b[0m       0.2569            0.2569        4.5827  0.0006  0.3037\n",
      "      7            0.3600        0.5474       0.2569            0.2569        4.3741  0.0006  0.2763\n",
      "      8            0.3800        \u001b[32m0.4503\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        4.0089  0.0006  0.2909\n",
      "      9            \u001b[36m0.4200\u001b[0m        \u001b[32m0.3806\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m3.6271\u001b[0m  0.0006  0.3036\n",
      "     10            \u001b[36m0.4800\u001b[0m        \u001b[32m0.3567\u001b[0m       0.2674            0.2674        \u001b[94m3.2868\u001b[0m  0.0005  0.2901\n",
      "     11            \u001b[36m0.6000\u001b[0m        \u001b[32m0.3376\u001b[0m       0.2708            0.2708        \u001b[94m3.0011\u001b[0m  0.0005  0.2843\n",
      "     12            \u001b[36m0.7200\u001b[0m        \u001b[32m0.2847\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m2.7519\u001b[0m  0.0005  0.2767\n",
      "     13            \u001b[36m0.8000\u001b[0m        \u001b[32m0.2243\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m2.5437\u001b[0m  0.0005  0.2866\n",
      "     14            \u001b[36m0.8600\u001b[0m        \u001b[32m0.1995\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m2.3613\u001b[0m  0.0005  0.2906\n",
      "     15            \u001b[36m0.9200\u001b[0m        0.1997       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        \u001b[94m2.2189\u001b[0m  0.0004  0.2971\n",
      "     16            \u001b[36m0.9800\u001b[0m        \u001b[32m0.1920\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m2.1020\u001b[0m  0.0004  0.2863\n",
      "     17            0.9800        \u001b[32m0.1692\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m2.0168\u001b[0m  0.0004  0.2913\n",
      "     18            0.9800        \u001b[32m0.1382\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.9427\u001b[0m  0.0004  0.2839\n",
      "     19            \u001b[36m1.0000\u001b[0m        0.1832       0.3438            0.3438        \u001b[94m1.8859\u001b[0m  0.0004  0.4060\n",
      "     20            1.0000        0.1531       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.8401\u001b[0m  0.0003  0.2891\n",
      "     21            1.0000        \u001b[32m0.1382\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.8047\u001b[0m  0.0003  0.2809\n",
      "     22            1.0000        0.1520       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.7749\u001b[0m  0.0003  0.2933\n",
      "     23            1.0000        \u001b[32m0.1260\u001b[0m       0.3576            0.3576        \u001b[94m1.7504\u001b[0m  0.0002  0.2785\n",
      "     24            1.0000        \u001b[32m0.1128\u001b[0m       0.3507            0.3507        \u001b[94m1.7357\u001b[0m  0.0002  0.2915\n",
      "     25            1.0000        \u001b[32m0.0873\u001b[0m       0.3576            0.3576        \u001b[94m1.7275\u001b[0m  0.0002  0.2895\n",
      "     26            1.0000        0.1284       0.3576            0.3576        \u001b[94m1.7257\u001b[0m  0.0002  0.2854\n",
      "     27            1.0000        0.1070       0.3681            0.3681        \u001b[94m1.7246\u001b[0m  0.0002  0.2965\n",
      "     28            1.0000        0.1066       0.3576            0.3576        \u001b[94m1.7239\u001b[0m  0.0001  0.2839\n",
      "     29            1.0000        \u001b[32m0.0809\u001b[0m       0.3576            0.3576        \u001b[94m1.7224\u001b[0m  0.0001  0.2940\n",
      "     30            1.0000        0.0991       0.3576            0.3576        \u001b[94m1.7202\u001b[0m  0.0001  0.3042\n",
      "     31            1.0000        \u001b[32m0.0795\u001b[0m       0.3472            0.3472        \u001b[94m1.7183\u001b[0m  0.0001  0.3010\n",
      "     32            1.0000        0.0887       0.3472            0.3472        \u001b[94m1.7162\u001b[0m  0.0001  0.3005\n",
      "     33            1.0000        0.1137       0.3472            0.3472        \u001b[94m1.7149\u001b[0m  0.0000  0.2934\n",
      "     34            1.0000        0.0984       0.3438            0.3438        \u001b[94m1.7136\u001b[0m  0.0000  0.2955\n",
      "     35            1.0000        \u001b[32m0.0692\u001b[0m       0.3403            0.3403        \u001b[94m1.7125\u001b[0m  0.0000  0.2919\n",
      "     36            1.0000        0.0709       0.3403            0.3403        \u001b[94m1.7113\u001b[0m  0.0000  0.2943\n",
      "     37            1.0000        0.0894       0.3368            0.3368        \u001b[94m1.7102\u001b[0m  0.0000  0.2900\n",
      "     38            1.0000        \u001b[32m0.0636\u001b[0m       0.3368            0.3368        \u001b[94m1.7094\u001b[0m  0.0000  0.2855\n",
      "     39            1.0000        0.1212       0.3368            0.3368        \u001b[94m1.7089\u001b[0m  0.0000  0.2989\n",
      "     40            1.0000        0.0745       0.3368            0.3368        \u001b[94m1.7082\u001b[0m  0.0000  0.3182\n",
      "Training model for subject 5 with 60 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2333\u001b[0m        \u001b[32m1.6476\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.3764\u001b[0m  0.0006  0.3090\n",
      "      2            0.2333        \u001b[32m1.4071\u001b[0m       0.2500            0.2500        4.9987  0.0006  0.2883\n",
      "      3            0.2333        \u001b[32m1.1414\u001b[0m       0.2500            0.2500        4.5929  0.0006  0.2854\n",
      "      4            0.2333        \u001b[32m0.9104\u001b[0m       0.2500            0.2500        4.4498  0.0006  0.3411\n",
      "      5            \u001b[36m0.2500\u001b[0m        \u001b[32m0.7830\u001b[0m       0.2500            0.2500        \u001b[94m4.2241\u001b[0m  0.0006  0.3523\n",
      "      6            \u001b[36m0.2833\u001b[0m        \u001b[32m0.6248\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m3.9960\u001b[0m  0.0006  0.3638\n",
      "      7            \u001b[36m0.3167\u001b[0m        \u001b[32m0.5645\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m3.8102\u001b[0m  0.0006  0.3566\n",
      "      8            \u001b[36m0.3333\u001b[0m        \u001b[32m0.5531\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m3.5011\u001b[0m  0.0006  0.3772\n",
      "      9            \u001b[36m0.4167\u001b[0m        \u001b[32m0.5448\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m3.2028\u001b[0m  0.0006  0.3634\n",
      "     10            \u001b[36m0.4833\u001b[0m        \u001b[32m0.4616\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        \u001b[94m2.9361\u001b[0m  0.0005  0.3997\n",
      "     11            \u001b[36m0.5833\u001b[0m        \u001b[32m0.3746\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        \u001b[94m2.6222\u001b[0m  0.0005  0.3970\n",
      "     12            \u001b[36m0.6833\u001b[0m        \u001b[32m0.3464\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m2.3613\u001b[0m  0.0005  0.3037\n",
      "     13            \u001b[36m0.8000\u001b[0m        \u001b[32m0.2936\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        \u001b[94m2.1397\u001b[0m  0.0005  0.3112\n",
      "     14            \u001b[36m0.9167\u001b[0m        0.3143       0.3021            0.3021        \u001b[94m1.9807\u001b[0m  0.0005  0.4121\n",
      "     15            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2745\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m1.8553\u001b[0m  0.0004  0.2949\n",
      "     16            \u001b[36m0.9833\u001b[0m        \u001b[32m0.2543\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.7725\u001b[0m  0.0004  0.2878\n",
      "     17            0.9833        \u001b[32m0.2093\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.7153\u001b[0m  0.0004  0.3013\n",
      "     18            \u001b[36m1.0000\u001b[0m        0.2207       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.6747\u001b[0m  0.0004  0.3100\n",
      "     19            1.0000        0.2151       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.6465\u001b[0m  0.0004  0.2973\n",
      "     20            1.0000        0.2160       0.3715            0.3715        \u001b[94m1.6180\u001b[0m  0.0003  0.3014\n",
      "     21            1.0000        \u001b[32m0.1694\u001b[0m       0.3681            0.3681        \u001b[94m1.5920\u001b[0m  0.0003  0.2913\n",
      "     22            1.0000        0.1870       0.3681            0.3681        \u001b[94m1.5746\u001b[0m  0.0003  0.3165\n",
      "     23            1.0000        \u001b[32m0.1330\u001b[0m       0.3715            0.3715        \u001b[94m1.5627\u001b[0m  0.0002  0.3182\n",
      "     24            1.0000        0.1496       0.3611            0.3611        \u001b[94m1.5555\u001b[0m  0.0002  0.2867\n",
      "     25            1.0000        0.1346       0.3507            0.3507        \u001b[94m1.5501\u001b[0m  0.0002  0.3053\n",
      "     26            1.0000        0.1719       0.3472            0.3472        \u001b[94m1.5472\u001b[0m  0.0002  0.3005\n",
      "     27            1.0000        0.1502       0.3438            0.3438        1.5476  0.0002  0.3079\n",
      "     28            1.0000        \u001b[32m0.1128\u001b[0m       0.3472            0.3472        1.5473  0.0001  0.3088\n",
      "     29            1.0000        \u001b[32m0.1080\u001b[0m       0.3542            0.3542        1.5478  0.0001  0.2984\n",
      "     30            1.0000        0.1261       0.3472            0.3472        1.5479  0.0001  0.3076\n",
      "     31            1.0000        \u001b[32m0.1026\u001b[0m       0.3507            0.3507        1.5485  0.0001  0.3015\n",
      "     32            1.0000        0.1163       0.3472            0.3472        1.5488  0.0001  0.3086\n",
      "     33            1.0000        0.1616       0.3472            0.3472        1.5489  0.0000  0.3073\n",
      "     34            1.0000        0.1081       0.3438            0.3438        1.5487  0.0000  0.2953\n",
      "     35            1.0000        0.1133       0.3403            0.3403        1.5486  0.0000  0.2917\n",
      "     36            1.0000        0.1186       0.3403            0.3403        1.5486  0.0000  0.2996\n",
      "     37            1.0000        0.1600       0.3368            0.3368        1.5487  0.0000  0.2999\n",
      "     38            1.0000        0.1172       0.3368            0.3368        1.5486  0.0000  0.2959\n",
      "     39            1.0000        0.1112       0.3368            0.3368        1.5483  0.0000  0.3121\n",
      "     40            1.0000        0.1079       0.3368            0.3368        1.5484  0.0000  0.3095\n",
      "Training model for subject 5 with 70 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3714\u001b[0m        \u001b[32m1.7427\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m3.3194\u001b[0m  0.0006  0.3083\n",
      "      2            0.2714        \u001b[32m1.2474\u001b[0m       0.2535            0.2535        \u001b[94m2.8349\u001b[0m  0.0006  0.3174\n",
      "      3            0.2857        \u001b[32m1.0780\u001b[0m       0.2535            0.2535        3.0296  0.0006  0.3115\n",
      "      4            0.3000        \u001b[32m0.7960\u001b[0m       0.2569            0.2569        3.1542  0.0006  0.3047\n",
      "      5            0.3143        \u001b[32m0.7436\u001b[0m       0.2569            0.2569        3.2591  0.0006  0.3171\n",
      "      6            0.3286        \u001b[32m0.7188\u001b[0m       0.2569            0.2569        3.4400  0.0006  0.2974\n",
      "      7            0.3143        \u001b[32m0.6442\u001b[0m       0.2569            0.2569        3.5565  0.0006  0.3121\n",
      "      8            0.3286        \u001b[32m0.4420\u001b[0m       0.2604            0.2604        3.5793  0.0006  0.3858\n",
      "      9            0.3571        0.5561       0.2604            0.2604        3.4987  0.0006  0.3826\n",
      "     10            \u001b[36m0.4000\u001b[0m        \u001b[32m0.3481\u001b[0m       0.2639            0.2639        3.3192  0.0005  0.4018\n",
      "     11            \u001b[36m0.4714\u001b[0m        0.4378       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        3.1024  0.0005  0.3915\n",
      "     12            \u001b[36m0.5429\u001b[0m        \u001b[32m0.3133\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        2.9252  0.0005  0.3886\n",
      "     13            \u001b[36m0.6143\u001b[0m        0.3430       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m2.7277\u001b[0m  0.0005  0.3817\n",
      "     14            \u001b[36m0.7143\u001b[0m        0.3517       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m2.5212\u001b[0m  0.0005  0.4259\n",
      "     15            \u001b[36m0.8429\u001b[0m        \u001b[32m0.3087\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        \u001b[94m2.3462\u001b[0m  0.0004  0.3255\n",
      "     16            \u001b[36m0.9000\u001b[0m        \u001b[32m0.2804\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m2.1986\u001b[0m  0.0004  0.3562\n",
      "     17            \u001b[36m0.9286\u001b[0m        \u001b[32m0.2173\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m2.0863\u001b[0m  0.0004  0.3099\n",
      "     18            \u001b[36m0.9857\u001b[0m        0.3026       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.9927\u001b[0m  0.0004  0.3142\n",
      "     19            0.9857        \u001b[32m0.2029\u001b[0m       0.3542            0.3542        \u001b[94m1.9059\u001b[0m  0.0004  0.3166\n",
      "     20            \u001b[36m1.0000\u001b[0m        0.2168       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.8405\u001b[0m  0.0003  0.3178\n",
      "     21            1.0000        \u001b[32m0.1673\u001b[0m       0.3542            0.3542        \u001b[94m1.7882\u001b[0m  0.0003  0.3144\n",
      "     22            1.0000        0.1813       0.3611            0.3611        \u001b[94m1.7452\u001b[0m  0.0003  0.3092\n",
      "     23            1.0000        0.1679       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.7102\u001b[0m  0.0002  0.3263\n",
      "     24            1.0000        0.1703       0.3646            0.3646        \u001b[94m1.6845\u001b[0m  0.0002  0.3279\n",
      "     25            1.0000        \u001b[32m0.1563\u001b[0m       0.3611            0.3611        \u001b[94m1.6622\u001b[0m  0.0002  0.3249\n",
      "     26            1.0000        0.1677       0.3646            0.3646        \u001b[94m1.6466\u001b[0m  0.0002  0.3236\n",
      "     27            1.0000        0.1701       0.3611            0.3611        \u001b[94m1.6368\u001b[0m  0.0002  0.3328\n",
      "     28            1.0000        0.1704       0.3576            0.3576        \u001b[94m1.6321\u001b[0m  0.0001  0.3584\n",
      "     29            1.0000        \u001b[32m0.1383\u001b[0m       0.3646            0.3646        \u001b[94m1.6277\u001b[0m  0.0001  0.3310\n",
      "     30            1.0000        \u001b[32m0.1141\u001b[0m       0.3681            0.3681        \u001b[94m1.6255\u001b[0m  0.0001  0.3777\n",
      "     31            1.0000        0.1210       0.3681            0.3681        \u001b[94m1.6253\u001b[0m  0.0001  0.4738\n",
      "     32            1.0000        0.1325       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.6248\u001b[0m  0.0001  0.4834\n",
      "     33            1.0000        0.1206       0.3715            0.3715        1.6252  0.0000  0.5010\n",
      "     34            1.0000        0.1419       0.3715            0.3715        1.6250  0.0000  0.4839\n",
      "     35            1.0000        0.1270       0.3681            0.3681        \u001b[94m1.6248\u001b[0m  0.0000  0.4734\n",
      "     36            1.0000        0.1340       0.3681            0.3681        \u001b[94m1.6245\u001b[0m  0.0000  0.4099\n",
      "     37            1.0000        0.1344       0.3681            0.3681        \u001b[94m1.6242\u001b[0m  0.0000  0.4225\n",
      "     38            1.0000        0.1635       0.3681            0.3681        \u001b[94m1.6242\u001b[0m  0.0000  0.4150\n",
      "     39            1.0000        \u001b[32m0.0956\u001b[0m       0.3681            0.3681        \u001b[94m1.6238\u001b[0m  0.0000  0.3398\n",
      "     40            1.0000        0.1086       0.3646            0.3646        \u001b[94m1.6231\u001b[0m  0.0000  0.3823\n",
      "Training model for subject 5 with 80 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2750\u001b[0m        \u001b[32m1.6337\u001b[0m       \u001b[35m0.2465\u001b[0m            \u001b[31m0.2465\u001b[0m        \u001b[94m5.6391\u001b[0m  0.0006  0.5368\n",
      "      2            \u001b[36m0.3125\u001b[0m        \u001b[32m1.3764\u001b[0m       0.2431            0.2431        \u001b[94m4.8357\u001b[0m  0.0006  0.4213\n",
      "      3            0.2875        \u001b[32m1.0057\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m4.4136\u001b[0m  0.0006  0.4321\n",
      "      4            0.3000        \u001b[32m0.9633\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m3.7163\u001b[0m  0.0006  0.4789\n",
      "      5            0.3000        \u001b[32m0.8533\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m3.2292\u001b[0m  0.0006  0.4989\n",
      "      6            \u001b[36m0.3625\u001b[0m        0.8879       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m2.9179\u001b[0m  0.0006  0.5079\n",
      "      7            \u001b[36m0.4750\u001b[0m        \u001b[32m0.6826\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m2.7158\u001b[0m  0.0006  0.4671\n",
      "      8            \u001b[36m0.5125\u001b[0m        \u001b[32m0.5971\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m2.6095\u001b[0m  0.0006  0.5011\n",
      "      9            \u001b[36m0.5375\u001b[0m        \u001b[32m0.5522\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        \u001b[94m2.5869\u001b[0m  0.0006  0.4972\n",
      "     10            \u001b[36m0.5750\u001b[0m        \u001b[32m0.4449\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m2.5091\u001b[0m  0.0005  0.4922\n",
      "     11            \u001b[36m0.6625\u001b[0m        0.5224       0.3229            0.3229        \u001b[94m2.2894\u001b[0m  0.0005  0.4331\n",
      "     12            \u001b[36m0.7250\u001b[0m        0.4451       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m2.1391\u001b[0m  0.0005  0.4858\n",
      "     13            \u001b[36m0.8250\u001b[0m        \u001b[32m0.4050\u001b[0m       0.3299            0.3299        \u001b[94m1.9944\u001b[0m  0.0005  0.4432\n",
      "     14            \u001b[36m0.8875\u001b[0m        \u001b[32m0.3476\u001b[0m       0.3264            0.3264        \u001b[94m1.9028\u001b[0m  0.0005  0.4259\n",
      "     15            \u001b[36m0.9125\u001b[0m        \u001b[32m0.3294\u001b[0m       0.3229            0.3229        \u001b[94m1.8451\u001b[0m  0.0004  0.4470\n",
      "     16            \u001b[36m0.9500\u001b[0m        \u001b[32m0.3019\u001b[0m       0.3264            0.3264        \u001b[94m1.8078\u001b[0m  0.0004  0.4580\n",
      "     17            \u001b[36m0.9625\u001b[0m        \u001b[32m0.2980\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m1.7876\u001b[0m  0.0004  0.4364\n",
      "     18            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2498\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.7738\u001b[0m  0.0004  0.4708\n",
      "     19            \u001b[36m0.9875\u001b[0m        0.2619       0.3368            0.3368        \u001b[94m1.7469\u001b[0m  0.0004  0.4328\n",
      "     20            0.9875        \u001b[32m0.2214\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.7028\u001b[0m  0.0003  0.4319\n",
      "     21            0.9875        0.2654       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.6688\u001b[0m  0.0003  0.4645\n",
      "     22            0.9875        0.2311       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.6393\u001b[0m  0.0003  0.4669\n",
      "     23            \u001b[36m1.0000\u001b[0m        0.2284       0.3715            0.3715        \u001b[94m1.6237\u001b[0m  0.0002  0.4117\n",
      "     24            1.0000        \u001b[32m0.2131\u001b[0m       0.3611            0.3611        \u001b[94m1.6049\u001b[0m  0.0002  0.4413\n",
      "     25            1.0000        0.2230       0.3611            0.3611        \u001b[94m1.5895\u001b[0m  0.0002  0.4368\n",
      "     26            1.0000        \u001b[32m0.2049\u001b[0m       0.3576            0.3576        \u001b[94m1.5785\u001b[0m  0.0002  0.4227\n",
      "     27            1.0000        0.2076       0.3681            0.3681        \u001b[94m1.5691\u001b[0m  0.0002  0.4276\n",
      "     28            1.0000        0.2188       0.3715            0.3715        \u001b[94m1.5594\u001b[0m  0.0001  0.4385\n",
      "     29            1.0000        0.2087       0.3681            0.3681        \u001b[94m1.5508\u001b[0m  0.0001  0.4329\n",
      "     30            1.0000        \u001b[32m0.1696\u001b[0m       0.3611            0.3611        \u001b[94m1.5448\u001b[0m  0.0001  0.4447\n",
      "     31            1.0000        0.1814       0.3715            0.3715        \u001b[94m1.5395\u001b[0m  0.0001  0.4474\n",
      "     32            1.0000        \u001b[32m0.1590\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.5361\u001b[0m  0.0001  0.4117\n",
      "     33            1.0000        \u001b[32m0.1451\u001b[0m       0.3750            0.3750        \u001b[94m1.5328\u001b[0m  0.0000  0.4467\n",
      "     34            1.0000        0.1913       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.5298\u001b[0m  0.0000  0.4882\n",
      "     35            1.0000        0.1507       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.5276\u001b[0m  0.0000  0.4905\n",
      "     36            1.0000        \u001b[32m0.1329\u001b[0m       0.3854            0.3854        \u001b[94m1.5254\u001b[0m  0.0000  0.4976\n",
      "     37            1.0000        0.1986       0.3854            0.3854        \u001b[94m1.5233\u001b[0m  0.0000  0.4142\n",
      "     38            1.0000        0.1860       0.3854            0.3854        \u001b[94m1.5214\u001b[0m  0.0000  0.3944\n",
      "     39            1.0000        0.2185       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.5196\u001b[0m  0.0000  0.3926\n",
      "     40            1.0000        0.1764       0.3889            0.3889        \u001b[94m1.5180\u001b[0m  0.0000  0.4170\n",
      "Training model for subject 5 with 90 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3556\u001b[0m        \u001b[32m1.6809\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m2.5112\u001b[0m  0.0006  0.5062\n",
      "      2            0.3222        \u001b[32m1.4039\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        2.8296  0.0006  0.3611\n",
      "      3            0.3000        \u001b[32m1.1300\u001b[0m       0.2569            0.2569        2.9518  0.0006  0.3419\n",
      "      4            0.3111        \u001b[32m1.0454\u001b[0m       0.2604            0.2604        2.8571  0.0006  0.3464\n",
      "      5            0.3111        \u001b[32m0.8922\u001b[0m       0.2569            0.2569        2.7134  0.0006  0.3355\n",
      "      6            0.3556        \u001b[32m0.7439\u001b[0m       0.2604            0.2604        \u001b[94m2.4673\u001b[0m  0.0006  0.3374\n",
      "      7            \u001b[36m0.4667\u001b[0m        \u001b[32m0.7266\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        \u001b[94m2.1160\u001b[0m  0.0006  0.3828\n",
      "      8            \u001b[36m0.6889\u001b[0m        0.7532       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m1.8140\u001b[0m  0.0006  0.3731\n",
      "      9            \u001b[36m0.8111\u001b[0m        \u001b[32m0.5837\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m1.6472\u001b[0m  0.0006  0.3413\n",
      "     10            \u001b[36m0.8778\u001b[0m        \u001b[32m0.5505\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.5656\u001b[0m  0.0005  0.3643\n",
      "     11            \u001b[36m0.8889\u001b[0m        0.5649       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.5630\u001b[0m  0.0005  0.3505\n",
      "     12            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4465\u001b[0m       0.3576            0.3576        1.5803  0.0005  0.3704\n",
      "     13            0.8889        \u001b[32m0.4303\u001b[0m       0.3646            0.3646        1.6066  0.0005  0.3571\n",
      "     14            0.8889        0.4307       0.3715            0.3715        1.6290  0.0005  0.3378\n",
      "     15            0.9000        \u001b[32m0.3845\u001b[0m       0.3715            0.3715        1.6304  0.0004  0.3395\n",
      "     16            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3602\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        1.6128  0.0004  0.3518\n",
      "     17            \u001b[36m0.9556\u001b[0m        \u001b[32m0.3421\u001b[0m       0.3715            0.3715        1.5926  0.0004  0.3577\n",
      "     18            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3110\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        1.5747  0.0004  0.3443\n",
      "     19            1.0000        \u001b[32m0.2987\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.5517\u001b[0m  0.0004  0.3414\n",
      "     20            1.0000        \u001b[32m0.2627\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.5388\u001b[0m  0.0003  0.3475\n",
      "     21            1.0000        0.3141       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.5290\u001b[0m  0.0003  0.3340\n",
      "     22            1.0000        \u001b[32m0.2535\u001b[0m       0.4028            0.4028        \u001b[94m1.5218\u001b[0m  0.0003  0.3413\n",
      "     23            1.0000        \u001b[32m0.2151\u001b[0m       0.3993            0.3993        \u001b[94m1.5138\u001b[0m  0.0002  0.3493\n",
      "     24            1.0000        0.2513       0.3993            0.3993        \u001b[94m1.5065\u001b[0m  0.0002  0.3675\n",
      "     25            1.0000        \u001b[32m0.1824\u001b[0m       0.3924            0.3924        \u001b[94m1.4999\u001b[0m  0.0002  0.3479\n",
      "     26            1.0000        0.2015       0.4028            0.4028        \u001b[94m1.4955\u001b[0m  0.0002  0.3536\n",
      "     27            1.0000        0.2379       0.3993            0.3993        \u001b[94m1.4940\u001b[0m  0.0002  0.3527\n",
      "     28            1.0000        0.1835       0.3993            0.3993        \u001b[94m1.4917\u001b[0m  0.0001  0.3339\n",
      "     29            1.0000        0.2252       0.3993            0.3993        \u001b[94m1.4890\u001b[0m  0.0001  0.3513\n",
      "     30            1.0000        \u001b[32m0.1707\u001b[0m       0.3993            0.3993        \u001b[94m1.4859\u001b[0m  0.0001  0.3441\n",
      "     31            1.0000        0.2427       0.3993            0.3993        \u001b[94m1.4834\u001b[0m  0.0001  0.3854\n",
      "     32            1.0000        0.1725       0.3993            0.3993        \u001b[94m1.4817\u001b[0m  0.0001  0.4330\n",
      "     33            1.0000        0.1763       0.3993            0.3993        \u001b[94m1.4808\u001b[0m  0.0000  0.4479\n",
      "     34            1.0000        0.1909       0.3993            0.3993        \u001b[94m1.4796\u001b[0m  0.0000  0.4163\n",
      "     35            1.0000        0.2325       0.3993            0.3993        \u001b[94m1.4787\u001b[0m  0.0000  0.4192\n",
      "     36            1.0000        0.2128       0.3993            0.3993        \u001b[94m1.4779\u001b[0m  0.0000  0.4358\n",
      "     37            1.0000        0.2002       0.4028            0.4028        \u001b[94m1.4771\u001b[0m  0.0000  0.5094\n",
      "     38            1.0000        0.2031       0.4028            0.4028        \u001b[94m1.4763\u001b[0m  0.0000  0.3454\n",
      "     39            1.0000        0.2090       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.4755\u001b[0m  0.0000  0.3752\n",
      "     40            1.0000        0.2023       0.4028            0.4028        \u001b[94m1.4749\u001b[0m  0.0000  0.3798\n",
      "Training model for subject 5 with 100 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2400\u001b[0m        \u001b[32m1.5035\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.0689\u001b[0m  0.0006  0.3752\n",
      "      2            \u001b[36m0.2800\u001b[0m        \u001b[32m1.3462\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m2.6913\u001b[0m  0.0006  0.3559\n",
      "      3            \u001b[36m0.4200\u001b[0m        \u001b[32m1.0645\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m2.3808\u001b[0m  0.0006  0.3616\n",
      "      4            \u001b[36m0.5000\u001b[0m        \u001b[32m1.0146\u001b[0m       0.3090            0.3090        \u001b[94m2.3635\u001b[0m  0.0006  0.3723\n",
      "      5            0.4600        \u001b[32m0.8844\u001b[0m       0.2986            0.2986        \u001b[94m2.3619\u001b[0m  0.0006  0.3566\n",
      "      6            0.4100        0.8857       0.2882            0.2882        2.4081  0.0006  0.3692\n",
      "      7            0.4600        \u001b[32m0.7321\u001b[0m       0.3090            0.3090        2.3713  0.0006  0.3677\n",
      "      8            \u001b[36m0.5700\u001b[0m        \u001b[32m0.5681\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m2.2392\u001b[0m  0.0006  0.3561\n",
      "      9            \u001b[36m0.6800\u001b[0m        \u001b[32m0.5594\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m2.0611\u001b[0m  0.0006  0.3715\n",
      "     10            \u001b[36m0.7600\u001b[0m        0.6067       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.9184\u001b[0m  0.0005  0.3634\n",
      "     11            \u001b[36m0.8000\u001b[0m        \u001b[32m0.5279\u001b[0m       0.3542            0.3542        \u001b[94m1.8344\u001b[0m  0.0005  0.3564\n",
      "     12            \u001b[36m0.8600\u001b[0m        \u001b[32m0.5147\u001b[0m       0.3507            0.3507        \u001b[94m1.7564\u001b[0m  0.0005  0.3575\n",
      "     13            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3895\u001b[0m       0.3299            0.3299        \u001b[94m1.6890\u001b[0m  0.0005  0.3551\n",
      "     14            \u001b[36m0.9300\u001b[0m        0.4087       0.3333            0.3333        \u001b[94m1.6284\u001b[0m  0.0005  0.3602\n",
      "     15            \u001b[36m0.9700\u001b[0m        0.4271       0.3507            0.3507        \u001b[94m1.5656\u001b[0m  0.0004  0.3661\n",
      "     16            0.9700        \u001b[32m0.3529\u001b[0m       0.3472            0.3472        \u001b[94m1.5230\u001b[0m  0.0004  0.3624\n",
      "     17            \u001b[36m0.9800\u001b[0m        \u001b[32m0.3478\u001b[0m       0.3576            0.3576        \u001b[94m1.4896\u001b[0m  0.0004  0.3512\n",
      "     18            \u001b[36m0.9900\u001b[0m        \u001b[32m0.3467\u001b[0m       0.3576            0.3576        \u001b[94m1.4682\u001b[0m  0.0004  0.3694\n",
      "     19            0.9900        \u001b[32m0.3290\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.4621\u001b[0m  0.0004  0.3618\n",
      "     20            0.9900        \u001b[32m0.3171\u001b[0m       0.3646            0.3646        1.4631  0.0003  0.3737\n",
      "     21            0.9900        \u001b[32m0.2685\u001b[0m       0.3611            0.3611        1.4690  0.0003  0.4646\n",
      "     22            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2233\u001b[0m       0.3646            0.3646        1.4709  0.0003  0.3684\n",
      "     23            1.0000        \u001b[32m0.2194\u001b[0m       0.3681            0.3681        1.4737  0.0002  0.3591\n",
      "     24            1.0000        0.2421       0.3750            0.3750        1.4750  0.0002  0.3544\n",
      "     25            1.0000        0.2234       0.3715            0.3715        1.4757  0.0002  0.3706\n",
      "     26            1.0000        0.2322       0.3681            0.3681        1.4760  0.0002  0.3668\n",
      "     27            1.0000        0.2644       0.3715            0.3715        1.4764  0.0002  0.4193\n",
      "     28            1.0000        0.2194       0.3750            0.3750        1.4786  0.0001  0.3899\n",
      "     29            1.0000        \u001b[32m0.1628\u001b[0m       0.3785            0.3785        1.4789  0.0001  0.4552\n",
      "     30            1.0000        0.1879       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        1.4779  0.0001  0.4222\n",
      "     31            1.0000        0.2114       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        1.4760  0.0001  0.4393\n",
      "     32            1.0000        0.1675       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        1.4745  0.0001  0.4284\n",
      "     33            1.0000        0.2059       0.3924            0.3924        1.4730  0.0000  0.4407\n",
      "     34            1.0000        0.2089       0.3924            0.3924        1.4728  0.0000  0.3802\n",
      "     35            1.0000        0.1896       0.3924            0.3924        1.4728  0.0000  0.3717\n",
      "     36            1.0000        0.1972       0.3924            0.3924        1.4730  0.0000  0.3683\n",
      "     37            1.0000        0.1695       0.3924            0.3924        1.4732  0.0000  0.3549\n",
      "     38            1.0000        0.1892       0.3924            0.3924        1.4732  0.0000  0.3680\n",
      "     39            1.0000        \u001b[32m0.1626\u001b[0m       0.3889            0.3889        1.4732  0.0000  0.3620\n",
      "     40            1.0000        0.1914       0.3889            0.3889        1.4733  0.0000  0.3774\n",
      "Training model for subject 5 with 110 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2545\u001b[0m        \u001b[32m1.8706\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m8.8027\u001b[0m  0.0006  0.3754\n",
      "      2            0.2545        \u001b[32m1.4842\u001b[0m       0.2500            0.2500        \u001b[94m5.8329\u001b[0m  0.0006  0.3546\n",
      "      3            0.2545        \u001b[32m1.2005\u001b[0m       0.2500            0.2500        \u001b[94m3.7659\u001b[0m  0.0006  0.3799\n",
      "      4            \u001b[36m0.3455\u001b[0m        \u001b[32m1.0124\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m2.3848\u001b[0m  0.0006  0.3704\n",
      "      5            \u001b[36m0.4182\u001b[0m        \u001b[32m0.9196\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        \u001b[94m1.9808\u001b[0m  0.0006  0.3731\n",
      "      6            0.4000        \u001b[32m0.8294\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        2.1112  0.0006  0.3702\n",
      "      7            \u001b[36m0.4364\u001b[0m        0.9247       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        2.1653  0.0006  0.3762\n",
      "      8            0.4273        \u001b[32m0.7609\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        2.3098  0.0006  0.3677\n",
      "      9            \u001b[36m0.4636\u001b[0m        0.7719       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        2.2097  0.0006  0.3805\n",
      "     10            \u001b[36m0.5000\u001b[0m        \u001b[32m0.6919\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        2.1487  0.0005  0.3701\n",
      "     11            \u001b[36m0.5545\u001b[0m        \u001b[32m0.5462\u001b[0m       0.3507            0.3507        2.0834  0.0005  0.3718\n",
      "     12            \u001b[36m0.6182\u001b[0m        0.5617       0.3507            0.3507        2.0136  0.0005  0.3820\n",
      "     13            \u001b[36m0.6364\u001b[0m        \u001b[32m0.5443\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.9784\u001b[0m  0.0005  0.3842\n",
      "     14            \u001b[36m0.6545\u001b[0m        \u001b[32m0.5101\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.9482\u001b[0m  0.0005  0.3700\n",
      "     15            \u001b[36m0.7364\u001b[0m        \u001b[32m0.4368\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.8855\u001b[0m  0.0004  0.3724\n",
      "     16            \u001b[36m0.7727\u001b[0m        0.4725       0.3750            0.3750        \u001b[94m1.8307\u001b[0m  0.0004  0.3818\n",
      "     17            \u001b[36m0.8182\u001b[0m        0.4959       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.8044\u001b[0m  0.0004  0.3756\n",
      "     18            \u001b[36m0.8636\u001b[0m        \u001b[32m0.4171\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.7686\u001b[0m  0.0004  0.3805\n",
      "     19            \u001b[36m0.9091\u001b[0m        \u001b[32m0.3756\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.7161\u001b[0m  0.0004  0.3647\n",
      "     20            \u001b[36m0.9455\u001b[0m        \u001b[32m0.3257\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.6810\u001b[0m  0.0003  0.3740\n",
      "     21            \u001b[36m0.9636\u001b[0m        \u001b[32m0.3252\u001b[0m       0.4028            0.4028        \u001b[94m1.6543\u001b[0m  0.0003  0.3653\n",
      "     22            \u001b[36m0.9727\u001b[0m        0.3846       0.3993            0.3993        \u001b[94m1.6376\u001b[0m  0.0003  0.4316\n",
      "     23            \u001b[36m0.9818\u001b[0m        0.3488       0.3993            0.3993        \u001b[94m1.6254\u001b[0m  0.0002  0.4253\n",
      "     24            \u001b[36m0.9909\u001b[0m        \u001b[32m0.2997\u001b[0m       0.3993            0.3993        \u001b[94m1.6169\u001b[0m  0.0002  0.4332\n",
      "     25            0.9909        \u001b[32m0.2801\u001b[0m       0.4028            0.4028        \u001b[94m1.6119\u001b[0m  0.0002  0.4452\n",
      "     26            0.9909        0.3738       0.4028            0.4028        \u001b[94m1.5998\u001b[0m  0.0002  0.4307\n",
      "     27            0.9909        \u001b[32m0.2726\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.5878\u001b[0m  0.0002  0.4838\n",
      "     28            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2464\u001b[0m       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.5771\u001b[0m  0.0001  0.4506\n",
      "     29            1.0000        0.2626       0.4167            0.4167        \u001b[94m1.5665\u001b[0m  0.0001  0.3812\n",
      "     30            1.0000        0.2589       0.4132            0.4132        \u001b[94m1.5595\u001b[0m  0.0001  0.4902\n",
      "     31            1.0000        0.2739       0.4062            0.4062        \u001b[94m1.5535\u001b[0m  0.0001  0.3636\n",
      "     32            1.0000        0.2686       0.4062            0.4062        \u001b[94m1.5484\u001b[0m  0.0001  0.3760\n",
      "     33            1.0000        0.2695       0.4167            0.4167        \u001b[94m1.5442\u001b[0m  0.0000  0.3728\n",
      "     34            1.0000        0.2465       0.4167            0.4167        \u001b[94m1.5397\u001b[0m  0.0000  0.3698\n",
      "     35            1.0000        \u001b[32m0.2286\u001b[0m       0.4132            0.4132        \u001b[94m1.5354\u001b[0m  0.0000  0.3701\n",
      "     36            1.0000        0.2460       0.4132            0.4132        \u001b[94m1.5316\u001b[0m  0.0000  0.3816\n",
      "     37            1.0000        0.2596       0.4167            0.4167        \u001b[94m1.5288\u001b[0m  0.0000  0.3728\n",
      "     38            1.0000        0.2704       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.5266\u001b[0m  0.0000  0.3733\n",
      "     39            1.0000        0.2670       0.4167            0.4167        \u001b[94m1.5248\u001b[0m  0.0000  0.3822\n",
      "     40            1.0000        0.2537       0.4201            0.4201        \u001b[94m1.5235\u001b[0m  0.0000  0.3821\n",
      "Training model for subject 5 with 120 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.5901\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.9560\u001b[0m  0.0006  0.3924\n",
      "      2            \u001b[36m0.3083\u001b[0m        \u001b[32m1.2627\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m2.5374\u001b[0m  0.0006  0.3870\n",
      "      3            \u001b[36m0.3583\u001b[0m        \u001b[32m1.1814\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m1.8986\u001b[0m  0.0006  0.3950\n",
      "      4            \u001b[36m0.5500\u001b[0m        \u001b[32m1.0302\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m1.7206\u001b[0m  0.0006  0.3882\n",
      "      5            \u001b[36m0.5750\u001b[0m        \u001b[32m0.9265\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        1.7458  0.0006  0.3924\n",
      "      6            \u001b[36m0.6250\u001b[0m        \u001b[32m0.8201\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.6862\u001b[0m  0.0006  0.3779\n",
      "      7            \u001b[36m0.8000\u001b[0m        \u001b[32m0.8003\u001b[0m       0.3472            0.3472        \u001b[94m1.5694\u001b[0m  0.0006  0.3689\n",
      "      8            \u001b[36m0.8083\u001b[0m        \u001b[32m0.7388\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.5242\u001b[0m  0.0006  0.3945\n",
      "      9            \u001b[36m0.8500\u001b[0m        \u001b[32m0.6886\u001b[0m       0.3472            0.3472        \u001b[94m1.4986\u001b[0m  0.0006  0.3891\n",
      "     10            \u001b[36m0.9083\u001b[0m        \u001b[32m0.5929\u001b[0m       0.3542            0.3542        1.5006  0.0005  0.4032\n",
      "     11            \u001b[36m0.9333\u001b[0m        0.5944       0.3507            0.3507        1.5133  0.0005  0.3886\n",
      "     12            0.9333        0.6385       0.3611            0.3611        1.5200  0.0005  0.3839\n",
      "     13            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5179\u001b[0m       0.3611            0.3611        1.5145  0.0005  0.3820\n",
      "     14            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4963\u001b[0m       0.3542            0.3542        1.5207  0.0005  0.3968\n",
      "     15            0.9667        \u001b[32m0.4891\u001b[0m       0.3611            0.3611        1.5288  0.0004  0.4582\n",
      "     16            \u001b[36m0.9750\u001b[0m        \u001b[32m0.4252\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        1.5374  0.0004  0.4901\n",
      "     17            \u001b[36m0.9833\u001b[0m        \u001b[32m0.4209\u001b[0m       0.3611            0.3611        1.5417  0.0004  0.4346\n",
      "     18            0.9833        \u001b[32m0.3902\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        1.5576  0.0004  0.4791\n",
      "     19            \u001b[36m0.9917\u001b[0m        \u001b[32m0.3600\u001b[0m       0.3542            0.3542        1.5658  0.0004  0.4416\n",
      "     20            0.9917        \u001b[32m0.3349\u001b[0m       0.3507            0.3507        1.5708  0.0003  0.4551\n",
      "     21            \u001b[36m1.0000\u001b[0m        0.3438       0.3542            0.3542        1.5674  0.0003  0.4634\n",
      "     22            1.0000        \u001b[32m0.3249\u001b[0m       0.3646            0.3646        1.5649  0.0003  0.4062\n",
      "     23            1.0000        \u001b[32m0.2737\u001b[0m       0.3646            0.3646        1.5630  0.0002  0.3943\n",
      "     24            1.0000        0.3086       0.3646            0.3646        1.5577  0.0002  0.3910\n",
      "     25            1.0000        0.3237       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        1.5532  0.0002  0.3861\n",
      "     26            1.0000        0.2883       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        1.5517  0.0002  0.3936\n",
      "     27            1.0000        0.2755       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        1.5512  0.0002  0.4025\n",
      "     28            1.0000        0.2756       0.3854            0.3854        1.5491  0.0001  0.3864\n",
      "     29            1.0000        \u001b[32m0.2642\u001b[0m       0.3854            0.3854        1.5463  0.0001  0.3896\n",
      "     30            1.0000        \u001b[32m0.2309\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        1.5438  0.0001  0.4011\n",
      "     31            1.0000        0.2695       0.3854            0.3854        1.5411  0.0001  0.3769\n",
      "     32            1.0000        \u001b[32m0.2209\u001b[0m       0.3854            0.3854        1.5380  0.0001  0.3710\n",
      "     33            1.0000        0.2574       0.3854            0.3854        1.5348  0.0000  0.3743\n",
      "     34            1.0000        0.2247       0.3854            0.3854        1.5322  0.0000  0.4046\n",
      "     35            1.0000        0.2526       0.3889            0.3889        1.5298  0.0000  0.3832\n",
      "     36            1.0000        0.2503       0.3889            0.3889        1.5277  0.0000  0.4943\n",
      "     37            1.0000        \u001b[32m0.2113\u001b[0m       0.3854            0.3854        1.5258  0.0000  0.3884\n",
      "     38            1.0000        0.2346       0.3819            0.3819        1.5242  0.0000  0.3973\n",
      "     39            1.0000        0.2640       0.3750            0.3750        1.5229  0.0000  0.3909\n",
      "     40            1.0000        0.2515       0.3715            0.3715        1.5217  0.0000  0.4001\n",
      "Training model for subject 5 with 130 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2308\u001b[0m        \u001b[32m1.5922\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.1755\u001b[0m  0.0006  0.4143\n",
      "      2            \u001b[36m0.3769\u001b[0m        \u001b[32m1.3093\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m1.6968\u001b[0m  0.0006  0.4045\n",
      "      3            \u001b[36m0.4538\u001b[0m        \u001b[32m1.1443\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        1.7331  0.0006  0.3810\n",
      "      4            \u001b[36m0.4769\u001b[0m        \u001b[32m0.9474\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        1.8919  0.0006  0.4104\n",
      "      5            \u001b[36m0.4846\u001b[0m        \u001b[32m0.9220\u001b[0m       0.3750            0.3750        2.0337  0.0006  0.4161\n",
      "      6            \u001b[36m0.5077\u001b[0m        \u001b[32m0.8257\u001b[0m       0.3611            0.3611        1.9377  0.0006  0.3943\n",
      "      7            \u001b[36m0.5923\u001b[0m        \u001b[32m0.7984\u001b[0m       0.3576            0.3576        1.8516  0.0006  0.4388\n",
      "      8            \u001b[36m0.6538\u001b[0m        \u001b[32m0.6901\u001b[0m       0.3611            0.3611        1.7264  0.0006  0.4530\n",
      "      9            \u001b[36m0.7154\u001b[0m        0.6964       0.3542            0.3542        1.6975  0.0006  0.4814\n",
      "     10            \u001b[36m0.7385\u001b[0m        \u001b[32m0.6365\u001b[0m       0.3715            0.3715        \u001b[94m1.6845\u001b[0m  0.0005  0.4664\n",
      "     11            \u001b[36m0.8077\u001b[0m        \u001b[32m0.6085\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.6820\u001b[0m  0.0005  0.4888\n",
      "     12            \u001b[36m0.8231\u001b[0m        \u001b[32m0.5084\u001b[0m       0.3681            0.3681        \u001b[94m1.6755\u001b[0m  0.0005  0.5173\n",
      "     13            0.8231        \u001b[32m0.4747\u001b[0m       0.3611            0.3611        \u001b[94m1.6729\u001b[0m  0.0005  0.4687\n",
      "     14            \u001b[36m0.8615\u001b[0m        0.5076       0.3715            0.3715        \u001b[94m1.6650\u001b[0m  0.0005  0.4171\n",
      "     15            \u001b[36m0.9077\u001b[0m        \u001b[32m0.4462\u001b[0m       0.3819            0.3819        \u001b[94m1.6431\u001b[0m  0.0004  0.4026\n",
      "     16            \u001b[36m0.9308\u001b[0m        0.4751       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.6140\u001b[0m  0.0004  0.4058\n",
      "     17            \u001b[36m0.9385\u001b[0m        \u001b[32m0.3853\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.5729\u001b[0m  0.0004  0.3984\n",
      "     18            \u001b[36m0.9462\u001b[0m        0.4510       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.5326\u001b[0m  0.0004  0.3919\n",
      "     19            \u001b[36m0.9615\u001b[0m        \u001b[32m0.3840\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.4949\u001b[0m  0.0004  0.3933\n",
      "     20            \u001b[36m0.9923\u001b[0m        \u001b[32m0.3810\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.4609\u001b[0m  0.0003  0.3837\n",
      "     21            0.9846        \u001b[32m0.3155\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.4341\u001b[0m  0.0003  0.3949\n",
      "     22            0.9846        0.3723       0.4167            0.4167        \u001b[94m1.4178\u001b[0m  0.0003  0.3887\n",
      "     23            0.9846        0.3458       0.4132            0.4132        \u001b[94m1.4091\u001b[0m  0.0002  0.4130\n",
      "     24            0.9923        0.3325       0.4097            0.4097        \u001b[94m1.4083\u001b[0m  0.0002  0.4047\n",
      "     25            0.9923        \u001b[32m0.2735\u001b[0m       0.4201            0.4201        \u001b[94m1.4069\u001b[0m  0.0002  0.4165\n",
      "     26            0.9923        0.3351       0.4167            0.4167        \u001b[94m1.4045\u001b[0m  0.0002  0.4155\n",
      "     27            0.9923        0.3343       0.4201            0.4201        \u001b[94m1.4038\u001b[0m  0.0002  0.3861\n",
      "     28            0.9923        0.2978       0.4167            0.4167        1.4046  0.0001  0.3960\n",
      "     29            0.9923        \u001b[32m0.2187\u001b[0m       0.4132            0.4132        1.4051  0.0001  0.4090\n",
      "     30            0.9923        0.2477       0.4167            0.4167        1.4063  0.0001  0.4251\n",
      "     31            0.9923        0.2799       0.4167            0.4167        1.4057  0.0001  0.4035\n",
      "     32            0.9923        0.2270       0.4167            0.4167        1.4053  0.0001  0.3923\n",
      "     33            0.9923        0.2542       0.4167            0.4167        1.4046  0.0000  0.3970\n",
      "     34            0.9923        0.2580       0.4167            0.4167        \u001b[94m1.4035\u001b[0m  0.0000  0.4115\n",
      "     35            0.9923        0.2703       0.4167            0.4167        \u001b[94m1.4018\u001b[0m  0.0000  0.4025\n",
      "     36            0.9923        0.2737       0.4167            0.4167        \u001b[94m1.4009\u001b[0m  0.0000  0.3966\n",
      "     37            0.9923        0.2633       0.4167            0.4167        \u001b[94m1.3999\u001b[0m  0.0000  0.3949\n",
      "     38            0.9923        0.2822       0.4201            0.4201        \u001b[94m1.3988\u001b[0m  0.0000  0.4088\n",
      "     39            0.9923        0.3134       0.4201            0.4201        \u001b[94m1.3982\u001b[0m  0.0000  0.4474\n",
      "     40            0.9923        0.2523       0.4201            0.4201        \u001b[94m1.3977\u001b[0m  0.0000  0.4647\n",
      "Training model for subject 5 with 140 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.4571\u001b[0m        \u001b[32m1.6381\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m1.5718\u001b[0m  0.0006  0.4609\n",
      "      2            \u001b[36m0.4857\u001b[0m        \u001b[32m1.2252\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.4553\u001b[0m  0.0006  0.4657\n",
      "      3            \u001b[36m0.5786\u001b[0m        \u001b[32m1.0919\u001b[0m       0.3507            0.3507        \u001b[94m1.4246\u001b[0m  0.0006  0.4593\n",
      "      4            \u001b[36m0.6571\u001b[0m        \u001b[32m0.9792\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        1.6076  0.0006  0.4683\n",
      "      5            0.5071        \u001b[32m0.8908\u001b[0m       0.3854            0.3854        1.9263  0.0006  0.3951\n",
      "      6            0.5000        \u001b[32m0.8232\u001b[0m       0.3785            0.3785        2.0644  0.0006  0.5178\n",
      "      7            0.5143        \u001b[32m0.7525\u001b[0m       0.3715            0.3715        2.0628  0.0006  0.3901\n",
      "      8            0.5500        \u001b[32m0.6932\u001b[0m       0.3681            0.3681        2.0089  0.0006  0.4141\n",
      "      9            0.6214        \u001b[32m0.6372\u001b[0m       0.3715            0.3715        1.8706  0.0006  0.4085\n",
      "     10            \u001b[36m0.7429\u001b[0m        0.6821       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        1.6936  0.0005  0.3901\n",
      "     11            \u001b[36m0.8429\u001b[0m        \u001b[32m0.5148\u001b[0m       0.3819            0.3819        1.6174  0.0005  0.3921\n",
      "     12            \u001b[36m0.8857\u001b[0m        \u001b[32m0.4864\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        1.5514  0.0005  0.3988\n",
      "     13            \u001b[36m0.8929\u001b[0m        \u001b[32m0.4460\u001b[0m       0.3854            0.3854        1.5200  0.0005  0.4139\n",
      "     14            \u001b[36m0.9071\u001b[0m        0.4518       0.3854            0.3854        1.4800  0.0005  0.4081\n",
      "     15            \u001b[36m0.9429\u001b[0m        \u001b[32m0.4383\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        1.4552  0.0004  0.3935\n",
      "     16            \u001b[36m0.9643\u001b[0m        \u001b[32m0.4268\u001b[0m       0.3993            0.3993        1.4433  0.0004  0.3907\n",
      "     17            0.9643        \u001b[32m0.4060\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        1.4356  0.0004  0.4103\n",
      "     18            \u001b[36m0.9786\u001b[0m        \u001b[32m0.3559\u001b[0m       0.4028            0.4028        1.4331  0.0004  0.4079\n",
      "     19            \u001b[36m0.9857\u001b[0m        0.3661       0.4028            0.4028        1.4326  0.0004  0.3982\n",
      "     20            \u001b[36m0.9929\u001b[0m        0.4100       0.4028            0.4028        1.4318  0.0003  0.3921\n",
      "     21            \u001b[36m1.0000\u001b[0m        0.3610       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        1.4331  0.0003  0.3941\n",
      "     22            0.9929        \u001b[32m0.3340\u001b[0m       0.4062            0.4062        1.4377  0.0003  0.3993\n",
      "     23            0.9857        \u001b[32m0.2697\u001b[0m       0.4062            0.4062        1.4435  0.0002  0.3849\n",
      "     24            0.9857        \u001b[32m0.2612\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        1.4442  0.0002  0.4033\n",
      "     25            1.0000        0.2743       0.4132            0.4132        1.4400  0.0002  0.3945\n",
      "     26            1.0000        0.2749       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        1.4376  0.0002  0.3910\n",
      "     27            1.0000        0.2699       0.4167            0.4167        1.4366  0.0002  0.3990\n",
      "     28            1.0000        0.2689       0.4201            0.4201        1.4356  0.0001  0.3966\n",
      "     29            1.0000        \u001b[32m0.2334\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        1.4338  0.0001  0.4151\n",
      "     30            1.0000        0.2594       0.4271            0.4271        1.4329  0.0001  0.4696\n",
      "     31            1.0000        0.2616       0.4236            0.4236        1.4322  0.0001  0.4597\n",
      "     32            1.0000        0.2405       0.4236            0.4236        1.4301  0.0001  0.4937\n",
      "     33            1.0000        0.2968       0.4236            0.4236        1.4295  0.0000  0.4644\n",
      "     34            1.0000        0.2477       0.4271            0.4271        1.4282  0.0000  0.4729\n",
      "     35            1.0000        0.2663       0.4271            0.4271        1.4257  0.0000  0.4699\n",
      "     36            1.0000        \u001b[32m0.2242\u001b[0m       0.4271            0.4271        \u001b[94m1.4243\u001b[0m  0.0000  0.5174\n",
      "     37            1.0000        0.2485       0.4271            0.4271        \u001b[94m1.4230\u001b[0m  0.0000  0.3906\n",
      "     38            1.0000        0.2768       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.4219\u001b[0m  0.0000  0.3840\n",
      "     39            1.0000        0.2527       0.4340            0.4340        \u001b[94m1.4201\u001b[0m  0.0000  0.4024\n",
      "     40            1.0000        0.2665       0.4340            0.4340        \u001b[94m1.4194\u001b[0m  0.0000  0.3849\n",
      "Training model for subject 5 with 150 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2467\u001b[0m        \u001b[32m1.6358\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m3.0799\u001b[0m  0.0006  0.4090\n",
      "      2            \u001b[36m0.4733\u001b[0m        \u001b[32m1.5156\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        \u001b[94m1.6036\u001b[0m  0.0006  0.4155\n",
      "      3            \u001b[36m0.5133\u001b[0m        \u001b[32m1.1220\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m1.5519\u001b[0m  0.0006  0.4039\n",
      "      4            0.4733        1.1503       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        2.0225  0.0006  0.3967\n",
      "      5            0.4000        \u001b[32m0.9654\u001b[0m       0.3368            0.3368        2.7453  0.0006  0.3990\n",
      "      6            0.3867        0.9790       0.3090            0.3090        3.3096  0.0006  0.3981\n",
      "      7            0.3867        \u001b[32m0.8153\u001b[0m       0.3056            0.3056        3.3862  0.0006  0.4235\n",
      "      8            0.4133        0.8321       0.3229            0.3229        3.1786  0.0006  0.4007\n",
      "      9            0.4533        \u001b[32m0.7236\u001b[0m       0.3368            0.3368        2.7638  0.0006  0.3982\n",
      "     10            0.4667        0.7248       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        2.4355  0.0005  0.4086\n",
      "     11            0.5067        \u001b[32m0.6829\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        2.1785  0.0005  0.3915\n",
      "     12            \u001b[36m0.5600\u001b[0m        \u001b[32m0.6313\u001b[0m       0.3715            0.3715        2.0500  0.0005  0.3934\n",
      "     13            \u001b[36m0.6333\u001b[0m        \u001b[32m0.6084\u001b[0m       0.3715            0.3715        1.9178  0.0005  0.4233\n",
      "     14            \u001b[36m0.7000\u001b[0m        \u001b[32m0.5795\u001b[0m       0.3750            0.3750        1.8411  0.0005  0.3939\n",
      "     15            \u001b[36m0.8000\u001b[0m        \u001b[32m0.5696\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        1.7837  0.0004  0.3935\n",
      "     16            \u001b[36m0.8133\u001b[0m        \u001b[32m0.5191\u001b[0m       0.3819            0.3819        1.7171  0.0004  0.3869\n",
      "     17            \u001b[36m0.8733\u001b[0m        \u001b[32m0.4824\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        1.6532  0.0004  0.3804\n",
      "     18            \u001b[36m0.9200\u001b[0m        \u001b[32m0.4788\u001b[0m       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        1.6027  0.0004  0.4013\n",
      "     19            \u001b[36m0.9267\u001b[0m        0.4933       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        1.5779  0.0004  0.4041\n",
      "     20            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3911\u001b[0m       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        1.5706  0.0003  0.4098\n",
      "     21            \u001b[36m0.9467\u001b[0m        \u001b[32m0.3441\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        1.5609  0.0003  0.4848\n",
      "     22            \u001b[36m0.9533\u001b[0m        0.3701       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        1.5538  0.0003  0.5165\n",
      "     23            \u001b[36m0.9600\u001b[0m        0.3782       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.5379\u001b[0m  0.0002  0.4538\n",
      "     24            \u001b[36m0.9733\u001b[0m        0.3693       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.5247\u001b[0m  0.0002  0.4728\n",
      "     25            \u001b[36m0.9867\u001b[0m        0.3530       0.4410            0.4410        \u001b[94m1.5124\u001b[0m  0.0002  0.5841\n",
      "     26            0.9867        \u001b[32m0.3134\u001b[0m       0.4271            0.4271        \u001b[94m1.5048\u001b[0m  0.0002  0.5098\n",
      "     27            \u001b[36m0.9933\u001b[0m        0.3624       0.4201            0.4201        \u001b[94m1.5000\u001b[0m  0.0002  0.4486\n",
      "     28            0.9933        0.3310       0.4236            0.4236        \u001b[94m1.4941\u001b[0m  0.0001  0.4018\n",
      "     29            0.9933        0.3265       0.4201            0.4201        \u001b[94m1.4913\u001b[0m  0.0001  0.3921\n",
      "     30            0.9933        0.3280       0.4271            0.4271        \u001b[94m1.4894\u001b[0m  0.0001  0.3997\n",
      "     31            0.9933        0.3252       0.4236            0.4236        \u001b[94m1.4867\u001b[0m  0.0001  0.4121\n",
      "     32            0.9933        \u001b[32m0.3045\u001b[0m       0.4236            0.4236        \u001b[94m1.4844\u001b[0m  0.0001  0.4061\n",
      "     33            0.9933        0.3147       0.4236            0.4236        \u001b[94m1.4821\u001b[0m  0.0000  0.4014\n",
      "     34            0.9933        0.3327       0.4236            0.4236        \u001b[94m1.4794\u001b[0m  0.0000  0.3862\n",
      "     35            0.9933        \u001b[32m0.2961\u001b[0m       0.4236            0.4236        \u001b[94m1.4772\u001b[0m  0.0000  0.3960\n",
      "     36            \u001b[36m1.0000\u001b[0m        0.3402       0.4236            0.4236        \u001b[94m1.4753\u001b[0m  0.0000  0.3933\n",
      "     37            1.0000        0.3310       0.4236            0.4236        \u001b[94m1.4738\u001b[0m  0.0000  0.4098\n",
      "     38            1.0000        0.3490       0.4271            0.4271        \u001b[94m1.4724\u001b[0m  0.0000  0.4195\n",
      "     39            1.0000        0.4072       0.4306            0.4306        \u001b[94m1.4718\u001b[0m  0.0000  0.4022\n",
      "     40            1.0000        0.3319       0.4340            0.4340        \u001b[94m1.4708\u001b[0m  0.0000  0.4344\n",
      "Training model for subject 5 with 160 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3750\u001b[0m        \u001b[32m1.6579\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m2.1598\u001b[0m  0.0006  0.4210\n",
      "      2            0.2687        \u001b[32m1.3324\u001b[0m       0.2569            0.2569        3.0103  0.0006  0.3949\n",
      "      3            0.2750        \u001b[32m1.1653\u001b[0m       0.2604            0.2604        3.4220  0.0006  0.3974\n",
      "      4            0.2625        \u001b[32m1.0925\u001b[0m       0.2500            0.2500        4.0148  0.0006  0.3900\n",
      "      5            0.2750        \u001b[32m1.0080\u001b[0m       0.2535            0.2535        4.4114  0.0006  0.3934\n",
      "      6            0.3063        \u001b[32m0.8204\u001b[0m       0.2708            0.2708        4.2945  0.0006  0.3987\n",
      "      7            0.3500        \u001b[32m0.7968\u001b[0m       0.2917            0.2917        4.1278  0.0006  0.4182\n",
      "      8            \u001b[36m0.3937\u001b[0m        \u001b[32m0.7812\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        3.9516  0.0006  0.3862\n",
      "      9            \u001b[36m0.4313\u001b[0m        \u001b[32m0.6987\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        3.5751  0.0006  0.3931\n",
      "     10            \u001b[36m0.4813\u001b[0m        0.7532       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        3.2479  0.0005  0.4044\n",
      "     11            \u001b[36m0.4875\u001b[0m        0.7237       0.3715            0.3715        2.9851  0.0005  0.4467\n",
      "     12            \u001b[36m0.5188\u001b[0m        \u001b[32m0.5813\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        2.7995  0.0005  0.4498\n",
      "     13            \u001b[36m0.5437\u001b[0m        0.6020       0.3750            0.3750        2.5628  0.0005  0.5039\n",
      "     14            \u001b[36m0.5875\u001b[0m        0.6176       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        2.3235  0.0005  0.4706\n",
      "     15            \u001b[36m0.6000\u001b[0m        \u001b[32m0.5518\u001b[0m       0.3854            0.3854        \u001b[94m2.1545\u001b[0m  0.0004  0.4852\n",
      "     16            \u001b[36m0.6625\u001b[0m        0.5657       0.3785            0.3785        \u001b[94m2.0221\u001b[0m  0.0004  0.4848\n",
      "     17            \u001b[36m0.6875\u001b[0m        0.5760       0.3750            0.3750        \u001b[94m1.9099\u001b[0m  0.0004  0.3964\n",
      "     18            \u001b[36m0.7500\u001b[0m        \u001b[32m0.4639\u001b[0m       0.3819            0.3819        \u001b[94m1.8118\u001b[0m  0.0004  0.4104\n",
      "     19            \u001b[36m0.8187\u001b[0m        \u001b[32m0.4420\u001b[0m       0.3785            0.3785        \u001b[94m1.7114\u001b[0m  0.0004  0.3988\n",
      "     20            \u001b[36m0.8688\u001b[0m        \u001b[32m0.3730\u001b[0m       0.3819            0.3819        \u001b[94m1.6367\u001b[0m  0.0003  0.4006\n",
      "     21            \u001b[36m0.9313\u001b[0m        0.4015       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.5808\u001b[0m  0.0003  0.3963\n",
      "     22            \u001b[36m0.9500\u001b[0m        0.4311       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.5429\u001b[0m  0.0003  0.4084\n",
      "     23            \u001b[36m0.9625\u001b[0m        \u001b[32m0.3715\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.5110\u001b[0m  0.0002  0.4027\n",
      "     24            \u001b[36m0.9812\u001b[0m        0.3895       0.3889            0.3889        \u001b[94m1.4828\u001b[0m  0.0002  0.4069\n",
      "     25            \u001b[36m0.9875\u001b[0m        0.3880       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.4619\u001b[0m  0.0002  0.4078\n",
      "     26            0.9875        0.4405       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.4473\u001b[0m  0.0002  0.4205\n",
      "     27            \u001b[36m0.9938\u001b[0m        \u001b[32m0.3687\u001b[0m       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.4300\u001b[0m  0.0002  0.4036\n",
      "     28            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3398\u001b[0m       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.4179\u001b[0m  0.0001  0.3937\n",
      "     29            1.0000        \u001b[32m0.3348\u001b[0m       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.4083\u001b[0m  0.0001  0.3955\n",
      "     30            1.0000        0.3417       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.4002\u001b[0m  0.0001  0.4091\n",
      "     31            1.0000        0.3376       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.3930\u001b[0m  0.0001  0.3955\n",
      "     32            1.0000        0.3474       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.3888\u001b[0m  0.0001  0.3932\n",
      "     33            1.0000        \u001b[32m0.3129\u001b[0m       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.3863\u001b[0m  0.0000  0.3945\n",
      "     34            1.0000        0.3221       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.3836\u001b[0m  0.0000  0.4115\n",
      "     35            1.0000        0.3258       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.3812\u001b[0m  0.0000  0.5059\n",
      "     36            1.0000        0.3549       0.4653            0.4653        \u001b[94m1.3795\u001b[0m  0.0000  0.4003\n",
      "     37            1.0000        \u001b[32m0.3096\u001b[0m       0.4688            0.4688        \u001b[94m1.3787\u001b[0m  0.0000  0.3975\n",
      "     38            1.0000        \u001b[32m0.3079\u001b[0m       0.4688            0.4688        \u001b[94m1.3773\u001b[0m  0.0000  0.4147\n",
      "     39            1.0000        0.3334       0.4653            0.4653        \u001b[94m1.3761\u001b[0m  0.0000  0.4153\n",
      "     40            1.0000        0.3252       0.4653            0.4653        1.3761  0.0000  0.4016\n",
      "Training model for subject 5 with 170 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2471\u001b[0m        \u001b[32m1.7990\u001b[0m       \u001b[35m0.2465\u001b[0m            \u001b[31m0.2465\u001b[0m        \u001b[94m3.3798\u001b[0m  0.0006  0.4267\n",
      "      2            0.2471        \u001b[32m1.4517\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        3.6635  0.0006  0.4922\n",
      "      3            0.2471        \u001b[32m1.2482\u001b[0m       0.2500            0.2500        3.7188  0.0006  0.4718\n",
      "      4            \u001b[36m0.2529\u001b[0m        \u001b[32m1.0561\u001b[0m       0.2500            0.2500        3.7282  0.0006  0.4669\n",
      "      5            \u001b[36m0.2588\u001b[0m        1.1090       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        4.0047  0.0006  0.4673\n",
      "      6            0.2529        \u001b[32m0.9824\u001b[0m       0.2500            0.2500        4.4466  0.0006  0.4615\n",
      "      7            0.2529        0.9983       0.2500            0.2500        4.3158  0.0006  0.4180\n",
      "      8            0.2529        0.9966       0.2535            0.2535        4.1575  0.0006  0.4063\n",
      "      9            0.2588        \u001b[32m0.7847\u001b[0m       0.2535            0.2535        3.8374  0.0006  0.4005\n",
      "     10            \u001b[36m0.2647\u001b[0m        0.7975       0.2535            0.2535        3.5195  0.0005  0.3838\n",
      "     11            \u001b[36m0.2765\u001b[0m        \u001b[32m0.7729\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m3.0053\u001b[0m  0.0005  0.3962\n",
      "     12            \u001b[36m0.3353\u001b[0m        \u001b[32m0.7230\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m2.5275\u001b[0m  0.0005  0.4069\n",
      "     13            \u001b[36m0.4471\u001b[0m        \u001b[32m0.6780\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m2.1481\u001b[0m  0.0005  0.3910\n",
      "     14            \u001b[36m0.5765\u001b[0m        \u001b[32m0.6181\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m1.8718\u001b[0m  0.0005  0.3851\n",
      "     15            \u001b[36m0.6824\u001b[0m        0.6340       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.6907\u001b[0m  0.0004  0.3906\n",
      "     16            \u001b[36m0.7412\u001b[0m        0.6227       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.5824\u001b[0m  0.0004  0.4055\n",
      "     17            \u001b[36m0.7824\u001b[0m        \u001b[32m0.6158\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.5220\u001b[0m  0.0004  0.3992\n",
      "     18            \u001b[36m0.8118\u001b[0m        \u001b[32m0.5671\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.4869\u001b[0m  0.0004  0.4080\n",
      "     19            \u001b[36m0.8235\u001b[0m        \u001b[32m0.5497\u001b[0m       0.3819            0.3819        \u001b[94m1.4607\u001b[0m  0.0004  0.3986\n",
      "     20            \u001b[36m0.8529\u001b[0m        \u001b[32m0.5475\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.4367\u001b[0m  0.0003  0.3914\n",
      "     21            \u001b[36m0.8765\u001b[0m        \u001b[32m0.4615\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.4066\u001b[0m  0.0003  0.3947\n",
      "     22            \u001b[36m0.9059\u001b[0m        0.4691       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.3787\u001b[0m  0.0003  0.3883\n",
      "     23            \u001b[36m0.9294\u001b[0m        \u001b[32m0.4604\u001b[0m       0.3958            0.3958        \u001b[94m1.3608\u001b[0m  0.0002  0.3871\n",
      "     24            \u001b[36m0.9471\u001b[0m        0.5529       0.3958            0.3958        \u001b[94m1.3499\u001b[0m  0.0002  0.4086\n",
      "     25            \u001b[36m0.9529\u001b[0m        \u001b[32m0.4494\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.3364\u001b[0m  0.0002  0.4108\n",
      "     26            0.9529        0.4967       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.3258\u001b[0m  0.0002  0.4095\n",
      "     27            \u001b[36m0.9588\u001b[0m        0.4579       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.3152\u001b[0m  0.0002  0.3964\n",
      "     28            0.9588        \u001b[32m0.4311\u001b[0m       0.4132            0.4132        \u001b[94m1.3073\u001b[0m  0.0001  0.4013\n",
      "     29            0.9588        0.4449       0.4167            0.4167        \u001b[94m1.3028\u001b[0m  0.0001  0.3880\n",
      "     30            \u001b[36m0.9647\u001b[0m        0.4425       0.4132            0.4132        \u001b[94m1.3009\u001b[0m  0.0001  0.3864\n",
      "     31            0.9647        \u001b[32m0.4133\u001b[0m       0.4201            0.4201        \u001b[94m1.2995\u001b[0m  0.0001  0.4295\n",
      "     32            0.9647        0.4657       0.4201            0.4201        \u001b[94m1.2990\u001b[0m  0.0001  0.4934\n",
      "     33            0.9647        0.4487       0.4201            0.4201        \u001b[94m1.2985\u001b[0m  0.0000  0.5098\n",
      "     34            0.9647        0.4184       0.4132            0.4132        \u001b[94m1.2977\u001b[0m  0.0000  0.4702\n",
      "     35            0.9647        0.4168       0.4132            0.4132        1.2977  0.0000  0.4756\n",
      "     36            0.9647        0.4220       0.4167            0.4167        1.2979  0.0000  0.5192\n",
      "     37            0.9647        \u001b[32m0.3835\u001b[0m       0.4132            0.4132        \u001b[94m1.2969\u001b[0m  0.0000  0.4048\n",
      "     38            0.9647        0.4255       0.4132            0.4132        \u001b[94m1.2966\u001b[0m  0.0000  0.3994\n",
      "     39            0.9647        0.4120       0.4132            0.4132        \u001b[94m1.2966\u001b[0m  0.0000  0.5079\n",
      "     40            0.9647        \u001b[32m0.3625\u001b[0m       0.4132            0.4132        \u001b[94m1.2961\u001b[0m  0.0000  0.4107\n",
      "Training model for subject 5 with 180 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.6248\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.4189\u001b[0m  0.0006  0.4131\n",
      "      2            \u001b[36m0.3611\u001b[0m        \u001b[32m1.4026\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        \u001b[94m2.8137\u001b[0m  0.0006  0.3787\n",
      "      3            0.2611        \u001b[32m1.2236\u001b[0m       0.2569            0.2569        3.1606  0.0006  0.4085\n",
      "      4            0.2500        \u001b[32m1.1596\u001b[0m       0.2500            0.2500        3.7074  0.0006  0.3855\n",
      "      5            0.2500        \u001b[32m1.0286\u001b[0m       0.2500            0.2500        4.0312  0.0006  0.3947\n",
      "      6            0.2500        \u001b[32m0.9967\u001b[0m       0.2500            0.2500        4.1343  0.0006  0.4024\n",
      "      7            0.2556        \u001b[32m0.9179\u001b[0m       0.2535            0.2535        3.9922  0.0006  0.3967\n",
      "      8            0.2611        0.9633       0.2535            0.2535        3.5474  0.0006  0.4124\n",
      "      9            0.2667        \u001b[32m0.8167\u001b[0m       0.2535            0.2535        3.0915  0.0006  0.3948\n",
      "     10            0.2889        \u001b[32m0.7684\u001b[0m       0.2569            0.2569        \u001b[94m2.7435\u001b[0m  0.0005  0.4017\n",
      "     11            0.3500        0.7856       0.2743            0.2743        \u001b[94m2.3693\u001b[0m  0.0005  0.3995\n",
      "     12            \u001b[36m0.4556\u001b[0m        \u001b[32m0.7602\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m2.0425\u001b[0m  0.0005  0.4028\n",
      "     13            \u001b[36m0.5611\u001b[0m        \u001b[32m0.6769\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m1.8249\u001b[0m  0.0005  0.3924\n",
      "     14            \u001b[36m0.6611\u001b[0m        0.6987       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m1.7022\u001b[0m  0.0005  0.3943\n",
      "     15            \u001b[36m0.7444\u001b[0m        \u001b[32m0.6230\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.6072\u001b[0m  0.0004  0.4037\n",
      "     16            \u001b[36m0.8111\u001b[0m        \u001b[32m0.5799\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.5261\u001b[0m  0.0004  0.4026\n",
      "     17            \u001b[36m0.8278\u001b[0m        \u001b[32m0.5780\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.4911\u001b[0m  0.0004  0.4014\n",
      "     18            \u001b[36m0.8500\u001b[0m        \u001b[32m0.5612\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.4413\u001b[0m  0.0004  0.3816\n",
      "     19            \u001b[36m0.8944\u001b[0m        0.5715       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.3948\u001b[0m  0.0004  0.4047\n",
      "     20            \u001b[36m0.9056\u001b[0m        0.5838       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.3552\u001b[0m  0.0003  0.4076\n",
      "     21            \u001b[36m0.9222\u001b[0m        \u001b[32m0.5070\u001b[0m       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.3325\u001b[0m  0.0003  0.4404\n",
      "     22            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4029\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.3151\u001b[0m  0.0003  0.4712\n",
      "     23            0.9333        0.4875       0.4444            0.4444        \u001b[94m1.3032\u001b[0m  0.0002  0.5012\n",
      "     24            0.9333        0.5329       0.4444            0.4444        \u001b[94m1.2971\u001b[0m  0.0002  0.4510\n",
      "     25            \u001b[36m0.9389\u001b[0m        0.4213       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.2886\u001b[0m  0.0002  0.5068\n",
      "     26            \u001b[36m0.9444\u001b[0m        0.4375       0.4514            0.4514        \u001b[94m1.2804\u001b[0m  0.0002  0.5020\n",
      "     27            0.9444        0.5437       0.4479            0.4479        \u001b[94m1.2769\u001b[0m  0.0002  0.3980\n",
      "     28            \u001b[36m0.9500\u001b[0m        0.5222       0.4410            0.4410        1.2774  0.0001  0.3964\n",
      "     29            \u001b[36m0.9667\u001b[0m        0.4138       0.4479            0.4479        1.2770  0.0001  0.3895\n",
      "     30            \u001b[36m0.9722\u001b[0m        0.4867       0.4514            0.4514        \u001b[94m1.2762\u001b[0m  0.0001  0.4080\n",
      "     31            0.9722        0.4188       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.2753\u001b[0m  0.0001  0.4078\n",
      "     32            0.9722        0.4035       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2740\u001b[0m  0.0001  0.4234\n",
      "     33            0.9722        0.4055       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2716\u001b[0m  0.0000  0.4206\n",
      "     34            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3990\u001b[0m       0.4688            0.4688        \u001b[94m1.2695\u001b[0m  0.0000  0.4132\n",
      "     35            0.9778        0.4088       0.4688            0.4688        \u001b[94m1.2686\u001b[0m  0.0000  0.3909\n",
      "     36            0.9778        0.4128       \u001b[35m0.4722\u001b[0m            \u001b[31m0.4722\u001b[0m        \u001b[94m1.2669\u001b[0m  0.0000  0.4078\n",
      "     37            0.9722        \u001b[32m0.3488\u001b[0m       0.4688            0.4688        \u001b[94m1.2648\u001b[0m  0.0000  0.4090\n",
      "     38            0.9722        0.4189       0.4688            0.4688        \u001b[94m1.2639\u001b[0m  0.0000  0.4007\n",
      "     39            0.9722        0.4499       0.4688            0.4688        \u001b[94m1.2632\u001b[0m  0.0000  0.4024\n",
      "     40            0.9722        0.4589       0.4653            0.4653        1.2633  0.0000  0.4264\n",
      "Training model for subject 5 with 190 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2684\u001b[0m        \u001b[32m1.5138\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m3.2153\u001b[0m  0.0006  0.4198\n",
      "      2            \u001b[36m0.3684\u001b[0m        \u001b[32m1.3864\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        \u001b[94m1.8589\u001b[0m  0.0006  0.3973\n",
      "      3            \u001b[36m0.4263\u001b[0m        \u001b[32m1.1848\u001b[0m       0.2882            0.2882        \u001b[94m1.8518\u001b[0m  0.0006  0.4040\n",
      "      4            0.3105        \u001b[32m1.1393\u001b[0m       0.2639            0.2639        2.1462  0.0006  0.4057\n",
      "      5            0.3579        \u001b[32m1.0998\u001b[0m       0.2535            0.2535        2.1887  0.0006  0.4049\n",
      "      6            0.4158        \u001b[32m1.0184\u001b[0m       0.2639            0.2639        1.9970  0.0006  0.4685\n",
      "      7            \u001b[36m0.5158\u001b[0m        \u001b[32m0.9889\u001b[0m       0.2639            0.2639        1.8886  0.0006  0.4393\n",
      "      8            0.4684        \u001b[32m0.9219\u001b[0m       0.2674            0.2674        2.0192  0.0006  0.4007\n",
      "      9            0.4368        \u001b[32m0.8390\u001b[0m       0.2708            0.2708        2.1367  0.0006  0.4121\n",
      "     10            0.4579        0.8617       0.2743            0.2743        2.1003  0.0005  0.4788\n",
      "     11            0.4947        \u001b[32m0.7675\u001b[0m       0.2674            0.2674        2.0277  0.0005  0.4592\n",
      "     12            \u001b[36m0.5368\u001b[0m        \u001b[32m0.7546\u001b[0m       0.2708            0.2708        1.8716  0.0005  0.4625\n",
      "     13            \u001b[36m0.6368\u001b[0m        \u001b[32m0.7295\u001b[0m       0.3125            0.3125        \u001b[94m1.6925\u001b[0m  0.0005  0.4772\n",
      "     14            \u001b[36m0.7421\u001b[0m        \u001b[32m0.7054\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.5539\u001b[0m  0.0005  0.4713\n",
      "     15            \u001b[36m0.8263\u001b[0m        \u001b[32m0.6520\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.4563\u001b[0m  0.0004  0.5016\n",
      "     16            \u001b[36m0.8684\u001b[0m        \u001b[32m0.6085\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.4074\u001b[0m  0.0004  0.5225\n",
      "     17            \u001b[36m0.8895\u001b[0m        \u001b[32m0.5781\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.3853\u001b[0m  0.0004  0.4141\n",
      "     18            \u001b[36m0.9053\u001b[0m        0.6665       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.3587\u001b[0m  0.0004  0.3966\n",
      "     19            \u001b[36m0.9263\u001b[0m        0.6264       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.3477\u001b[0m  0.0004  0.4070\n",
      "     20            \u001b[36m0.9316\u001b[0m        \u001b[32m0.5091\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.3324\u001b[0m  0.0003  0.3952\n",
      "     21            0.9316        0.5491       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.3205\u001b[0m  0.0003  0.4163\n",
      "     22            \u001b[36m0.9368\u001b[0m        0.5790       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.3109\u001b[0m  0.0003  0.3924\n",
      "     23            \u001b[36m0.9421\u001b[0m        0.5326       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.3000\u001b[0m  0.0002  0.4003\n",
      "     24            \u001b[36m0.9474\u001b[0m        \u001b[32m0.4968\u001b[0m       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2930\u001b[0m  0.0002  0.3994\n",
      "     25            \u001b[36m0.9579\u001b[0m        \u001b[32m0.4350\u001b[0m       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.2874\u001b[0m  0.0002  0.4004\n",
      "     26            \u001b[36m0.9632\u001b[0m        0.5402       0.4653            0.4653        \u001b[94m1.2836\u001b[0m  0.0002  0.3912\n",
      "     27            0.9632        0.4849       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2801\u001b[0m  0.0002  0.4137\n",
      "     28            0.9632        0.4444       0.4653            0.4653        \u001b[94m1.2793\u001b[0m  0.0001  0.4088\n",
      "     29            0.9632        0.5367       0.4653            0.4653        1.2807  0.0001  0.3931\n",
      "     30            0.9632        0.4877       0.4688            0.4688        1.2808  0.0001  0.4033\n",
      "     31            0.9632        0.5503       0.4618            0.4618        1.2799  0.0001  0.3957\n",
      "     32            0.9632        0.4641       0.4583            0.4583        1.2796  0.0001  0.4004\n",
      "     33            0.9632        \u001b[32m0.4221\u001b[0m       0.4514            0.4514        1.2793  0.0000  0.3982\n",
      "     34            0.9632        0.4606       0.4479            0.4479        \u001b[94m1.2791\u001b[0m  0.0000  0.4158\n",
      "     35            0.9632        \u001b[32m0.4124\u001b[0m       0.4479            0.4479        \u001b[94m1.2789\u001b[0m  0.0000  0.4002\n",
      "     36            0.9632        0.4947       0.4479            0.4479        1.2790  0.0000  0.3924\n",
      "     37            0.9632        0.4627       0.4479            0.4479        1.2792  0.0000  0.4071\n",
      "     38            0.9632        0.4239       0.4479            0.4479        1.2791  0.0000  0.3963\n",
      "     39            0.9632        0.4396       0.4514            0.4514        \u001b[94m1.2786\u001b[0m  0.0000  0.4528\n",
      "     40            0.9632        0.4404       0.4514            0.4514        1.2787  0.0000  0.4699\n",
      "Training model for subject 5 with 200 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2650\u001b[0m        \u001b[32m1.6428\u001b[0m       \u001b[35m0.2396\u001b[0m            \u001b[31m0.2396\u001b[0m        \u001b[94m3.0183\u001b[0m  0.0006  0.6087\n",
      "      2            \u001b[36m0.2700\u001b[0m        \u001b[32m1.3889\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m2.4685\u001b[0m  0.0006  0.5602\n",
      "      3            \u001b[36m0.3700\u001b[0m        \u001b[32m1.1996\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        2.5462  0.0006  0.6416\n",
      "      4            \u001b[36m0.3900\u001b[0m        \u001b[32m1.0081\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        2.9183  0.0006  0.5219\n",
      "      5            0.3200        \u001b[32m0.9703\u001b[0m       0.2569            0.2569        2.9801  0.0006  0.4988\n",
      "      6            0.3600        \u001b[32m0.9476\u001b[0m       0.2674            0.2674        2.5074  0.0006  0.4878\n",
      "      7            \u001b[36m0.5100\u001b[0m        \u001b[32m0.7920\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m1.9134\u001b[0m  0.0006  0.5094\n",
      "      8            \u001b[36m0.6500\u001b[0m        \u001b[32m0.7741\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m1.6151\u001b[0m  0.0006  0.5007\n",
      "      9            \u001b[36m0.7150\u001b[0m        \u001b[32m0.6985\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.5194\u001b[0m  0.0006  0.5145\n",
      "     10            \u001b[36m0.7750\u001b[0m        \u001b[32m0.6627\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.4576\u001b[0m  0.0005  0.4907\n",
      "     11            \u001b[36m0.8150\u001b[0m        \u001b[32m0.6275\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.4067\u001b[0m  0.0005  0.5119\n",
      "     12            \u001b[36m0.8550\u001b[0m        0.6411       0.3715            0.3715        \u001b[94m1.3872\u001b[0m  0.0005  0.5030\n",
      "     13            \u001b[36m0.9050\u001b[0m        \u001b[32m0.5277\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.3482\u001b[0m  0.0005  0.4850\n",
      "     14            \u001b[36m0.9150\u001b[0m        0.5383       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.3225\u001b[0m  0.0005  0.5103\n",
      "     15            \u001b[36m0.9250\u001b[0m        \u001b[32m0.5048\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.3129\u001b[0m  0.0004  0.5105\n",
      "     16            \u001b[36m0.9350\u001b[0m        0.5360       0.4097            0.4097        \u001b[94m1.3059\u001b[0m  0.0004  0.5035\n",
      "     17            \u001b[36m0.9400\u001b[0m        \u001b[32m0.4464\u001b[0m       0.4062            0.4062        1.3089  0.0004  0.4995\n",
      "     18            \u001b[36m0.9600\u001b[0m        \u001b[32m0.4430\u001b[0m       0.4097            0.4097        1.3183  0.0004  0.5173\n",
      "     19            \u001b[36m0.9750\u001b[0m        \u001b[32m0.4171\u001b[0m       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        1.3119  0.0004  0.5028\n",
      "     20            0.9750        0.4422       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.3002\u001b[0m  0.0003  0.4956\n",
      "     21            \u001b[36m0.9800\u001b[0m        \u001b[32m0.3753\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.2938\u001b[0m  0.0003  0.4970\n",
      "     22            0.9800        0.4145       0.4444            0.4444        \u001b[94m1.2877\u001b[0m  0.0003  0.5001\n",
      "     23            0.9800        0.4050       0.4479            0.4479        \u001b[94m1.2833\u001b[0m  0.0002  0.6318\n",
      "     24            0.9800        \u001b[32m0.3702\u001b[0m       0.4410            0.4410        \u001b[94m1.2815\u001b[0m  0.0002  0.5691\n",
      "     25            \u001b[36m0.9850\u001b[0m        0.4161       0.4410            0.4410        \u001b[94m1.2769\u001b[0m  0.0002  0.6361\n",
      "     26            \u001b[36m0.9900\u001b[0m        \u001b[32m0.3646\u001b[0m       0.4479            0.4479        \u001b[94m1.2715\u001b[0m  0.0002  0.5870\n",
      "     27            \u001b[36m0.9950\u001b[0m        \u001b[32m0.3601\u001b[0m       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.2703\u001b[0m  0.0002  0.5906\n",
      "     28            0.9950        \u001b[32m0.3344\u001b[0m       0.4514            0.4514        \u001b[94m1.2688\u001b[0m  0.0001  0.5829\n",
      "     29            0.9950        0.3468       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        1.2699  0.0001  0.5163\n",
      "     30            0.9950        \u001b[32m0.3270\u001b[0m       0.4549            0.4549        1.2708  0.0001  0.5150\n",
      "     31            0.9950        \u001b[32m0.3211\u001b[0m       0.4549            0.4549        1.2709  0.0001  0.5177\n",
      "     32            0.9950        \u001b[32m0.2931\u001b[0m       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        1.2711  0.0001  0.5009\n",
      "     33            0.9950        0.3600       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        1.2708  0.0000  0.5309\n",
      "     34            0.9950        0.2980       0.4549            0.4549        1.2693  0.0000  0.4927\n",
      "     35            0.9900        0.3282       0.4514            0.4514        \u001b[94m1.2684\u001b[0m  0.0000  0.5168\n",
      "     36            0.9900        0.2950       0.4549            0.4549        \u001b[94m1.2676\u001b[0m  0.0000  0.5360\n",
      "     37            0.9900        0.3035       0.4549            0.4549        \u001b[94m1.2670\u001b[0m  0.0000  0.5212\n",
      "     38            0.9900        0.3018       0.4549            0.4549        \u001b[94m1.2668\u001b[0m  0.0000  0.5068\n",
      "     39            0.9900        0.3270       0.4549            0.4549        \u001b[94m1.2668\u001b[0m  0.0000  0.4959\n",
      "     40            0.9900        0.3192       0.4549            0.4549        \u001b[94m1.2667\u001b[0m  0.0000  0.5235\n",
      "Training model for subject 5 with 210 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2524\u001b[0m        \u001b[32m1.5228\u001b[0m       \u001b[35m0.2465\u001b[0m            \u001b[31m0.2465\u001b[0m        \u001b[94m4.0864\u001b[0m  0.0006  0.5047\n",
      "      2            0.2524        \u001b[32m1.2983\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        5.0522  0.0006  0.5035\n",
      "      3            \u001b[36m0.2571\u001b[0m        \u001b[32m1.1671\u001b[0m       0.2500            0.2500        \u001b[94m3.9677\u001b[0m  0.0006  0.5011\n",
      "      4            \u001b[36m0.2952\u001b[0m        \u001b[32m0.9954\u001b[0m       0.2500            0.2500        \u001b[94m2.8779\u001b[0m  0.0006  0.5021\n",
      "      5            \u001b[36m0.4048\u001b[0m        \u001b[32m0.8631\u001b[0m       0.2500            0.2500        \u001b[94m2.4651\u001b[0m  0.0006  0.4953\n",
      "      6            \u001b[36m0.5048\u001b[0m        0.8876       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m2.1360\u001b[0m  0.0006  0.5004\n",
      "      7            \u001b[36m0.5143\u001b[0m        \u001b[32m0.8017\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m1.9987\u001b[0m  0.0006  0.5359\n",
      "      8            0.4857        \u001b[32m0.7920\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m1.9448\u001b[0m  0.0006  0.5794\n",
      "      9            \u001b[36m0.6000\u001b[0m        \u001b[32m0.6587\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m1.7846\u001b[0m  0.0006  0.5804\n",
      "     10            \u001b[36m0.7286\u001b[0m        0.6776       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.6181\u001b[0m  0.0005  0.6026\n",
      "     11            \u001b[36m0.7762\u001b[0m        \u001b[32m0.5833\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.5102\u001b[0m  0.0005  0.5991\n",
      "     12            \u001b[36m0.8286\u001b[0m        \u001b[32m0.5390\u001b[0m       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.4285\u001b[0m  0.0005  0.5375\n",
      "     13            \u001b[36m0.8571\u001b[0m        0.5636       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.3688\u001b[0m  0.0005  0.5132\n",
      "     14            \u001b[36m0.8857\u001b[0m        \u001b[32m0.5266\u001b[0m       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.3209\u001b[0m  0.0005  0.4963\n",
      "     15            \u001b[36m0.9143\u001b[0m        \u001b[32m0.5010\u001b[0m       0.4583            0.4583        \u001b[94m1.2800\u001b[0m  0.0004  0.5053\n",
      "     16            \u001b[36m0.9429\u001b[0m        \u001b[32m0.4164\u001b[0m       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.2531\u001b[0m  0.0004  0.5031\n",
      "     17            \u001b[36m0.9571\u001b[0m        0.4400       0.4549            0.4549        \u001b[94m1.2265\u001b[0m  0.0004  0.5200\n",
      "     18            \u001b[36m0.9667\u001b[0m        0.4325       0.4583            0.4583        \u001b[94m1.2080\u001b[0m  0.0004  0.5021\n",
      "     19            0.9667        \u001b[32m0.3908\u001b[0m       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2024\u001b[0m  0.0004  0.5020\n",
      "     20            \u001b[36m0.9714\u001b[0m        0.3960       \u001b[35m0.4931\u001b[0m            \u001b[31m0.4931\u001b[0m        1.2029  0.0003  0.5043\n",
      "     21            \u001b[36m0.9762\u001b[0m        \u001b[32m0.3844\u001b[0m       0.4861            0.4861        1.2042  0.0003  0.5051\n",
      "     22            0.9762        \u001b[32m0.3694\u001b[0m       0.4931            0.4931        1.2048  0.0003  0.4980\n",
      "     23            0.9714        0.4136       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        1.2092  0.0002  0.5987\n",
      "     24            \u001b[36m0.9810\u001b[0m        \u001b[32m0.3493\u001b[0m       \u001b[35m0.5000\u001b[0m            \u001b[31m0.5000\u001b[0m        1.2049  0.0002  0.5075\n",
      "     25            \u001b[36m0.9952\u001b[0m        \u001b[32m0.3333\u001b[0m       0.4965            0.4965        \u001b[94m1.1951\u001b[0m  0.0002  0.5062\n",
      "     26            0.9952        \u001b[32m0.2966\u001b[0m       \u001b[35m0.5035\u001b[0m            \u001b[31m0.5035\u001b[0m        \u001b[94m1.1901\u001b[0m  0.0002  0.5062\n",
      "     27            0.9952        \u001b[32m0.2934\u001b[0m       0.4965            0.4965        \u001b[94m1.1887\u001b[0m  0.0002  0.5062\n",
      "     28            0.9952        0.3153       0.4965            0.4965        \u001b[94m1.1869\u001b[0m  0.0001  0.4974\n",
      "     29            0.9952        \u001b[32m0.2933\u001b[0m       0.4965            0.4965        \u001b[94m1.1851\u001b[0m  0.0001  0.5095\n",
      "     30            0.9952        0.3175       0.4965            0.4965        \u001b[94m1.1839\u001b[0m  0.0001  0.5161\n",
      "     31            0.9952        \u001b[32m0.2902\u001b[0m       0.5000            0.5000        \u001b[94m1.1818\u001b[0m  0.0001  0.5488\n",
      "     32            0.9952        0.3116       0.5000            0.5000        \u001b[94m1.1796\u001b[0m  0.0001  0.5973\n",
      "     33            0.9952        \u001b[32m0.2290\u001b[0m       0.5035            0.5035        \u001b[94m1.1791\u001b[0m  0.0000  0.5936\n",
      "     34            0.9952        0.2872       0.5035            0.5035        \u001b[94m1.1781\u001b[0m  0.0000  0.6047\n",
      "     35            0.9952        0.2877       0.5035            0.5035        \u001b[94m1.1771\u001b[0m  0.0000  0.7259\n",
      "     36            0.9952        0.2504       0.5035            0.5035        \u001b[94m1.1764\u001b[0m  0.0000  0.5035\n",
      "     37            0.9952        0.3005       0.5000            0.5000        \u001b[94m1.1758\u001b[0m  0.0000  0.5112\n",
      "     38            0.9952        0.2766       0.5000            0.5000        \u001b[94m1.1754\u001b[0m  0.0000  0.5325\n",
      "     39            0.9952        0.3201       0.5000            0.5000        \u001b[94m1.1751\u001b[0m  0.0000  0.5648\n",
      "     40            0.9952        0.3439       0.5035            0.5035        \u001b[94m1.1748\u001b[0m  0.0000  0.5452\n",
      "Training model for subject 5 with 220 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2545\u001b[0m        \u001b[32m1.5557\u001b[0m       \u001b[35m0.2431\u001b[0m            \u001b[31m0.2431\u001b[0m        \u001b[94m3.3245\u001b[0m  0.0006  0.4934\n",
      "      2            \u001b[36m0.3591\u001b[0m        \u001b[32m1.2674\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m2.9206\u001b[0m  0.0006  0.4926\n",
      "      3            \u001b[36m0.3636\u001b[0m        \u001b[32m1.1116\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m2.4811\u001b[0m  0.0006  0.5127\n",
      "      4            \u001b[36m0.4364\u001b[0m        \u001b[32m1.0345\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m1.9755\u001b[0m  0.0006  0.5040\n",
      "      5            \u001b[36m0.5591\u001b[0m        \u001b[32m0.9726\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m1.6736\u001b[0m  0.0006  0.5036\n",
      "      6            \u001b[36m0.6682\u001b[0m        \u001b[32m0.8812\u001b[0m       0.3333            0.3333        \u001b[94m1.5354\u001b[0m  0.0006  0.4906\n",
      "      7            0.6591        \u001b[32m0.8469\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.5108\u001b[0m  0.0006  0.5306\n",
      "      8            \u001b[36m0.7000\u001b[0m        \u001b[32m0.7830\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.4756\u001b[0m  0.0006  0.4931\n",
      "      9            \u001b[36m0.7591\u001b[0m        \u001b[32m0.6788\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.3990\u001b[0m  0.0006  0.5011\n",
      "     10            \u001b[36m0.8273\u001b[0m        \u001b[32m0.6555\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.3318\u001b[0m  0.0005  0.5259\n",
      "     11            \u001b[36m0.8818\u001b[0m        0.7017       0.4479            0.4479        \u001b[94m1.2936\u001b[0m  0.0005  0.5022\n",
      "     12            \u001b[36m0.9182\u001b[0m        \u001b[32m0.6490\u001b[0m       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.2717\u001b[0m  0.0005  0.5011\n",
      "     13            0.8909        \u001b[32m0.5964\u001b[0m       0.4514            0.4514        \u001b[94m1.2634\u001b[0m  0.0005  0.5111\n",
      "     14            \u001b[36m0.9227\u001b[0m        \u001b[32m0.5116\u001b[0m       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2525\u001b[0m  0.0005  0.5087\n",
      "     15            \u001b[36m0.9364\u001b[0m        0.5323       0.4514            0.4514        \u001b[94m1.2410\u001b[0m  0.0004  0.6002\n",
      "     16            \u001b[36m0.9409\u001b[0m        \u001b[32m0.4745\u001b[0m       0.4549            0.4549        \u001b[94m1.2297\u001b[0m  0.0004  0.6154\n",
      "     17            0.9409        \u001b[32m0.4631\u001b[0m       0.4618            0.4618        \u001b[94m1.2247\u001b[0m  0.0004  0.6003\n",
      "     18            \u001b[36m0.9591\u001b[0m        0.5179       0.4618            0.4618        \u001b[94m1.2219\u001b[0m  0.0004  0.6204\n",
      "     19            \u001b[36m0.9636\u001b[0m        \u001b[32m0.4600\u001b[0m       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        \u001b[94m1.2172\u001b[0m  0.0004  0.5219\n",
      "     20            \u001b[36m0.9727\u001b[0m        \u001b[32m0.4148\u001b[0m       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        \u001b[94m1.2154\u001b[0m  0.0003  0.5229\n",
      "     21            0.9636        \u001b[32m0.3928\u001b[0m       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        1.2164  0.0003  0.5167\n",
      "     22            0.9727        0.4034       0.4826            0.4826        \u001b[94m1.2147\u001b[0m  0.0003  0.5156\n",
      "     23            \u001b[36m0.9773\u001b[0m        \u001b[32m0.3613\u001b[0m       0.4861            0.4861        \u001b[94m1.2102\u001b[0m  0.0002  0.5108\n",
      "     24            0.9773        \u001b[32m0.3590\u001b[0m       0.4896            0.4896        \u001b[94m1.2077\u001b[0m  0.0002  0.5959\n",
      "     25            0.9773        0.3660       \u001b[35m0.5035\u001b[0m            \u001b[31m0.5035\u001b[0m        \u001b[94m1.2053\u001b[0m  0.0002  0.4926\n",
      "     26            \u001b[36m0.9818\u001b[0m        \u001b[32m0.3147\u001b[0m       0.5000            0.5000        \u001b[94m1.2052\u001b[0m  0.0002  0.4977\n",
      "     27            \u001b[36m0.9909\u001b[0m        0.3469       0.5000            0.5000        \u001b[94m1.2043\u001b[0m  0.0002  0.5101\n",
      "     28            \u001b[36m1.0000\u001b[0m        0.3320       0.5035            0.5035        \u001b[94m1.2040\u001b[0m  0.0001  0.5025\n",
      "     29            1.0000        0.3760       \u001b[35m0.5069\u001b[0m            \u001b[31m0.5069\u001b[0m        \u001b[94m1.2031\u001b[0m  0.0001  0.5065\n",
      "     30            1.0000        0.3834       0.5069            0.5069        \u001b[94m1.2022\u001b[0m  0.0001  0.5061\n",
      "     31            0.9955        0.3424       0.5035            0.5035        \u001b[94m1.2015\u001b[0m  0.0001  0.5063\n",
      "     32            0.9955        0.3555       0.5035            0.5035        \u001b[94m1.2014\u001b[0m  0.0001  0.5005\n",
      "     33            0.9955        0.3180       0.5035            0.5035        \u001b[94m1.2010\u001b[0m  0.0000  0.5008\n",
      "     34            0.9955        0.3168       0.5000            0.5000        \u001b[94m1.2008\u001b[0m  0.0000  0.5115\n",
      "     35            0.9955        \u001b[32m0.2798\u001b[0m       0.5035            0.5035        \u001b[94m1.2004\u001b[0m  0.0000  0.5027\n",
      "     36            0.9955        0.2911       0.5035            0.5035        \u001b[94m1.1996\u001b[0m  0.0000  0.5046\n",
      "     37            0.9955        0.2866       0.5035            0.5035        \u001b[94m1.1994\u001b[0m  0.0000  0.5290\n",
      "     38            0.9955        \u001b[32m0.2700\u001b[0m       0.5035            0.5035        \u001b[94m1.1989\u001b[0m  0.0000  0.5537\n",
      "     39            0.9955        0.3514       0.5035            0.5035        \u001b[94m1.1988\u001b[0m  0.0000  0.5841\n",
      "     40            0.9955        0.3245       0.5069            0.5069        1.1989  0.0000  0.5801\n",
      "Training model for subject 5 with 230 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2522\u001b[0m        \u001b[32m1.6843\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.7857\u001b[0m  0.0006  0.5655\n",
      "      2            0.2522        \u001b[32m1.3637\u001b[0m       0.2500            0.2500        5.4008  0.0006  0.6302\n",
      "      3            0.2522        \u001b[32m1.2195\u001b[0m       0.2500            0.2500        \u001b[94m4.7576\u001b[0m  0.0006  0.5066\n",
      "      4            0.2522        \u001b[32m1.1783\u001b[0m       0.2500            0.2500        \u001b[94m3.6183\u001b[0m  0.0006  0.5034\n",
      "      5            \u001b[36m0.2739\u001b[0m        \u001b[32m1.0956\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m2.7801\u001b[0m  0.0006  0.5020\n",
      "      6            \u001b[36m0.4087\u001b[0m        \u001b[32m0.9953\u001b[0m       0.2639            0.2639        \u001b[94m2.1547\u001b[0m  0.0006  0.4913\n",
      "      7            \u001b[36m0.4783\u001b[0m        \u001b[32m0.9489\u001b[0m       0.2604            0.2604        \u001b[94m1.8692\u001b[0m  0.0006  0.5016\n",
      "      8            \u001b[36m0.5522\u001b[0m        \u001b[32m0.8870\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        \u001b[94m1.7012\u001b[0m  0.0006  0.5032\n",
      "      9            \u001b[36m0.6217\u001b[0m        \u001b[32m0.7397\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.5773\u001b[0m  0.0006  0.4993\n",
      "     10            \u001b[36m0.6739\u001b[0m        0.7919       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.5155\u001b[0m  0.0005  0.5035\n",
      "     11            \u001b[36m0.7174\u001b[0m        0.7611       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.4496\u001b[0m  0.0005  0.5045\n",
      "     12            \u001b[36m0.8043\u001b[0m        0.7463       0.3611            0.3611        \u001b[94m1.3971\u001b[0m  0.0005  0.5062\n",
      "     13            \u001b[36m0.8261\u001b[0m        0.7500       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.3616\u001b[0m  0.0005  0.4936\n",
      "     14            \u001b[36m0.8522\u001b[0m        \u001b[32m0.7105\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.3355\u001b[0m  0.0005  0.5015\n",
      "     15            \u001b[36m0.8652\u001b[0m        \u001b[32m0.5612\u001b[0m       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.3189\u001b[0m  0.0004  0.4982\n",
      "     16            \u001b[36m0.8913\u001b[0m        \u001b[32m0.5202\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.3079\u001b[0m  0.0004  0.5011\n",
      "     17            \u001b[36m0.9087\u001b[0m        0.5843       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.2897\u001b[0m  0.0004  0.4970\n",
      "     18            \u001b[36m0.9217\u001b[0m        \u001b[32m0.5030\u001b[0m       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.2728\u001b[0m  0.0004  0.4984\n",
      "     19            \u001b[36m0.9304\u001b[0m        0.5115       0.4410            0.4410        \u001b[94m1.2659\u001b[0m  0.0004  0.5111\n",
      "     20            \u001b[36m0.9391\u001b[0m        \u001b[32m0.4846\u001b[0m       0.4410            0.4410        \u001b[94m1.2613\u001b[0m  0.0003  0.5155\n",
      "     21            0.9391        0.5159       0.4653            0.4653        \u001b[94m1.2567\u001b[0m  0.0003  0.4987\n",
      "     22            \u001b[36m0.9522\u001b[0m        0.4922       0.4549            0.4549        1.2581  0.0003  0.6236\n",
      "     23            0.9522        \u001b[32m0.4250\u001b[0m       0.4514            0.4514        1.2573  0.0002  0.6102\n",
      "     24            0.9435        0.4829       0.4479            0.4479        \u001b[94m1.2536\u001b[0m  0.0002  0.6490\n",
      "     25            0.9522        0.4469       0.4618            0.4618        \u001b[94m1.2473\u001b[0m  0.0002  0.6291\n",
      "     26            \u001b[36m0.9565\u001b[0m        0.4501       0.4583            0.4583        \u001b[94m1.2423\u001b[0m  0.0002  0.5339\n",
      "     27            \u001b[36m0.9652\u001b[0m        \u001b[32m0.4141\u001b[0m       0.4653            0.4653        \u001b[94m1.2363\u001b[0m  0.0002  0.6066\n",
      "     28            \u001b[36m0.9696\u001b[0m        0.4502       0.4653            0.4653        \u001b[94m1.2349\u001b[0m  0.0001  0.5153\n",
      "     29            0.9696        \u001b[32m0.3802\u001b[0m       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2332\u001b[0m  0.0001  0.4905\n",
      "     30            0.9696        0.3820       0.4688            0.4688        \u001b[94m1.2309\u001b[0m  0.0001  0.5079\n",
      "     31            0.9696        0.4134       \u001b[35m0.4722\u001b[0m            \u001b[31m0.4722\u001b[0m        \u001b[94m1.2298\u001b[0m  0.0001  0.5067\n",
      "     32            0.9696        \u001b[32m0.3786\u001b[0m       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        \u001b[94m1.2292\u001b[0m  0.0001  0.5029\n",
      "     33            0.9696        \u001b[32m0.3610\u001b[0m       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        \u001b[94m1.2279\u001b[0m  0.0000  0.5069\n",
      "     34            0.9696        0.3828       0.4826            0.4826        \u001b[94m1.2274\u001b[0m  0.0000  0.5004\n",
      "     35            0.9696        0.4161       0.4792            0.4792        \u001b[94m1.2266\u001b[0m  0.0000  0.5009\n",
      "     36            0.9696        \u001b[32m0.3542\u001b[0m       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        \u001b[94m1.2259\u001b[0m  0.0000  0.4960\n",
      "     37            0.9696        \u001b[32m0.3482\u001b[0m       0.4861            0.4861        \u001b[94m1.2255\u001b[0m  0.0000  0.4997\n",
      "     38            0.9696        \u001b[32m0.3421\u001b[0m       0.4861            0.4861        \u001b[94m1.2253\u001b[0m  0.0000  0.5031\n",
      "     39            0.9696        0.3589       0.4861            0.4861        1.2255  0.0000  0.5175\n",
      "     40            0.9696        0.4195       0.4861            0.4861        1.2254  0.0000  0.4974\n",
      "Training model for subject 5 with 240 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2708\u001b[0m        \u001b[32m1.6343\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m2.4702\u001b[0m  0.0006  0.5480\n",
      "      2            \u001b[36m0.3625\u001b[0m        \u001b[32m1.3768\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m1.7814\u001b[0m  0.0006  0.5114\n",
      "      3            \u001b[36m0.4000\u001b[0m        \u001b[32m1.2118\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        2.0110  0.0006  0.4896\n",
      "      4            \u001b[36m0.4083\u001b[0m        \u001b[32m1.0881\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        2.1566  0.0006  0.5031\n",
      "      5            \u001b[36m0.4458\u001b[0m        \u001b[32m0.9940\u001b[0m       0.2812            0.2812        1.9744  0.0006  0.5565\n",
      "      6            \u001b[36m0.5083\u001b[0m        \u001b[32m0.9363\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        1.8001  0.0006  0.5848\n",
      "      7            \u001b[36m0.5542\u001b[0m        \u001b[32m0.9306\u001b[0m       0.3056            0.3056        \u001b[94m1.6975\u001b[0m  0.0006  0.5914\n",
      "      8            \u001b[36m0.5875\u001b[0m        \u001b[32m0.8727\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m1.5987\u001b[0m  0.0006  0.5933\n",
      "      9            \u001b[36m0.6667\u001b[0m        \u001b[32m0.8025\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.4916\u001b[0m  0.0006  0.6290\n",
      "     10            \u001b[36m0.7167\u001b[0m        \u001b[32m0.7775\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.3975\u001b[0m  0.0005  0.5134\n",
      "     11            \u001b[36m0.7625\u001b[0m        \u001b[32m0.6742\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.3524\u001b[0m  0.0005  0.4935\n",
      "     12            \u001b[36m0.7833\u001b[0m        0.7339       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        1.3668  0.0005  0.5020\n",
      "     13            \u001b[36m0.8333\u001b[0m        \u001b[32m0.6491\u001b[0m       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.3420\u001b[0m  0.0005  0.5090\n",
      "     14            \u001b[36m0.8542\u001b[0m        \u001b[32m0.5833\u001b[0m       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.3069\u001b[0m  0.0005  0.5039\n",
      "     15            \u001b[36m0.9000\u001b[0m        0.6138       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.2751\u001b[0m  0.0004  0.5217\n",
      "     16            \u001b[36m0.9042\u001b[0m        \u001b[32m0.5057\u001b[0m       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        \u001b[94m1.2633\u001b[0m  0.0004  0.4991\n",
      "     17            0.8958        0.5148       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        1.2660  0.0004  0.4915\n",
      "     18            0.9000        0.5766       \u001b[35m0.5035\u001b[0m            \u001b[31m0.5035\u001b[0m        1.2650  0.0004  0.5245\n",
      "     19            \u001b[36m0.9250\u001b[0m        0.5407       0.4861            0.4861        \u001b[94m1.2612\u001b[0m  0.0004  0.5060\n",
      "     20            \u001b[36m0.9292\u001b[0m        \u001b[32m0.4976\u001b[0m       0.4792            0.4792        \u001b[94m1.2551\u001b[0m  0.0003  0.4987\n",
      "     21            \u001b[36m0.9417\u001b[0m        \u001b[32m0.4462\u001b[0m       0.4965            0.4965        \u001b[94m1.2464\u001b[0m  0.0003  0.5026\n",
      "     22            \u001b[36m0.9625\u001b[0m        \u001b[32m0.4278\u001b[0m       \u001b[35m0.5069\u001b[0m            \u001b[31m0.5069\u001b[0m        \u001b[94m1.2415\u001b[0m  0.0003  0.5099\n",
      "     23            \u001b[36m0.9667\u001b[0m        0.4493       \u001b[35m0.5104\u001b[0m            \u001b[31m0.5104\u001b[0m        \u001b[94m1.2356\u001b[0m  0.0002  0.5087\n",
      "     24            0.9667        0.4382       0.5035            0.5035        \u001b[94m1.2312\u001b[0m  0.0002  0.5144\n",
      "     25            0.9667        \u001b[32m0.4131\u001b[0m       0.5035            0.5035        \u001b[94m1.2281\u001b[0m  0.0002  0.5163\n",
      "     26            \u001b[36m0.9708\u001b[0m        \u001b[32m0.3751\u001b[0m       0.5069            0.5069        \u001b[94m1.2239\u001b[0m  0.0002  0.5045\n",
      "     27            \u001b[36m0.9750\u001b[0m        0.4276       0.5104            0.5104        \u001b[94m1.2185\u001b[0m  0.0002  0.5126\n",
      "     28            0.9708        0.4161       \u001b[35m0.5139\u001b[0m            \u001b[31m0.5139\u001b[0m        \u001b[94m1.2160\u001b[0m  0.0001  0.5643\n",
      "     29            0.9750        0.3903       0.5069            0.5069        \u001b[94m1.2137\u001b[0m  0.0001  0.7049\n",
      "     30            \u001b[36m0.9792\u001b[0m        \u001b[32m0.3725\u001b[0m       0.5139            0.5139        \u001b[94m1.2136\u001b[0m  0.0001  0.5823\n",
      "     31            0.9792        \u001b[32m0.3556\u001b[0m       \u001b[35m0.5174\u001b[0m            \u001b[31m0.5174\u001b[0m        1.2143  0.0001  0.5999\n",
      "     32            0.9792        0.3641       0.5104            0.5104        1.2143  0.0001  0.6394\n",
      "     33            0.9792        0.3748       0.5069            0.5069        1.2150  0.0000  0.5179\n",
      "     34            0.9792        \u001b[32m0.3277\u001b[0m       0.5069            0.5069        1.2145  0.0000  0.5077\n",
      "     35            0.9792        0.3600       0.5069            0.5069        1.2142  0.0000  0.5000\n",
      "     36            0.9792        0.3867       0.5035            0.5035        1.2137  0.0000  0.5062\n",
      "     37            0.9792        0.3675       0.5035            0.5035        \u001b[94m1.2135\u001b[0m  0.0000  0.5060\n",
      "     38            0.9792        0.3767       0.5035            0.5035        \u001b[94m1.2135\u001b[0m  0.0000  0.5093\n",
      "     39            0.9792        \u001b[32m0.3112\u001b[0m       0.5069            0.5069        \u001b[94m1.2135\u001b[0m  0.0000  0.5162\n",
      "     40            0.9792        0.3367       0.5035            0.5035        1.2136  0.0000  0.4913\n",
      "Training model for subject 5 with 250 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3280\u001b[0m        \u001b[32m1.6454\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m5.0281\u001b[0m  0.0006  0.4946\n",
      "      2            0.2520        \u001b[32m1.4756\u001b[0m       0.2535            0.2535        \u001b[94m4.5988\u001b[0m  0.0006  0.5388\n",
      "      3            0.2680        \u001b[32m1.2253\u001b[0m       0.2604            0.2604        \u001b[94m2.9322\u001b[0m  0.0006  0.5010\n",
      "      4            0.2760        \u001b[32m1.1815\u001b[0m       0.2674            0.2674        \u001b[94m2.3174\u001b[0m  0.0006  0.5065\n",
      "      5            0.3080        \u001b[32m1.0335\u001b[0m       0.2639            0.2639        \u001b[94m2.1025\u001b[0m  0.0006  0.4978\n",
      "      6            \u001b[36m0.3960\u001b[0m        \u001b[32m0.9449\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m1.8303\u001b[0m  0.0006  0.4997\n",
      "      7            \u001b[36m0.5120\u001b[0m        1.0088       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        \u001b[94m1.6324\u001b[0m  0.0006  0.5105\n",
      "      8            \u001b[36m0.6360\u001b[0m        1.0148       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.5508\u001b[0m  0.0006  0.5181\n",
      "      9            \u001b[36m0.6800\u001b[0m        \u001b[32m0.8855\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.5087\u001b[0m  0.0006  0.5021\n",
      "     10            \u001b[36m0.7320\u001b[0m        \u001b[32m0.7740\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.4341\u001b[0m  0.0005  0.5015\n",
      "     11            \u001b[36m0.7680\u001b[0m        0.7965       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.3915\u001b[0m  0.0005  0.5787\n",
      "     12            \u001b[36m0.8040\u001b[0m        \u001b[32m0.6763\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.3632\u001b[0m  0.0005  0.5827\n",
      "     13            \u001b[36m0.8560\u001b[0m        0.6991       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.3256\u001b[0m  0.0005  0.5986\n",
      "     14            \u001b[36m0.9040\u001b[0m        0.6838       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.3060\u001b[0m  0.0005  0.6637\n",
      "     15            0.9040        \u001b[32m0.6236\u001b[0m       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2943\u001b[0m  0.0004  0.6266\n",
      "     16            \u001b[36m0.9080\u001b[0m        \u001b[32m0.6147\u001b[0m       \u001b[35m0.4722\u001b[0m            \u001b[31m0.4722\u001b[0m        \u001b[94m1.2843\u001b[0m  0.0004  0.5087\n",
      "     17            \u001b[36m0.9240\u001b[0m        0.6334       0.4688            0.4688        \u001b[94m1.2756\u001b[0m  0.0004  0.5274\n",
      "     18            \u001b[36m0.9400\u001b[0m        \u001b[32m0.5479\u001b[0m       0.4688            0.4688        \u001b[94m1.2728\u001b[0m  0.0004  0.5112\n",
      "     19            0.9400        0.5521       \u001b[35m0.4931\u001b[0m            \u001b[31m0.4931\u001b[0m        1.2738  0.0004  0.5166\n",
      "     20            \u001b[36m0.9440\u001b[0m        \u001b[32m0.5143\u001b[0m       0.4896            0.4896        1.2778  0.0003  0.5028\n",
      "     21            0.9440        0.5357       0.4931            0.4931        1.2866  0.0003  0.5035\n",
      "     22            \u001b[36m0.9480\u001b[0m        \u001b[32m0.4951\u001b[0m       0.4896            0.4896        1.2871  0.0003  0.5202\n",
      "     23            \u001b[36m0.9520\u001b[0m        0.5338       \u001b[35m0.5000\u001b[0m            \u001b[31m0.5000\u001b[0m        1.2788  0.0002  0.5017\n",
      "     24            \u001b[36m0.9560\u001b[0m        \u001b[32m0.4345\u001b[0m       \u001b[35m0.5139\u001b[0m            \u001b[31m0.5139\u001b[0m        \u001b[94m1.2715\u001b[0m  0.0002  0.5007\n",
      "     25            \u001b[36m0.9600\u001b[0m        0.4585       0.5104            0.5104        \u001b[94m1.2650\u001b[0m  0.0002  0.5196\n",
      "     26            0.9560        \u001b[32m0.4302\u001b[0m       0.5104            0.5104        \u001b[94m1.2589\u001b[0m  0.0002  0.5229\n",
      "     27            \u001b[36m0.9720\u001b[0m        0.4380       \u001b[35m0.5174\u001b[0m            \u001b[31m0.5174\u001b[0m        \u001b[94m1.2508\u001b[0m  0.0002  0.5189\n",
      "     28            \u001b[36m0.9760\u001b[0m        \u001b[32m0.4263\u001b[0m       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        \u001b[94m1.2435\u001b[0m  0.0001  0.5084\n",
      "     29            \u001b[36m0.9840\u001b[0m        \u001b[32m0.3373\u001b[0m       0.5174            0.5174        \u001b[94m1.2389\u001b[0m  0.0001  0.6038\n",
      "     30            0.9840        0.4355       \u001b[35m0.5243\u001b[0m            \u001b[31m0.5243\u001b[0m        \u001b[94m1.2368\u001b[0m  0.0001  0.5013\n",
      "     31            0.9840        0.3801       0.5208            0.5208        \u001b[94m1.2349\u001b[0m  0.0001  0.5222\n",
      "     32            \u001b[36m0.9880\u001b[0m        0.4271       0.5243            0.5243        \u001b[94m1.2332\u001b[0m  0.0001  0.5072\n",
      "     33            0.9840        0.3520       0.5208            0.5208        \u001b[94m1.2330\u001b[0m  0.0000  0.5440\n",
      "     34            0.9800        0.3718       0.5243            0.5243        \u001b[94m1.2322\u001b[0m  0.0000  0.6001\n",
      "     35            0.9800        0.3751       0.5243            0.5243        \u001b[94m1.2322\u001b[0m  0.0000  0.5995\n",
      "     36            0.9840        0.3917       \u001b[35m0.5278\u001b[0m            \u001b[31m0.5278\u001b[0m        1.2323  0.0000  0.6166\n",
      "     37            0.9800        0.3539       0.5278            0.5278        \u001b[94m1.2315\u001b[0m  0.0000  0.6512\n",
      "     38            0.9800        0.3607       0.5278            0.5278        \u001b[94m1.2311\u001b[0m  0.0000  0.5196\n",
      "     39            0.9800        0.3756       0.5278            0.5278        \u001b[94m1.2309\u001b[0m  0.0000  0.5164\n",
      "     40            0.9800        0.4078       0.5278            0.5278        1.2312  0.0000  0.5127\n",
      "Training model for subject 5 with 260 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2462\u001b[0m        \u001b[32m1.5531\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.4450\u001b[0m  0.0006  0.6085\n",
      "      2            \u001b[36m0.3346\u001b[0m        \u001b[32m1.2902\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m2.6725\u001b[0m  0.0006  0.6003\n",
      "      3            \u001b[36m0.4038\u001b[0m        \u001b[32m1.1103\u001b[0m       0.2500            0.2500        \u001b[94m2.5902\u001b[0m  0.0006  0.6134\n",
      "      4            \u001b[36m0.4538\u001b[0m        \u001b[32m1.0286\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m2.3011\u001b[0m  0.0006  0.6059\n",
      "      5            \u001b[36m0.4692\u001b[0m        \u001b[32m0.8989\u001b[0m       0.2535            0.2535        \u001b[94m1.9730\u001b[0m  0.0006  0.5954\n",
      "      6            \u001b[36m0.6000\u001b[0m        \u001b[32m0.8724\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m1.6323\u001b[0m  0.0006  0.6184\n",
      "      7            \u001b[36m0.7000\u001b[0m        \u001b[32m0.7927\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.4766\u001b[0m  0.0006  0.6004\n",
      "      8            \u001b[36m0.7462\u001b[0m        \u001b[32m0.7857\u001b[0m       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.3976\u001b[0m  0.0006  0.6030\n",
      "      9            \u001b[36m0.8346\u001b[0m        \u001b[32m0.6779\u001b[0m       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.3182\u001b[0m  0.0006  0.6127\n",
      "     10            \u001b[36m0.8615\u001b[0m        \u001b[32m0.6238\u001b[0m       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.2917\u001b[0m  0.0005  0.6276\n",
      "     11            \u001b[36m0.8846\u001b[0m        \u001b[32m0.5978\u001b[0m       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.2812\u001b[0m  0.0005  0.6117\n",
      "     12            \u001b[36m0.9077\u001b[0m        \u001b[32m0.5304\u001b[0m       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.2645\u001b[0m  0.0005  0.6197\n",
      "     13            \u001b[36m0.9269\u001b[0m        \u001b[32m0.4766\u001b[0m       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.2567\u001b[0m  0.0005  0.6059\n",
      "     14            \u001b[36m0.9423\u001b[0m        \u001b[32m0.4753\u001b[0m       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2503\u001b[0m  0.0005  0.6397\n",
      "     15            \u001b[36m0.9615\u001b[0m        \u001b[32m0.4162\u001b[0m       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.2373\u001b[0m  0.0004  0.7191\n",
      "     16            0.9615        0.4294       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        1.2490  0.0004  0.7157\n",
      "     17            0.9615        \u001b[32m0.3901\u001b[0m       0.4722            0.4722        1.2678  0.0004  0.7651\n",
      "     18            \u001b[36m0.9654\u001b[0m        \u001b[32m0.3794\u001b[0m       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        1.2463  0.0004  0.7099\n",
      "     19            \u001b[36m0.9846\u001b[0m        \u001b[32m0.3736\u001b[0m       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        \u001b[94m1.2047\u001b[0m  0.0004  0.6052\n",
      "     20            0.9808        \u001b[32m0.3079\u001b[0m       \u001b[35m0.5069\u001b[0m            \u001b[31m0.5069\u001b[0m        \u001b[94m1.1763\u001b[0m  0.0003  0.6022\n",
      "     21            0.9846        \u001b[32m0.3055\u001b[0m       \u001b[35m0.5104\u001b[0m            \u001b[31m0.5104\u001b[0m        \u001b[94m1.1673\u001b[0m  0.0003  0.6116\n",
      "     22            0.9846        \u001b[32m0.3047\u001b[0m       \u001b[35m0.5243\u001b[0m            \u001b[31m0.5243\u001b[0m        \u001b[94m1.1634\u001b[0m  0.0003  0.6001\n",
      "     23            \u001b[36m0.9923\u001b[0m        0.3066       0.5243            0.5243        \u001b[94m1.1578\u001b[0m  0.0002  0.6083\n",
      "     24            \u001b[36m0.9962\u001b[0m        \u001b[32m0.2831\u001b[0m       0.5069            0.5069        \u001b[94m1.1517\u001b[0m  0.0002  0.6092\n",
      "     25            0.9962        0.3173       0.5000            0.5000        1.1523  0.0002  0.6066\n",
      "     26            0.9962        0.3013       0.5104            0.5104        \u001b[94m1.1492\u001b[0m  0.0002  0.6110\n",
      "     27            0.9962        0.2870       0.5000            0.5000        \u001b[94m1.1449\u001b[0m  0.0002  0.6047\n",
      "     28            0.9962        \u001b[32m0.2603\u001b[0m       0.5139            0.5139        \u001b[94m1.1424\u001b[0m  0.0001  0.6129\n",
      "     29            0.9962        0.3006       0.5243            0.5243        1.1432  0.0001  0.7373\n",
      "     30            \u001b[36m1.0000\u001b[0m        0.2886       0.5243            0.5243        1.1430  0.0001  0.6377\n",
      "     31            1.0000        \u001b[32m0.2500\u001b[0m       \u001b[35m0.5278\u001b[0m            \u001b[31m0.5278\u001b[0m        \u001b[94m1.1413\u001b[0m  0.0001  0.6237\n",
      "     32            1.0000        \u001b[32m0.2460\u001b[0m       0.5278            0.5278        \u001b[94m1.1394\u001b[0m  0.0001  0.6090\n",
      "     33            1.0000        \u001b[32m0.2236\u001b[0m       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        \u001b[94m1.1368\u001b[0m  0.0000  0.6167\n",
      "     34            1.0000        0.2459       0.5243            0.5243        \u001b[94m1.1351\u001b[0m  0.0000  0.6891\n",
      "     35            1.0000        0.2431       0.5243            0.5243        \u001b[94m1.1338\u001b[0m  0.0000  0.7250\n",
      "     36            1.0000        \u001b[32m0.2156\u001b[0m       0.5278            0.5278        \u001b[94m1.1321\u001b[0m  0.0000  0.7083\n",
      "     37            1.0000        0.2297       0.5278            0.5278        \u001b[94m1.1313\u001b[0m  0.0000  0.7708\n",
      "     38            1.0000        0.2250       0.5278            0.5278        \u001b[94m1.1310\u001b[0m  0.0000  0.6377\n",
      "     39            1.0000        0.2767       0.5278            0.5278        \u001b[94m1.1309\u001b[0m  0.0000  0.6189\n",
      "     40            1.0000        0.2161       0.5312            0.5312        \u001b[94m1.1308\u001b[0m  0.0000  0.6157\n",
      "Training model for subject 5 with 270 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2667\u001b[0m        \u001b[32m1.6403\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m3.0991\u001b[0m  0.0006  0.6110\n",
      "      2            \u001b[36m0.3222\u001b[0m        \u001b[32m1.3399\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        \u001b[94m2.1183\u001b[0m  0.0006  0.6112\n",
      "      3            \u001b[36m0.4444\u001b[0m        \u001b[32m1.1883\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m1.6621\u001b[0m  0.0006  0.6145\n",
      "      4            \u001b[36m0.5222\u001b[0m        \u001b[32m1.0780\u001b[0m       0.2986            0.2986        1.6994  0.0006  0.5969\n",
      "      5            \u001b[36m0.5407\u001b[0m        \u001b[32m0.9964\u001b[0m       0.3194            0.3194        \u001b[94m1.6521\u001b[0m  0.0006  0.5920\n",
      "      6            \u001b[36m0.6148\u001b[0m        \u001b[32m0.9066\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.5806\u001b[0m  0.0006  0.6021\n",
      "      7            \u001b[36m0.7185\u001b[0m        \u001b[32m0.8514\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.4105\u001b[0m  0.0006  0.6156\n",
      "      8            \u001b[36m0.7852\u001b[0m        \u001b[32m0.7842\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.3334\u001b[0m  0.0006  0.6112\n",
      "      9            \u001b[36m0.8370\u001b[0m        \u001b[32m0.7192\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.2842\u001b[0m  0.0006  0.5973\n",
      "     10            \u001b[36m0.8889\u001b[0m        \u001b[32m0.7025\u001b[0m       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.2576\u001b[0m  0.0005  0.5983\n",
      "     11            0.8667        \u001b[32m0.6281\u001b[0m       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2447\u001b[0m  0.0005  0.5994\n",
      "     12            0.8741        \u001b[32m0.5693\u001b[0m       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        \u001b[94m1.2333\u001b[0m  0.0005  0.5922\n",
      "     13            \u001b[36m0.8963\u001b[0m        \u001b[32m0.5530\u001b[0m       \u001b[35m0.5069\u001b[0m            \u001b[31m0.5069\u001b[0m        \u001b[94m1.1936\u001b[0m  0.0005  0.6323\n",
      "     14            \u001b[36m0.9296\u001b[0m        \u001b[32m0.5119\u001b[0m       0.5069            0.5069        \u001b[94m1.1867\u001b[0m  0.0005  0.7221\n",
      "     15            \u001b[36m0.9333\u001b[0m        0.5830       0.4931            0.4931        1.2001  0.0004  0.7229\n",
      "     16            0.9333        \u001b[32m0.4903\u001b[0m       0.4931            0.4931        1.2017  0.0004  0.7683\n",
      "     17            \u001b[36m0.9481\u001b[0m        \u001b[32m0.4432\u001b[0m       \u001b[35m0.5104\u001b[0m            \u001b[31m0.5104\u001b[0m        \u001b[94m1.1774\u001b[0m  0.0004  0.6775\n",
      "     18            0.9370        \u001b[32m0.4294\u001b[0m       0.5104            0.5104        1.1850  0.0004  0.6117\n",
      "     19            0.9407        0.4498       \u001b[35m0.5174\u001b[0m            \u001b[31m0.5174\u001b[0m        1.1917  0.0004  0.6084\n",
      "     20            \u001b[36m0.9704\u001b[0m        \u001b[32m0.3888\u001b[0m       0.5139            0.5139        1.1943  0.0003  0.5824\n",
      "     21            0.9704        \u001b[32m0.3651\u001b[0m       0.5174            0.5174        \u001b[94m1.1774\u001b[0m  0.0003  0.6101\n",
      "     22            \u001b[36m0.9741\u001b[0m        0.3803       \u001b[35m0.5347\u001b[0m            \u001b[31m0.5347\u001b[0m        \u001b[94m1.1565\u001b[0m  0.0003  0.6063\n",
      "     23            \u001b[36m0.9815\u001b[0m        \u001b[32m0.3262\u001b[0m       \u001b[35m0.5382\u001b[0m            \u001b[31m0.5382\u001b[0m        1.1603  0.0002  0.6017\n",
      "     24            \u001b[36m0.9889\u001b[0m        0.3608       0.5312            0.5312        1.1605  0.0002  0.5992\n",
      "     25            0.9852        0.3415       0.5174            0.5174        1.1600  0.0002  0.6136\n",
      "     26            0.9852        0.3579       0.5278            0.5278        1.1591  0.0002  0.6164\n",
      "     27            0.9852        \u001b[32m0.3217\u001b[0m       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        \u001b[94m1.1544\u001b[0m  0.0002  0.6256\n",
      "     28            0.9889        \u001b[32m0.3010\u001b[0m       0.5417            0.5417        \u001b[94m1.1517\u001b[0m  0.0001  0.6262\n",
      "     29            0.9889        0.3350       0.5417            0.5417        \u001b[94m1.1476\u001b[0m  0.0001  0.6005\n",
      "     30            \u001b[36m0.9926\u001b[0m        \u001b[32m0.2926\u001b[0m       0.5382            0.5382        \u001b[94m1.1420\u001b[0m  0.0001  0.7229\n",
      "     31            \u001b[36m0.9963\u001b[0m        0.2995       \u001b[35m0.5486\u001b[0m            \u001b[31m0.5486\u001b[0m        \u001b[94m1.1374\u001b[0m  0.0001  0.5997\n",
      "     32            0.9963        \u001b[32m0.2847\u001b[0m       0.5486            0.5486        \u001b[94m1.1352\u001b[0m  0.0001  0.5965\n",
      "     33            0.9963        0.3307       0.5486            0.5486        1.1361  0.0000  0.7260\n",
      "     34            0.9963        0.3088       0.5451            0.5451        1.1370  0.0000  0.7688\n",
      "     35            0.9963        \u001b[32m0.2844\u001b[0m       0.5451            0.5451        1.1381  0.0000  0.6885\n",
      "     36            0.9963        0.3060       0.5486            0.5486        1.1387  0.0000  0.7312\n",
      "     37            0.9963        \u001b[32m0.2650\u001b[0m       0.5486            0.5486        1.1385  0.0000  0.6256\n",
      "     38            0.9963        0.2911       0.5486            0.5486        1.1390  0.0000  0.6058\n",
      "     39            0.9963        0.2871       0.5486            0.5486        1.1387  0.0000  0.6182\n",
      "     40            0.9963        0.2893       0.5486            0.5486        1.1388  0.0000  0.6208\n",
      "Training model for subject 6 with 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2000\u001b[0m        \u001b[32m2.4431\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m2.7738\u001b[0m  0.0006  0.3253\n",
      "      2            \u001b[36m0.6000\u001b[0m        \u001b[32m0.8718\u001b[0m       0.2569            0.2569        \u001b[94m2.4841\u001b[0m  0.0006  0.3304\n",
      "      3            0.5000        \u001b[32m0.6837\u001b[0m       0.2500            0.2500        3.0072  0.0006  0.3098\n",
      "      4            0.6000        \u001b[32m0.3557\u001b[0m       0.2500            0.2500        3.0922  0.0006  0.3328\n",
      "      5            \u001b[36m0.8000\u001b[0m        0.4036       0.2500            0.2500        3.0677  0.0006  0.3195\n",
      "      6            \u001b[36m0.9000\u001b[0m        \u001b[32m0.1834\u001b[0m       0.2500            0.2500        3.2455  0.0006  0.3322\n",
      "      7            0.9000        0.1959       0.2500            0.2500        3.3267  0.0006  0.3245\n",
      "      8            0.9000        \u001b[32m0.0735\u001b[0m       0.2500            0.2500        3.3188  0.0006  0.3152\n",
      "      9            0.9000        \u001b[32m0.0546\u001b[0m       0.2535            0.2535        3.2126  0.0006  0.3358\n",
      "     10            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0484\u001b[0m       0.2535            0.2535        3.0745  0.0005  0.3119\n",
      "     11            1.0000        0.0526       0.2535            0.2535        2.9267  0.0005  0.3135\n",
      "     12            1.0000        0.0732       0.2569            0.2569        2.8016  0.0005  0.3322\n",
      "     13            1.0000        \u001b[32m0.0285\u001b[0m       0.2569            0.2569        2.6739  0.0005  0.3321\n",
      "     14            1.0000        0.0502       0.2535            0.2535        2.5681  0.0005  0.3201\n",
      "     15            1.0000        \u001b[32m0.0232\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m2.4736\u001b[0m  0.0004  0.3191\n",
      "     16            1.0000        0.0239       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m2.3981\u001b[0m  0.0004  0.3357\n",
      "     17            1.0000        0.0386       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m2.3406\u001b[0m  0.0004  0.3144\n",
      "     18            1.0000        0.0475       0.2674            0.2674        \u001b[94m2.2867\u001b[0m  0.0004  0.3222\n",
      "     19            1.0000        \u001b[32m0.0183\u001b[0m       0.2708            0.2708        \u001b[94m2.2511\u001b[0m  0.0004  0.3286\n",
      "     20            1.0000        0.0198       0.2639            0.2639        \u001b[94m2.2220\u001b[0m  0.0003  0.3183\n",
      "     21            1.0000        0.0249       0.2674            0.2674        \u001b[94m2.1981\u001b[0m  0.0003  0.3237\n",
      "     22            1.0000        \u001b[32m0.0073\u001b[0m       0.2639            0.2639        \u001b[94m2.1823\u001b[0m  0.0003  0.3138\n",
      "     23            1.0000        0.0194       0.2708            0.2708        \u001b[94m2.1722\u001b[0m  0.0002  0.3247\n",
      "     24            1.0000        0.0123       0.2674            0.2674        \u001b[94m2.1668\u001b[0m  0.0002  0.3150\n",
      "     25            1.0000        0.0212       0.2674            0.2674        \u001b[94m2.1622\u001b[0m  0.0002  0.3233\n",
      "     26            1.0000        0.0223       0.2674            0.2674        \u001b[94m2.1600\u001b[0m  0.0002  0.3156\n",
      "     27            1.0000        0.0160       0.2639            0.2639        2.1601  0.0002  0.3782\n",
      "     28            1.0000        0.0173       0.2674            0.2674        2.1618  0.0001  0.4295\n",
      "     29            1.0000        0.0143       0.2674            0.2674        2.1638  0.0001  0.4259\n",
      "     30            1.0000        0.0125       0.2674            0.2674        2.1641  0.0001  0.4274\n",
      "     31            1.0000        0.0152       0.2674            0.2674        2.1656  0.0001  0.4288\n",
      "     32            1.0000        0.0190       0.2674            0.2674        2.1650  0.0001  0.4327\n",
      "     33            1.0000        0.0174       0.2674            0.2674        2.1683  0.0000  0.3517\n",
      "     34            1.0000        0.0167       0.2674            0.2674        2.1670  0.0000  0.3420\n",
      "     35            1.0000        0.0104       0.2674            0.2674        2.1666  0.0000  0.3229\n",
      "     36            1.0000        0.0143       0.2674            0.2674        2.1650  0.0000  0.3298\n",
      "     37            1.0000        0.0467       0.2674            0.2674        2.1630  0.0000  0.3334\n",
      "     38            1.0000        0.0122       0.2674            0.2674        2.1617  0.0000  0.3189\n",
      "     39            1.0000        \u001b[32m0.0067\u001b[0m       0.2674            0.2674        2.1639  0.0000  0.3177\n",
      "     40            1.0000        0.0219       0.2674            0.2674        2.1657  0.0000  0.3308\n",
      "Training model for subject 6 with 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3500\u001b[0m        \u001b[32m1.6172\u001b[0m       \u001b[35m0.2326\u001b[0m            \u001b[31m0.2326\u001b[0m        \u001b[94m1.6596\u001b[0m  0.0006  0.2770\n",
      "      2            \u001b[36m0.8000\u001b[0m        \u001b[32m1.0636\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        1.6768  0.0006  0.2631\n",
      "      3            \u001b[36m0.9000\u001b[0m        \u001b[32m0.7419\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        1.9608  0.0006  0.2827\n",
      "      4            0.6500        \u001b[32m0.5248\u001b[0m       0.2917            0.2917        2.4212  0.0006  0.2772\n",
      "      5            0.6000        \u001b[32m0.3374\u001b[0m       0.2847            0.2847        2.8911  0.0006  0.2719\n",
      "      6            0.6000        0.4607       0.2743            0.2743        3.2622  0.0006  0.2698\n",
      "      7            0.6000        \u001b[32m0.2646\u001b[0m       0.2847            0.2847        3.3580  0.0006  0.2660\n",
      "      8            0.6000        \u001b[32m0.1695\u001b[0m       0.2917            0.2917        3.2908  0.0006  0.2809\n",
      "      9            0.6500        \u001b[32m0.1094\u001b[0m       0.2951            0.2951        3.1575  0.0006  0.2721\n",
      "     10            0.7500        \u001b[32m0.0973\u001b[0m       0.2986            0.2986        2.9876  0.0005  0.4185\n",
      "     11            0.9000        0.1559       0.3090            0.3090        2.7629  0.0005  0.2655\n",
      "     12            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0556\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        2.5678  0.0005  0.2833\n",
      "     13            1.0000        0.0743       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        2.4098  0.0005  0.2687\n",
      "     14            1.0000        \u001b[32m0.0340\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        2.2964  0.0005  0.2679\n",
      "     15            1.0000        0.0344       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        2.1938  0.0004  0.2784\n",
      "     16            1.0000        0.0413       0.3542            0.3542        2.1122  0.0004  0.2565\n",
      "     17            1.0000        \u001b[32m0.0172\u001b[0m       0.3576            0.3576        2.0436  0.0004  0.2842\n",
      "     18            1.0000        0.0239       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        1.9881  0.0004  0.2858\n",
      "     19            1.0000        0.0340       0.3681            0.3681        1.9454  0.0004  0.2621\n",
      "     20            1.0000        0.0185       0.3681            0.3681        1.9115  0.0003  0.2781\n",
      "     21            1.0000        0.0235       0.3646            0.3646        1.8831  0.0003  0.2676\n",
      "     22            1.0000        \u001b[32m0.0164\u001b[0m       0.3611            0.3611        1.8598  0.0003  0.2839\n",
      "     23            1.0000        0.0520       0.3646            0.3646        1.8347  0.0002  0.2730\n",
      "     24            1.0000        0.0180       0.3715            0.3715        1.8154  0.0002  0.2885\n",
      "     25            1.0000        \u001b[32m0.0116\u001b[0m       0.3715            0.3715        1.8003  0.0002  0.2726\n",
      "     26            1.0000        0.0168       0.3646            0.3646        1.7891  0.0002  0.2701\n",
      "     27            1.0000        0.0246       0.3542            0.3542        1.7805  0.0002  0.2656\n",
      "     28            1.0000        0.0216       0.3542            0.3542        1.7746  0.0001  0.2828\n",
      "     29            1.0000        0.0171       0.3576            0.3576        1.7700  0.0001  0.2702\n",
      "     30            1.0000        0.0203       0.3576            0.3576        1.7659  0.0001  0.2694\n",
      "     31            1.0000        0.0140       0.3576            0.3576        1.7627  0.0001  0.2838\n",
      "     32            1.0000        0.0134       0.3576            0.3576        1.7601  0.0001  0.2818\n",
      "     33            1.0000        0.0215       0.3576            0.3576        1.7581  0.0000  0.2731\n",
      "     34            1.0000        0.0186       0.3542            0.3542        1.7561  0.0000  0.2683\n",
      "     35            1.0000        0.0211       0.3576            0.3576        1.7544  0.0000  0.2728\n",
      "     36            1.0000        0.0182       0.3576            0.3576        1.7531  0.0000  0.2605\n",
      "     37            1.0000        0.0365       0.3576            0.3576        1.7517  0.0000  0.3378\n",
      "     38            1.0000        0.0134       0.3576            0.3576        1.7504  0.0000  0.3606\n",
      "     39            1.0000        0.0204       0.3646            0.3646        1.7493  0.0000  0.3621\n",
      "     40            1.0000        0.0193       0.3646            0.3646        1.7483  0.0000  0.3504\n",
      "Training model for subject 6 with 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3000\u001b[0m        \u001b[32m1.5866\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.3717\u001b[0m  0.0006  0.3535\n",
      "      2            \u001b[36m0.4667\u001b[0m        \u001b[32m0.9382\u001b[0m       0.2361            0.2361        \u001b[94m3.0133\u001b[0m  0.0006  0.3499\n",
      "      3            \u001b[36m0.5333\u001b[0m        \u001b[32m0.7623\u001b[0m       0.2396            0.2396        \u001b[94m2.7621\u001b[0m  0.0006  0.3932\n",
      "      4            0.5333        \u001b[32m0.5742\u001b[0m       0.2431            0.2431        \u001b[94m2.6701\u001b[0m  0.0006  0.3325\n",
      "      5            \u001b[36m0.6000\u001b[0m        \u001b[32m0.4903\u001b[0m       0.2431            0.2431        \u001b[94m2.6091\u001b[0m  0.0006  0.2817\n",
      "      6            0.6000        \u001b[32m0.3125\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        2.7165  0.0006  0.2547\n",
      "      7            \u001b[36m0.6333\u001b[0m        \u001b[32m0.1741\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        2.8013  0.0006  0.2831\n",
      "      8            \u001b[36m0.6667\u001b[0m        0.2241       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        2.7694  0.0006  0.2660\n",
      "      9            \u001b[36m0.7333\u001b[0m        \u001b[32m0.1433\u001b[0m       0.2778            0.2778        2.6584  0.0006  0.2701\n",
      "     10            \u001b[36m0.8667\u001b[0m        \u001b[32m0.1405\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m2.4950\u001b[0m  0.0005  0.2840\n",
      "     11            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1325\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m2.3142\u001b[0m  0.0005  0.2691\n",
      "     12            1.0000        \u001b[32m0.0755\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m2.1700\u001b[0m  0.0005  0.2652\n",
      "     13            1.0000        \u001b[32m0.0719\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m2.0653\u001b[0m  0.0005  0.2690\n",
      "     14            1.0000        0.0747       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m1.9696\u001b[0m  0.0005  0.2533\n",
      "     15            1.0000        0.0723       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m1.8864\u001b[0m  0.0004  0.2649\n",
      "     16            1.0000        \u001b[32m0.0713\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.8300\u001b[0m  0.0004  0.2629\n",
      "     17            1.0000        \u001b[32m0.0571\u001b[0m       0.3333            0.3333        \u001b[94m1.7891\u001b[0m  0.0004  0.2678\n",
      "     18            1.0000        \u001b[32m0.0424\u001b[0m       0.3368            0.3368        \u001b[94m1.7624\u001b[0m  0.0004  0.2577\n",
      "     19            1.0000        0.0445       0.3264            0.3264        \u001b[94m1.7449\u001b[0m  0.0004  0.2514\n",
      "     20            1.0000        0.0448       0.3299            0.3299        \u001b[94m1.7369\u001b[0m  0.0003  0.2785\n",
      "     21            1.0000        0.0428       0.3333            0.3333        \u001b[94m1.7317\u001b[0m  0.0003  0.2716\n",
      "     22            1.0000        \u001b[32m0.0349\u001b[0m       0.3368            0.3368        1.7327  0.0003  0.2628\n",
      "     23            1.0000        0.0370       0.3403            0.3403        1.7346  0.0002  0.2689\n",
      "     24            1.0000        \u001b[32m0.0229\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        1.7375  0.0002  0.2568\n",
      "     25            1.0000        0.0332       0.3507            0.3507        1.7414  0.0002  0.2718\n",
      "     26            1.0000        0.0250       0.3507            0.3507        1.7455  0.0002  0.2480\n",
      "     27            1.0000        0.0357       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        1.7489  0.0002  0.4048\n",
      "     28            1.0000        0.0234       0.3507            0.3507        1.7521  0.0001  0.2850\n",
      "     29            1.0000        0.0409       0.3472            0.3472        1.7547  0.0001  0.2652\n",
      "     30            1.0000        0.0401       0.3472            0.3472        1.7558  0.0001  0.2862\n",
      "     31            1.0000        0.0312       0.3472            0.3472        1.7576  0.0001  0.2667\n",
      "     32            1.0000        \u001b[32m0.0200\u001b[0m       0.3472            0.3472        1.7589  0.0001  0.2514\n",
      "     33            1.0000        0.0216       0.3438            0.3438        1.7599  0.0000  0.2791\n",
      "     34            1.0000        0.0290       0.3438            0.3438        1.7605  0.0000  0.2594\n",
      "     35            1.0000        0.0274       0.3438            0.3438        1.7614  0.0000  0.2596\n",
      "     36            1.0000        0.0292       0.3368            0.3368        1.7623  0.0000  0.2672\n",
      "     37            1.0000        0.0376       0.3333            0.3333        1.7626  0.0000  0.2807\n",
      "     38            1.0000        0.0251       0.3299            0.3299        1.7630  0.0000  0.2627\n",
      "     39            1.0000        0.0423       0.3333            0.3333        1.7634  0.0000  0.2840\n",
      "     40            1.0000        0.0370       0.3333            0.3333        1.7637  0.0000  0.2962\n",
      "Training model for subject 6 with 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.5250\u001b[0m        \u001b[32m1.8573\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m1.8594\u001b[0m  0.0006  0.2822\n",
      "      2            0.4750        \u001b[32m1.1812\u001b[0m       0.2535            0.2535        2.5038  0.0006  0.2682\n",
      "      3            \u001b[36m0.5750\u001b[0m        \u001b[32m0.9996\u001b[0m       0.2292            0.2292        2.2412  0.0006  0.3022\n",
      "      4            \u001b[36m0.6750\u001b[0m        \u001b[32m0.6433\u001b[0m       0.2153            0.2153        2.0527  0.0006  0.2868\n",
      "      5            \u001b[36m0.8750\u001b[0m        \u001b[32m0.5762\u001b[0m       0.2153            0.2153        1.9766  0.0006  0.2796\n",
      "      6            0.8750        \u001b[32m0.4574\u001b[0m       0.2431            0.2431        1.9602  0.0006  0.2687\n",
      "      7            0.8750        \u001b[32m0.3995\u001b[0m       0.2431            0.2431        1.9761  0.0006  0.2600\n",
      "      8            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3510\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        1.9855  0.0006  0.3158\n",
      "      9            \u001b[36m0.9250\u001b[0m        0.3969       0.2535            0.2535        1.9875  0.0006  0.3614\n",
      "     10            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2402\u001b[0m       0.2604            0.2604        1.9747  0.0005  0.3671\n",
      "     11            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2263\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        1.9127  0.0005  0.3278\n",
      "     12            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1759\u001b[0m       0.2847            0.2847        \u001b[94m1.8492\u001b[0m  0.0005  0.3375\n",
      "     13            1.0000        0.1770       0.2812            0.2812        \u001b[94m1.8057\u001b[0m  0.0005  0.3337\n",
      "     14            1.0000        \u001b[32m0.1527\u001b[0m       0.2847            0.2847        \u001b[94m1.7591\u001b[0m  0.0005  0.3537\n",
      "     15            1.0000        \u001b[32m0.1213\u001b[0m       0.2778            0.2778        \u001b[94m1.7286\u001b[0m  0.0004  0.3591\n",
      "     16            1.0000        0.1754       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m1.7110\u001b[0m  0.0004  0.3260\n",
      "     17            1.0000        0.1482       0.2986            0.2986        \u001b[94m1.7023\u001b[0m  0.0004  0.3002\n",
      "     18            1.0000        \u001b[32m0.0998\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        \u001b[94m1.6924\u001b[0m  0.0004  0.2760\n",
      "     19            1.0000        \u001b[32m0.0892\u001b[0m       0.3021            0.3021        \u001b[94m1.6850\u001b[0m  0.0004  0.2831\n",
      "     20            1.0000        \u001b[32m0.0803\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m1.6797\u001b[0m  0.0003  0.2627\n",
      "     21            1.0000        0.1098       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        \u001b[94m1.6755\u001b[0m  0.0003  0.2630\n",
      "     22            1.0000        0.1095       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m1.6716\u001b[0m  0.0003  0.2701\n",
      "     23            1.0000        0.0814       0.3229            0.3229        \u001b[94m1.6674\u001b[0m  0.0002  0.2763\n",
      "     24            1.0000        \u001b[32m0.0801\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m1.6647\u001b[0m  0.0002  0.2709\n",
      "     25            1.0000        \u001b[32m0.0510\u001b[0m       0.3299            0.3299        \u001b[94m1.6628\u001b[0m  0.0002  0.2693\n",
      "     26            1.0000        0.1023       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m1.6615\u001b[0m  0.0002  0.2563\n",
      "     27            1.0000        0.0897       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.6606\u001b[0m  0.0002  0.2812\n",
      "     28            1.0000        0.0636       0.3403            0.3403        \u001b[94m1.6599\u001b[0m  0.0001  0.2855\n",
      "     29            1.0000        0.0734       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.6592\u001b[0m  0.0001  0.2629\n",
      "     30            1.0000        0.0695       0.3438            0.3438        \u001b[94m1.6586\u001b[0m  0.0001  0.2731\n",
      "     31            1.0000        0.0724       0.3438            0.3438        \u001b[94m1.6580\u001b[0m  0.0001  0.2667\n",
      "     32            1.0000        0.0641       0.3403            0.3403        \u001b[94m1.6572\u001b[0m  0.0001  0.2699\n",
      "     33            1.0000        0.0616       0.3403            0.3403        \u001b[94m1.6564\u001b[0m  0.0000  0.2661\n",
      "     34            1.0000        0.0802       0.3403            0.3403        \u001b[94m1.6558\u001b[0m  0.0000  0.2897\n",
      "     35            1.0000        0.0612       0.3403            0.3403        \u001b[94m1.6551\u001b[0m  0.0000  0.2640\n",
      "     36            1.0000        0.0759       0.3403            0.3403        \u001b[94m1.6548\u001b[0m  0.0000  0.2827\n",
      "     37            1.0000        0.0791       0.3403            0.3403        \u001b[94m1.6541\u001b[0m  0.0000  0.2795\n",
      "     38            1.0000        0.0818       0.3403            0.3403        \u001b[94m1.6536\u001b[0m  0.0000  0.3736\n",
      "     39            1.0000        \u001b[32m0.0508\u001b[0m       0.3403            0.3403        \u001b[94m1.6531\u001b[0m  0.0000  0.2809\n",
      "     40            1.0000        0.0567       0.3403            0.3403        \u001b[94m1.6528\u001b[0m  0.0000  0.2682\n",
      "Training model for subject 6 with 50 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2600\u001b[0m        \u001b[32m1.7886\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.9362\u001b[0m  0.0006  0.2815\n",
      "      2            0.2600        \u001b[32m1.3261\u001b[0m       0.2500            0.2500        5.7235  0.0006  0.2836\n",
      "      3            0.2600        \u001b[32m1.0273\u001b[0m       0.2500            0.2500        6.1435  0.0006  0.2890\n",
      "      4            0.2600        \u001b[32m0.8198\u001b[0m       0.2500            0.2500        5.8258  0.0006  0.2831\n",
      "      5            0.2600        \u001b[32m0.7575\u001b[0m       0.2500            0.2500        5.3436  0.0006  0.2904\n",
      "      6            0.2600        \u001b[32m0.6804\u001b[0m       0.2500            0.2500        \u001b[94m4.8767\u001b[0m  0.0006  0.2804\n",
      "      7            \u001b[36m0.2800\u001b[0m        \u001b[32m0.4771\u001b[0m       0.2500            0.2500        \u001b[94m4.4755\u001b[0m  0.0006  0.2980\n",
      "      8            \u001b[36m0.3000\u001b[0m        \u001b[32m0.4251\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m4.0187\u001b[0m  0.0006  0.2816\n",
      "      9            \u001b[36m0.4200\u001b[0m        0.4283       0.2569            0.2569        \u001b[94m3.6113\u001b[0m  0.0006  0.3034\n",
      "     10            \u001b[36m0.5400\u001b[0m        \u001b[32m0.3490\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m3.2510\u001b[0m  0.0005  0.2836\n",
      "     11            \u001b[36m0.6600\u001b[0m        \u001b[32m0.2848\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m3.0357\u001b[0m  0.0005  0.2888\n",
      "     12            \u001b[36m0.7400\u001b[0m        \u001b[32m0.2588\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m2.8328\u001b[0m  0.0005  0.2854\n",
      "     13            \u001b[36m0.8200\u001b[0m        0.3054       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m2.6372\u001b[0m  0.0005  0.2990\n",
      "     14            \u001b[36m0.9000\u001b[0m        \u001b[32m0.2446\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        \u001b[94m2.4630\u001b[0m  0.0005  0.2833\n",
      "     15            \u001b[36m0.9600\u001b[0m        \u001b[32m0.1954\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m2.3136\u001b[0m  0.0004  0.2906\n",
      "     16            \u001b[36m0.9800\u001b[0m        \u001b[32m0.1815\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m2.1718\u001b[0m  0.0004  0.3009\n",
      "     17            0.9800        \u001b[32m0.1585\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m2.0586\u001b[0m  0.0004  0.3407\n",
      "     18            \u001b[36m1.0000\u001b[0m        0.1764       0.3264            0.3264        \u001b[94m1.9773\u001b[0m  0.0004  0.3594\n",
      "     19            1.0000        0.1984       0.3160            0.3160        \u001b[94m1.9155\u001b[0m  0.0004  0.3900\n",
      "     20            1.0000        0.1854       0.3229            0.3229        \u001b[94m1.8759\u001b[0m  0.0003  0.3482\n",
      "     21            1.0000        \u001b[32m0.1433\u001b[0m       0.3160            0.3160        \u001b[94m1.8409\u001b[0m  0.0003  0.3469\n",
      "     22            1.0000        \u001b[32m0.1023\u001b[0m       0.3056            0.3056        \u001b[94m1.8167\u001b[0m  0.0003  0.3359\n",
      "     23            1.0000        0.1423       0.3160            0.3160        \u001b[94m1.7974\u001b[0m  0.0002  0.3632\n",
      "     24            1.0000        0.1340       0.3229            0.3229        \u001b[94m1.7805\u001b[0m  0.0002  0.3953\n",
      "     25            1.0000        \u001b[32m0.0954\u001b[0m       0.3299            0.3299        \u001b[94m1.7691\u001b[0m  0.0002  0.2859\n",
      "     26            1.0000        \u001b[32m0.0889\u001b[0m       0.3368            0.3368        \u001b[94m1.7613\u001b[0m  0.0002  0.2940\n",
      "     27            1.0000        0.0927       0.3264            0.3264        \u001b[94m1.7543\u001b[0m  0.0002  0.2897\n",
      "     28            1.0000        0.1115       0.3299            0.3299        \u001b[94m1.7476\u001b[0m  0.0001  0.2811\n",
      "     29            1.0000        \u001b[32m0.0837\u001b[0m       0.3299            0.3299        \u001b[94m1.7440\u001b[0m  0.0001  0.3065\n",
      "     30            1.0000        0.1363       0.3333            0.3333        \u001b[94m1.7417\u001b[0m  0.0001  0.2974\n",
      "     31            1.0000        0.1024       0.3333            0.3333        \u001b[94m1.7398\u001b[0m  0.0001  0.2919\n",
      "     32            1.0000        0.0892       0.3368            0.3368        \u001b[94m1.7388\u001b[0m  0.0001  0.3019\n",
      "     33            1.0000        0.1053       0.3368            0.3368        \u001b[94m1.7376\u001b[0m  0.0000  0.3022\n",
      "     34            1.0000        0.0917       0.3368            0.3368        \u001b[94m1.7367\u001b[0m  0.0000  0.3022\n",
      "     35            1.0000        0.0845       0.3368            0.3368        \u001b[94m1.7358\u001b[0m  0.0000  0.3065\n",
      "     36            1.0000        \u001b[32m0.0785\u001b[0m       0.3368            0.3368        \u001b[94m1.7347\u001b[0m  0.0000  0.2879\n",
      "     37            1.0000        0.0870       0.3368            0.3368        \u001b[94m1.7338\u001b[0m  0.0000  0.2899\n",
      "     38            1.0000        0.1104       0.3368            0.3368        \u001b[94m1.7332\u001b[0m  0.0000  0.2919\n",
      "     39            1.0000        \u001b[32m0.0777\u001b[0m       0.3368            0.3368        \u001b[94m1.7326\u001b[0m  0.0000  0.2830\n",
      "     40            1.0000        0.0968       0.3333            0.3333        \u001b[94m1.7323\u001b[0m  0.0000  0.3014\n",
      "Training model for subject 6 with 60 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.8227\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m6.1256\u001b[0m  0.0006  0.3035\n",
      "      2            0.2500        \u001b[32m1.2671\u001b[0m       0.2500            0.2500        \u001b[94m5.0793\u001b[0m  0.0006  0.2925\n",
      "      3            \u001b[36m0.2667\u001b[0m        \u001b[32m1.0328\u001b[0m       0.2465            0.2465        \u001b[94m3.8603\u001b[0m  0.0006  0.3000\n",
      "      4            \u001b[36m0.4667\u001b[0m        \u001b[32m0.8581\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m3.2731\u001b[0m  0.0006  0.3112\n",
      "      5            0.4500        \u001b[32m0.7905\u001b[0m       0.3160            0.3160        3.3343  0.0006  0.3047\n",
      "      6            0.4333        \u001b[32m0.7398\u001b[0m       0.2917            0.2917        3.4846  0.0006  0.3009\n",
      "      7            0.4000        \u001b[32m0.5840\u001b[0m       0.2708            0.2708        3.5634  0.0006  0.3152\n",
      "      8            0.3667        \u001b[32m0.4861\u001b[0m       0.2674            0.2674        3.5880  0.0006  0.2919\n",
      "      9            0.4000        \u001b[32m0.4506\u001b[0m       0.2674            0.2674        3.4717  0.0006  0.3068\n",
      "     10            0.4000        \u001b[32m0.4198\u001b[0m       0.2569            0.2569        3.3059  0.0005  0.2899\n",
      "     11            0.4000        \u001b[32m0.3434\u001b[0m       0.2604            0.2604        \u001b[94m3.1156\u001b[0m  0.0005  0.3079\n",
      "     12            0.4500        \u001b[32m0.2918\u001b[0m       0.2743            0.2743        \u001b[94m2.9283\u001b[0m  0.0005  0.2980\n",
      "     13            \u001b[36m0.5500\u001b[0m        \u001b[32m0.2453\u001b[0m       0.2778            0.2778        \u001b[94m2.7132\u001b[0m  0.0005  0.3038\n",
      "     14            \u001b[36m0.6667\u001b[0m        \u001b[32m0.2426\u001b[0m       0.2917            0.2917        \u001b[94m2.5171\u001b[0m  0.0005  0.2983\n",
      "     15            \u001b[36m0.8333\u001b[0m        \u001b[32m0.2306\u001b[0m       0.3021            0.3021        \u001b[94m2.3118\u001b[0m  0.0004  0.2913\n",
      "     16            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2093\u001b[0m       0.3090            0.3090        \u001b[94m2.1443\u001b[0m  0.0004  0.2785\n",
      "     17            \u001b[36m0.9667\u001b[0m        0.2159       0.3160            0.3160        \u001b[94m2.0339\u001b[0m  0.0004  0.2991\n",
      "     18            \u001b[36m0.9833\u001b[0m        \u001b[32m0.1669\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m1.9334\u001b[0m  0.0004  0.3158\n",
      "     19            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1564\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m1.8563\u001b[0m  0.0004  0.3236\n",
      "     20            1.0000        \u001b[32m0.1503\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.7960\u001b[0m  0.0003  0.2984\n",
      "     21            1.0000        \u001b[32m0.1362\u001b[0m       0.3368            0.3368        \u001b[94m1.7521\u001b[0m  0.0003  0.3038\n",
      "     22            1.0000        0.1554       0.3368            0.3368        \u001b[94m1.7184\u001b[0m  0.0003  0.3002\n",
      "     23            1.0000        0.1382       0.3403            0.3403        \u001b[94m1.6930\u001b[0m  0.0002  0.3689\n",
      "     24            1.0000        \u001b[32m0.1146\u001b[0m       0.3403            0.3403        \u001b[94m1.6725\u001b[0m  0.0002  0.4705\n",
      "     25            1.0000        0.1345       0.3368            0.3368        \u001b[94m1.6608\u001b[0m  0.0002  0.3606\n",
      "     26            1.0000        0.1448       0.3194            0.3194        \u001b[94m1.6537\u001b[0m  0.0002  0.3601\n",
      "     27            1.0000        \u001b[32m0.0793\u001b[0m       0.3160            0.3160        \u001b[94m1.6495\u001b[0m  0.0002  0.3603\n",
      "     28            1.0000        0.1191       0.3125            0.3125        \u001b[94m1.6469\u001b[0m  0.0001  0.3616\n",
      "     29            1.0000        0.1228       0.3125            0.3125        \u001b[94m1.6451\u001b[0m  0.0001  0.4130\n",
      "     30            1.0000        0.1161       0.3160            0.3160        \u001b[94m1.6444\u001b[0m  0.0001  0.3165\n",
      "     31            1.0000        0.0985       0.3125            0.3125        1.6448  0.0001  0.3002\n",
      "     32            1.0000        0.1157       0.3160            0.3160        1.6446  0.0001  0.3111\n",
      "     33            1.0000        0.1072       0.3160            0.3160        1.6449  0.0000  0.3027\n",
      "     34            1.0000        0.0907       0.3125            0.3125        1.6445  0.0000  0.2926\n",
      "     35            1.0000        0.1121       0.3125            0.3125        \u001b[94m1.6442\u001b[0m  0.0000  0.3028\n",
      "     36            1.0000        0.1226       0.3125            0.3125        \u001b[94m1.6440\u001b[0m  0.0000  0.2993\n",
      "     37            1.0000        0.1129       0.3160            0.3160        \u001b[94m1.6434\u001b[0m  0.0000  0.3075\n",
      "     38            1.0000        0.1174       0.3194            0.3194        \u001b[94m1.6430\u001b[0m  0.0000  0.2976\n",
      "     39            1.0000        0.1161       0.3194            0.3194        \u001b[94m1.6423\u001b[0m  0.0000  0.2994\n",
      "     40            1.0000        0.1259       0.3194            0.3194        \u001b[94m1.6419\u001b[0m  0.0000  0.3166\n",
      "Training model for subject 6 with 70 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3000\u001b[0m        \u001b[32m1.5945\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m3.4848\u001b[0m  0.0006  0.3154\n",
      "      2            0.2429        \u001b[32m1.2832\u001b[0m       0.2500            0.2500        4.5306  0.0006  0.3035\n",
      "      3            0.2286        \u001b[32m1.2280\u001b[0m       0.2500            0.2500        4.8912  0.0006  0.3128\n",
      "      4            0.2286        \u001b[32m0.9690\u001b[0m       0.2500            0.2500        5.0323  0.0006  0.3137\n",
      "      5            0.2286        \u001b[32m0.8352\u001b[0m       0.2500            0.2500        4.9571  0.0006  0.3199\n",
      "      6            0.2286        \u001b[32m0.7622\u001b[0m       0.2500            0.2500        4.6215  0.0006  0.3057\n",
      "      7            0.2429        \u001b[32m0.6467\u001b[0m       0.2500            0.2500        4.3200  0.0006  0.2966\n",
      "      8            0.2571        \u001b[32m0.5722\u001b[0m       0.2500            0.2500        4.0299  0.0006  0.3149\n",
      "      9            \u001b[36m0.3286\u001b[0m        \u001b[32m0.4616\u001b[0m       0.2500            0.2500        3.6388  0.0006  0.3104\n",
      "     10            \u001b[36m0.3571\u001b[0m        0.4650       0.2674            0.2674        \u001b[94m3.3127\u001b[0m  0.0005  0.3187\n",
      "     11            \u001b[36m0.4429\u001b[0m        0.5038       0.2708            0.2708        \u001b[94m3.1138\u001b[0m  0.0005  0.3239\n",
      "     12            \u001b[36m0.5429\u001b[0m        \u001b[32m0.3500\u001b[0m       0.2812            0.2812        \u001b[94m2.9741\u001b[0m  0.0005  0.3156\n",
      "     13            \u001b[36m0.6429\u001b[0m        \u001b[32m0.3032\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m2.8280\u001b[0m  0.0005  0.3078\n",
      "     14            \u001b[36m0.6714\u001b[0m        \u001b[32m0.3009\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m2.6694\u001b[0m  0.0005  0.3042\n",
      "     15            \u001b[36m0.7714\u001b[0m        \u001b[32m0.2824\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m2.5270\u001b[0m  0.0004  0.3140\n",
      "     16            \u001b[36m0.8286\u001b[0m        0.2938       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m2.4174\u001b[0m  0.0004  0.3149\n",
      "     17            \u001b[36m0.8571\u001b[0m        0.3369       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m2.3197\u001b[0m  0.0004  0.3126\n",
      "     18            \u001b[36m0.9286\u001b[0m        \u001b[32m0.2370\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m2.2076\u001b[0m  0.0004  0.3489\n",
      "     19            \u001b[36m0.9571\u001b[0m        \u001b[32m0.2094\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m2.0973\u001b[0m  0.0004  0.3155\n",
      "     20            \u001b[36m0.9714\u001b[0m        0.2287       0.3507            0.3507        \u001b[94m2.0020\u001b[0m  0.0003  0.3060\n",
      "     21            \u001b[36m0.9857\u001b[0m        \u001b[32m0.1699\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.9288\u001b[0m  0.0003  0.3266\n",
      "     22            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1695\u001b[0m       0.3542            0.3542        \u001b[94m1.8641\u001b[0m  0.0003  0.3032\n",
      "     23            1.0000        \u001b[32m0.1507\u001b[0m       0.3646            0.3646        \u001b[94m1.8142\u001b[0m  0.0002  0.3267\n",
      "     24            1.0000        \u001b[32m0.1414\u001b[0m       0.3542            0.3542        \u001b[94m1.7735\u001b[0m  0.0002  0.3233\n",
      "     25            1.0000        \u001b[32m0.1300\u001b[0m       0.3576            0.3576        \u001b[94m1.7425\u001b[0m  0.0002  0.3422\n",
      "     26            1.0000        0.1824       0.3611            0.3611        \u001b[94m1.7226\u001b[0m  0.0002  0.3711\n",
      "     27            1.0000        0.1883       0.3611            0.3611        \u001b[94m1.7091\u001b[0m  0.0002  0.3971\n",
      "     28            1.0000        0.1434       0.3472            0.3472        \u001b[94m1.6978\u001b[0m  0.0001  0.3876\n",
      "     29            1.0000        0.1659       0.3472            0.3472        \u001b[94m1.6900\u001b[0m  0.0001  0.4134\n",
      "     30            1.0000        \u001b[32m0.1192\u001b[0m       0.3507            0.3507        \u001b[94m1.6842\u001b[0m  0.0001  0.3725\n",
      "     31            1.0000        0.1573       0.3507            0.3507        \u001b[94m1.6790\u001b[0m  0.0001  0.4261\n",
      "     32            1.0000        0.1269       0.3576            0.3576        \u001b[94m1.6767\u001b[0m  0.0001  0.4056\n",
      "     33            1.0000        0.1390       0.3542            0.3542        \u001b[94m1.6747\u001b[0m  0.0000  0.3130\n",
      "     34            1.0000        0.1828       0.3542            0.3542        \u001b[94m1.6718\u001b[0m  0.0000  0.3348\n",
      "     35            1.0000        0.1309       0.3611            0.3611        \u001b[94m1.6688\u001b[0m  0.0000  0.3247\n",
      "     36            1.0000        0.1210       0.3611            0.3611        \u001b[94m1.6664\u001b[0m  0.0000  0.3125\n",
      "     37            1.0000        0.1463       0.3611            0.3611        \u001b[94m1.6653\u001b[0m  0.0000  0.3325\n",
      "     38            1.0000        0.1697       0.3611            0.3611        \u001b[94m1.6642\u001b[0m  0.0000  0.3169\n",
      "     39            1.0000        0.1442       0.3611            0.3611        \u001b[94m1.6635\u001b[0m  0.0000  0.4330\n",
      "     40            1.0000        0.1353       0.3611            0.3611        \u001b[94m1.6624\u001b[0m  0.0000  0.3257\n",
      "Training model for subject 6 with 80 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2750\u001b[0m        \u001b[32m1.7380\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m2.6804\u001b[0m  0.0006  0.3272\n",
      "      2            \u001b[36m0.4125\u001b[0m        \u001b[32m1.2230\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m2.4719\u001b[0m  0.0006  0.3335\n",
      "      3            \u001b[36m0.4500\u001b[0m        \u001b[32m1.1025\u001b[0m       0.3125            0.3125        \u001b[94m2.3461\u001b[0m  0.0006  0.3458\n",
      "      4            0.4000        \u001b[32m0.9212\u001b[0m       0.2674            0.2674        \u001b[94m2.3134\u001b[0m  0.0006  0.3191\n",
      "      5            0.4000        \u001b[32m0.8603\u001b[0m       0.2708            0.2708        2.3820  0.0006  0.3289\n",
      "      6            \u001b[36m0.4625\u001b[0m        \u001b[32m0.8456\u001b[0m       0.2708            0.2708        2.3463  0.0006  0.3303\n",
      "      7            \u001b[36m0.6000\u001b[0m        \u001b[32m0.7066\u001b[0m       0.2951            0.2951        \u001b[94m2.1339\u001b[0m  0.0006  0.3080\n",
      "      8            \u001b[36m0.6500\u001b[0m        \u001b[32m0.5928\u001b[0m       0.3021            0.3021        \u001b[94m2.0936\u001b[0m  0.0006  0.3386\n",
      "      9            \u001b[36m0.7125\u001b[0m        \u001b[32m0.5265\u001b[0m       0.3125            0.3125        2.0982  0.0006  0.3364\n",
      "     10            0.7125        0.5833       0.3160            0.3160        2.1216  0.0005  0.3251\n",
      "     11            \u001b[36m0.7375\u001b[0m        \u001b[32m0.4495\u001b[0m       0.3229            0.3229        2.1070  0.0005  0.3152\n",
      "     12            \u001b[36m0.7500\u001b[0m        0.4590       0.3194            0.3194        \u001b[94m2.0853\u001b[0m  0.0005  0.3334\n",
      "     13            \u001b[36m0.8125\u001b[0m        \u001b[32m0.4337\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m2.0612\u001b[0m  0.0005  0.3321\n",
      "     14            \u001b[36m0.8375\u001b[0m        \u001b[32m0.3872\u001b[0m       0.3368            0.3368        \u001b[94m1.9899\u001b[0m  0.0005  0.3377\n",
      "     15            \u001b[36m0.8500\u001b[0m        \u001b[32m0.3265\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.9367\u001b[0m  0.0004  0.3124\n",
      "     16            \u001b[36m0.9125\u001b[0m        0.3605       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.8468\u001b[0m  0.0004  0.3458\n",
      "     17            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2920\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.7696\u001b[0m  0.0004  0.3514\n",
      "     18            \u001b[36m0.9750\u001b[0m        0.3110       0.3819            0.3819        \u001b[94m1.7005\u001b[0m  0.0004  0.3434\n",
      "     19            \u001b[36m0.9875\u001b[0m        \u001b[32m0.2565\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.6476\u001b[0m  0.0004  0.3250\n",
      "     20            \u001b[36m1.0000\u001b[0m        0.2751       0.3889            0.3889        \u001b[94m1.6030\u001b[0m  0.0003  0.3344\n",
      "     21            1.0000        \u001b[32m0.2161\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.5715\u001b[0m  0.0003  0.3501\n",
      "     22            1.0000        0.2442       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.5485\u001b[0m  0.0003  0.3351\n",
      "     23            1.0000        \u001b[32m0.1909\u001b[0m       0.3924            0.3924        \u001b[94m1.5325\u001b[0m  0.0002  0.3550\n",
      "     24            1.0000        \u001b[32m0.1815\u001b[0m       0.4028            0.4028        \u001b[94m1.5141\u001b[0m  0.0002  0.3426\n",
      "     25            1.0000        0.1866       0.4028            0.4028        \u001b[94m1.4991\u001b[0m  0.0002  0.3503\n",
      "     26            1.0000        \u001b[32m0.1709\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.4873\u001b[0m  0.0002  0.3904\n",
      "     27            1.0000        0.1853       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.4795\u001b[0m  0.0002  0.4100\n",
      "     28            1.0000        0.1735       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.4734\u001b[0m  0.0001  0.3870\n",
      "     29            1.0000        0.1733       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.4684\u001b[0m  0.0001  0.3956\n",
      "     30            1.0000        \u001b[32m0.1595\u001b[0m       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.4642\u001b[0m  0.0001  0.3872\n",
      "     31            1.0000        \u001b[32m0.1316\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.4598\u001b[0m  0.0001  0.4591\n",
      "     32            1.0000        0.1669       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.4559\u001b[0m  0.0001  0.4272\n",
      "     33            1.0000        0.1673       0.4514            0.4514        \u001b[94m1.4527\u001b[0m  0.0000  0.3316\n",
      "     34            1.0000        0.1624       0.4514            0.4514        \u001b[94m1.4496\u001b[0m  0.0000  0.3304\n",
      "     35            1.0000        0.1453       0.4479            0.4479        \u001b[94m1.4467\u001b[0m  0.0000  0.3232\n",
      "     36            1.0000        0.1822       0.4479            0.4479        \u001b[94m1.4444\u001b[0m  0.0000  0.3251\n",
      "     37            1.0000        \u001b[32m0.1228\u001b[0m       0.4514            0.4514        \u001b[94m1.4424\u001b[0m  0.0000  0.3318\n",
      "     38            1.0000        0.1726       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.4408\u001b[0m  0.0000  0.3314\n",
      "     39            1.0000        0.1876       0.4583            0.4583        \u001b[94m1.4393\u001b[0m  0.0000  0.3401\n",
      "     40            1.0000        0.1455       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.4382\u001b[0m  0.0000  0.3447\n",
      "Training model for subject 6 with 90 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2889\u001b[0m        \u001b[32m1.6859\u001b[0m       \u001b[35m0.2292\u001b[0m            \u001b[31m0.2292\u001b[0m        \u001b[94m3.2214\u001b[0m  0.0006  0.3535\n",
      "      2            \u001b[36m0.3000\u001b[0m        \u001b[32m1.2729\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m2.0797\u001b[0m  0.0006  0.3411\n",
      "      3            \u001b[36m0.3778\u001b[0m        \u001b[32m1.2220\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m1.8282\u001b[0m  0.0006  0.3451\n",
      "      4            0.3444        \u001b[32m1.0500\u001b[0m       0.2674            0.2674        \u001b[94m1.8055\u001b[0m  0.0006  0.3427\n",
      "      5            \u001b[36m0.5444\u001b[0m        \u001b[32m0.9595\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        \u001b[94m1.6481\u001b[0m  0.0006  0.3461\n",
      "      6            \u001b[36m0.7889\u001b[0m        \u001b[32m0.7855\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4976\u001b[0m  0.0006  0.3511\n",
      "      7            \u001b[36m0.8222\u001b[0m        \u001b[32m0.7167\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.4960\u001b[0m  0.0006  0.3431\n",
      "      8            \u001b[36m0.8333\u001b[0m        \u001b[32m0.6339\u001b[0m       0.4028            0.4028        1.5083  0.0006  0.3362\n",
      "      9            0.8333        \u001b[32m0.5374\u001b[0m       0.3889            0.3889        1.5422  0.0006  0.3406\n",
      "     10            0.8222        0.5394       0.3958            0.3958        1.5919  0.0005  0.3509\n",
      "     11            0.8111        \u001b[32m0.4702\u001b[0m       0.3958            0.3958        1.6359  0.0005  0.3389\n",
      "     12            \u001b[36m0.8667\u001b[0m        0.4719       0.3889            0.3889        1.6852  0.0005  0.3395\n",
      "     13            \u001b[36m0.9111\u001b[0m        \u001b[32m0.3680\u001b[0m       0.3750            0.3750        1.6905  0.0005  0.4623\n",
      "     14            \u001b[36m0.9556\u001b[0m        0.3724       0.3750            0.3750        1.6864  0.0005  0.3488\n",
      "     15            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3547\u001b[0m       0.3785            0.3785        1.6749  0.0004  0.3457\n",
      "     16            0.9667        \u001b[32m0.2885\u001b[0m       0.3785            0.3785        1.6396  0.0004  0.3293\n",
      "     17            \u001b[36m0.9778\u001b[0m        0.3156       0.3854            0.3854        1.5975  0.0004  0.3398\n",
      "     18            0.9778        \u001b[32m0.2675\u001b[0m       0.4062            0.4062        1.5505  0.0004  0.3269\n",
      "     19            \u001b[36m1.0000\u001b[0m        0.2680       0.4062            0.4062        1.5225  0.0004  0.3562\n",
      "     20            1.0000        \u001b[32m0.2489\u001b[0m       0.4062            0.4062        1.5083  0.0003  0.3441\n",
      "     21            1.0000        \u001b[32m0.2043\u001b[0m       0.3993            0.3993        1.5035  0.0003  0.3413\n",
      "     22            1.0000        \u001b[32m0.1630\u001b[0m       0.4028            0.4028        1.5028  0.0003  0.3481\n",
      "     23            1.0000        0.1963       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        1.5003  0.0002  0.3868\n",
      "     24            1.0000        0.1898       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.4954\u001b[0m  0.0002  0.4006\n",
      "     25            1.0000        0.2232       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.4898\u001b[0m  0.0002  0.4174\n",
      "     26            1.0000        0.2193       0.4271            0.4271        \u001b[94m1.4792\u001b[0m  0.0002  0.3930\n",
      "     27            1.0000        0.1983       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.4711\u001b[0m  0.0002  0.4086\n",
      "     28            1.0000        0.2010       0.4340            0.4340        \u001b[94m1.4675\u001b[0m  0.0001  0.4178\n",
      "     29            1.0000        \u001b[32m0.1606\u001b[0m       0.4306            0.4306        \u001b[94m1.4671\u001b[0m  0.0001  0.3911\n",
      "     30            1.0000        0.2435       0.4271            0.4271        1.4680  0.0001  0.4494\n",
      "     31            1.0000        0.1756       0.4271            0.4271        \u001b[94m1.4671\u001b[0m  0.0001  0.3484\n",
      "     32            1.0000        0.1770       0.4306            0.4306        \u001b[94m1.4663\u001b[0m  0.0001  0.3717\n",
      "     33            1.0000        0.1751       0.4271            0.4271        \u001b[94m1.4661\u001b[0m  0.0000  0.3388\n",
      "     34            1.0000        \u001b[32m0.1603\u001b[0m       0.4201            0.4201        \u001b[94m1.4652\u001b[0m  0.0000  0.3438\n",
      "     35            1.0000        0.1777       0.4201            0.4201        \u001b[94m1.4642\u001b[0m  0.0000  0.3494\n",
      "     36            1.0000        \u001b[32m0.1492\u001b[0m       0.4167            0.4167        \u001b[94m1.4636\u001b[0m  0.0000  0.3482\n",
      "     37            1.0000        0.1851       0.4132            0.4132        \u001b[94m1.4632\u001b[0m  0.0000  0.3556\n",
      "     38            1.0000        \u001b[32m0.1474\u001b[0m       0.4132            0.4132        \u001b[94m1.4627\u001b[0m  0.0000  0.3617\n",
      "     39            1.0000        0.1897       0.4097            0.4097        \u001b[94m1.4624\u001b[0m  0.0000  0.3594\n",
      "     40            1.0000        0.1814       0.4097            0.4097        \u001b[94m1.4620\u001b[0m  0.0000  0.3368\n",
      "Training model for subject 6 with 100 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3200\u001b[0m        \u001b[32m1.8020\u001b[0m       \u001b[35m0.2396\u001b[0m            \u001b[31m0.2396\u001b[0m        \u001b[94m2.8147\u001b[0m  0.0006  0.3779\n",
      "      2            \u001b[36m0.4200\u001b[0m        \u001b[32m1.2577\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m2.7169\u001b[0m  0.0006  0.3865\n",
      "      3            0.4000        \u001b[32m1.0690\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        2.7272  0.0006  0.3535\n",
      "      4            \u001b[36m0.4300\u001b[0m        \u001b[32m0.9709\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        2.7215  0.0006  0.3711\n",
      "      5            \u001b[36m0.4600\u001b[0m        \u001b[32m0.8753\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        2.8519  0.0006  0.3480\n",
      "      6            \u001b[36m0.5300\u001b[0m        \u001b[32m0.7630\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        2.9918  0.0006  0.3634\n",
      "      7            \u001b[36m0.5400\u001b[0m        \u001b[32m0.7515\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        2.9409  0.0006  0.3515\n",
      "      8            \u001b[36m0.5500\u001b[0m        \u001b[32m0.6012\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        2.7836  0.0006  0.3688\n",
      "      9            \u001b[36m0.5800\u001b[0m        \u001b[32m0.5280\u001b[0m       0.3576            0.3576        \u001b[94m2.5797\u001b[0m  0.0006  0.3570\n",
      "     10            \u001b[36m0.6300\u001b[0m        \u001b[32m0.4904\u001b[0m       0.3542            0.3542        \u001b[94m2.3733\u001b[0m  0.0005  0.3433\n",
      "     11            \u001b[36m0.6900\u001b[0m        0.5332       0.3438            0.3438        \u001b[94m2.1350\u001b[0m  0.0005  0.3676\n",
      "     12            \u001b[36m0.8200\u001b[0m        0.4957       0.3472            0.3472        \u001b[94m1.9368\u001b[0m  0.0005  0.3579\n",
      "     13            \u001b[36m0.8700\u001b[0m        \u001b[32m0.4220\u001b[0m       0.3472            0.3472        \u001b[94m1.8300\u001b[0m  0.0005  0.3705\n",
      "     14            \u001b[36m0.9200\u001b[0m        \u001b[32m0.4176\u001b[0m       0.3542            0.3542        \u001b[94m1.7478\u001b[0m  0.0005  0.3687\n",
      "     15            \u001b[36m0.9500\u001b[0m        0.4357       0.3542            0.3542        \u001b[94m1.6924\u001b[0m  0.0004  0.3599\n",
      "     16            \u001b[36m0.9700\u001b[0m        \u001b[32m0.3336\u001b[0m       0.3542            0.3542        \u001b[94m1.6628\u001b[0m  0.0004  0.3626\n",
      "     17            \u001b[36m0.9800\u001b[0m        \u001b[32m0.3314\u001b[0m       0.3542            0.3542        \u001b[94m1.6483\u001b[0m  0.0004  0.3624\n",
      "     18            0.9800        \u001b[32m0.2851\u001b[0m       0.3507            0.3507        \u001b[94m1.6352\u001b[0m  0.0004  0.3580\n",
      "     19            \u001b[36m0.9900\u001b[0m        0.2992       0.3507            0.3507        \u001b[94m1.6216\u001b[0m  0.0004  0.3774\n",
      "     20            0.9900        \u001b[32m0.2820\u001b[0m       0.3438            0.3438        \u001b[94m1.6139\u001b[0m  0.0003  0.4380\n",
      "     21            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2679\u001b[0m       0.3542            0.3542        \u001b[94m1.6055\u001b[0m  0.0003  0.4552\n",
      "     22            1.0000        0.2856       0.3438            0.3438        \u001b[94m1.5920\u001b[0m  0.0003  0.4424\n",
      "     23            1.0000        0.2718       0.3403            0.3403        \u001b[94m1.5758\u001b[0m  0.0002  0.4331\n",
      "     24            1.0000        \u001b[32m0.2315\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.5604\u001b[0m  0.0002  0.4401\n",
      "     25            1.0000        \u001b[32m0.1996\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.5490\u001b[0m  0.0002  0.4249\n",
      "     26            1.0000        0.2625       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.5415\u001b[0m  0.0002  0.3922\n",
      "     27            1.0000        0.2082       0.3785            0.3785        \u001b[94m1.5376\u001b[0m  0.0002  0.3646\n",
      "     28            1.0000        0.2043       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.5369\u001b[0m  0.0001  0.4842\n",
      "     29            1.0000        0.2276       0.3819            0.3819        \u001b[94m1.5361\u001b[0m  0.0001  0.3645\n",
      "     30            1.0000        0.2141       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.5353\u001b[0m  0.0001  0.3651\n",
      "     31            1.0000        0.2256       0.3924            0.3924        1.5364  0.0001  0.3746\n",
      "     32            1.0000        \u001b[32m0.1926\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        1.5383  0.0001  0.3698\n",
      "     33            1.0000        0.2148       0.3924            0.3924        1.5399  0.0000  0.3579\n",
      "     34            1.0000        0.2388       0.3924            0.3924        1.5412  0.0000  0.3615\n",
      "     35            1.0000        \u001b[32m0.1596\u001b[0m       0.3924            0.3924        1.5419  0.0000  0.3784\n",
      "     36            1.0000        0.1930       0.3924            0.3924        1.5419  0.0000  0.3631\n",
      "     37            1.0000        0.1897       0.3889            0.3889        1.5416  0.0000  0.3570\n",
      "     38            1.0000        0.1946       0.3854            0.3854        1.5412  0.0000  0.3688\n",
      "     39            1.0000        0.1895       0.3854            0.3854        1.5409  0.0000  0.3746\n",
      "     40            1.0000        0.2101       0.3854            0.3854        1.5408  0.0000  0.3671\n",
      "Training model for subject 6 with 110 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3000\u001b[0m        \u001b[32m1.8276\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m3.7623\u001b[0m  0.0006  0.3733\n",
      "      2            0.2818        \u001b[32m1.3691\u001b[0m       0.2465            0.2465        \u001b[94m3.5224\u001b[0m  0.0006  0.4156\n",
      "      3            \u001b[36m0.3182\u001b[0m        \u001b[32m1.1535\u001b[0m       0.2431            0.2431        \u001b[94m2.9479\u001b[0m  0.0006  0.3858\n",
      "      4            \u001b[36m0.4364\u001b[0m        \u001b[32m1.0402\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m2.5316\u001b[0m  0.0006  0.3699\n",
      "      5            \u001b[36m0.5182\u001b[0m        \u001b[32m0.8570\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m2.3944\u001b[0m  0.0006  0.3912\n",
      "      6            0.5182        0.8928       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m2.3066\u001b[0m  0.0006  0.3760\n",
      "      7            \u001b[36m0.5455\u001b[0m        \u001b[32m0.8032\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m2.1940\u001b[0m  0.0006  0.3735\n",
      "      8            \u001b[36m0.5545\u001b[0m        \u001b[32m0.7404\u001b[0m       0.3646            0.3646        \u001b[94m2.0634\u001b[0m  0.0006  0.3848\n",
      "      9            \u001b[36m0.5636\u001b[0m        \u001b[32m0.6299\u001b[0m       0.3611            0.3611        \u001b[94m1.9903\u001b[0m  0.0006  0.3822\n",
      "     10            \u001b[36m0.5909\u001b[0m        \u001b[32m0.5761\u001b[0m       0.3542            0.3542        \u001b[94m1.9220\u001b[0m  0.0005  0.3663\n",
      "     11            \u001b[36m0.6455\u001b[0m        \u001b[32m0.5710\u001b[0m       0.3472            0.3472        \u001b[94m1.8776\u001b[0m  0.0005  0.3767\n",
      "     12            \u001b[36m0.6818\u001b[0m        0.5763       0.3611            0.3611        \u001b[94m1.8352\u001b[0m  0.0005  0.3772\n",
      "     13            \u001b[36m0.7545\u001b[0m        \u001b[32m0.5273\u001b[0m       0.3646            0.3646        \u001b[94m1.7904\u001b[0m  0.0005  0.3777\n",
      "     14            \u001b[36m0.8000\u001b[0m        \u001b[32m0.5027\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.7299\u001b[0m  0.0005  0.4351\n",
      "     15            \u001b[36m0.8545\u001b[0m        \u001b[32m0.3965\u001b[0m       0.3785            0.3785        \u001b[94m1.6719\u001b[0m  0.0004  0.4610\n",
      "     16            \u001b[36m0.8909\u001b[0m        0.4636       0.3785            0.3785        \u001b[94m1.6087\u001b[0m  0.0004  0.4855\n",
      "     17            \u001b[36m0.9545\u001b[0m        0.4029       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.5657\u001b[0m  0.0004  0.4498\n",
      "     18            \u001b[36m0.9818\u001b[0m        \u001b[32m0.3640\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.5218\u001b[0m  0.0004  0.4487\n",
      "     19            0.9818        0.3722       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.4906\u001b[0m  0.0004  0.5043\n",
      "     20            \u001b[36m0.9909\u001b[0m        \u001b[32m0.3492\u001b[0m       0.4201            0.4201        \u001b[94m1.4751\u001b[0m  0.0003  0.4599\n",
      "     21            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3174\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.4670\u001b[0m  0.0003  0.3671\n",
      "     22            1.0000        \u001b[32m0.3068\u001b[0m       0.4236            0.4236        \u001b[94m1.4650\u001b[0m  0.0003  0.3709\n",
      "     23            1.0000        \u001b[32m0.2969\u001b[0m       0.4271            0.4271        \u001b[94m1.4644\u001b[0m  0.0002  0.3814\n",
      "     24            1.0000        0.3145       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        1.4651  0.0002  0.3938\n",
      "     25            1.0000        0.3553       0.4340            0.4340        \u001b[94m1.4637\u001b[0m  0.0002  0.3842\n",
      "     26            1.0000        0.3655       0.4340            0.4340        1.4642  0.0002  0.3851\n",
      "     27            1.0000        0.3028       0.4236            0.4236        1.4675  0.0002  0.3926\n",
      "     28            1.0000        0.3053       0.4271            0.4271        1.4710  0.0001  0.3847\n",
      "     29            1.0000        \u001b[32m0.2854\u001b[0m       0.4201            0.4201        1.4747  0.0001  0.4006\n",
      "     30            1.0000        0.3018       0.4201            0.4201        1.4772  0.0001  0.3910\n",
      "     31            1.0000        \u001b[32m0.2339\u001b[0m       0.4201            0.4201        1.4792  0.0001  0.3747\n",
      "     32            1.0000        0.2695       0.4201            0.4201        1.4810  0.0001  0.3812\n",
      "     33            1.0000        0.2791       0.4201            0.4201        1.4817  0.0000  0.3681\n",
      "     34            1.0000        0.2846       0.4201            0.4201        1.4822  0.0000  0.4754\n",
      "     35            1.0000        0.2564       0.4167            0.4167        1.4821  0.0000  0.3770\n",
      "     36            1.0000        0.2748       0.4167            0.4167        1.4818  0.0000  0.3932\n",
      "     37            1.0000        0.2466       0.4167            0.4167        1.4814  0.0000  0.4165\n",
      "     38            1.0000        0.2446       0.4201            0.4201        1.4811  0.0000  0.3997\n",
      "     39            1.0000        0.2651       0.4201            0.4201        1.4809  0.0000  0.3621\n",
      "     40            1.0000        \u001b[32m0.2325\u001b[0m       0.4201            0.4201        1.4808  0.0000  0.3814\n",
      "Training model for subject 6 with 120 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2417\u001b[0m        \u001b[32m1.8084\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m1.9500\u001b[0m  0.0006  0.3936\n",
      "      2            \u001b[36m0.2667\u001b[0m        \u001b[32m1.3176\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        2.0905  0.0006  0.3967\n",
      "      3            \u001b[36m0.5750\u001b[0m        \u001b[32m1.1645\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.9031\u001b[0m  0.0006  0.3841\n",
      "      4            0.5583        \u001b[32m1.1473\u001b[0m       0.3403            0.3403        \u001b[94m1.8151\u001b[0m  0.0006  0.3910\n",
      "      5            0.5000        \u001b[32m1.0126\u001b[0m       0.2812            0.2812        1.9115  0.0006  0.3785\n",
      "      6            0.4750        \u001b[32m0.8828\u001b[0m       0.2951            0.2951        2.0679  0.0006  0.3892\n",
      "      7            0.5333        \u001b[32m0.8279\u001b[0m       0.3299            0.3299        2.2296  0.0006  0.3899\n",
      "      8            0.5417        \u001b[32m0.7216\u001b[0m       0.3611            0.3611        2.3148  0.0006  0.4465\n",
      "      9            0.5583        \u001b[32m0.6848\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        2.2994  0.0006  0.4734\n",
      "     10            \u001b[36m0.6083\u001b[0m        \u001b[32m0.5891\u001b[0m       0.3715            0.3715        2.1734  0.0005  0.4630\n",
      "     11            \u001b[36m0.6750\u001b[0m        \u001b[32m0.5657\u001b[0m       0.3750            0.3750        1.9992  0.0005  0.4329\n",
      "     12            \u001b[36m0.8333\u001b[0m        0.5864       0.3576            0.3576        1.8536  0.0005  0.4533\n",
      "     13            \u001b[36m0.8750\u001b[0m        \u001b[32m0.5254\u001b[0m       0.3681            0.3681        \u001b[94m1.7645\u001b[0m  0.0005  0.4922\n",
      "     14            \u001b[36m0.9250\u001b[0m        \u001b[32m0.4690\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.6952\u001b[0m  0.0005  0.3875\n",
      "     15            \u001b[36m0.9417\u001b[0m        0.5047       0.3993            0.3993        \u001b[94m1.6412\u001b[0m  0.0004  0.4158\n",
      "     16            \u001b[36m0.9833\u001b[0m        \u001b[32m0.4184\u001b[0m       0.3993            0.3993        \u001b[94m1.6055\u001b[0m  0.0004  0.3814\n",
      "     17            0.9833        \u001b[32m0.4181\u001b[0m       0.3993            0.3993        \u001b[94m1.5760\u001b[0m  0.0004  0.3825\n",
      "     18            0.9833        0.4182       0.3958            0.3958        \u001b[94m1.5682\u001b[0m  0.0004  0.3866\n",
      "     19            \u001b[36m0.9917\u001b[0m        \u001b[32m0.3539\u001b[0m       0.3889            0.3889        \u001b[94m1.5645\u001b[0m  0.0004  0.4005\n",
      "     20            0.9917        0.4052       0.3924            0.3924        \u001b[94m1.5524\u001b[0m  0.0003  0.3857\n",
      "     21            \u001b[36m1.0000\u001b[0m        0.3655       0.3889            0.3889        \u001b[94m1.5454\u001b[0m  0.0003  0.3979\n",
      "     22            1.0000        \u001b[32m0.3273\u001b[0m       0.3854            0.3854        \u001b[94m1.5450\u001b[0m  0.0003  0.3753\n",
      "     23            1.0000        \u001b[32m0.2892\u001b[0m       0.3819            0.3819        1.5493  0.0002  0.3900\n",
      "     24            1.0000        0.2936       0.3854            0.3854        1.5557  0.0002  0.3841\n",
      "     25            1.0000        0.3215       0.3819            0.3819        1.5588  0.0002  0.3900\n",
      "     26            1.0000        0.2911       0.3715            0.3715        1.5606  0.0002  0.3777\n",
      "     27            1.0000        0.3042       0.3715            0.3715        1.5596  0.0002  0.3950\n",
      "     28            1.0000        \u001b[32m0.2889\u001b[0m       0.3750            0.3750        1.5589  0.0001  0.3878\n",
      "     29            1.0000        \u001b[32m0.2836\u001b[0m       0.3715            0.3715        1.5582  0.0001  0.3958\n",
      "     30            1.0000        0.3050       0.3750            0.3750        1.5556  0.0001  0.3999\n",
      "     31            1.0000        \u001b[32m0.2493\u001b[0m       0.3750            0.3750        1.5532  0.0001  0.4010\n",
      "     32            1.0000        0.2762       0.3750            0.3750        1.5505  0.0001  0.3911\n",
      "     33            1.0000        0.2862       0.3715            0.3715        1.5481  0.0000  0.3743\n",
      "     34            1.0000        0.2576       0.3715            0.3715        1.5467  0.0000  0.3955\n",
      "     35            1.0000        \u001b[32m0.2444\u001b[0m       0.3646            0.3646        1.5451  0.0000  0.3938\n",
      "     36            1.0000        0.2474       0.3681            0.3681        \u001b[94m1.5439\u001b[0m  0.0000  0.3761\n",
      "     37            1.0000        \u001b[32m0.2382\u001b[0m       0.3681            0.3681        \u001b[94m1.5429\u001b[0m  0.0000  0.3954\n",
      "     38            1.0000        0.2710       0.3681            0.3681        \u001b[94m1.5422\u001b[0m  0.0000  0.3964\n",
      "     39            1.0000        0.2390       0.3681            0.3681        \u001b[94m1.5417\u001b[0m  0.0000  0.3875\n",
      "     40            1.0000        0.2408       0.3681            0.3681        \u001b[94m1.5413\u001b[0m  0.0000  0.4060\n",
      "Training model for subject 6 with 130 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2615\u001b[0m        \u001b[32m1.5018\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m3.0636\u001b[0m  0.0006  0.5020\n",
      "      2            \u001b[36m0.3231\u001b[0m        \u001b[32m1.3075\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m2.7948\u001b[0m  0.0006  0.4638\n",
      "      3            \u001b[36m0.3615\u001b[0m        \u001b[32m1.0802\u001b[0m       0.2639            0.2639        \u001b[94m2.6364\u001b[0m  0.0006  0.4706\n",
      "      4            0.3462        \u001b[32m0.9273\u001b[0m       0.2639            0.2639        \u001b[94m2.6248\u001b[0m  0.0006  0.4700\n",
      "      5            0.2846        \u001b[32m0.8218\u001b[0m       0.2604            0.2604        3.0430  0.0006  0.4553\n",
      "      6            0.2615        \u001b[32m0.7772\u001b[0m       0.2500            0.2500        3.5660  0.0006  0.4879\n",
      "      7            0.2538        \u001b[32m0.7173\u001b[0m       0.2500            0.2500        3.8126  0.0006  0.4144\n",
      "      8            0.2692        \u001b[32m0.6880\u001b[0m       0.2500            0.2500        3.8738  0.0006  0.3998\n",
      "      9            0.2692        \u001b[32m0.6880\u001b[0m       0.2535            0.2535        3.7331  0.0006  0.3941\n",
      "     10            0.2769        \u001b[32m0.6034\u001b[0m       0.2535            0.2535        3.5237  0.0005  0.3858\n",
      "     11            0.3231        \u001b[32m0.5546\u001b[0m       0.2639            0.2639        3.2322  0.0005  0.3826\n",
      "     12            \u001b[36m0.4077\u001b[0m        \u001b[32m0.5515\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        2.8244  0.0005  0.4083\n",
      "     13            \u001b[36m0.5308\u001b[0m        \u001b[32m0.5035\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        \u001b[94m2.4725\u001b[0m  0.0005  0.4006\n",
      "     14            \u001b[36m0.6077\u001b[0m        \u001b[32m0.4536\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m2.2363\u001b[0m  0.0005  0.4069\n",
      "     15            \u001b[36m0.7154\u001b[0m        0.4846       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m2.0678\u001b[0m  0.0004  0.3933\n",
      "     16            \u001b[36m0.7846\u001b[0m        \u001b[32m0.4311\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.8949\u001b[0m  0.0004  0.3842\n",
      "     17            \u001b[36m0.8308\u001b[0m        \u001b[32m0.3952\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.7567\u001b[0m  0.0004  0.3958\n",
      "     18            \u001b[36m0.8923\u001b[0m        \u001b[32m0.3140\u001b[0m       0.3681            0.3681        \u001b[94m1.6666\u001b[0m  0.0004  0.4019\n",
      "     19            \u001b[36m0.9231\u001b[0m        0.3313       0.3681            0.3681        \u001b[94m1.5974\u001b[0m  0.0004  0.4110\n",
      "     20            \u001b[36m0.9462\u001b[0m        0.3179       0.3750            0.3750        \u001b[94m1.5532\u001b[0m  0.0003  0.3924\n",
      "     21            \u001b[36m0.9692\u001b[0m        \u001b[32m0.2918\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.5147\u001b[0m  0.0003  0.3999\n",
      "     22            \u001b[36m0.9923\u001b[0m        0.3198       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.4856\u001b[0m  0.0003  0.3999\n",
      "     23            0.9923        0.3225       0.3819            0.3819        \u001b[94m1.4630\u001b[0m  0.0002  0.3947\n",
      "     24            0.9846        0.3041       0.3854            0.3854        \u001b[94m1.4464\u001b[0m  0.0002  0.3913\n",
      "     25            0.9923        \u001b[32m0.2742\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.4327\u001b[0m  0.0002  0.4046\n",
      "     26            0.9923        \u001b[32m0.2741\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.4203\u001b[0m  0.0002  0.3897\n",
      "     27            0.9923        \u001b[32m0.2215\u001b[0m       0.3993            0.3993        \u001b[94m1.4098\u001b[0m  0.0002  0.4010\n",
      "     28            0.9923        0.3012       0.4097            0.4097        \u001b[94m1.4012\u001b[0m  0.0001  0.3912\n",
      "     29            0.9923        \u001b[32m0.2182\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.3927\u001b[0m  0.0001  0.4000\n",
      "     30            0.9923        0.2753       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.3851\u001b[0m  0.0001  0.3933\n",
      "     31            0.9923        0.2564       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.3786\u001b[0m  0.0001  0.4155\n",
      "     32            0.9923        0.2574       0.4236            0.4236        \u001b[94m1.3743\u001b[0m  0.0001  0.4654\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.2767       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.3716\u001b[0m  0.0000  0.4527\n",
      "     34            1.0000        0.2290       0.4167            0.4167        \u001b[94m1.3699\u001b[0m  0.0000  0.6114\n",
      "     35            1.0000        0.2295       0.4167            0.4167        \u001b[94m1.3687\u001b[0m  0.0000  0.4845\n",
      "     36            1.0000        0.2341       0.4236            0.4236        \u001b[94m1.3676\u001b[0m  0.0000  0.4782\n",
      "     37            1.0000        0.2309       0.4236            0.4236        \u001b[94m1.3665\u001b[0m  0.0000  0.5602\n",
      "     38            1.0000        \u001b[32m0.2077\u001b[0m       0.4236            0.4236        \u001b[94m1.3655\u001b[0m  0.0000  0.4106\n",
      "     39            1.0000        0.2219       0.4236            0.4236        \u001b[94m1.3645\u001b[0m  0.0000  0.3983\n",
      "     40            1.0000        0.2270       0.4236            0.4236        \u001b[94m1.3638\u001b[0m  0.0000  0.3988\n",
      "Training model for subject 6 with 140 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.7383\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.2177\u001b[0m  0.0006  0.3889\n",
      "      2            \u001b[36m0.2786\u001b[0m        \u001b[32m1.4672\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m2.0677\u001b[0m  0.0006  0.3968\n",
      "      3            \u001b[36m0.4000\u001b[0m        \u001b[32m1.1927\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m1.8791\u001b[0m  0.0006  0.4017\n",
      "      4            0.3143        \u001b[32m0.9504\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        2.2436  0.0006  0.3921\n",
      "      5            0.2929        0.9633       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        2.5194  0.0006  0.4055\n",
      "      6            0.2929        \u001b[32m0.8974\u001b[0m       0.2708            0.2708        2.8686  0.0006  0.3849\n",
      "      7            0.3143        \u001b[32m0.7929\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        2.9482  0.0006  0.3987\n",
      "      8            0.3500        \u001b[32m0.7566\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        2.6947  0.0006  0.3969\n",
      "      9            0.3714        0.7898       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        2.4098  0.0006  0.3770\n",
      "     10            \u001b[36m0.4429\u001b[0m        \u001b[32m0.6235\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        2.1539  0.0005  0.4170\n",
      "     11            \u001b[36m0.6000\u001b[0m        0.6657       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        1.9551  0.0005  0.3811\n",
      "     12            \u001b[36m0.7214\u001b[0m        \u001b[32m0.5641\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.8171\u001b[0m  0.0005  0.3963\n",
      "     13            \u001b[36m0.8071\u001b[0m        \u001b[32m0.5609\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.6941\u001b[0m  0.0005  0.4010\n",
      "     14            \u001b[36m0.8429\u001b[0m        \u001b[32m0.5336\u001b[0m       0.3958            0.3958        \u001b[94m1.6107\u001b[0m  0.0005  0.3996\n",
      "     15            \u001b[36m0.8857\u001b[0m        \u001b[32m0.5153\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.5674\u001b[0m  0.0004  0.4109\n",
      "     16            \u001b[36m0.9357\u001b[0m        \u001b[32m0.4510\u001b[0m       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.5360\u001b[0m  0.0004  0.4071\n",
      "     17            \u001b[36m0.9429\u001b[0m        0.4846       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.5001\u001b[0m  0.0004  0.4045\n",
      "     18            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4047\u001b[0m       0.4097            0.4097        \u001b[94m1.4658\u001b[0m  0.0004  0.4017\n",
      "     19            \u001b[36m0.9714\u001b[0m        \u001b[32m0.4012\u001b[0m       0.4097            0.4097        \u001b[94m1.4429\u001b[0m  0.0004  0.3903\n",
      "     20            \u001b[36m0.9786\u001b[0m        0.4092       0.4132            0.4132        \u001b[94m1.4394\u001b[0m  0.0003  0.3981\n",
      "     21            \u001b[36m0.9857\u001b[0m        \u001b[32m0.3731\u001b[0m       0.4201            0.4201        1.4481  0.0003  0.3973\n",
      "     22            0.9857        \u001b[32m0.3477\u001b[0m       0.4201            0.4201        1.4486  0.0003  0.4081\n",
      "     23            0.9857        0.3611       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        1.4400  0.0002  0.4955\n",
      "     24            0.9857        0.3613       0.4271            0.4271        \u001b[94m1.4360\u001b[0m  0.0002  0.4755\n",
      "     25            0.9857        \u001b[32m0.3355\u001b[0m       0.4271            0.4271        \u001b[94m1.4323\u001b[0m  0.0002  0.4997\n",
      "     26            0.9857        \u001b[32m0.3012\u001b[0m       0.4271            0.4271        \u001b[94m1.4249\u001b[0m  0.0002  0.4834\n",
      "     27            0.9857        \u001b[32m0.2797\u001b[0m       0.4236            0.4236        \u001b[94m1.4162\u001b[0m  0.0002  0.4863\n",
      "     28            \u001b[36m0.9929\u001b[0m        0.3317       0.4236            0.4236        \u001b[94m1.4107\u001b[0m  0.0001  0.4827\n",
      "     29            0.9929        0.2864       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.4047\u001b[0m  0.0001  0.4102\n",
      "     30            0.9929        0.3913       0.4340            0.4340        \u001b[94m1.4022\u001b[0m  0.0001  0.4175\n",
      "     31            0.9929        \u001b[32m0.2787\u001b[0m       0.4271            0.4271        \u001b[94m1.3990\u001b[0m  0.0001  0.4182\n",
      "     32            0.9929        0.2936       0.4306            0.4306        \u001b[94m1.3967\u001b[0m  0.0001  0.4047\n",
      "     33            0.9929        0.3283       0.4306            0.4306        \u001b[94m1.3946\u001b[0m  0.0000  0.4084\n",
      "     34            0.9929        \u001b[32m0.2608\u001b[0m       0.4306            0.4306        \u001b[94m1.3922\u001b[0m  0.0000  0.4057\n",
      "     35            0.9929        0.2745       0.4340            0.4340        \u001b[94m1.3905\u001b[0m  0.0000  0.4075\n",
      "     36            0.9929        0.2779       0.4340            0.4340        \u001b[94m1.3883\u001b[0m  0.0000  0.4429\n",
      "     37            \u001b[36m1.0000\u001b[0m        0.3357       0.4340            0.4340        \u001b[94m1.3866\u001b[0m  0.0000  0.4063\n",
      "     38            1.0000        0.3239       0.4306            0.4306        \u001b[94m1.3850\u001b[0m  0.0000  0.4188\n",
      "     39            1.0000        0.3016       0.4306            0.4306        \u001b[94m1.3840\u001b[0m  0.0000  0.4048\n",
      "     40            1.0000        0.3047       0.4306            0.4306        \u001b[94m1.3831\u001b[0m  0.0000  0.5180\n",
      "Training model for subject 6 with 150 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2467\u001b[0m        \u001b[32m1.6430\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m1.8804\u001b[0m  0.0006  0.4026\n",
      "      2            0.2467        \u001b[32m1.3420\u001b[0m       0.2500            0.2500        2.1885  0.0006  0.4077\n",
      "      3            0.2467        \u001b[32m1.2506\u001b[0m       0.2500            0.2500        3.4603  0.0006  0.3867\n",
      "      4            0.2467        \u001b[32m1.0658\u001b[0m       0.2500            0.2500        3.5852  0.0006  0.3891\n",
      "      5            \u001b[36m0.2533\u001b[0m        \u001b[32m0.9761\u001b[0m       0.2604            0.2604        3.2844  0.0006  0.3953\n",
      "      6            \u001b[36m0.2667\u001b[0m        \u001b[32m0.9357\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        2.9962  0.0006  0.4022\n",
      "      7            \u001b[36m0.2867\u001b[0m        \u001b[32m0.8920\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        2.6226  0.0006  0.3975\n",
      "      8            \u001b[36m0.3067\u001b[0m        \u001b[32m0.7763\u001b[0m       0.2639            0.2639        2.7155  0.0006  0.4086\n",
      "      9            \u001b[36m0.3200\u001b[0m        \u001b[32m0.7541\u001b[0m       0.2674            0.2674        2.7726  0.0006  0.4140\n",
      "     10            \u001b[36m0.3400\u001b[0m        \u001b[32m0.7300\u001b[0m       0.2708            0.2708        2.5645  0.0005  0.4101\n",
      "     11            \u001b[36m0.4133\u001b[0m        \u001b[32m0.6644\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        2.3494  0.0005  0.3955\n",
      "     12            0.4133        \u001b[32m0.5583\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        2.2919  0.0005  0.3986\n",
      "     13            \u001b[36m0.4667\u001b[0m        0.6953       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        2.1761  0.0005  0.4385\n",
      "     14            \u001b[36m0.5400\u001b[0m        0.5683       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        2.0090  0.0005  0.4645\n",
      "     15            \u001b[36m0.6667\u001b[0m        0.5598       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m1.8556\u001b[0m  0.0004  0.4531\n",
      "     16            \u001b[36m0.7067\u001b[0m        \u001b[32m0.4928\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.7475\u001b[0m  0.0004  0.4844\n",
      "     17            \u001b[36m0.8000\u001b[0m        \u001b[32m0.4806\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.6570\u001b[0m  0.0004  0.4914\n",
      "     18            \u001b[36m0.8400\u001b[0m        \u001b[32m0.4338\u001b[0m       0.3819            0.3819        \u001b[94m1.5839\u001b[0m  0.0004  0.4644\n",
      "     19            \u001b[36m0.8800\u001b[0m        \u001b[32m0.4088\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.5276\u001b[0m  0.0004  0.5056\n",
      "     20            \u001b[36m0.9067\u001b[0m        \u001b[32m0.3915\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.4930\u001b[0m  0.0003  0.4119\n",
      "     21            \u001b[36m0.9333\u001b[0m        0.4340       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.4554\u001b[0m  0.0003  0.4066\n",
      "     22            \u001b[36m0.9467\u001b[0m        0.4373       0.4132            0.4132        \u001b[94m1.4264\u001b[0m  0.0003  0.4027\n",
      "     23            0.9467        \u001b[32m0.3667\u001b[0m       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.4088\u001b[0m  0.0002  0.4153\n",
      "     24            \u001b[36m0.9533\u001b[0m        \u001b[32m0.3638\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.3962\u001b[0m  0.0002  0.4061\n",
      "     25            0.9533        0.3845       0.4271            0.4271        \u001b[94m1.3838\u001b[0m  0.0002  0.4012\n",
      "     26            \u001b[36m0.9600\u001b[0m        0.3698       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.3756\u001b[0m  0.0002  0.4055\n",
      "     27            \u001b[36m0.9733\u001b[0m        0.3733       0.4236            0.4236        \u001b[94m1.3686\u001b[0m  0.0002  0.4112\n",
      "     28            \u001b[36m0.9800\u001b[0m        \u001b[32m0.3084\u001b[0m       0.4236            0.4236        \u001b[94m1.3607\u001b[0m  0.0001  0.4167\n",
      "     29            0.9800        0.3370       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.3531\u001b[0m  0.0001  0.3861\n",
      "     30            \u001b[36m0.9867\u001b[0m        0.3458       0.4340            0.4340        \u001b[94m1.3468\u001b[0m  0.0001  0.3906\n",
      "     31            0.9867        \u001b[32m0.2779\u001b[0m       0.4375            0.4375        \u001b[94m1.3428\u001b[0m  0.0001  0.4132\n",
      "     32            0.9867        \u001b[32m0.2713\u001b[0m       0.4375            0.4375        \u001b[94m1.3385\u001b[0m  0.0001  0.3921\n",
      "     33            0.9867        0.3274       0.4375            0.4375        \u001b[94m1.3355\u001b[0m  0.0000  0.3966\n",
      "     34            0.9867        0.2983       0.4306            0.4306        \u001b[94m1.3327\u001b[0m  0.0000  0.4004\n",
      "     35            0.9867        \u001b[32m0.2533\u001b[0m       0.4306            0.4306        \u001b[94m1.3314\u001b[0m  0.0000  0.4054\n",
      "     36            0.9867        0.2903       0.4306            0.4306        \u001b[94m1.3304\u001b[0m  0.0000  0.4066\n",
      "     37            0.9867        0.3013       0.4236            0.4236        \u001b[94m1.3304\u001b[0m  0.0000  0.4080\n",
      "     38            0.9867        0.3627       0.4271            0.4271        \u001b[94m1.3296\u001b[0m  0.0000  0.4122\n",
      "     39            0.9867        0.3111       0.4236            0.4236        1.3296  0.0000  0.4116\n",
      "     40            0.9867        0.3177       0.4271            0.4271        1.3296  0.0000  0.3869\n",
      "Training model for subject 6 with 160 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.7931\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.4678\u001b[0m  0.0006  0.3968\n",
      "      2            0.2500        \u001b[32m1.4560\u001b[0m       0.2500            0.2500        4.4850  0.0006  0.3878\n",
      "      3            0.2500        \u001b[32m1.2677\u001b[0m       0.2500            0.2500        \u001b[94m4.1202\u001b[0m  0.0006  0.4013\n",
      "      4            0.2500        \u001b[32m1.1137\u001b[0m       0.2500            0.2500        \u001b[94m3.7379\u001b[0m  0.0006  0.4339\n",
      "      5            0.2500        \u001b[32m1.0392\u001b[0m       0.2500            0.2500        \u001b[94m3.3031\u001b[0m  0.0006  0.4539\n",
      "      6            \u001b[36m0.2625\u001b[0m        \u001b[32m0.9198\u001b[0m       0.2500            0.2500        \u001b[94m3.1816\u001b[0m  0.0006  0.4861\n",
      "      7            \u001b[36m0.2687\u001b[0m        \u001b[32m0.8400\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m3.0644\u001b[0m  0.0006  0.4715\n",
      "      8            \u001b[36m0.2938\u001b[0m        \u001b[32m0.7980\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m2.9152\u001b[0m  0.0006  0.4662\n",
      "      9            \u001b[36m0.3563\u001b[0m        \u001b[32m0.7223\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m2.7048\u001b[0m  0.0006  0.4985\n",
      "     10            \u001b[36m0.4250\u001b[0m        0.7695       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m2.5184\u001b[0m  0.0005  0.4733\n",
      "     11            \u001b[36m0.4375\u001b[0m        \u001b[32m0.6642\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m2.3557\u001b[0m  0.0005  0.4036\n",
      "     12            \u001b[36m0.4688\u001b[0m        \u001b[32m0.5795\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m2.2251\u001b[0m  0.0005  0.3967\n",
      "     13            \u001b[36m0.5125\u001b[0m        0.6004       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m2.0614\u001b[0m  0.0005  0.3972\n",
      "     14            \u001b[36m0.5500\u001b[0m        \u001b[32m0.5110\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.9511\u001b[0m  0.0005  0.3972\n",
      "     15            \u001b[36m0.6062\u001b[0m        0.5636       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.8467\u001b[0m  0.0004  0.3886\n",
      "     16            \u001b[36m0.6562\u001b[0m        \u001b[32m0.5098\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.7415\u001b[0m  0.0004  0.4943\n",
      "     17            \u001b[36m0.7312\u001b[0m        \u001b[32m0.4768\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.6369\u001b[0m  0.0004  0.4028\n",
      "     18            \u001b[36m0.7875\u001b[0m        \u001b[32m0.4340\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.5517\u001b[0m  0.0004  0.4095\n",
      "     19            \u001b[36m0.8750\u001b[0m        0.4644       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.4785\u001b[0m  0.0004  0.4085\n",
      "     20            \u001b[36m0.9125\u001b[0m        0.4404       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.4245\u001b[0m  0.0003  0.3976\n",
      "     21            \u001b[36m0.9250\u001b[0m        0.4848       0.4201            0.4201        \u001b[94m1.3798\u001b[0m  0.0003  0.3934\n",
      "     22            \u001b[36m0.9375\u001b[0m        \u001b[32m0.4078\u001b[0m       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.3509\u001b[0m  0.0003  0.3934\n",
      "     23            \u001b[36m0.9500\u001b[0m        \u001b[32m0.3558\u001b[0m       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.3340\u001b[0m  0.0002  0.4024\n",
      "     24            \u001b[36m0.9625\u001b[0m        \u001b[32m0.3484\u001b[0m       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.3239\u001b[0m  0.0002  0.4103\n",
      "     25            \u001b[36m0.9875\u001b[0m        0.3621       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.3185\u001b[0m  0.0002  0.4214\n",
      "     26            0.9875        0.3903       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.3169\u001b[0m  0.0002  0.4137\n",
      "     27            \u001b[36m1.0000\u001b[0m        0.3520       0.4549            0.4549        \u001b[94m1.3149\u001b[0m  0.0002  0.4030\n",
      "     28            1.0000        0.4011       0.4444            0.4444        \u001b[94m1.3112\u001b[0m  0.0001  0.4053\n",
      "     29            1.0000        0.3494       0.4479            0.4479        \u001b[94m1.3071\u001b[0m  0.0001  0.4006\n",
      "     30            1.0000        0.3579       0.4514            0.4514        \u001b[94m1.3045\u001b[0m  0.0001  0.4202\n",
      "     31            1.0000        0.3735       0.4549            0.4549        \u001b[94m1.3022\u001b[0m  0.0001  0.3935\n",
      "     32            1.0000        0.3554       0.4549            0.4549        \u001b[94m1.3005\u001b[0m  0.0001  0.3977\n",
      "     33            1.0000        \u001b[32m0.3340\u001b[0m       0.4549            0.4549        \u001b[94m1.2991\u001b[0m  0.0000  0.4073\n",
      "     34            1.0000        \u001b[32m0.2954\u001b[0m       0.4549            0.4549        1.2996  0.0000  0.4352\n",
      "     35            1.0000        0.3071       0.4583            0.4583        1.3001  0.0000  0.4580\n",
      "     36            1.0000        0.3267       0.4583            0.4583        1.2997  0.0000  0.4906\n",
      "     37            1.0000        0.3217       0.4549            0.4549        1.2993  0.0000  0.4499\n",
      "     38            1.0000        0.3571       0.4479            0.4479        \u001b[94m1.2987\u001b[0m  0.0000  0.4538\n",
      "     39            1.0000        0.3187       0.4479            0.4479        \u001b[94m1.2980\u001b[0m  0.0000  0.5153\n",
      "     40            1.0000        0.3345       0.4479            0.4479        \u001b[94m1.2972\u001b[0m  0.0000  0.5162\n",
      "Training model for subject 6 with 170 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2471\u001b[0m        \u001b[32m1.6909\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m7.8669\u001b[0m  0.0006  0.4469\n",
      "      2            0.2471        \u001b[32m1.4229\u001b[0m       0.2500            0.2500        \u001b[94m6.7610\u001b[0m  0.0006  0.4062\n",
      "      3            0.2471        \u001b[32m1.2244\u001b[0m       0.2500            0.2500        \u001b[94m4.5997\u001b[0m  0.0006  0.3917\n",
      "      4            \u001b[36m0.3647\u001b[0m        \u001b[32m1.0460\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m3.5947\u001b[0m  0.0006  0.5136\n",
      "      5            \u001b[36m0.4235\u001b[0m        1.0935       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        \u001b[94m3.1290\u001b[0m  0.0006  0.3923\n",
      "      6            \u001b[36m0.4588\u001b[0m        \u001b[32m0.9022\u001b[0m       0.2674            0.2674        \u001b[94m2.6783\u001b[0m  0.0006  0.4032\n",
      "      7            0.4588        0.9778       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m2.5058\u001b[0m  0.0006  0.4123\n",
      "      8            0.4353        \u001b[32m0.8500\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        \u001b[94m2.3274\u001b[0m  0.0006  0.4151\n",
      "      9            0.4059        \u001b[32m0.8020\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m2.2506\u001b[0m  0.0006  0.4150\n",
      "     10            0.4529        \u001b[32m0.7261\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m2.1007\u001b[0m  0.0005  0.4011\n",
      "     11            \u001b[36m0.5176\u001b[0m        0.7568       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m1.9209\u001b[0m  0.0005  0.3999\n",
      "     12            \u001b[36m0.5882\u001b[0m        \u001b[32m0.6238\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        \u001b[94m1.8042\u001b[0m  0.0005  0.4005\n",
      "     13            \u001b[36m0.6588\u001b[0m        0.6798       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.6950\u001b[0m  0.0005  0.4083\n",
      "     14            \u001b[36m0.6941\u001b[0m        \u001b[32m0.5935\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.5933\u001b[0m  0.0005  0.3923\n",
      "     15            \u001b[36m0.7176\u001b[0m        0.6197       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.5112\u001b[0m  0.0004  0.4094\n",
      "     16            \u001b[36m0.7824\u001b[0m        \u001b[32m0.5864\u001b[0m       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.4398\u001b[0m  0.0004  0.3972\n",
      "     17            \u001b[36m0.8059\u001b[0m        \u001b[32m0.5359\u001b[0m       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.4048\u001b[0m  0.0004  0.3984\n",
      "     18            \u001b[36m0.8412\u001b[0m        0.6095       0.4340            0.4340        \u001b[94m1.3797\u001b[0m  0.0004  0.3940\n",
      "     19            \u001b[36m0.8824\u001b[0m        \u001b[32m0.5166\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.3680\u001b[0m  0.0004  0.4021\n",
      "     20            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4943\u001b[0m       0.4410            0.4410        \u001b[94m1.3642\u001b[0m  0.0003  0.4117\n",
      "     21            \u001b[36m0.9235\u001b[0m        0.5114       0.4375            0.4375        \u001b[94m1.3611\u001b[0m  0.0003  0.3977\n",
      "     22            0.9235        \u001b[32m0.4374\u001b[0m       0.4306            0.4306        1.3636  0.0003  0.3973\n",
      "     23            0.9235        0.4818       0.4201            0.4201        1.3673  0.0002  0.4000\n",
      "     24            \u001b[36m0.9412\u001b[0m        0.5031       0.4132            0.4132        1.3649  0.0002  0.5004\n",
      "     25            \u001b[36m0.9471\u001b[0m        0.4533       0.4132            0.4132        \u001b[94m1.3559\u001b[0m  0.0002  0.5081\n",
      "     26            0.9471        \u001b[32m0.4132\u001b[0m       0.4132            0.4132        \u001b[94m1.3493\u001b[0m  0.0002  0.4744\n",
      "     27            0.9471        0.4601       0.4167            0.4167        \u001b[94m1.3452\u001b[0m  0.0002  0.4739\n",
      "     28            \u001b[36m0.9529\u001b[0m        0.4203       0.4271            0.4271        \u001b[94m1.3413\u001b[0m  0.0001  0.4709\n",
      "     29            0.9529        0.4659       0.4271            0.4271        \u001b[94m1.3385\u001b[0m  0.0001  0.5147\n",
      "     30            0.9529        0.4608       0.4236            0.4236        \u001b[94m1.3345\u001b[0m  0.0001  0.3993\n",
      "     31            \u001b[36m0.9588\u001b[0m        0.4427       0.4271            0.4271        \u001b[94m1.3320\u001b[0m  0.0001  0.4126\n",
      "     32            0.9588        \u001b[32m0.3920\u001b[0m       0.4271            0.4271        \u001b[94m1.3311\u001b[0m  0.0001  0.4102\n",
      "     33            0.9529        \u001b[32m0.3751\u001b[0m       0.4271            0.4271        \u001b[94m1.3298\u001b[0m  0.0000  0.4017\n",
      "     34            0.9588        0.3870       0.4271            0.4271        \u001b[94m1.3274\u001b[0m  0.0000  0.3886\n",
      "     35            0.9588        0.3955       0.4236            0.4236        \u001b[94m1.3269\u001b[0m  0.0000  0.4105\n",
      "     36            0.9588        0.4206       0.4271            0.4271        1.3277  0.0000  0.4064\n",
      "     37            0.9588        0.4397       0.4271            0.4271        1.3272  0.0000  0.3981\n",
      "     38            0.9588        0.4121       0.4340            0.4340        \u001b[94m1.3264\u001b[0m  0.0000  0.4008\n",
      "     39            0.9588        0.4224       0.4340            0.4340        1.3265  0.0000  0.3944\n",
      "     40            0.9588        0.4713       0.4306            0.4306        \u001b[94m1.3259\u001b[0m  0.0000  0.4104\n",
      "Training model for subject 6 with 180 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2833\u001b[0m        \u001b[32m1.7425\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m2.9494\u001b[0m  0.0006  0.4077\n",
      "      2            0.2722        \u001b[32m1.4163\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m2.1546\u001b[0m  0.0006  0.3905\n",
      "      3            \u001b[36m0.3056\u001b[0m        \u001b[32m1.3033\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m1.6699\u001b[0m  0.0006  0.3996\n",
      "      4            \u001b[36m0.4500\u001b[0m        \u001b[32m1.1198\u001b[0m       0.2674            0.2674        1.8208  0.0006  0.3859\n",
      "      5            0.3444        \u001b[32m1.0607\u001b[0m       0.2639            0.2639        2.4552  0.0006  0.3960\n",
      "      6            0.3222        \u001b[32m1.0578\u001b[0m       0.2535            0.2535        2.9231  0.0006  0.4175\n",
      "      7            0.3667        \u001b[32m0.9917\u001b[0m       0.2569            0.2569        2.9591  0.0006  0.4035\n",
      "      8            0.4222        \u001b[32m0.8688\u001b[0m       0.2604            0.2604        2.7473  0.0006  0.3966\n",
      "      9            \u001b[36m0.4611\u001b[0m        \u001b[32m0.8534\u001b[0m       0.2708            0.2708        2.3987  0.0006  0.3980\n",
      "     10            \u001b[36m0.5222\u001b[0m        \u001b[32m0.7393\u001b[0m       0.2917            0.2917        2.0879  0.0005  0.4061\n",
      "     11            \u001b[36m0.5722\u001b[0m        \u001b[32m0.7325\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        1.8563  0.0005  0.3983\n",
      "     12            \u001b[36m0.6500\u001b[0m        0.7547       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        1.6813  0.0005  0.4073\n",
      "     13            \u001b[36m0.6889\u001b[0m        0.7494       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.5975\u001b[0m  0.0005  0.3984\n",
      "     14            \u001b[36m0.7333\u001b[0m        \u001b[32m0.6383\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.5385\u001b[0m  0.0005  0.5075\n",
      "     15            \u001b[36m0.8000\u001b[0m        0.6605       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.4785\u001b[0m  0.0004  0.4953\n",
      "     16            \u001b[36m0.8444\u001b[0m        0.6801       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.4401\u001b[0m  0.0004  0.4743\n",
      "     17            \u001b[36m0.8556\u001b[0m        \u001b[32m0.5199\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.4073\u001b[0m  0.0004  0.5069\n",
      "     18            \u001b[36m0.8833\u001b[0m        0.5776       0.4028            0.4028        \u001b[94m1.3820\u001b[0m  0.0004  0.5633\n",
      "     19            0.8833        0.5857       0.4028            0.4028        \u001b[94m1.3690\u001b[0m  0.0004  0.4120\n",
      "     20            \u001b[36m0.8889\u001b[0m        \u001b[32m0.5188\u001b[0m       0.3993            0.3993        \u001b[94m1.3620\u001b[0m  0.0003  0.4032\n",
      "     21            \u001b[36m0.9111\u001b[0m        0.5208       0.4062            0.4062        \u001b[94m1.3560\u001b[0m  0.0003  0.3929\n",
      "     22            \u001b[36m0.9333\u001b[0m        0.5252       0.4028            0.4028        \u001b[94m1.3432\u001b[0m  0.0003  0.5064\n",
      "     23            \u001b[36m0.9389\u001b[0m        \u001b[32m0.4615\u001b[0m       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.3343\u001b[0m  0.0002  0.4090\n",
      "     24            \u001b[36m0.9444\u001b[0m        0.4704       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.3203\u001b[0m  0.0002  0.3974\n",
      "     25            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4610\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.3065\u001b[0m  0.0002  0.4087\n",
      "     26            \u001b[36m0.9611\u001b[0m        0.5302       0.4236            0.4236        \u001b[94m1.2991\u001b[0m  0.0002  0.4142\n",
      "     27            0.9611        \u001b[32m0.4031\u001b[0m       0.4236            0.4236        \u001b[94m1.2936\u001b[0m  0.0002  0.3962\n",
      "     28            0.9611        0.5050       0.4201            0.4201        \u001b[94m1.2894\u001b[0m  0.0001  0.4014\n",
      "     29            \u001b[36m0.9667\u001b[0m        0.4727       0.4236            0.4236        \u001b[94m1.2861\u001b[0m  0.0001  0.4061\n",
      "     30            0.9667        0.4421       0.4236            0.4236        \u001b[94m1.2828\u001b[0m  0.0001  0.3979\n",
      "     31            0.9667        0.4333       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.2804\u001b[0m  0.0001  0.4081\n",
      "     32            0.9667        0.4429       0.4340            0.4340        \u001b[94m1.2787\u001b[0m  0.0001  0.4007\n",
      "     33            0.9667        0.4386       0.4340            0.4340        \u001b[94m1.2772\u001b[0m  0.0000  0.3958\n",
      "     34            0.9667        0.4099       0.4340            0.4340        \u001b[94m1.2762\u001b[0m  0.0000  0.3927\n",
      "     35            0.9667        0.4830       0.4340            0.4340        \u001b[94m1.2751\u001b[0m  0.0000  0.4025\n",
      "     36            0.9667        0.4699       0.4340            0.4340        \u001b[94m1.2743\u001b[0m  0.0000  0.3942\n",
      "     37            0.9611        0.4408       0.4236            0.4236        \u001b[94m1.2733\u001b[0m  0.0000  0.4018\n",
      "     38            0.9611        0.4330       0.4236            0.4236        \u001b[94m1.2733\u001b[0m  0.0000  0.4093\n",
      "     39            0.9667        0.4668       0.4271            0.4271        \u001b[94m1.2723\u001b[0m  0.0000  0.4078\n",
      "     40            0.9611        0.4643       0.4236            0.4236        1.2724  0.0000  0.4193\n",
      "Training model for subject 6 with 190 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2474\u001b[0m        \u001b[32m1.6582\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.3342\u001b[0m  0.0006  0.3950\n",
      "      2            \u001b[36m0.3105\u001b[0m        \u001b[32m1.3971\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m3.2686\u001b[0m  0.0006  0.4011\n",
      "      3            0.2947        \u001b[32m1.2745\u001b[0m       0.2604            0.2604        \u001b[94m3.0904\u001b[0m  0.0006  0.4541\n",
      "      4            0.2737        \u001b[32m1.2078\u001b[0m       0.2535            0.2535        \u001b[94m2.9384\u001b[0m  0.0006  0.4748\n",
      "      5            0.2632        \u001b[32m1.0137\u001b[0m       0.2535            0.2535        \u001b[94m2.7835\u001b[0m  0.0006  0.4628\n",
      "      6            0.2895        \u001b[32m0.9594\u001b[0m       0.2639            0.2639        \u001b[94m2.5643\u001b[0m  0.0006  0.4718\n",
      "      7            \u001b[36m0.3368\u001b[0m        \u001b[32m0.9059\u001b[0m       0.2569            0.2569        \u001b[94m2.2914\u001b[0m  0.0006  0.4923\n",
      "      8            \u001b[36m0.3421\u001b[0m        \u001b[32m0.8843\u001b[0m       0.2639            0.2639        \u001b[94m2.2455\u001b[0m  0.0006  0.4989\n",
      "      9            \u001b[36m0.3789\u001b[0m        \u001b[32m0.8352\u001b[0m       0.2674            0.2674        \u001b[94m2.0969\u001b[0m  0.0006  0.3949\n",
      "     10            \u001b[36m0.4474\u001b[0m        \u001b[32m0.8334\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m1.9523\u001b[0m  0.0005  0.4072\n",
      "     11            \u001b[36m0.5368\u001b[0m        \u001b[32m0.8093\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m1.8497\u001b[0m  0.0005  0.4165\n",
      "     12            \u001b[36m0.5737\u001b[0m        \u001b[32m0.7315\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m1.7656\u001b[0m  0.0005  0.3948\n",
      "     13            \u001b[36m0.6421\u001b[0m        0.7603       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m1.6739\u001b[0m  0.0005  0.4067\n",
      "     14            \u001b[36m0.6632\u001b[0m        \u001b[32m0.6626\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.5800\u001b[0m  0.0005  0.3927\n",
      "     15            \u001b[36m0.6895\u001b[0m        0.6800       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.5137\u001b[0m  0.0004  0.4010\n",
      "     16            \u001b[36m0.7526\u001b[0m        0.7041       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.4538\u001b[0m  0.0004  0.4065\n",
      "     17            \u001b[36m0.8000\u001b[0m        \u001b[32m0.6606\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.4239\u001b[0m  0.0004  0.3954\n",
      "     18            \u001b[36m0.8211\u001b[0m        0.6696       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.3883\u001b[0m  0.0004  0.4206\n",
      "     19            \u001b[36m0.8579\u001b[0m        \u001b[32m0.5896\u001b[0m       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.3591\u001b[0m  0.0004  0.4123\n",
      "     20            \u001b[36m0.8737\u001b[0m        0.6293       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.3419\u001b[0m  0.0003  0.4055\n",
      "     21            \u001b[36m0.8895\u001b[0m        \u001b[32m0.5321\u001b[0m       0.4236            0.4236        \u001b[94m1.3310\u001b[0m  0.0003  0.4186\n",
      "     22            \u001b[36m0.9105\u001b[0m        \u001b[32m0.4812\u001b[0m       0.4201            0.4201        \u001b[94m1.3123\u001b[0m  0.0003  0.4045\n",
      "     23            \u001b[36m0.9158\u001b[0m        0.4858       0.4201            0.4201        \u001b[94m1.3004\u001b[0m  0.0002  0.4103\n",
      "     24            \u001b[36m0.9368\u001b[0m        \u001b[32m0.4460\u001b[0m       0.4236            0.4236        \u001b[94m1.2933\u001b[0m  0.0002  0.4060\n",
      "     25            \u001b[36m0.9421\u001b[0m        0.5464       0.4132            0.4132        \u001b[94m1.2853\u001b[0m  0.0002  0.3885\n",
      "     26            \u001b[36m0.9474\u001b[0m        0.5237       0.3993            0.3993        \u001b[94m1.2840\u001b[0m  0.0002  0.3897\n",
      "     27            \u001b[36m0.9526\u001b[0m        0.4514       0.3993            0.3993        \u001b[94m1.2829\u001b[0m  0.0002  0.5009\n",
      "     28            \u001b[36m0.9632\u001b[0m        0.5299       0.3924            0.3924        \u001b[94m1.2802\u001b[0m  0.0001  0.4020\n",
      "     29            0.9632        0.5041       0.3958            0.3958        \u001b[94m1.2778\u001b[0m  0.0001  0.3932\n",
      "     30            \u001b[36m0.9789\u001b[0m        \u001b[32m0.4460\u001b[0m       0.4097            0.4097        \u001b[94m1.2743\u001b[0m  0.0001  0.4068\n",
      "     31            0.9789        0.5037       0.4132            0.4132        \u001b[94m1.2734\u001b[0m  0.0001  0.3937\n",
      "     32            0.9789        0.4841       0.4167            0.4167        \u001b[94m1.2712\u001b[0m  0.0001  0.4740\n",
      "     33            0.9737        0.4509       0.4201            0.4201        \u001b[94m1.2696\u001b[0m  0.0000  0.4906\n",
      "     34            0.9737        0.4662       0.4201            0.4201        \u001b[94m1.2686\u001b[0m  0.0000  0.4810\n",
      "     35            0.9737        \u001b[32m0.3819\u001b[0m       0.4236            0.4236        \u001b[94m1.2674\u001b[0m  0.0000  0.4641\n",
      "     36            0.9737        0.4028       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.2669\u001b[0m  0.0000  0.5166\n",
      "     37            0.9789        0.4490       0.4236            0.4236        \u001b[94m1.2666\u001b[0m  0.0000  0.5090\n",
      "     38            0.9789        \u001b[32m0.3618\u001b[0m       0.4271            0.4271        \u001b[94m1.2660\u001b[0m  0.0000  0.4072\n",
      "     39            0.9789        0.4948       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.2659\u001b[0m  0.0000  0.3965\n",
      "     40            0.9789        0.5022       0.4306            0.4306        1.2659  0.0000  0.4026\n",
      "Training model for subject 6 with 200 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2550\u001b[0m        \u001b[32m1.7122\u001b[0m       \u001b[35m0.2465\u001b[0m            \u001b[31m0.2465\u001b[0m        \u001b[94m2.0900\u001b[0m  0.0006  0.5064\n",
      "      2            \u001b[36m0.3400\u001b[0m        \u001b[32m1.4890\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m1.7364\u001b[0m  0.0006  0.5060\n",
      "      3            \u001b[36m0.4750\u001b[0m        \u001b[32m1.2776\u001b[0m       0.2674            0.2674        \u001b[94m1.6340\u001b[0m  0.0006  0.4884\n",
      "      4            0.3250        \u001b[32m1.0404\u001b[0m       0.2535            0.2535        2.0998  0.0006  0.5033\n",
      "      5            0.3850        1.0505       0.2604            0.2604        2.0843  0.0006  0.5007\n",
      "      6            \u001b[36m0.4850\u001b[0m        \u001b[32m0.9968\u001b[0m       0.2674            0.2674        1.8607  0.0006  0.5068\n",
      "      7            \u001b[36m0.5800\u001b[0m        \u001b[32m0.8758\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        1.6662  0.0006  0.5136\n",
      "      8            \u001b[36m0.6800\u001b[0m        \u001b[32m0.8410\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m1.5293\u001b[0m  0.0006  0.4961\n",
      "      9            \u001b[36m0.7150\u001b[0m        \u001b[32m0.7551\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m1.4439\u001b[0m  0.0006  0.5021\n",
      "     10            \u001b[36m0.7850\u001b[0m        0.8124       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.3817\u001b[0m  0.0005  0.5028\n",
      "     11            \u001b[36m0.8100\u001b[0m        \u001b[32m0.6720\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.3341\u001b[0m  0.0005  0.5072\n",
      "     12            \u001b[36m0.8500\u001b[0m        0.7087       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.3037\u001b[0m  0.0005  0.5114\n",
      "     13            \u001b[36m0.8600\u001b[0m        \u001b[32m0.6465\u001b[0m       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.2930\u001b[0m  0.0005  0.4959\n",
      "     14            \u001b[36m0.8800\u001b[0m        \u001b[32m0.6072\u001b[0m       0.4306            0.4306        \u001b[94m1.2817\u001b[0m  0.0005  0.4917\n",
      "     15            \u001b[36m0.9050\u001b[0m        \u001b[32m0.5790\u001b[0m       0.4097            0.4097        \u001b[94m1.2690\u001b[0m  0.0004  0.5410\n",
      "     16            \u001b[36m0.9250\u001b[0m        \u001b[32m0.4944\u001b[0m       0.4028            0.4028        \u001b[94m1.2629\u001b[0m  0.0004  0.5217\n",
      "     17            \u001b[36m0.9300\u001b[0m        0.5318       0.4236            0.4236        \u001b[94m1.2560\u001b[0m  0.0004  0.5162\n",
      "     18            \u001b[36m0.9350\u001b[0m        0.5473       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.2492\u001b[0m  0.0004  0.6098\n",
      "     19            \u001b[36m0.9500\u001b[0m        0.5227       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.2444\u001b[0m  0.0004  0.6013\n",
      "     20            \u001b[36m0.9650\u001b[0m        \u001b[32m0.4638\u001b[0m       0.4444            0.4444        \u001b[94m1.2368\u001b[0m  0.0003  0.6121\n",
      "     21            0.9650        \u001b[32m0.4410\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        1.2392  0.0003  0.6000\n",
      "     22            0.9650        0.4417       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        1.2385  0.0003  0.5569\n",
      "     23            \u001b[36m0.9700\u001b[0m        \u001b[32m0.3826\u001b[0m       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        \u001b[94m1.2357\u001b[0m  0.0002  0.5177\n",
      "     24            \u001b[36m0.9850\u001b[0m        0.3966       0.4826            0.4826        \u001b[94m1.2274\u001b[0m  0.0002  0.5085\n",
      "     25            \u001b[36m0.9900\u001b[0m        0.3834       0.4757            0.4757        \u001b[94m1.2144\u001b[0m  0.0002  0.5165\n",
      "     26            0.9900        0.3956       0.4792            0.4792        \u001b[94m1.2087\u001b[0m  0.0002  0.4999\n",
      "     27            0.9900        \u001b[32m0.3732\u001b[0m       0.4653            0.4653        \u001b[94m1.2041\u001b[0m  0.0002  0.5297\n",
      "     28            0.9900        \u001b[32m0.3546\u001b[0m       0.4618            0.4618        \u001b[94m1.2020\u001b[0m  0.0001  0.5078\n",
      "     29            0.9900        0.3629       0.4618            0.4618        1.2033  0.0001  0.5376\n",
      "     30            0.9900        \u001b[32m0.3458\u001b[0m       0.4653            0.4653        1.2036  0.0001  0.5064\n",
      "     31            0.9900        0.3569       0.4722            0.4722        1.2047  0.0001  0.5173\n",
      "     32            0.9900        0.3690       0.4653            0.4653        1.2048  0.0001  0.5021\n",
      "     33            0.9900        0.3674       0.4618            0.4618        1.2049  0.0000  0.5044\n",
      "     34            \u001b[36m0.9950\u001b[0m        \u001b[32m0.3316\u001b[0m       0.4549            0.4549        1.2048  0.0000  0.6315\n",
      "     35            0.9950        \u001b[32m0.3054\u001b[0m       0.4653            0.4653        1.2045  0.0000  0.5028\n",
      "     36            0.9950        0.3270       0.4757            0.4757        1.2038  0.0000  0.5032\n",
      "     37            0.9950        0.3324       0.4722            0.4722        1.2031  0.0000  0.5136\n",
      "     38            0.9900        \u001b[32m0.2815\u001b[0m       0.4688            0.4688        1.2025  0.0000  0.4978\n",
      "     39            0.9900        0.3610       0.4653            0.4653        1.2021  0.0000  0.5111\n",
      "     40            0.9900        0.3716       0.4653            0.4653        \u001b[94m1.2016\u001b[0m  0.0000  0.5097\n",
      "Training model for subject 6 with 210 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2524\u001b[0m        \u001b[32m1.6157\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m1.7771\u001b[0m  0.0006  0.5295\n",
      "      2            \u001b[36m0.3762\u001b[0m        \u001b[32m1.2830\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m1.6462\u001b[0m  0.0006  0.5948\n",
      "      3            0.3238        \u001b[32m1.2025\u001b[0m       0.2674            0.2674        1.8831  0.0006  0.5865\n",
      "      4            0.3571        \u001b[32m1.0709\u001b[0m       0.2882            0.2882        2.3188  0.0006  0.5649\n",
      "      5            \u001b[36m0.4476\u001b[0m        \u001b[32m0.9799\u001b[0m       0.2569            0.2569        2.4051  0.0006  0.6191\n",
      "      6            0.4429        \u001b[32m0.8339\u001b[0m       0.2674            0.2674        2.2999  0.0006  0.5650\n",
      "      7            \u001b[36m0.4762\u001b[0m        0.8652       0.2778            0.2778        2.1120  0.0006  0.5183\n",
      "      8            \u001b[36m0.5524\u001b[0m        \u001b[32m0.8088\u001b[0m       0.2535            0.2535        1.8878  0.0006  0.5001\n",
      "      9            \u001b[36m0.6476\u001b[0m        0.8307       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        1.6708  0.0006  0.5179\n",
      "     10            \u001b[36m0.7000\u001b[0m        \u001b[32m0.6761\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.5250\u001b[0m  0.0005  0.4939\n",
      "     11            \u001b[36m0.7667\u001b[0m        \u001b[32m0.6739\u001b[0m       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.4167\u001b[0m  0.0005  0.5012\n",
      "     12            \u001b[36m0.8286\u001b[0m        \u001b[32m0.6675\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.3480\u001b[0m  0.0005  0.4936\n",
      "     13            \u001b[36m0.8762\u001b[0m        \u001b[32m0.6270\u001b[0m       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.3083\u001b[0m  0.0005  0.4993\n",
      "     14            \u001b[36m0.8905\u001b[0m        \u001b[32m0.5447\u001b[0m       0.4306            0.4306        \u001b[94m1.2864\u001b[0m  0.0005  0.5026\n",
      "     15            \u001b[36m0.9238\u001b[0m        \u001b[32m0.4514\u001b[0m       0.4340            0.4340        \u001b[94m1.2700\u001b[0m  0.0004  0.5116\n",
      "     16            \u001b[36m0.9286\u001b[0m        0.5709       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.2665\u001b[0m  0.0004  0.4908\n",
      "     17            0.9238        0.4865       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        1.2693  0.0004  0.5088\n",
      "     18            \u001b[36m0.9476\u001b[0m        \u001b[32m0.4426\u001b[0m       0.4618            0.4618        \u001b[94m1.2655\u001b[0m  0.0004  0.4900\n",
      "     19            \u001b[36m0.9571\u001b[0m        0.4612       0.4549            0.4549        \u001b[94m1.2611\u001b[0m  0.0004  0.5025\n",
      "     20            0.9571        \u001b[32m0.4027\u001b[0m       0.4583            0.4583        \u001b[94m1.2560\u001b[0m  0.0003  0.5157\n",
      "     21            \u001b[36m0.9714\u001b[0m        0.4697       0.4618            0.4618        \u001b[94m1.2538\u001b[0m  0.0003  0.5027\n",
      "     22            \u001b[36m0.9762\u001b[0m        \u001b[32m0.3745\u001b[0m       0.4583            0.4583        \u001b[94m1.2514\u001b[0m  0.0003  0.5018\n",
      "     23            \u001b[36m0.9810\u001b[0m        0.4054       0.4618            0.4618        \u001b[94m1.2458\u001b[0m  0.0002  0.5167\n",
      "     24            0.9762        \u001b[32m0.3503\u001b[0m       0.4618            0.4618        \u001b[94m1.2340\u001b[0m  0.0002  0.5030\n",
      "     25            0.9810        0.3624       0.4618            0.4618        \u001b[94m1.2257\u001b[0m  0.0002  0.5472\n",
      "     26            0.9810        0.3518       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2198\u001b[0m  0.0002  0.6012\n",
      "     27            0.9810        0.3746       0.4653            0.4653        \u001b[94m1.2182\u001b[0m  0.0002  0.6037\n",
      "     28            0.9810        0.3643       0.4688            0.4688        1.2196  0.0001  0.6217\n",
      "     29            0.9810        0.3749       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        1.2215  0.0001  0.6389\n",
      "     30            \u001b[36m0.9857\u001b[0m        \u001b[32m0.3054\u001b[0m       0.4688            0.4688        1.2213  0.0001  0.5057\n",
      "     31            \u001b[36m0.9905\u001b[0m        0.3503       0.4688            0.4688        1.2210  0.0001  0.5169\n",
      "     32            0.9905        0.3546       0.4722            0.4722        1.2206  0.0001  0.5326\n",
      "     33            0.9905        0.3686       0.4688            0.4688        1.2207  0.0000  0.5026\n",
      "     34            0.9905        \u001b[32m0.3004\u001b[0m       0.4688            0.4688        1.2208  0.0000  0.5144\n",
      "     35            0.9905        0.3171       0.4688            0.4688        1.2204  0.0000  0.4979\n",
      "     36            0.9905        0.3273       0.4722            0.4722        1.2203  0.0000  0.5170\n",
      "     37            0.9905        0.3479       0.4722            0.4722        1.2198  0.0000  0.5174\n",
      "     38            0.9905        \u001b[32m0.2902\u001b[0m       0.4722            0.4722        1.2194  0.0000  0.4893\n",
      "     39            0.9905        0.3211       0.4688            0.4688        1.2192  0.0000  0.5128\n",
      "     40            0.9905        \u001b[32m0.2773\u001b[0m       0.4688            0.4688        1.2191  0.0000  0.4872\n",
      "Training model for subject 6 with 220 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2545\u001b[0m        \u001b[32m1.6395\u001b[0m       \u001b[35m0.2465\u001b[0m            \u001b[31m0.2465\u001b[0m        \u001b[94m2.9852\u001b[0m  0.0006  0.5498\n",
      "      2            \u001b[36m0.3818\u001b[0m        \u001b[32m1.3404\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m2.5126\u001b[0m  0.0006  0.5932\n",
      "      3            \u001b[36m0.4000\u001b[0m        \u001b[32m1.1360\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m2.5106\u001b[0m  0.0006  0.5062\n",
      "      4            \u001b[36m0.4045\u001b[0m        \u001b[32m1.0303\u001b[0m       0.2569            0.2569        \u001b[94m2.1237\u001b[0m  0.0006  0.4842\n",
      "      5            0.3045        \u001b[32m1.0266\u001b[0m       0.2604            0.2604        \u001b[94m2.0711\u001b[0m  0.0006  0.5010\n",
      "      6            0.3182        \u001b[32m0.9007\u001b[0m       0.2708            0.2708        \u001b[94m2.0417\u001b[0m  0.0006  0.5328\n",
      "      7            0.3545        \u001b[32m0.8422\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        \u001b[94m1.9471\u001b[0m  0.0006  0.5140\n",
      "      8            \u001b[36m0.5318\u001b[0m        \u001b[32m0.7711\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m1.6869\u001b[0m  0.0006  0.4915\n",
      "      9            \u001b[36m0.6409\u001b[0m        0.7917       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.4968\u001b[0m  0.0006  0.5981\n",
      "     10            \u001b[36m0.7545\u001b[0m        \u001b[32m0.6758\u001b[0m       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.3951\u001b[0m  0.0005  0.6171\n",
      "     11            \u001b[36m0.7818\u001b[0m        0.7341       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.3397\u001b[0m  0.0005  0.5649\n",
      "     12            \u001b[36m0.8545\u001b[0m        0.6838       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.3062\u001b[0m  0.0005  0.6393\n",
      "     13            \u001b[36m0.8818\u001b[0m        \u001b[32m0.6434\u001b[0m       0.4410            0.4410        \u001b[94m1.2795\u001b[0m  0.0005  0.6114\n",
      "     14            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6318\u001b[0m       0.4340            0.4340        \u001b[94m1.2752\u001b[0m  0.0005  0.5383\n",
      "     15            \u001b[36m0.9045\u001b[0m        \u001b[32m0.6296\u001b[0m       0.4444            0.4444        1.2819  0.0004  0.4991\n",
      "     16            0.9045        \u001b[32m0.5382\u001b[0m       0.4479            0.4479        1.2787  0.0004  0.5327\n",
      "     17            0.9045        \u001b[32m0.5025\u001b[0m       0.4410            0.4410        \u001b[94m1.2718\u001b[0m  0.0004  0.5171\n",
      "     18            \u001b[36m0.9182\u001b[0m        0.5035       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        \u001b[94m1.2511\u001b[0m  0.0004  0.5272\n",
      "     19            \u001b[36m0.9455\u001b[0m        \u001b[32m0.4781\u001b[0m       0.4792            0.4792        \u001b[94m1.2308\u001b[0m  0.0004  0.5120\n",
      "     20            \u001b[36m0.9545\u001b[0m        \u001b[32m0.4476\u001b[0m       0.4792            0.4792        \u001b[94m1.2214\u001b[0m  0.0003  0.4964\n",
      "     21            \u001b[36m0.9682\u001b[0m        \u001b[32m0.4288\u001b[0m       0.4618            0.4618        \u001b[94m1.2210\u001b[0m  0.0003  0.5026\n",
      "     22            \u001b[36m0.9727\u001b[0m        0.4309       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        \u001b[94m1.2181\u001b[0m  0.0003  0.5232\n",
      "     23            0.9682        0.4724       0.4792            0.4792        \u001b[94m1.2145\u001b[0m  0.0002  0.4950\n",
      "     24            0.9636        \u001b[32m0.4106\u001b[0m       0.4826            0.4826        \u001b[94m1.2143\u001b[0m  0.0002  0.5030\n",
      "     25            0.9591        \u001b[32m0.3549\u001b[0m       0.4688            0.4688        \u001b[94m1.2141\u001b[0m  0.0002  0.5190\n",
      "     26            0.9636        \u001b[32m0.3429\u001b[0m       0.4826            0.4826        \u001b[94m1.2121\u001b[0m  0.0002  0.5070\n",
      "     27            0.9636        \u001b[32m0.3321\u001b[0m       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        \u001b[94m1.2105\u001b[0m  0.0002  0.5116\n",
      "     28            0.9682        0.3404       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        \u001b[94m1.2075\u001b[0m  0.0001  0.5146\n",
      "     29            \u001b[36m0.9773\u001b[0m        0.3754       0.4931            0.4931        \u001b[94m1.2065\u001b[0m  0.0001  0.4981\n",
      "     30            0.9773        0.3688       0.4896            0.4896        \u001b[94m1.2060\u001b[0m  0.0001  0.5082\n",
      "     31            0.9773        0.3708       0.4965            0.4965        \u001b[94m1.2053\u001b[0m  0.0001  0.5041\n",
      "     32            0.9773        0.3606       \u001b[35m0.5069\u001b[0m            \u001b[31m0.5069\u001b[0m        \u001b[94m1.2029\u001b[0m  0.0001  0.5500\n",
      "     33            0.9773        0.3745       0.5069            0.5069        \u001b[94m1.2000\u001b[0m  0.0000  0.5797\n",
      "     34            \u001b[36m0.9818\u001b[0m        0.3337       \u001b[35m0.5104\u001b[0m            \u001b[31m0.5104\u001b[0m        \u001b[94m1.1967\u001b[0m  0.0000  0.5866\n",
      "     35            0.9818        0.3666       0.5104            0.5104        \u001b[94m1.1944\u001b[0m  0.0000  0.5972\n",
      "     36            0.9818        \u001b[32m0.3188\u001b[0m       \u001b[35m0.5139\u001b[0m            \u001b[31m0.5139\u001b[0m        \u001b[94m1.1933\u001b[0m  0.0000  0.5977\n",
      "     37            0.9818        \u001b[32m0.2899\u001b[0m       0.5139            0.5139        \u001b[94m1.1925\u001b[0m  0.0000  0.5439\n",
      "     38            0.9818        0.3656       0.5104            0.5104        \u001b[94m1.1919\u001b[0m  0.0000  0.5033\n",
      "     39            \u001b[36m0.9864\u001b[0m        0.3376       0.5139            0.5139        \u001b[94m1.1918\u001b[0m  0.0000  0.5230\n",
      "     40            0.9864        0.3384       0.5104            0.5104        \u001b[94m1.1917\u001b[0m  0.0000  0.5081\n",
      "Training model for subject 6 with 230 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2870\u001b[0m        \u001b[32m1.7031\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m2.1463\u001b[0m  0.0006  0.5143\n",
      "      2            \u001b[36m0.5043\u001b[0m        \u001b[32m1.4438\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m1.3504\u001b[0m  0.0006  0.4980\n",
      "      3            0.4087        \u001b[32m1.2113\u001b[0m       0.2639            0.2639        1.6021  0.0006  0.5214\n",
      "      4            0.3174        \u001b[32m1.1086\u001b[0m       0.2535            0.2535        2.1769  0.0006  0.5097\n",
      "      5            0.3261        1.1265       0.2674            0.2674        2.4945  0.0006  0.4916\n",
      "      6            0.3435        \u001b[32m1.0136\u001b[0m       0.2708            0.2708        2.5007  0.0006  0.4854\n",
      "      7            0.3565        \u001b[32m0.9997\u001b[0m       0.2812            0.2812        2.2058  0.0006  0.4985\n",
      "      8            0.3957        \u001b[32m0.9265\u001b[0m       0.2743            0.2743        1.9336  0.0006  0.4989\n",
      "      9            \u001b[36m0.5304\u001b[0m        \u001b[32m0.8588\u001b[0m       0.2951            0.2951        1.6170  0.0006  0.4851\n",
      "     10            \u001b[36m0.6565\u001b[0m        \u001b[32m0.8330\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        1.4327  0.0005  0.5024\n",
      "     11            \u001b[36m0.7391\u001b[0m        \u001b[32m0.6641\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.3356\u001b[0m  0.0005  0.4898\n",
      "     12            \u001b[36m0.7826\u001b[0m        0.7270       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.2864\u001b[0m  0.0005  0.4876\n",
      "     13            \u001b[36m0.8087\u001b[0m        0.6677       0.4236            0.4236        \u001b[94m1.2803\u001b[0m  0.0005  0.4989\n",
      "     14            \u001b[36m0.8348\u001b[0m        \u001b[32m0.6609\u001b[0m       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.2707\u001b[0m  0.0005  0.6032\n",
      "     15            \u001b[36m0.8696\u001b[0m        \u001b[32m0.5855\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.2548\u001b[0m  0.0004  0.5060\n",
      "     16            \u001b[36m0.9130\u001b[0m        \u001b[32m0.5783\u001b[0m       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.2375\u001b[0m  0.0004  0.5528\n",
      "     17            \u001b[36m0.9348\u001b[0m        \u001b[32m0.4776\u001b[0m       0.4410            0.4410        \u001b[94m1.2258\u001b[0m  0.0004  0.5629\n",
      "     18            \u001b[36m0.9391\u001b[0m        0.5105       0.4340            0.4340        \u001b[94m1.2207\u001b[0m  0.0004  0.5823\n",
      "     19            \u001b[36m0.9435\u001b[0m        \u001b[32m0.4693\u001b[0m       0.4444            0.4444        \u001b[94m1.2136\u001b[0m  0.0004  0.5868\n",
      "     20            \u001b[36m0.9478\u001b[0m        0.5342       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.2020\u001b[0m  0.0003  0.6223\n",
      "     21            \u001b[36m0.9565\u001b[0m        0.4922       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.1906\u001b[0m  0.0003  0.5069\n",
      "     22            \u001b[36m0.9609\u001b[0m        0.4877       \u001b[35m0.5000\u001b[0m            \u001b[31m0.5000\u001b[0m        \u001b[94m1.1836\u001b[0m  0.0003  0.4867\n",
      "     23            0.9609        \u001b[32m0.4271\u001b[0m       0.4965            0.4965        \u001b[94m1.1796\u001b[0m  0.0002  0.5104\n",
      "     24            0.9609        0.4665       \u001b[35m0.5035\u001b[0m            \u001b[31m0.5035\u001b[0m        \u001b[94m1.1733\u001b[0m  0.0002  0.5028\n",
      "     25            \u001b[36m0.9652\u001b[0m        \u001b[32m0.4131\u001b[0m       0.5035            0.5035        \u001b[94m1.1638\u001b[0m  0.0002  0.5062\n",
      "     26            \u001b[36m0.9783\u001b[0m        \u001b[32m0.3999\u001b[0m       0.4931            0.4931        \u001b[94m1.1583\u001b[0m  0.0002  0.5169\n",
      "     27            0.9739        0.4010       0.4965            0.4965        \u001b[94m1.1555\u001b[0m  0.0002  0.4963\n",
      "     28            0.9739        \u001b[32m0.3909\u001b[0m       0.4931            0.4931        \u001b[94m1.1552\u001b[0m  0.0001  0.4996\n",
      "     29            0.9739        0.4142       0.4931            0.4931        1.1553  0.0001  0.5105\n",
      "     30            0.9739        0.4337       0.4965            0.4965        \u001b[94m1.1535\u001b[0m  0.0001  0.5083\n",
      "     31            0.9739        \u001b[32m0.3758\u001b[0m       \u001b[35m0.5104\u001b[0m            \u001b[31m0.5104\u001b[0m        \u001b[94m1.1524\u001b[0m  0.0001  0.5089\n",
      "     32            0.9783        0.3777       \u001b[35m0.5139\u001b[0m            \u001b[31m0.5139\u001b[0m        \u001b[94m1.1512\u001b[0m  0.0001  0.5001\n",
      "     33            0.9783        0.3800       \u001b[35m0.5174\u001b[0m            \u001b[31m0.5174\u001b[0m        \u001b[94m1.1505\u001b[0m  0.0000  0.4999\n",
      "     34            0.9783        0.4032       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        \u001b[94m1.1502\u001b[0m  0.0000  0.4973\n",
      "     35            0.9783        \u001b[32m0.3671\u001b[0m       0.5208            0.5208        \u001b[94m1.1492\u001b[0m  0.0000  0.5105\n",
      "     36            \u001b[36m0.9826\u001b[0m        0.3959       \u001b[35m0.5243\u001b[0m            \u001b[31m0.5243\u001b[0m        \u001b[94m1.1486\u001b[0m  0.0000  0.4952\n",
      "     37            0.9826        0.3763       0.5208            0.5208        \u001b[94m1.1483\u001b[0m  0.0000  0.5062\n",
      "     38            0.9826        0.3854       0.5208            0.5208        \u001b[94m1.1483\u001b[0m  0.0000  0.4890\n",
      "     39            0.9826        \u001b[32m0.3550\u001b[0m       0.5243            0.5243        \u001b[94m1.1482\u001b[0m  0.0000  0.5481\n",
      "     40            0.9826        0.3570       0.5174            0.5174        \u001b[94m1.1480\u001b[0m  0.0000  0.5885\n",
      "Training model for subject 6 with 240 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3500\u001b[0m        \u001b[32m1.7403\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m1.9994\u001b[0m  0.0006  0.5931\n",
      "      2            0.2500        \u001b[32m1.4132\u001b[0m       0.2500            0.2500        2.5552  0.0006  0.5550\n",
      "      3            0.2500        \u001b[32m1.2447\u001b[0m       0.2500            0.2500        2.9763  0.0006  0.6526\n",
      "      4            0.2542        \u001b[32m1.1401\u001b[0m       0.2465            0.2465        2.6356  0.0006  0.5037\n",
      "      5            0.3458        \u001b[32m1.0682\u001b[0m       0.2500            0.2500        2.1417  0.0006  0.5063\n",
      "      6            \u001b[36m0.4042\u001b[0m        \u001b[32m0.9664\u001b[0m       0.2569            0.2569        2.0671  0.0006  0.5022\n",
      "      7            \u001b[36m0.5042\u001b[0m        1.0441       0.2708            0.2708        \u001b[94m1.8904\u001b[0m  0.0006  0.4920\n",
      "      8            \u001b[36m0.5875\u001b[0m        \u001b[32m0.8792\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        \u001b[94m1.6795\u001b[0m  0.0006  0.5022\n",
      "      9            \u001b[36m0.6625\u001b[0m        0.9277       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m1.5233\u001b[0m  0.0006  0.4967\n",
      "     10            \u001b[36m0.7292\u001b[0m        \u001b[32m0.7792\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.4192\u001b[0m  0.0005  0.5026\n",
      "     11            \u001b[36m0.7958\u001b[0m        0.7917       0.3611            0.3611        \u001b[94m1.3565\u001b[0m  0.0005  0.5026\n",
      "     12            \u001b[36m0.8625\u001b[0m        0.8013       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.3170\u001b[0m  0.0005  0.4984\n",
      "     13            \u001b[36m0.8958\u001b[0m        \u001b[32m0.6985\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.3046\u001b[0m  0.0005  0.4877\n",
      "     14            \u001b[36m0.9208\u001b[0m        \u001b[32m0.6720\u001b[0m       0.3889            0.3889        \u001b[94m1.2942\u001b[0m  0.0005  0.4873\n",
      "     15            0.9208        0.6878       0.3924            0.3924        \u001b[94m1.2836\u001b[0m  0.0004  0.5080\n",
      "     16            \u001b[36m0.9333\u001b[0m        \u001b[32m0.6486\u001b[0m       0.3958            0.3958        \u001b[94m1.2792\u001b[0m  0.0004  0.5064\n",
      "     17            \u001b[36m0.9500\u001b[0m        \u001b[32m0.6040\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        1.2806  0.0004  0.5080\n",
      "     18            \u001b[36m0.9542\u001b[0m        \u001b[32m0.5157\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.2758\u001b[0m  0.0004  0.4903\n",
      "     19            \u001b[36m0.9583\u001b[0m        0.5771       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.2620\u001b[0m  0.0004  0.5019\n",
      "     20            0.9542        \u001b[32m0.4766\u001b[0m       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.2571\u001b[0m  0.0003  0.5110\n",
      "     21            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4453\u001b[0m       0.4306            0.4306        \u001b[94m1.2563\u001b[0m  0.0003  0.5041\n",
      "     22            0.9542        0.4794       0.4201            0.4201        1.2641  0.0003  0.5061\n",
      "     23            0.9583        0.4687       0.4167            0.4167        1.2714  0.0002  0.5728\n",
      "     24            0.9625        \u001b[32m0.4220\u001b[0m       0.4306            0.4306        1.2697  0.0002  0.6001\n",
      "     25            \u001b[36m0.9792\u001b[0m        0.4515       0.4375            0.4375        \u001b[94m1.2549\u001b[0m  0.0002  0.6892\n",
      "     26            0.9750        \u001b[32m0.3943\u001b[0m       0.4340            0.4340        \u001b[94m1.2392\u001b[0m  0.0002  0.6448\n",
      "     27            0.9750        0.4340       0.4514            0.4514        \u001b[94m1.2264\u001b[0m  0.0002  0.5112\n",
      "     28            0.9750        0.4078       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.2188\u001b[0m  0.0001  0.5183\n",
      "     29            0.9792        0.3949       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.2138\u001b[0m  0.0001  0.4852\n",
      "     30            \u001b[36m0.9833\u001b[0m        0.4134       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2114\u001b[0m  0.0001  0.5286\n",
      "     31            \u001b[36m0.9917\u001b[0m        \u001b[32m0.3730\u001b[0m       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2111\u001b[0m  0.0001  0.5236\n",
      "     32            0.9917        0.3793       0.4653            0.4653        \u001b[94m1.2101\u001b[0m  0.0001  0.5290\n",
      "     33            0.9917        0.4048       0.4688            0.4688        \u001b[94m1.2093\u001b[0m  0.0000  0.5174\n",
      "     34            0.9917        0.3756       0.4688            0.4688        \u001b[94m1.2082\u001b[0m  0.0000  0.5055\n",
      "     35            0.9917        \u001b[32m0.3178\u001b[0m       0.4653            0.4653        \u001b[94m1.2069\u001b[0m  0.0000  0.5100\n",
      "     36            0.9917        0.3330       0.4653            0.4653        \u001b[94m1.2067\u001b[0m  0.0000  0.5089\n",
      "     37            0.9917        0.3829       0.4653            0.4653        \u001b[94m1.2062\u001b[0m  0.0000  0.5005\n",
      "     38            \u001b[36m0.9958\u001b[0m        0.3772       0.4653            0.4653        \u001b[94m1.2059\u001b[0m  0.0000  0.4954\n",
      "     39            0.9958        0.4089       0.4653            0.4653        \u001b[94m1.2057\u001b[0m  0.0000  0.5071\n",
      "     40            0.9958        0.3384       0.4653            0.4653        1.2060  0.0000  0.4963\n",
      "Training model for subject 6 with 250 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2600\u001b[0m        \u001b[32m1.6501\u001b[0m       \u001b[35m0.2465\u001b[0m            \u001b[31m0.2465\u001b[0m        \u001b[94m2.8361\u001b[0m  0.0006  0.4937\n",
      "      2            0.2520        \u001b[32m1.3513\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        3.7979  0.0006  0.4910\n",
      "      3            0.2560        \u001b[32m1.1873\u001b[0m       0.2465            0.2465        3.1897  0.0006  0.5078\n",
      "      4            \u001b[36m0.2720\u001b[0m        \u001b[32m1.0683\u001b[0m       0.2465            0.2465        2.8756  0.0006  0.4896\n",
      "      5            \u001b[36m0.3520\u001b[0m        \u001b[32m0.9863\u001b[0m       0.2396            0.2396        \u001b[94m2.4800\u001b[0m  0.0006  0.5166\n",
      "      6            \u001b[36m0.4560\u001b[0m        \u001b[32m0.9110\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m2.0279\u001b[0m  0.0006  0.5974\n",
      "      7            \u001b[36m0.5280\u001b[0m        \u001b[32m0.8944\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        \u001b[94m1.7515\u001b[0m  0.0006  0.5971\n",
      "      8            \u001b[36m0.5440\u001b[0m        \u001b[32m0.8314\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m1.6692\u001b[0m  0.0006  0.5891\n",
      "      9            0.5000        \u001b[32m0.8104\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        1.6719  0.0006  0.6122\n",
      "     10            \u001b[36m0.5560\u001b[0m        \u001b[32m0.7408\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m1.6189\u001b[0m  0.0005  0.6180\n",
      "     11            \u001b[36m0.6840\u001b[0m        0.7613       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.5099\u001b[0m  0.0005  0.5238\n",
      "     12            \u001b[36m0.7400\u001b[0m        \u001b[32m0.6808\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.4366\u001b[0m  0.0005  0.4837\n",
      "     13            \u001b[36m0.8040\u001b[0m        \u001b[32m0.6550\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.3800\u001b[0m  0.0005  0.4901\n",
      "     14            \u001b[36m0.8680\u001b[0m        \u001b[32m0.6332\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.3408\u001b[0m  0.0005  0.5026\n",
      "     15            \u001b[36m0.8960\u001b[0m        0.6358       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.3039\u001b[0m  0.0004  0.4962\n",
      "     16            \u001b[36m0.9040\u001b[0m        \u001b[32m0.5253\u001b[0m       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.2854\u001b[0m  0.0004  0.5019\n",
      "     17            \u001b[36m0.9160\u001b[0m        0.5550       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.2766\u001b[0m  0.0004  0.5133\n",
      "     18            \u001b[36m0.9200\u001b[0m        0.5611       0.4340            0.4340        \u001b[94m1.2696\u001b[0m  0.0004  0.4998\n",
      "     19            \u001b[36m0.9320\u001b[0m        \u001b[32m0.5234\u001b[0m       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.2536\u001b[0m  0.0004  0.5155\n",
      "     20            \u001b[36m0.9440\u001b[0m        \u001b[32m0.5006\u001b[0m       0.4514            0.4514        \u001b[94m1.2479\u001b[0m  0.0003  0.5058\n",
      "     21            \u001b[36m0.9560\u001b[0m        \u001b[32m0.4940\u001b[0m       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.2425\u001b[0m  0.0003  0.4953\n",
      "     22            0.9560        \u001b[32m0.4264\u001b[0m       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2362\u001b[0m  0.0003  0.5124\n",
      "     23            \u001b[36m0.9600\u001b[0m        \u001b[32m0.4118\u001b[0m       \u001b[35m0.4722\u001b[0m            \u001b[31m0.4722\u001b[0m        \u001b[94m1.2325\u001b[0m  0.0002  0.5165\n",
      "     24            0.9600        0.4773       0.4722            0.4722        \u001b[94m1.2257\u001b[0m  0.0002  0.4963\n",
      "     25            0.9600        0.4751       0.4722            0.4722        \u001b[94m1.2202\u001b[0m  0.0002  0.5143\n",
      "     26            0.9560        \u001b[32m0.3984\u001b[0m       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        \u001b[94m1.2139\u001b[0m  0.0002  0.5022\n",
      "     27            0.9560        0.4013       0.4757            0.4757        \u001b[94m1.2098\u001b[0m  0.0002  0.5132\n",
      "     28            \u001b[36m0.9640\u001b[0m        0.4293       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        1.2104  0.0001  0.5062\n",
      "     29            0.9640        \u001b[32m0.3872\u001b[0m       0.4792            0.4792        \u001b[94m1.2087\u001b[0m  0.0001  0.6106\n",
      "     30            0.9640        \u001b[32m0.3742\u001b[0m       0.4792            0.4792        \u001b[94m1.2060\u001b[0m  0.0001  0.6692\n",
      "     31            0.9640        \u001b[32m0.3695\u001b[0m       0.4792            0.4792        \u001b[94m1.2042\u001b[0m  0.0001  0.7409\n",
      "     32            \u001b[36m0.9680\u001b[0m        0.3972       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        \u001b[94m1.2035\u001b[0m  0.0001  0.6416\n",
      "     33            0.9680        0.3767       0.4826            0.4826        \u001b[94m1.2023\u001b[0m  0.0000  0.5110\n",
      "     34            0.9680        \u001b[32m0.3652\u001b[0m       0.4826            0.4826        \u001b[94m1.2007\u001b[0m  0.0000  0.5225\n",
      "     35            0.9680        0.3855       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        \u001b[94m1.1995\u001b[0m  0.0000  0.5064\n",
      "     36            0.9680        0.3726       0.4861            0.4861        \u001b[94m1.1986\u001b[0m  0.0000  0.5104\n",
      "     37            0.9680        0.4057       0.4861            0.4861        \u001b[94m1.1978\u001b[0m  0.0000  0.5182\n",
      "     38            0.9680        0.3937       0.4861            0.4861        \u001b[94m1.1973\u001b[0m  0.0000  0.5157\n",
      "     39            0.9680        0.3982       0.4861            0.4861        \u001b[94m1.1970\u001b[0m  0.0000  0.5139\n",
      "     40            0.9680        0.4013       0.4861            0.4861        1.1971  0.0000  0.5028\n",
      "Training model for subject 6 with 260 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.4192\u001b[0m        \u001b[32m1.5635\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m1.7619\u001b[0m  0.0006  0.6328\n",
      "      2            0.3000        \u001b[32m1.2389\u001b[0m       0.2674            0.2674        2.6280  0.0006  0.5892\n",
      "      3            0.2885        \u001b[32m1.1241\u001b[0m       0.2604            0.2604        2.9635  0.0006  0.6110\n",
      "      4            \u001b[36m0.4231\u001b[0m        \u001b[32m0.9879\u001b[0m       0.2708            0.2708        2.5715  0.0006  0.6053\n",
      "      5            \u001b[36m0.4577\u001b[0m        \u001b[32m0.9522\u001b[0m       0.2986            0.2986        2.0797  0.0006  0.5996\n",
      "      6            \u001b[36m0.5423\u001b[0m        \u001b[32m0.8647\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m1.7399\u001b[0m  0.0006  0.5897\n",
      "      7            \u001b[36m0.6654\u001b[0m        \u001b[32m0.8128\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.5207\u001b[0m  0.0006  0.6127\n",
      "      8            \u001b[36m0.7385\u001b[0m        \u001b[32m0.7371\u001b[0m       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.3948\u001b[0m  0.0006  0.6009\n",
      "      9            \u001b[36m0.8423\u001b[0m        \u001b[32m0.6710\u001b[0m       \u001b[35m0.4722\u001b[0m            \u001b[31m0.4722\u001b[0m        \u001b[94m1.2735\u001b[0m  0.0006  0.6118\n",
      "     10            \u001b[36m0.8538\u001b[0m        \u001b[32m0.6119\u001b[0m       0.4618            0.4618        \u001b[94m1.2334\u001b[0m  0.0005  0.7312\n",
      "     11            \u001b[36m0.8769\u001b[0m        \u001b[32m0.5505\u001b[0m       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        \u001b[94m1.2141\u001b[0m  0.0005  0.7537\n",
      "     12            \u001b[36m0.9115\u001b[0m        \u001b[32m0.5366\u001b[0m       \u001b[35m0.4931\u001b[0m            \u001b[31m0.4931\u001b[0m        \u001b[94m1.2090\u001b[0m  0.0005  0.6847\n",
      "     13            \u001b[36m0.9231\u001b[0m        \u001b[32m0.5311\u001b[0m       0.4896            0.4896        1.2100  0.0005  0.8156\n",
      "     14            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4693\u001b[0m       \u001b[35m0.5035\u001b[0m            \u001b[31m0.5035\u001b[0m        \u001b[94m1.2019\u001b[0m  0.0005  0.6543\n",
      "     15            \u001b[36m0.9654\u001b[0m        \u001b[32m0.3954\u001b[0m       0.5000            0.5000        \u001b[94m1.1982\u001b[0m  0.0004  0.5966\n",
      "     16            \u001b[36m0.9769\u001b[0m        0.4111       0.4965            0.4965        \u001b[94m1.1963\u001b[0m  0.0004  0.8201\n",
      "     17            \u001b[36m0.9885\u001b[0m        \u001b[32m0.3818\u001b[0m       \u001b[35m0.5139\u001b[0m            \u001b[31m0.5139\u001b[0m        \u001b[94m1.1765\u001b[0m  0.0004  0.6204\n",
      "     18            0.9769        \u001b[32m0.3806\u001b[0m       \u001b[35m0.5382\u001b[0m            \u001b[31m0.5382\u001b[0m        \u001b[94m1.1581\u001b[0m  0.0004  0.6002\n",
      "     19            \u001b[36m0.9923\u001b[0m        \u001b[32m0.3254\u001b[0m       \u001b[35m0.5451\u001b[0m            \u001b[31m0.5451\u001b[0m        \u001b[94m1.1422\u001b[0m  0.0004  0.6303\n",
      "     20            0.9923        \u001b[32m0.3012\u001b[0m       0.5451            0.5451        \u001b[94m1.1384\u001b[0m  0.0003  0.6132\n",
      "     21            0.9923        0.3058       0.5208            0.5208        1.1410  0.0003  0.6344\n",
      "     22            0.9923        0.3038       0.5278            0.5278        1.1407  0.0003  0.6253\n",
      "     23            \u001b[36m0.9962\u001b[0m        0.3056       0.5347            0.5347        \u001b[94m1.1272\u001b[0m  0.0002  0.6107\n",
      "     24            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2990\u001b[0m       \u001b[35m0.5556\u001b[0m            \u001b[31m0.5556\u001b[0m        \u001b[94m1.1085\u001b[0m  0.0002  0.6102\n",
      "     25            1.0000        \u001b[32m0.2543\u001b[0m       \u001b[35m0.5590\u001b[0m            \u001b[31m0.5590\u001b[0m        \u001b[94m1.1001\u001b[0m  0.0002  0.6126\n",
      "     26            1.0000        0.2570       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        \u001b[94m1.0987\u001b[0m  0.0002  0.6174\n",
      "     27            1.0000        0.2749       0.5625            0.5625        1.1018  0.0002  0.6382\n",
      "     28            1.0000        \u001b[32m0.2452\u001b[0m       0.5625            0.5625        1.1043  0.0001  0.5958\n",
      "     29            0.9962        \u001b[32m0.2226\u001b[0m       \u001b[35m0.5694\u001b[0m            \u001b[31m0.5694\u001b[0m        1.1035  0.0001  0.7075\n",
      "     30            0.9962        0.2577       0.5694            0.5694        1.1018  0.0001  0.7187\n",
      "     31            1.0000        0.2243       \u001b[35m0.5799\u001b[0m            \u001b[31m0.5799\u001b[0m        1.1008  0.0001  0.7139\n",
      "     32            1.0000        0.2410       0.5764            0.5764        1.0998  0.0001  0.7219\n",
      "     33            1.0000        \u001b[32m0.2100\u001b[0m       0.5764            0.5764        \u001b[94m1.0982\u001b[0m  0.0000  0.7247\n",
      "     34            1.0000        0.2446       0.5764            0.5764        \u001b[94m1.0979\u001b[0m  0.0000  0.6107\n",
      "     35            1.0000        0.2609       0.5729            0.5729        \u001b[94m1.0972\u001b[0m  0.0000  0.7097\n",
      "     36            1.0000        0.2288       0.5764            0.5764        \u001b[94m1.0970\u001b[0m  0.0000  0.5996\n",
      "     37            1.0000        0.2205       0.5764            0.5764        \u001b[94m1.0964\u001b[0m  0.0000  0.5938\n",
      "     38            1.0000        0.2493       0.5764            0.5764        \u001b[94m1.0962\u001b[0m  0.0000  0.5983\n",
      "     39            1.0000        0.2431       0.5764            0.5764        \u001b[94m1.0961\u001b[0m  0.0000  0.6181\n",
      "     40            1.0000        0.2261       0.5764            0.5764        \u001b[94m1.0960\u001b[0m  0.0000  0.6138\n",
      "Training model for subject 6 with 270 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3556\u001b[0m        \u001b[32m1.5936\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m1.4914\u001b[0m  0.0006  0.6023\n",
      "      2            \u001b[36m0.5704\u001b[0m        \u001b[32m1.3109\u001b[0m       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.2911\u001b[0m  0.0006  0.5954\n",
      "      3            0.3407        \u001b[32m1.2070\u001b[0m       0.2882            0.2882        1.6809  0.0006  0.5980\n",
      "      4            0.4259        \u001b[32m1.0826\u001b[0m       0.3264            0.3264        1.6342  0.0006  0.6059\n",
      "      5            \u001b[36m0.6704\u001b[0m        \u001b[32m0.9715\u001b[0m       0.4028            0.4028        1.3548  0.0006  0.5960\n",
      "      6            0.6556        \u001b[32m0.9155\u001b[0m       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        1.3407  0.0006  0.6127\n",
      "      7            \u001b[36m0.7037\u001b[0m        \u001b[32m0.7719\u001b[0m       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        \u001b[94m1.2898\u001b[0m  0.0006  0.5957\n",
      "      8            \u001b[36m0.7889\u001b[0m        \u001b[32m0.7181\u001b[0m       \u001b[35m0.5000\u001b[0m            \u001b[31m0.5000\u001b[0m        \u001b[94m1.2416\u001b[0m  0.0006  0.6301\n",
      "      9            \u001b[36m0.8407\u001b[0m        0.7191       \u001b[35m0.5139\u001b[0m            \u001b[31m0.5139\u001b[0m        \u001b[94m1.2108\u001b[0m  0.0006  0.7350\n",
      "     10            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6421\u001b[0m       0.5035            0.5035        \u001b[94m1.1889\u001b[0m  0.0005  0.6937\n",
      "     11            \u001b[36m0.8852\u001b[0m        \u001b[32m0.6116\u001b[0m       0.4896            0.4896        \u001b[94m1.1807\u001b[0m  0.0005  0.7565\n",
      "     12            \u001b[36m0.9037\u001b[0m        0.6280       0.4861            0.4861        \u001b[94m1.1796\u001b[0m  0.0005  0.6916\n",
      "     13            \u001b[36m0.9148\u001b[0m        \u001b[32m0.5851\u001b[0m       0.4965            0.4965        \u001b[94m1.1726\u001b[0m  0.0005  0.6299\n",
      "     14            \u001b[36m0.9259\u001b[0m        \u001b[32m0.5481\u001b[0m       0.5069            0.5069        \u001b[94m1.1626\u001b[0m  0.0005  0.5959\n",
      "     15            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4996\u001b[0m       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        \u001b[94m1.1474\u001b[0m  0.0004  0.6170\n",
      "     16            \u001b[36m0.9407\u001b[0m        \u001b[32m0.4379\u001b[0m       \u001b[35m0.5243\u001b[0m            \u001b[31m0.5243\u001b[0m        \u001b[94m1.1429\u001b[0m  0.0004  0.5988\n",
      "     17            \u001b[36m0.9519\u001b[0m        0.4801       0.5174            0.5174        \u001b[94m1.1371\u001b[0m  0.0004  0.5988\n",
      "     18            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4178\u001b[0m       0.5243            0.5243        \u001b[94m1.1347\u001b[0m  0.0004  0.6007\n",
      "     19            0.9667        0.4485       0.5208            0.5208        1.1417  0.0004  0.5980\n",
      "     20            \u001b[36m0.9704\u001b[0m        \u001b[32m0.4086\u001b[0m       0.5139            0.5139        1.1386  0.0003  0.6034\n",
      "     21            \u001b[36m0.9741\u001b[0m        \u001b[32m0.3569\u001b[0m       0.5139            0.5139        1.1425  0.0003  0.6100\n",
      "     22            \u001b[36m0.9815\u001b[0m        0.3796       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        1.1406  0.0003  0.6093\n",
      "     23            0.9815        0.3853       0.5243            0.5243        1.1355  0.0002  0.6093\n",
      "     24            0.9778        \u001b[32m0.3304\u001b[0m       0.5278            0.5278        \u001b[94m1.1238\u001b[0m  0.0002  0.6141\n",
      "     25            0.9815        0.3504       0.5312            0.5312        \u001b[94m1.1157\u001b[0m  0.0002  0.6104\n",
      "     26            0.9741        0.3313       0.5312            0.5312        \u001b[94m1.1119\u001b[0m  0.0002  0.6044\n",
      "     27            \u001b[36m0.9852\u001b[0m        \u001b[32m0.3158\u001b[0m       0.5312            0.5312        1.1139  0.0002  0.6144\n",
      "     28            0.9852        0.3378       0.5243            0.5243        1.1179  0.0001  0.7015\n",
      "     29            \u001b[36m0.9889\u001b[0m        0.3278       \u001b[35m0.5347\u001b[0m            \u001b[31m0.5347\u001b[0m        1.1184  0.0001  0.7137\n",
      "     30            \u001b[36m0.9963\u001b[0m        0.3298       0.5312            0.5312        1.1188  0.0001  0.8113\n",
      "     31            0.9963        \u001b[32m0.2939\u001b[0m       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        1.1174  0.0001  0.9352\n",
      "     32            0.9963        0.3053       0.5347            0.5347        1.1166  0.0001  0.6312\n",
      "     33            0.9963        0.3418       0.5347            0.5347        1.1160  0.0000  0.7479\n",
      "     34            0.9963        \u001b[32m0.2897\u001b[0m       0.5347            0.5347        1.1154  0.0000  0.6014\n",
      "     35            0.9963        \u001b[32m0.2667\u001b[0m       0.5347            0.5347        1.1154  0.0000  0.6522\n",
      "     36            0.9963        0.3113       0.5347            0.5347        1.1158  0.0000  0.6090\n",
      "     37            0.9963        0.2717       0.5312            0.5312        1.1156  0.0000  0.6142\n",
      "     38            0.9963        0.2974       0.5312            0.5312        1.1155  0.0000  0.6173\n",
      "     39            0.9963        0.3198       0.5382            0.5382        1.1154  0.0000  0.6103\n",
      "     40            0.9963        0.3032       0.5382            0.5382        1.1154  0.0000  0.6381\n",
      "Training model for subject 7 with 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.5000\u001b[0m        \u001b[32m2.1939\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m       \u001b[94m10.4318\u001b[0m  0.0006  0.3323\n",
      "      2            0.5000        \u001b[32m0.8295\u001b[0m       0.2500            0.2500       11.0685  0.0006  0.3177\n",
      "      3            0.5000        \u001b[32m0.4459\u001b[0m       0.2500            0.2500       10.8489  0.0006  0.4381\n",
      "      4            0.5000        \u001b[32m0.2821\u001b[0m       0.2500            0.2500       \u001b[94m10.2967\u001b[0m  0.0006  0.3112\n",
      "      5            0.5000        \u001b[32m0.2019\u001b[0m       0.2500            0.2500        \u001b[94m9.5724\u001b[0m  0.0006  0.3380\n",
      "      6            0.5000        0.2975       0.2500            0.2500        \u001b[94m8.6828\u001b[0m  0.0006  0.3227\n",
      "      7            0.5000        \u001b[32m0.1464\u001b[0m       0.2500            0.2500        \u001b[94m7.7748\u001b[0m  0.0006  0.3233\n",
      "      8            0.5000        \u001b[32m0.0984\u001b[0m       0.2500            0.2500        \u001b[94m7.0869\u001b[0m  0.0006  0.3204\n",
      "      9            \u001b[36m0.6000\u001b[0m        \u001b[32m0.0610\u001b[0m       0.2500            0.2500        \u001b[94m6.3916\u001b[0m  0.0006  0.3083\n",
      "     10            \u001b[36m0.9000\u001b[0m        \u001b[32m0.0599\u001b[0m       0.2500            0.2500        \u001b[94m5.7253\u001b[0m  0.0005  0.3345\n",
      "     11            0.9000        \u001b[32m0.0406\u001b[0m       0.2500            0.2500        \u001b[94m5.1064\u001b[0m  0.0005  0.3159\n",
      "     12            0.9000        \u001b[32m0.0318\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m4.5879\u001b[0m  0.0005  0.3354\n",
      "     13            0.9000        0.0364       0.2500            0.2500        \u001b[94m4.1639\u001b[0m  0.0005  0.3391\n",
      "     14            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0152\u001b[0m       0.2500            0.2500        \u001b[94m3.8189\u001b[0m  0.0005  0.3166\n",
      "     15            1.0000        0.0275       0.2500            0.2500        \u001b[94m3.5317\u001b[0m  0.0004  0.4258\n",
      "     16            1.0000        0.0264       0.2535            0.2535        \u001b[94m3.3005\u001b[0m  0.0004  0.4452\n",
      "     17            1.0000        0.0295       0.2535            0.2535        \u001b[94m3.1072\u001b[0m  0.0004  0.4230\n",
      "     18            1.0000        \u001b[32m0.0119\u001b[0m       0.2535            0.2535        \u001b[94m2.9510\u001b[0m  0.0004  0.4296\n",
      "     19            1.0000        0.0146       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m2.8339\u001b[0m  0.0004  0.4382\n",
      "     20            1.0000        0.0214       0.2569            0.2569        \u001b[94m2.7395\u001b[0m  0.0003  0.4017\n",
      "     21            1.0000        0.0217       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m2.6620\u001b[0m  0.0003  0.3324\n",
      "     22            1.0000        0.0291       0.2674            0.2674        \u001b[94m2.6077\u001b[0m  0.0003  0.3281\n",
      "     23            1.0000        0.0125       0.2674            0.2674        \u001b[94m2.5663\u001b[0m  0.0002  0.3395\n",
      "     24            1.0000        0.0168       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m2.5278\u001b[0m  0.0002  0.3202\n",
      "     25            1.0000        0.0130       0.2708            0.2708        \u001b[94m2.4970\u001b[0m  0.0002  0.3266\n",
      "     26            1.0000        0.0127       0.2674            0.2674        \u001b[94m2.4751\u001b[0m  0.0002  0.3254\n",
      "     27            1.0000        0.0144       0.2708            0.2708        \u001b[94m2.4605\u001b[0m  0.0002  0.3288\n",
      "     28            1.0000        \u001b[32m0.0096\u001b[0m       0.2708            0.2708        \u001b[94m2.4477\u001b[0m  0.0001  0.3165\n",
      "     29            1.0000        \u001b[32m0.0076\u001b[0m       0.2674            0.2674        \u001b[94m2.4322\u001b[0m  0.0001  0.3369\n",
      "     30            1.0000        0.0134       0.2708            0.2708        \u001b[94m2.4198\u001b[0m  0.0001  0.3164\n",
      "     31            1.0000        0.0128       0.2708            0.2708        \u001b[94m2.4101\u001b[0m  0.0001  0.3347\n",
      "     32            1.0000        0.0086       0.2708            0.2708        \u001b[94m2.4061\u001b[0m  0.0001  0.3186\n",
      "     33            1.0000        0.0142       0.2708            0.2708        \u001b[94m2.4013\u001b[0m  0.0000  0.3177\n",
      "     34            1.0000        0.0158       0.2708            0.2708        \u001b[94m2.3939\u001b[0m  0.0000  0.3224\n",
      "     35            1.0000        0.0101       0.2708            0.2708        \u001b[94m2.3900\u001b[0m  0.0000  0.3281\n",
      "     36            1.0000        0.0081       0.2708            0.2708        \u001b[94m2.3832\u001b[0m  0.0000  0.3166\n",
      "     37            1.0000        0.0133       0.2708            0.2708        \u001b[94m2.3827\u001b[0m  0.0000  0.3223\n",
      "     38            1.0000        0.0243       0.2708            0.2708        \u001b[94m2.3772\u001b[0m  0.0000  0.3379\n",
      "     39            1.0000        0.0141       0.2708            0.2708        2.3780  0.0000  0.3100\n",
      "     40            1.0000        \u001b[32m0.0059\u001b[0m       0.2708            0.2708        \u001b[94m2.3763\u001b[0m  0.0000  0.3384\n",
      "Training model for subject 7 with 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.4500\u001b[0m        \u001b[32m1.8022\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m1.7594\u001b[0m  0.0006  0.2860\n",
      "      2            0.4500        \u001b[32m0.8970\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        2.5948  0.0006  0.2830\n",
      "      3            0.4500        \u001b[32m0.8260\u001b[0m       0.2535            0.2535        3.4019  0.0006  0.2670\n",
      "      4            0.4500        \u001b[32m0.6422\u001b[0m       0.2535            0.2535        3.9334  0.0006  0.2857\n",
      "      5            0.4500        \u001b[32m0.4530\u001b[0m       0.2535            0.2535        4.0455  0.0006  0.2724\n",
      "      6            \u001b[36m0.7000\u001b[0m        \u001b[32m0.4271\u001b[0m       0.2535            0.2535        3.7860  0.0006  0.2795\n",
      "      7            \u001b[36m0.8000\u001b[0m        \u001b[32m0.1911\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        3.3877  0.0006  0.2803\n",
      "      8            0.8000        \u001b[32m0.1467\u001b[0m       0.2535            0.2535        3.1107  0.0006  0.2680\n",
      "      9            \u001b[36m0.9000\u001b[0m        \u001b[32m0.1261\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        2.8460  0.0006  0.2676\n",
      "     10            0.9000        \u001b[32m0.0910\u001b[0m       0.2708            0.2708        2.6436  0.0005  0.2855\n",
      "     11            0.9000        \u001b[32m0.0765\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        2.4789  0.0005  0.2643\n",
      "     12            0.9000        0.0853       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        2.3500  0.0005  0.2723\n",
      "     13            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0457\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        2.2321  0.0005  0.2621\n",
      "     14            1.0000        \u001b[32m0.0423\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        2.1269  0.0005  0.2771\n",
      "     15            1.0000        0.0543       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        2.0449  0.0004  0.2692\n",
      "     16            1.0000        0.0580       0.3438            0.3438        1.9795  0.0004  0.2747\n",
      "     17            1.0000        \u001b[32m0.0394\u001b[0m       0.3542            0.3542        1.9229  0.0004  0.2694\n",
      "     18            1.0000        \u001b[32m0.0358\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        1.8743  0.0004  0.3113\n",
      "     19            1.0000        0.0412       0.3542            0.3542        1.8368  0.0004  0.2693\n",
      "     20            1.0000        \u001b[32m0.0259\u001b[0m       0.3438            0.3438        1.8092  0.0003  0.2911\n",
      "     21            1.0000        0.0284       0.3403            0.3403        1.7867  0.0003  0.2755\n",
      "     22            1.0000        0.0292       0.3438            0.3438        1.7676  0.0003  0.2876\n",
      "     23            1.0000        \u001b[32m0.0178\u001b[0m       0.3507            0.3507        \u001b[94m1.7512\u001b[0m  0.0002  0.3621\n",
      "     24            1.0000        \u001b[32m0.0165\u001b[0m       0.3576            0.3576        \u001b[94m1.7372\u001b[0m  0.0002  0.3631\n",
      "     25            1.0000        0.0248       0.3576            0.3576        \u001b[94m1.7263\u001b[0m  0.0002  0.3912\n",
      "     26            1.0000        0.0272       0.3576            0.3576        \u001b[94m1.7178\u001b[0m  0.0002  0.3439\n",
      "     27            1.0000        0.0238       0.3576            0.3576        \u001b[94m1.7109\u001b[0m  0.0002  0.3609\n",
      "     28            1.0000        \u001b[32m0.0157\u001b[0m       0.3542            0.3542        \u001b[94m1.7048\u001b[0m  0.0001  0.3581\n",
      "     29            1.0000        0.0205       0.3542            0.3542        \u001b[94m1.6997\u001b[0m  0.0001  0.3469\n",
      "     30            1.0000        0.0185       0.3542            0.3542        \u001b[94m1.6962\u001b[0m  0.0001  0.3983\n",
      "     31            1.0000        0.0324       0.3542            0.3542        \u001b[94m1.6929\u001b[0m  0.0001  0.3006\n",
      "     32            1.0000        0.0363       0.3542            0.3542        \u001b[94m1.6907\u001b[0m  0.0001  0.2869\n",
      "     33            1.0000        0.0246       0.3542            0.3542        \u001b[94m1.6882\u001b[0m  0.0000  0.2716\n",
      "     34            1.0000        0.0199       0.3507            0.3507        \u001b[94m1.6859\u001b[0m  0.0000  0.2677\n",
      "     35            1.0000        0.0213       0.3507            0.3507        \u001b[94m1.6832\u001b[0m  0.0000  0.2897\n",
      "     36            1.0000        0.0163       0.3507            0.3507        \u001b[94m1.6814\u001b[0m  0.0000  0.2760\n",
      "     37            1.0000        0.0176       0.3507            0.3507        \u001b[94m1.6797\u001b[0m  0.0000  0.2844\n",
      "     38            1.0000        0.0208       0.3507            0.3507        \u001b[94m1.6784\u001b[0m  0.0000  0.2696\n",
      "     39            1.0000        \u001b[32m0.0150\u001b[0m       0.3507            0.3507        \u001b[94m1.6772\u001b[0m  0.0000  0.2681\n",
      "     40            1.0000        0.0177       0.3507            0.3507        \u001b[94m1.6763\u001b[0m  0.0000  0.2848\n",
      "Training model for subject 7 with 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.5000\u001b[0m        \u001b[32m1.7644\u001b[0m       \u001b[35m0.2326\u001b[0m            \u001b[31m0.2326\u001b[0m        \u001b[94m2.6322\u001b[0m  0.0006  0.2869\n",
      "      2            0.4333        \u001b[32m0.9840\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        2.9810  0.0006  0.2802\n",
      "      3            0.5000        \u001b[32m0.7839\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        2.9277  0.0006  0.2603\n",
      "      4            0.5000        \u001b[32m0.5706\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        2.7208  0.0006  0.2574\n",
      "      5            \u001b[36m0.5667\u001b[0m        \u001b[32m0.4537\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m2.5029\u001b[0m  0.0006  0.2506\n",
      "      6            \u001b[36m0.6333\u001b[0m        \u001b[32m0.3819\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m2.4192\u001b[0m  0.0006  0.2702\n",
      "      7            \u001b[36m0.7333\u001b[0m        \u001b[32m0.2196\u001b[0m       0.3125            0.3125        \u001b[94m2.3032\u001b[0m  0.0006  0.2669\n",
      "      8            \u001b[36m0.7667\u001b[0m        0.3016       0.3125            0.3125        \u001b[94m2.2452\u001b[0m  0.0006  0.2808\n",
      "      9            \u001b[36m0.9000\u001b[0m        \u001b[32m0.1727\u001b[0m       0.3056            0.3056        \u001b[94m2.2190\u001b[0m  0.0006  0.2839\n",
      "     10            \u001b[36m0.9333\u001b[0m        \u001b[32m0.1322\u001b[0m       0.3021            0.3021        2.2394  0.0005  0.2728\n",
      "     11            \u001b[36m0.9667\u001b[0m        \u001b[32m0.1295\u001b[0m       0.3056            0.3056        2.2639  0.0005  0.2635\n",
      "     12            \u001b[36m1.0000\u001b[0m        0.1303       0.3125            0.3125        2.2777  0.0005  0.2667\n",
      "     13            1.0000        0.1389       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        2.2533  0.0005  0.2678\n",
      "     14            1.0000        \u001b[32m0.0961\u001b[0m       0.2917            0.2917        \u001b[94m2.1871\u001b[0m  0.0005  0.2639\n",
      "     15            1.0000        0.1012       0.2951            0.2951        \u001b[94m2.1038\u001b[0m  0.0004  0.2593\n",
      "     16            1.0000        \u001b[32m0.0741\u001b[0m       0.3125            0.3125        \u001b[94m2.0243\u001b[0m  0.0004  0.2547\n",
      "     17            1.0000        \u001b[32m0.0562\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        \u001b[94m1.9471\u001b[0m  0.0004  0.2642\n",
      "     18            1.0000        0.0650       0.3125            0.3125        \u001b[94m1.8844\u001b[0m  0.0004  0.2502\n",
      "     19            1.0000        \u001b[32m0.0378\u001b[0m       0.3160            0.3160        \u001b[94m1.8437\u001b[0m  0.0004  0.3212\n",
      "     20            1.0000        0.0773       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m1.8155\u001b[0m  0.0003  0.2537\n",
      "     21            1.0000        0.0557       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.7997\u001b[0m  0.0003  0.2546\n",
      "     22            1.0000        0.0497       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.7896\u001b[0m  0.0003  0.2553\n",
      "     23            1.0000        0.0424       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.7824\u001b[0m  0.0002  0.2670\n",
      "     24            1.0000        0.0458       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.7785\u001b[0m  0.0002  0.2719\n",
      "     25            1.0000        \u001b[32m0.0354\u001b[0m       0.3750            0.3750        \u001b[94m1.7753\u001b[0m  0.0002  0.2624\n",
      "     26            1.0000        0.0470       0.3750            0.3750        \u001b[94m1.7729\u001b[0m  0.0002  0.2715\n",
      "     27            1.0000        0.0377       0.3785            0.3785        \u001b[94m1.7706\u001b[0m  0.0002  0.2845\n",
      "     28            1.0000        \u001b[32m0.0298\u001b[0m       0.3785            0.3785        \u001b[94m1.7687\u001b[0m  0.0001  0.2645\n",
      "     29            1.0000        0.0497       0.3750            0.3750        \u001b[94m1.7677\u001b[0m  0.0001  0.2670\n",
      "     30            1.0000        0.0579       0.3750            0.3750        \u001b[94m1.7667\u001b[0m  0.0001  0.2496\n",
      "     31            1.0000        0.0439       0.3785            0.3785        1.7671  0.0001  0.2727\n",
      "     32            1.0000        0.0362       0.3785            0.3785        1.7668  0.0001  0.2496\n",
      "     33            1.0000        0.0572       0.3785            0.3785        \u001b[94m1.7663\u001b[0m  0.0000  0.2834\n",
      "     34            1.0000        0.0337       0.3785            0.3785        \u001b[94m1.7658\u001b[0m  0.0000  0.2871\n",
      "     35            1.0000        0.0466       0.3785            0.3785        \u001b[94m1.7653\u001b[0m  0.0000  0.3295\n",
      "     36            1.0000        0.0351       0.3785            0.3785        \u001b[94m1.7647\u001b[0m  0.0000  0.3434\n",
      "     37            1.0000        0.0388       0.3785            0.3785        \u001b[94m1.7641\u001b[0m  0.0000  0.3736\n",
      "     38            1.0000        \u001b[32m0.0237\u001b[0m       0.3785            0.3785        \u001b[94m1.7631\u001b[0m  0.0000  0.3400\n",
      "     39            1.0000        0.0446       0.3785            0.3785        \u001b[94m1.7624\u001b[0m  0.0000  0.3572\n",
      "     40            1.0000        0.0282       0.3785            0.3785        \u001b[94m1.7619\u001b[0m  0.0000  0.3500\n",
      "Training model for subject 7 with 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3000\u001b[0m        \u001b[32m1.7168\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m5.5355\u001b[0m  0.0006  0.3424\n",
      "      2            0.3000        \u001b[32m1.3147\u001b[0m       0.2500            0.2500        \u001b[94m4.3725\u001b[0m  0.0006  0.3009\n",
      "      3            0.3000        \u001b[32m0.8052\u001b[0m       0.2500            0.2500        \u001b[94m3.7121\u001b[0m  0.0006  0.2827\n",
      "      4            \u001b[36m0.3500\u001b[0m        \u001b[32m0.7848\u001b[0m       0.2500            0.2500        \u001b[94m3.4214\u001b[0m  0.0006  0.2652\n",
      "      5            \u001b[36m0.4750\u001b[0m        \u001b[32m0.5665\u001b[0m       0.2500            0.2500        \u001b[94m3.1064\u001b[0m  0.0006  0.2799\n",
      "      6            \u001b[36m0.5500\u001b[0m        \u001b[32m0.4884\u001b[0m       0.2465            0.2465        \u001b[94m2.9717\u001b[0m  0.0006  0.2665\n",
      "      7            \u001b[36m0.5750\u001b[0m        \u001b[32m0.4217\u001b[0m       0.2431            0.2431        \u001b[94m2.8509\u001b[0m  0.0006  0.2603\n",
      "      8            0.5750        \u001b[32m0.3720\u001b[0m       0.2500            0.2500        \u001b[94m2.8384\u001b[0m  0.0006  0.2867\n",
      "      9            0.5500        \u001b[32m0.3632\u001b[0m       0.2500            0.2500        2.8797  0.0006  0.2642\n",
      "     10            0.5500        \u001b[32m0.2418\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        2.8817  0.0005  0.3879\n",
      "     11            \u001b[36m0.7000\u001b[0m        \u001b[32m0.2259\u001b[0m       0.2431            0.2431        \u001b[94m2.7363\u001b[0m  0.0005  0.2695\n",
      "     12            \u001b[36m0.8000\u001b[0m        0.2410       0.2500            0.2500        \u001b[94m2.5054\u001b[0m  0.0005  0.2643\n",
      "     13            \u001b[36m0.9250\u001b[0m        \u001b[32m0.1980\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        \u001b[94m2.3375\u001b[0m  0.0005  0.2828\n",
      "     14            0.9250        0.2456       0.2743            0.2743        \u001b[94m2.2293\u001b[0m  0.0005  0.2605\n",
      "     15            0.9250        \u001b[32m0.1636\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        \u001b[94m2.1384\u001b[0m  0.0004  0.2704\n",
      "     16            0.9250        \u001b[32m0.1320\u001b[0m       0.2882            0.2882        \u001b[94m2.0716\u001b[0m  0.0004  0.2681\n",
      "     17            \u001b[36m0.9750\u001b[0m        0.1456       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m2.0139\u001b[0m  0.0004  0.2690\n",
      "     18            \u001b[36m1.0000\u001b[0m        0.1371       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m1.9698\u001b[0m  0.0004  0.2792\n",
      "     19            1.0000        \u001b[32m0.1238\u001b[0m       0.2986            0.2986        \u001b[94m1.9338\u001b[0m  0.0004  0.2836\n",
      "     20            1.0000        \u001b[32m0.0987\u001b[0m       0.3021            0.3021        \u001b[94m1.9048\u001b[0m  0.0003  0.2878\n",
      "     21            1.0000        \u001b[32m0.0963\u001b[0m       0.3056            0.3056        \u001b[94m1.8797\u001b[0m  0.0003  0.2807\n",
      "     22            1.0000        0.1076       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m1.8590\u001b[0m  0.0003  0.2682\n",
      "     23            1.0000        \u001b[32m0.0729\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m1.8424\u001b[0m  0.0002  0.2684\n",
      "     24            1.0000        0.0827       0.3160            0.3160        \u001b[94m1.8302\u001b[0m  0.0002  0.2779\n",
      "     25            1.0000        0.1061       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m1.8200\u001b[0m  0.0002  0.2686\n",
      "     26            1.0000        0.0802       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m1.8108\u001b[0m  0.0002  0.2835\n",
      "     27            1.0000        \u001b[32m0.0713\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.8043\u001b[0m  0.0002  0.2751\n",
      "     28            1.0000        \u001b[32m0.0549\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.7990\u001b[0m  0.0001  0.2665\n",
      "     29            1.0000        0.0884       0.3472            0.3472        \u001b[94m1.7942\u001b[0m  0.0001  0.2692\n",
      "     30            1.0000        0.0852       0.3438            0.3438        \u001b[94m1.7904\u001b[0m  0.0001  0.2831\n",
      "     31            1.0000        0.0799       0.3472            0.3472        \u001b[94m1.7881\u001b[0m  0.0001  0.2696\n",
      "     32            1.0000        0.0838       0.3472            0.3472        \u001b[94m1.7864\u001b[0m  0.0001  0.2842\n",
      "     33            1.0000        0.0618       0.3507            0.3507        \u001b[94m1.7847\u001b[0m  0.0000  0.2730\n",
      "     34            1.0000        0.0678       0.3472            0.3472        \u001b[94m1.7838\u001b[0m  0.0000  0.2984\n",
      "     35            1.0000        0.0574       0.3438            0.3438        \u001b[94m1.7827\u001b[0m  0.0000  0.2798\n",
      "     36            1.0000        0.0757       0.3472            0.3472        \u001b[94m1.7816\u001b[0m  0.0000  0.2693\n",
      "     37            1.0000        0.0904       0.3472            0.3472        \u001b[94m1.7807\u001b[0m  0.0000  0.2722\n",
      "     38            1.0000        0.0631       0.3472            0.3472        \u001b[94m1.7796\u001b[0m  0.0000  0.2819\n",
      "     39            1.0000        0.0623       0.3438            0.3438        \u001b[94m1.7788\u001b[0m  0.0000  0.2676\n",
      "     40            1.0000        0.0790       0.3438            0.3438        \u001b[94m1.7781\u001b[0m  0.0000  0.2793\n",
      "Training model for subject 7 with 50 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2600\u001b[0m        \u001b[32m1.7490\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.4394\u001b[0m  0.0006  0.2902\n",
      "      2            \u001b[36m0.4400\u001b[0m        \u001b[32m1.2133\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m3.3683\u001b[0m  0.0006  0.2791\n",
      "      3            0.3400        \u001b[32m0.9986\u001b[0m       0.2569            0.2569        4.1928  0.0006  0.2993\n",
      "      4            0.2800        \u001b[32m0.7245\u001b[0m       0.2500            0.2500        4.8335  0.0006  0.3145\n",
      "      5            0.2600        \u001b[32m0.7040\u001b[0m       0.2500            0.2500        5.1291  0.0006  0.3432\n",
      "      6            0.2800        \u001b[32m0.6111\u001b[0m       0.2500            0.2500        5.0374  0.0006  0.3617\n",
      "      7            0.3000        \u001b[32m0.4947\u001b[0m       0.2500            0.2500        4.6912  0.0006  0.3759\n",
      "      8            0.3600        \u001b[32m0.4204\u001b[0m       0.2535            0.2535        4.1260  0.0006  0.3909\n",
      "      9            0.4400        \u001b[32m0.3211\u001b[0m       0.2604            0.2604        3.6560  0.0006  0.3745\n",
      "     10            \u001b[36m0.5400\u001b[0m        0.3268       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m3.2664\u001b[0m  0.0005  0.3874\n",
      "     11            \u001b[36m0.5800\u001b[0m        0.3514       0.2708            0.2708        \u001b[94m3.0663\u001b[0m  0.0005  0.3977\n",
      "     12            \u001b[36m0.6000\u001b[0m        \u001b[32m0.3067\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        \u001b[94m2.9365\u001b[0m  0.0005  0.3016\n",
      "     13            \u001b[36m0.6800\u001b[0m        \u001b[32m0.2631\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m2.7951\u001b[0m  0.0005  0.2742\n",
      "     14            \u001b[36m0.7200\u001b[0m        \u001b[32m0.2347\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m2.6623\u001b[0m  0.0005  0.2872\n",
      "     15            \u001b[36m0.8000\u001b[0m        \u001b[32m0.2059\u001b[0m       0.3125            0.3125        \u001b[94m2.5509\u001b[0m  0.0004  0.2861\n",
      "     16            \u001b[36m0.8400\u001b[0m        0.2162       0.2986            0.2986        \u001b[94m2.4427\u001b[0m  0.0004  0.4012\n",
      "     17            \u001b[36m0.8800\u001b[0m        \u001b[32m0.1834\u001b[0m       0.3056            0.3056        \u001b[94m2.3509\u001b[0m  0.0004  0.2837\n",
      "     18            \u001b[36m0.9000\u001b[0m        \u001b[32m0.1808\u001b[0m       0.3021            0.3021        \u001b[94m2.2600\u001b[0m  0.0004  0.2981\n",
      "     19            \u001b[36m0.9200\u001b[0m        \u001b[32m0.1670\u001b[0m       0.3160            0.3160        \u001b[94m2.1916\u001b[0m  0.0004  0.2811\n",
      "     20            \u001b[36m0.9800\u001b[0m        \u001b[32m0.1212\u001b[0m       0.3090            0.3090        \u001b[94m2.1293\u001b[0m  0.0003  0.2850\n",
      "     21            \u001b[36m1.0000\u001b[0m        0.1515       0.3056            0.3056        \u001b[94m2.0686\u001b[0m  0.0003  0.2922\n",
      "     22            1.0000        \u001b[32m0.0896\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m2.0140\u001b[0m  0.0003  0.2949\n",
      "     23            1.0000        0.1239       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m1.9665\u001b[0m  0.0002  0.3035\n",
      "     24            1.0000        0.1195       0.3229            0.3229        \u001b[94m1.9317\u001b[0m  0.0002  0.2857\n",
      "     25            1.0000        0.1451       0.3160            0.3160        \u001b[94m1.9023\u001b[0m  0.0002  0.3009\n",
      "     26            1.0000        \u001b[32m0.0711\u001b[0m       0.3229            0.3229        \u001b[94m1.8788\u001b[0m  0.0002  0.3075\n",
      "     27            1.0000        0.1007       0.3125            0.3125        \u001b[94m1.8602\u001b[0m  0.0002  0.2826\n",
      "     28            1.0000        0.0981       0.3056            0.3056        \u001b[94m1.8453\u001b[0m  0.0001  0.2888\n",
      "     29            1.0000        0.0898       0.2986            0.2986        \u001b[94m1.8342\u001b[0m  0.0001  0.2933\n",
      "     30            1.0000        0.1162       0.3090            0.3090        \u001b[94m1.8249\u001b[0m  0.0001  0.3033\n",
      "     31            1.0000        0.1006       0.3125            0.3125        \u001b[94m1.8175\u001b[0m  0.0001  0.2840\n",
      "     32            1.0000        0.1110       0.3056            0.3056        \u001b[94m1.8108\u001b[0m  0.0001  0.3014\n",
      "     33            1.0000        0.0901       0.3021            0.3021        \u001b[94m1.8056\u001b[0m  0.0000  0.2990\n",
      "     34            1.0000        0.1086       0.2986            0.2986        \u001b[94m1.8015\u001b[0m  0.0000  0.3041\n",
      "     35            1.0000        0.0782       0.3021            0.3021        \u001b[94m1.7980\u001b[0m  0.0000  0.2800\n",
      "     36            1.0000        0.0919       0.2986            0.2986        \u001b[94m1.7950\u001b[0m  0.0000  0.2859\n",
      "     37            1.0000        0.1005       0.3021            0.3021        \u001b[94m1.7925\u001b[0m  0.0000  0.3182\n",
      "     38            1.0000        0.1139       0.3090            0.3090        \u001b[94m1.7906\u001b[0m  0.0000  0.3108\n",
      "     39            1.0000        0.1166       0.3090            0.3090        \u001b[94m1.7890\u001b[0m  0.0000  0.3201\n",
      "     40            1.0000        0.1222       0.3090            0.3090        \u001b[94m1.7876\u001b[0m  0.0000  0.2996\n",
      "Training model for subject 7 with 60 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2667\u001b[0m        \u001b[32m1.6990\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.3349\u001b[0m  0.0006  0.3130\n",
      "      2            \u001b[36m0.4333\u001b[0m        \u001b[32m1.4553\u001b[0m       0.2465            0.2465        \u001b[94m4.0683\u001b[0m  0.0006  0.3009\n",
      "      3            0.3667        \u001b[32m1.0476\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        4.2067  0.0006  0.2861\n",
      "      4            0.3167        \u001b[32m0.9123\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        4.5296  0.0006  0.2935\n",
      "      5            0.3167        \u001b[32m0.8554\u001b[0m       0.2674            0.2674        4.6806  0.0006  0.2889\n",
      "      6            0.3500        \u001b[32m0.6847\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        4.5703  0.0006  0.3038\n",
      "      7            0.3667        \u001b[32m0.6666\u001b[0m       0.2812            0.2812        4.4280  0.0006  0.2919\n",
      "      8            0.4000        \u001b[32m0.5478\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        4.2248  0.0006  0.3079\n",
      "      9            0.4333        \u001b[32m0.5104\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m3.9235\u001b[0m  0.0006  0.2831\n",
      "     10            \u001b[36m0.4833\u001b[0m        \u001b[32m0.4880\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m3.6130\u001b[0m  0.0005  0.3395\n",
      "     11            0.4833        \u001b[32m0.4163\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        \u001b[94m3.3400\u001b[0m  0.0005  0.3607\n",
      "     12            \u001b[36m0.5167\u001b[0m        \u001b[32m0.3902\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m3.0797\u001b[0m  0.0005  0.3904\n",
      "     13            \u001b[36m0.6000\u001b[0m        \u001b[32m0.3068\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m2.8273\u001b[0m  0.0005  0.3727\n",
      "     14            \u001b[36m0.6667\u001b[0m        \u001b[32m0.2688\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m2.5919\u001b[0m  0.0005  0.3643\n",
      "     15            \u001b[36m0.7167\u001b[0m        0.2889       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m2.4006\u001b[0m  0.0004  0.3564\n",
      "     16            \u001b[36m0.8667\u001b[0m        \u001b[32m0.2332\u001b[0m       0.3403            0.3403        \u001b[94m2.2357\u001b[0m  0.0004  0.4336\n",
      "     17            \u001b[36m0.9167\u001b[0m        0.2496       0.3403            0.3403        \u001b[94m2.1048\u001b[0m  0.0004  0.4032\n",
      "     18            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1722\u001b[0m       0.3299            0.3299        \u001b[94m1.9889\u001b[0m  0.0004  0.3288\n",
      "     19            1.0000        0.2716       0.3229            0.3229        \u001b[94m1.8971\u001b[0m  0.0004  0.3096\n",
      "     20            1.0000        0.2048       0.3160            0.3160        \u001b[94m1.8250\u001b[0m  0.0003  0.2875\n",
      "     21            1.0000        0.1938       0.3299            0.3299        \u001b[94m1.7734\u001b[0m  0.0003  0.3093\n",
      "     22            1.0000        0.1994       0.3333            0.3333        \u001b[94m1.7397\u001b[0m  0.0003  0.3112\n",
      "     23            1.0000        0.1964       0.3472            0.3472        \u001b[94m1.7174\u001b[0m  0.0002  0.3041\n",
      "     24            1.0000        \u001b[32m0.1222\u001b[0m       0.3403            0.3403        \u001b[94m1.7017\u001b[0m  0.0002  0.3016\n",
      "     25            1.0000        0.1850       0.3368            0.3368        \u001b[94m1.6890\u001b[0m  0.0002  0.3030\n",
      "     26            1.0000        0.1381       0.3368            0.3368        \u001b[94m1.6800\u001b[0m  0.0002  0.2936\n",
      "     27            1.0000        0.1582       0.3403            0.3403        \u001b[94m1.6744\u001b[0m  0.0002  0.3042\n",
      "     28            1.0000        \u001b[32m0.1203\u001b[0m       0.3472            0.3472        \u001b[94m1.6719\u001b[0m  0.0001  0.4371\n",
      "     29            1.0000        0.1251       0.3472            0.3472        \u001b[94m1.6689\u001b[0m  0.0001  0.3231\n",
      "     30            1.0000        0.1285       0.3507            0.3507        \u001b[94m1.6673\u001b[0m  0.0001  0.3192\n",
      "     31            1.0000        0.1733       0.3403            0.3403        \u001b[94m1.6658\u001b[0m  0.0001  0.3229\n",
      "     32            1.0000        0.1259       0.3403            0.3403        \u001b[94m1.6647\u001b[0m  0.0001  0.3141\n",
      "     33            1.0000        \u001b[32m0.0979\u001b[0m       0.3438            0.3438        \u001b[94m1.6639\u001b[0m  0.0000  0.2888\n",
      "     34            1.0000        0.1485       0.3438            0.3438        \u001b[94m1.6630\u001b[0m  0.0000  0.3012\n",
      "     35            1.0000        0.1223       0.3368            0.3368        \u001b[94m1.6623\u001b[0m  0.0000  0.3026\n",
      "     36            1.0000        0.1128       0.3368            0.3368        \u001b[94m1.6614\u001b[0m  0.0000  0.2980\n",
      "     37            1.0000        0.1189       0.3368            0.3368        \u001b[94m1.6609\u001b[0m  0.0000  0.3113\n",
      "     38            1.0000        0.1312       0.3368            0.3368        \u001b[94m1.6602\u001b[0m  0.0000  0.3067\n",
      "     39            1.0000        0.1029       0.3403            0.3403        \u001b[94m1.6592\u001b[0m  0.0000  0.3140\n",
      "     40            1.0000        0.1424       0.3403            0.3403        \u001b[94m1.6583\u001b[0m  0.0000  0.2989\n",
      "Training model for subject 7 with 70 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2571\u001b[0m        \u001b[32m1.8613\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m6.0908\u001b[0m  0.0006  0.3189\n",
      "      2            0.2571        \u001b[32m1.3933\u001b[0m       0.2500            0.2500        \u001b[94m3.9976\u001b[0m  0.0006  0.3110\n",
      "      3            \u001b[36m0.4429\u001b[0m        \u001b[32m1.0915\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m2.7471\u001b[0m  0.0006  0.3162\n",
      "      4            \u001b[36m0.4714\u001b[0m        \u001b[32m0.8998\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m2.2210\u001b[0m  0.0006  0.3196\n",
      "      5            \u001b[36m0.4857\u001b[0m        \u001b[32m0.7915\u001b[0m       0.3264            0.3264        \u001b[94m1.9822\u001b[0m  0.0006  0.3208\n",
      "      6            \u001b[36m0.6286\u001b[0m        \u001b[32m0.7191\u001b[0m       0.3299            0.3299        \u001b[94m1.8633\u001b[0m  0.0006  0.3101\n",
      "      7            \u001b[36m0.7286\u001b[0m        \u001b[32m0.6573\u001b[0m       0.3194            0.3194        \u001b[94m1.7974\u001b[0m  0.0006  0.3258\n",
      "      8            \u001b[36m0.8286\u001b[0m        \u001b[32m0.5337\u001b[0m       0.3264            0.3264        \u001b[94m1.7143\u001b[0m  0.0006  0.3191\n",
      "      9            \u001b[36m0.8857\u001b[0m        \u001b[32m0.4710\u001b[0m       0.3333            0.3333        \u001b[94m1.6736\u001b[0m  0.0006  0.3106\n",
      "     10            \u001b[36m0.9429\u001b[0m        0.5167       0.3507            0.3507        \u001b[94m1.6526\u001b[0m  0.0005  0.3250\n",
      "     11            \u001b[36m0.9714\u001b[0m        \u001b[32m0.4052\u001b[0m       0.3576            0.3576        \u001b[94m1.6485\u001b[0m  0.0005  0.3138\n",
      "     12            0.9714        0.4058       0.3646            0.3646        \u001b[94m1.6354\u001b[0m  0.0005  0.3145\n",
      "     13            0.9714        \u001b[32m0.3214\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.6234\u001b[0m  0.0005  0.3735\n",
      "     14            \u001b[36m0.9857\u001b[0m        \u001b[32m0.2780\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.6018\u001b[0m  0.0005  0.3688\n",
      "     15            \u001b[36m1.0000\u001b[0m        0.3098       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.5840\u001b[0m  0.0004  0.4083\n",
      "     16            1.0000        \u001b[32m0.2748\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.5726\u001b[0m  0.0004  0.3599\n",
      "     17            1.0000        \u001b[32m0.2354\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.5701\u001b[0m  0.0004  0.3611\n",
      "     18            1.0000        \u001b[32m0.1866\u001b[0m       0.3924            0.3924        \u001b[94m1.5631\u001b[0m  0.0004  0.3903\n",
      "     19            1.0000        0.2311       0.3854            0.3854        \u001b[94m1.5528\u001b[0m  0.0004  0.3799\n",
      "     20            1.0000        0.2005       0.3958            0.3958        \u001b[94m1.5441\u001b[0m  0.0003  0.4174\n",
      "     21            1.0000        0.2059       0.3924            0.3924        \u001b[94m1.5374\u001b[0m  0.0003  0.3117\n",
      "     22            1.0000        \u001b[32m0.1698\u001b[0m       0.3958            0.3958        \u001b[94m1.5341\u001b[0m  0.0003  0.3195\n",
      "     23            1.0000        0.1780       0.3958            0.3958        \u001b[94m1.5315\u001b[0m  0.0002  0.3368\n",
      "     24            1.0000        \u001b[32m0.1527\u001b[0m       0.3993            0.3993        \u001b[94m1.5301\u001b[0m  0.0002  0.3205\n",
      "     25            1.0000        \u001b[32m0.1426\u001b[0m       0.3958            0.3958        \u001b[94m1.5300\u001b[0m  0.0002  0.3242\n",
      "     26            1.0000        0.1588       0.3889            0.3889        \u001b[94m1.5293\u001b[0m  0.0002  0.3247\n",
      "     27            1.0000        \u001b[32m0.1405\u001b[0m       0.3889            0.3889        \u001b[94m1.5285\u001b[0m  0.0002  0.3258\n",
      "     28            1.0000        0.1885       0.3924            0.3924        \u001b[94m1.5285\u001b[0m  0.0001  0.3266\n",
      "     29            1.0000        \u001b[32m0.1310\u001b[0m       0.3958            0.3958        1.5287  0.0001  0.3188\n",
      "     30            1.0000        0.1668       0.3958            0.3958        1.5293  0.0001  0.3089\n",
      "     31            1.0000        \u001b[32m0.1184\u001b[0m       0.3993            0.3993        1.5294  0.0001  0.3203\n",
      "     32            1.0000        0.1294       0.3924            0.3924        1.5287  0.0001  0.3180\n",
      "     33            1.0000        0.1534       0.3889            0.3889        \u001b[94m1.5279\u001b[0m  0.0000  0.3100\n",
      "     34            1.0000        0.1194       0.3854            0.3854        \u001b[94m1.5277\u001b[0m  0.0000  0.3123\n",
      "     35            1.0000        \u001b[32m0.1178\u001b[0m       0.3889            0.3889        \u001b[94m1.5273\u001b[0m  0.0000  0.3168\n",
      "     36            1.0000        0.1203       0.3889            0.3889        \u001b[94m1.5269\u001b[0m  0.0000  0.3272\n",
      "     37            1.0000        \u001b[32m0.1152\u001b[0m       0.3889            0.3889        \u001b[94m1.5267\u001b[0m  0.0000  0.3143\n",
      "     38            1.0000        0.1160       0.3924            0.3924        \u001b[94m1.5264\u001b[0m  0.0000  0.3266\n",
      "     39            1.0000        0.1152       0.3924            0.3924        \u001b[94m1.5260\u001b[0m  0.0000  0.3233\n",
      "     40            1.0000        \u001b[32m0.0989\u001b[0m       0.3958            0.3958        \u001b[94m1.5257\u001b[0m  0.0000  0.4508\n",
      "Training model for subject 7 with 80 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3375\u001b[0m        \u001b[32m1.8201\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m1.9591\u001b[0m  0.0006  0.3449\n",
      "      2            \u001b[36m0.4625\u001b[0m        \u001b[32m1.2793\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        2.9135  0.0006  0.3252\n",
      "      3            0.4250        \u001b[32m1.1484\u001b[0m       0.2917            0.2917        3.2338  0.0006  0.3412\n",
      "      4            0.4500        \u001b[32m1.0396\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        3.1497  0.0006  0.3255\n",
      "      5            \u001b[36m0.5000\u001b[0m        \u001b[32m0.9290\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        2.8255  0.0006  0.3325\n",
      "      6            0.4875        \u001b[32m0.8928\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        2.3952  0.0006  0.3269\n",
      "      7            \u001b[36m0.5500\u001b[0m        \u001b[32m0.6780\u001b[0m       0.3576            0.3576        2.1746  0.0006  0.3180\n",
      "      8            \u001b[36m0.6500\u001b[0m        \u001b[32m0.6015\u001b[0m       0.3576            0.3576        2.0515  0.0006  0.3156\n",
      "      9            \u001b[36m0.7500\u001b[0m        0.6098       0.3542            0.3542        1.9991  0.0006  0.3339\n",
      "     10            \u001b[36m0.8000\u001b[0m        \u001b[32m0.5345\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.9287\u001b[0m  0.0005  0.3403\n",
      "     11            \u001b[36m0.8625\u001b[0m        \u001b[32m0.4697\u001b[0m       0.3542            0.3542        \u001b[94m1.8604\u001b[0m  0.0005  0.3576\n",
      "     12            \u001b[36m0.8875\u001b[0m        \u001b[32m0.4292\u001b[0m       0.3611            0.3611        \u001b[94m1.8031\u001b[0m  0.0005  0.3330\n",
      "     13            \u001b[36m0.9125\u001b[0m        0.4905       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.7630\u001b[0m  0.0005  0.3316\n",
      "     14            \u001b[36m0.9250\u001b[0m        \u001b[32m0.3793\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.7154\u001b[0m  0.0005  0.3879\n",
      "     15            \u001b[36m0.9625\u001b[0m        0.4394       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.6688\u001b[0m  0.0004  0.4130\n",
      "     16            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3644\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.6370\u001b[0m  0.0004  0.3852\n",
      "     17            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3186\u001b[0m       0.3993            0.3993        \u001b[94m1.6072\u001b[0m  0.0004  0.3833\n",
      "     18            1.0000        \u001b[32m0.2828\u001b[0m       0.3924            0.3924        \u001b[94m1.5929\u001b[0m  0.0004  0.4064\n",
      "     19            1.0000        \u001b[32m0.2665\u001b[0m       0.3993            0.3993        \u001b[94m1.5843\u001b[0m  0.0004  0.3967\n",
      "     20            1.0000        0.2670       0.3854            0.3854        \u001b[94m1.5816\u001b[0m  0.0003  0.3938\n",
      "     21            1.0000        \u001b[32m0.2652\u001b[0m       0.3889            0.3889        \u001b[94m1.5798\u001b[0m  0.0003  0.3923\n",
      "     22            1.0000        \u001b[32m0.2206\u001b[0m       0.3889            0.3889        1.5807  0.0003  0.3312\n",
      "     23            1.0000        0.2278       0.3854            0.3854        \u001b[94m1.5747\u001b[0m  0.0002  0.3590\n",
      "     24            1.0000        0.2405       0.3889            0.3889        1.5756  0.0002  0.3481\n",
      "     25            1.0000        0.2543       0.3924            0.3924        1.5791  0.0002  0.3285\n",
      "     26            1.0000        \u001b[32m0.2146\u001b[0m       0.3924            0.3924        1.5823  0.0002  0.3262\n",
      "     27            1.0000        0.2344       0.3958            0.3958        1.5852  0.0002  0.3243\n",
      "     28            1.0000        \u001b[32m0.1963\u001b[0m       0.3993            0.3993        1.5863  0.0001  0.3246\n",
      "     29            1.0000        \u001b[32m0.1859\u001b[0m       0.3958            0.3958        1.5862  0.0001  0.3343\n",
      "     30            1.0000        0.2187       0.3958            0.3958        1.5868  0.0001  0.3332\n",
      "     31            1.0000        0.1900       0.3958            0.3958        1.5868  0.0001  0.3374\n",
      "     32            1.0000        \u001b[32m0.1827\u001b[0m       0.3993            0.3993        1.5860  0.0001  0.3312\n",
      "     33            1.0000        \u001b[32m0.1797\u001b[0m       0.3993            0.3993        1.5859  0.0000  0.3350\n",
      "     34            1.0000        0.1895       0.3924            0.3924        1.5858  0.0000  0.3445\n",
      "     35            1.0000        0.1908       0.3924            0.3924        1.5851  0.0000  0.3480\n",
      "     36            1.0000        0.1941       0.3958            0.3958        1.5853  0.0000  0.3407\n",
      "     37            1.0000        0.1892       0.3958            0.3958        1.5847  0.0000  0.3348\n",
      "     38            1.0000        0.2040       0.3924            0.3924        1.5845  0.0000  0.3396\n",
      "     39            1.0000        \u001b[32m0.1567\u001b[0m       0.3889            0.3889        1.5842  0.0000  0.3483\n",
      "     40            1.0000        0.1954       0.3889            0.3889        1.5839  0.0000  0.3478\n",
      "Training model for subject 7 with 90 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2556\u001b[0m        \u001b[32m1.6744\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m3.4096\u001b[0m  0.0006  0.3431\n",
      "      2            \u001b[36m0.3556\u001b[0m        \u001b[32m1.2701\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m2.4446\u001b[0m  0.0006  0.3323\n",
      "      3            \u001b[36m0.4444\u001b[0m        \u001b[32m1.1565\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        \u001b[94m2.2752\u001b[0m  0.0006  0.3507\n",
      "      4            0.4444        \u001b[32m1.0302\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        \u001b[94m2.0986\u001b[0m  0.0006  0.3467\n",
      "      5            \u001b[36m0.5000\u001b[0m        \u001b[32m0.8724\u001b[0m       0.2882            0.2882        \u001b[94m1.9955\u001b[0m  0.0006  0.3483\n",
      "      6            \u001b[36m0.5333\u001b[0m        0.9460       0.2917            0.2917        \u001b[94m1.9709\u001b[0m  0.0006  0.3371\n",
      "      7            0.5333        \u001b[32m0.7643\u001b[0m       0.2951            0.2951        2.0090  0.0006  0.3419\n",
      "      8            \u001b[36m0.5778\u001b[0m        \u001b[32m0.7079\u001b[0m       0.2986            0.2986        1.9815  0.0006  0.3422\n",
      "      9            \u001b[36m0.6556\u001b[0m        \u001b[32m0.5967\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m1.9355\u001b[0m  0.0006  0.3468\n",
      "     10            \u001b[36m0.6889\u001b[0m        0.6259       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m1.9264\u001b[0m  0.0005  0.3465\n",
      "     11            \u001b[36m0.7556\u001b[0m        \u001b[32m0.5965\u001b[0m       0.3194            0.3194        1.9320  0.0005  0.3424\n",
      "     12            \u001b[36m0.8111\u001b[0m        \u001b[32m0.4848\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m1.8918\u001b[0m  0.0005  0.3734\n",
      "     13            \u001b[36m0.8556\u001b[0m        0.5025       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.7930\u001b[0m  0.0005  0.4297\n",
      "     14            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4078\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.7001\u001b[0m  0.0005  0.4249\n",
      "     15            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3943\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.6001\u001b[0m  0.0004  0.4171\n",
      "     16            \u001b[36m0.9444\u001b[0m        \u001b[32m0.3557\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.5237\u001b[0m  0.0004  0.4103\n",
      "     17            \u001b[36m0.9556\u001b[0m        \u001b[32m0.3406\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.4754\u001b[0m  0.0004  0.4493\n",
      "     18            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3057\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4454\u001b[0m  0.0004  0.3817\n",
      "     19            1.0000        0.3102       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.4342\u001b[0m  0.0004  0.4330\n",
      "     20            1.0000        \u001b[32m0.2794\u001b[0m       0.4132            0.4132        \u001b[94m1.4325\u001b[0m  0.0003  0.4575\n",
      "     21            1.0000        0.3169       0.4097            0.4097        \u001b[94m1.4299\u001b[0m  0.0003  0.3618\n",
      "     22            1.0000        \u001b[32m0.2113\u001b[0m       0.4132            0.4132        \u001b[94m1.4284\u001b[0m  0.0003  0.3514\n",
      "     23            1.0000        0.2327       0.4062            0.4062        1.4309  0.0002  0.3667\n",
      "     24            1.0000        \u001b[32m0.2110\u001b[0m       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        1.4340  0.0002  0.3620\n",
      "     25            1.0000        \u001b[32m0.2080\u001b[0m       0.4132            0.4132        1.4369  0.0002  0.3431\n",
      "     26            1.0000        \u001b[32m0.2059\u001b[0m       0.4132            0.4132        1.4381  0.0002  0.3460\n",
      "     27            1.0000        0.2712       0.4132            0.4132        1.4401  0.0002  0.3638\n",
      "     28            1.0000        0.2304       0.4167            0.4167        1.4413  0.0001  0.3352\n",
      "     29            1.0000        0.2605       0.4028            0.4028        1.4424  0.0001  0.3575\n",
      "     30            1.0000        0.2356       0.4028            0.4028        1.4421  0.0001  0.3466\n",
      "     31            1.0000        \u001b[32m0.1994\u001b[0m       0.4097            0.4097        1.4407  0.0001  0.3336\n",
      "     32            1.0000        0.2009       0.4028            0.4028        1.4399  0.0001  0.3604\n",
      "     33            1.0000        \u001b[32m0.1910\u001b[0m       0.3993            0.3993        1.4395  0.0000  0.3612\n",
      "     34            1.0000        0.2003       0.3993            0.3993        1.4390  0.0000  0.3450\n",
      "     35            1.0000        0.2111       0.3993            0.3993        1.4389  0.0000  0.3527\n",
      "     36            1.0000        \u001b[32m0.1689\u001b[0m       0.3993            0.3993        1.4385  0.0000  0.3535\n",
      "     37            1.0000        \u001b[32m0.1543\u001b[0m       0.3958            0.3958        1.4379  0.0000  0.3535\n",
      "     38            1.0000        0.2240       0.3958            0.3958        1.4376  0.0000  0.3393\n",
      "     39            1.0000        0.1827       0.3958            0.3958        1.4371  0.0000  0.3643\n",
      "     40            1.0000        0.2503       0.3993            0.3993        1.4365  0.0000  0.3435\n",
      "Training model for subject 7 with 100 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2900\u001b[0m        \u001b[32m1.9006\u001b[0m       \u001b[35m0.2118\u001b[0m            \u001b[31m0.2118\u001b[0m        \u001b[94m2.9971\u001b[0m  0.0006  0.3838\n",
      "      2            \u001b[36m0.3700\u001b[0m        \u001b[32m1.4204\u001b[0m       \u001b[35m0.2257\u001b[0m            \u001b[31m0.2257\u001b[0m        \u001b[94m2.7211\u001b[0m  0.0006  0.3659\n",
      "      3            0.3300        \u001b[32m1.1644\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        3.1272  0.0006  0.3634\n",
      "      4            0.3200        \u001b[32m1.0726\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        3.4586  0.0006  0.3473\n",
      "      5            0.3400        \u001b[32m0.9881\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        3.4659  0.0006  0.3708\n",
      "      6            0.3700        \u001b[32m0.8783\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        3.3100  0.0006  0.3656\n",
      "      7            \u001b[36m0.4100\u001b[0m        \u001b[32m0.7704\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        3.1514  0.0006  0.3711\n",
      "      8            \u001b[36m0.4500\u001b[0m        \u001b[32m0.7082\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        2.9058  0.0006  0.3514\n",
      "      9            \u001b[36m0.4900\u001b[0m        \u001b[32m0.6623\u001b[0m       0.3264            0.3264        \u001b[94m2.6675\u001b[0m  0.0006  0.4106\n",
      "     10            \u001b[36m0.5700\u001b[0m        \u001b[32m0.5679\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m2.4036\u001b[0m  0.0005  0.4125\n",
      "     11            \u001b[36m0.6200\u001b[0m        0.6308       0.3333            0.3333        \u001b[94m2.2693\u001b[0m  0.0005  0.4163\n",
      "     12            \u001b[36m0.6800\u001b[0m        \u001b[32m0.5493\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m2.1620\u001b[0m  0.0005  0.4141\n",
      "     13            \u001b[36m0.7100\u001b[0m        \u001b[32m0.4924\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m2.1022\u001b[0m  0.0005  0.4225\n",
      "     14            \u001b[36m0.7400\u001b[0m        \u001b[32m0.4417\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m2.0251\u001b[0m  0.0005  0.4263\n",
      "     15            \u001b[36m0.8100\u001b[0m        \u001b[32m0.4199\u001b[0m       0.3681            0.3681        \u001b[94m1.9425\u001b[0m  0.0004  0.4862\n",
      "     16            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4080\u001b[0m       0.3576            0.3576        \u001b[94m1.8633\u001b[0m  0.0004  0.4073\n",
      "     17            \u001b[36m0.9400\u001b[0m        \u001b[32m0.3794\u001b[0m       0.3438            0.3438        \u001b[94m1.7852\u001b[0m  0.0004  0.3634\n",
      "     18            \u001b[36m0.9700\u001b[0m        0.4276       0.3472            0.3472        \u001b[94m1.7150\u001b[0m  0.0004  0.3497\n",
      "     19            0.9700        \u001b[32m0.2933\u001b[0m       0.3507            0.3507        \u001b[94m1.6584\u001b[0m  0.0004  0.3691\n",
      "     20            \u001b[36m0.9900\u001b[0m        0.3230       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.6196\u001b[0m  0.0003  0.3673\n",
      "     21            0.9900        0.3231       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.5927\u001b[0m  0.0003  0.3500\n",
      "     22            \u001b[36m1.0000\u001b[0m        0.2951       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.5774\u001b[0m  0.0003  0.3929\n",
      "     23            1.0000        \u001b[32m0.2904\u001b[0m       0.3819            0.3819        \u001b[94m1.5615\u001b[0m  0.0002  0.3694\n",
      "     24            1.0000        \u001b[32m0.2585\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.5442\u001b[0m  0.0002  0.3979\n",
      "     25            1.0000        \u001b[32m0.2332\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.5276\u001b[0m  0.0002  0.3715\n",
      "     26            1.0000        0.2958       0.3958            0.3958        \u001b[94m1.5139\u001b[0m  0.0002  0.3703\n",
      "     27            1.0000        0.2922       0.3958            0.3958        \u001b[94m1.5069\u001b[0m  0.0002  0.3672\n",
      "     28            1.0000        \u001b[32m0.2330\u001b[0m       0.3889            0.3889        \u001b[94m1.5014\u001b[0m  0.0001  0.3583\n",
      "     29            1.0000        0.2648       0.3924            0.3924        \u001b[94m1.4976\u001b[0m  0.0001  0.4765\n",
      "     30            1.0000        0.2577       0.3889            0.3889        \u001b[94m1.4945\u001b[0m  0.0001  0.3749\n",
      "     31            1.0000        \u001b[32m0.1949\u001b[0m       0.3924            0.3924        \u001b[94m1.4927\u001b[0m  0.0001  0.3558\n",
      "     32            1.0000        0.2491       0.4028            0.4028        \u001b[94m1.4918\u001b[0m  0.0001  0.3695\n",
      "     33            1.0000        0.2289       0.3993            0.3993        \u001b[94m1.4906\u001b[0m  0.0000  0.3618\n",
      "     34            1.0000        0.2089       0.4028            0.4028        \u001b[94m1.4899\u001b[0m  0.0000  0.3627\n",
      "     35            1.0000        0.2187       0.3993            0.3993        \u001b[94m1.4889\u001b[0m  0.0000  0.3660\n",
      "     36            1.0000        0.2540       0.4028            0.4028        \u001b[94m1.4878\u001b[0m  0.0000  0.3603\n",
      "     37            1.0000        0.2374       0.4028            0.4028        \u001b[94m1.4868\u001b[0m  0.0000  0.3581\n",
      "     38            1.0000        0.2243       0.4028            0.4028        \u001b[94m1.4860\u001b[0m  0.0000  0.3705\n",
      "     39            1.0000        0.2456       0.4028            0.4028        \u001b[94m1.4852\u001b[0m  0.0000  0.3733\n",
      "     40            1.0000        0.2619       0.4028            0.4028        \u001b[94m1.4846\u001b[0m  0.0000  0.3597\n",
      "Training model for subject 7 with 110 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3455\u001b[0m        \u001b[32m1.5752\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m2.5764\u001b[0m  0.0006  0.3826\n",
      "      2            0.2727        \u001b[32m1.3031\u001b[0m       0.2465            0.2465        3.0004  0.0006  0.3822\n",
      "      3            0.3000        \u001b[32m1.0844\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        3.0512  0.0006  0.3820\n",
      "      4            \u001b[36m0.4455\u001b[0m        \u001b[32m0.9614\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        2.7922  0.0006  0.4074\n",
      "      5            \u001b[36m0.5000\u001b[0m        \u001b[32m0.7680\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m2.5498\u001b[0m  0.0006  0.4339\n",
      "      6            \u001b[36m0.5273\u001b[0m        0.7866       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m2.4024\u001b[0m  0.0006  0.4427\n",
      "      7            \u001b[36m0.5545\u001b[0m        \u001b[32m0.6705\u001b[0m       0.3576            0.3576        \u001b[94m2.3393\u001b[0m  0.0006  0.4429\n",
      "      8            \u001b[36m0.5727\u001b[0m        0.6809       0.3472            0.3472        2.3418  0.0006  0.4387\n",
      "      9            \u001b[36m0.6273\u001b[0m        \u001b[32m0.6513\u001b[0m       0.3264            0.3264        \u001b[94m2.3190\u001b[0m  0.0006  0.4426\n",
      "     10            \u001b[36m0.6545\u001b[0m        \u001b[32m0.5675\u001b[0m       0.3194            0.3194        \u001b[94m2.2185\u001b[0m  0.0005  0.4862\n",
      "     11            \u001b[36m0.7000\u001b[0m        0.6058       0.3299            0.3299        \u001b[94m2.1431\u001b[0m  0.0005  0.3715\n",
      "     12            \u001b[36m0.7727\u001b[0m        \u001b[32m0.4849\u001b[0m       0.3333            0.3333        \u001b[94m2.0768\u001b[0m  0.0005  0.3935\n",
      "     13            \u001b[36m0.8364\u001b[0m        0.5014       0.3299            0.3299        \u001b[94m1.9939\u001b[0m  0.0005  0.3651\n",
      "     14            \u001b[36m0.9091\u001b[0m        \u001b[32m0.3752\u001b[0m       0.3368            0.3368        \u001b[94m1.9156\u001b[0m  0.0005  0.3839\n",
      "     15            \u001b[36m0.9364\u001b[0m        0.4075       0.3403            0.3403        \u001b[94m1.8658\u001b[0m  0.0004  0.3704\n",
      "     16            \u001b[36m0.9636\u001b[0m        0.3772       0.3438            0.3438        \u001b[94m1.7937\u001b[0m  0.0004  0.3725\n",
      "     17            0.9636        0.4076       0.3472            0.3472        \u001b[94m1.7139\u001b[0m  0.0004  0.3605\n",
      "     18            \u001b[36m0.9909\u001b[0m        0.4119       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.6619\u001b[0m  0.0004  0.3725\n",
      "     19            0.9909        \u001b[32m0.3490\u001b[0m       0.3646            0.3646        \u001b[94m1.6210\u001b[0m  0.0004  0.3825\n",
      "     20            0.9909        \u001b[32m0.3038\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.5912\u001b[0m  0.0003  0.3828\n",
      "     21            0.9909        \u001b[32m0.2787\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.5750\u001b[0m  0.0003  0.3631\n",
      "     22            \u001b[36m1.0000\u001b[0m        0.3227       0.3785            0.3785        \u001b[94m1.5674\u001b[0m  0.0003  0.3828\n",
      "     23            1.0000        0.2973       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        1.5676  0.0002  0.3818\n",
      "     24            1.0000        0.2859       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        1.5692  0.0002  0.3822\n",
      "     25            1.0000        \u001b[32m0.2524\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        1.5718  0.0002  0.3714\n",
      "     26            1.0000        \u001b[32m0.2479\u001b[0m       0.3924            0.3924        1.5707  0.0002  0.3767\n",
      "     27            1.0000        0.2656       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.5646\u001b[0m  0.0002  0.3839\n",
      "     28            1.0000        \u001b[32m0.2399\u001b[0m       0.4028            0.4028        \u001b[94m1.5554\u001b[0m  0.0001  0.4049\n",
      "     29            1.0000        0.2782       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.5473\u001b[0m  0.0001  0.3580\n",
      "     30            1.0000        \u001b[32m0.2347\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.5388\u001b[0m  0.0001  0.3884\n",
      "     31            1.0000        0.2413       0.4062            0.4062        \u001b[94m1.5320\u001b[0m  0.0001  0.3656\n",
      "     32            1.0000        0.2476       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.5256\u001b[0m  0.0001  0.3792\n",
      "     33            1.0000        \u001b[32m0.2223\u001b[0m       0.4132            0.4132        \u001b[94m1.5206\u001b[0m  0.0000  0.3816\n",
      "     34            1.0000        \u001b[32m0.2078\u001b[0m       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.5168\u001b[0m  0.0000  0.3862\n",
      "     35            1.0000        0.2181       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.5141\u001b[0m  0.0000  0.3880\n",
      "     36            1.0000        \u001b[32m0.1944\u001b[0m       0.4167            0.4167        \u001b[94m1.5123\u001b[0m  0.0000  0.3723\n",
      "     37            1.0000        0.2013       0.4167            0.4167        \u001b[94m1.5111\u001b[0m  0.0000  0.3761\n",
      "     38            1.0000        0.2465       0.4167            0.4167        \u001b[94m1.5103\u001b[0m  0.0000  0.4162\n",
      "     39            1.0000        0.2382       0.4167            0.4167        \u001b[94m1.5099\u001b[0m  0.0000  0.4452\n",
      "     40            1.0000        \u001b[32m0.1881\u001b[0m       0.4167            0.4167        \u001b[94m1.5096\u001b[0m  0.0000  0.4768\n",
      "Training model for subject 7 with 120 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2583\u001b[0m        \u001b[32m1.6203\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.2405\u001b[0m  0.0006  0.4719\n",
      "      2            \u001b[36m0.3833\u001b[0m        \u001b[32m1.4192\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m2.2551\u001b[0m  0.0006  0.4649\n",
      "      3            \u001b[36m0.4583\u001b[0m        \u001b[32m1.1405\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m2.2067\u001b[0m  0.0006  0.4906\n",
      "      4            0.4000        \u001b[32m0.9802\u001b[0m       0.3194            0.3194        2.3597  0.0006  0.4915\n",
      "      5            0.3500        \u001b[32m0.8728\u001b[0m       0.2812            0.2812        2.6893  0.0006  0.3821\n",
      "      6            0.3750        \u001b[32m0.8560\u001b[0m       0.2986            0.2986        2.4569  0.0006  0.3875\n",
      "      7            \u001b[36m0.4667\u001b[0m        \u001b[32m0.7640\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m2.1851\u001b[0m  0.0006  0.3970\n",
      "      8            \u001b[36m0.5417\u001b[0m        \u001b[32m0.7029\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m2.0381\u001b[0m  0.0006  0.3848\n",
      "      9            \u001b[36m0.6667\u001b[0m        \u001b[32m0.6190\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.9390\u001b[0m  0.0006  0.3995\n",
      "     10            \u001b[36m0.7250\u001b[0m        \u001b[32m0.5550\u001b[0m       0.3542            0.3542        \u001b[94m1.8911\u001b[0m  0.0005  0.3960\n",
      "     11            \u001b[36m0.7583\u001b[0m        \u001b[32m0.5131\u001b[0m       0.3576            0.3576        \u001b[94m1.8832\u001b[0m  0.0005  0.3760\n",
      "     12            \u001b[36m0.7833\u001b[0m        0.5146       0.3750            0.3750        \u001b[94m1.8725\u001b[0m  0.0005  0.4859\n",
      "     13            \u001b[36m0.8167\u001b[0m        \u001b[32m0.4511\u001b[0m       0.3611            0.3611        \u001b[94m1.8496\u001b[0m  0.0005  0.3922\n",
      "     14            \u001b[36m0.8583\u001b[0m        \u001b[32m0.4317\u001b[0m       0.3611            0.3611        \u001b[94m1.8227\u001b[0m  0.0005  0.3898\n",
      "     15            \u001b[36m0.8750\u001b[0m        \u001b[32m0.4274\u001b[0m       0.3576            0.3576        \u001b[94m1.7924\u001b[0m  0.0004  0.3838\n",
      "     16            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3756\u001b[0m       0.3472            0.3472        \u001b[94m1.7474\u001b[0m  0.0004  0.3858\n",
      "     17            \u001b[36m0.9583\u001b[0m        0.3826       0.3438            0.3438        \u001b[94m1.6712\u001b[0m  0.0004  0.3967\n",
      "     18            \u001b[36m0.9833\u001b[0m        \u001b[32m0.3189\u001b[0m       0.3681            0.3681        \u001b[94m1.5975\u001b[0m  0.0004  0.3836\n",
      "     19            0.9833        0.3275       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.5399\u001b[0m  0.0004  0.3922\n",
      "     20            0.9833        \u001b[32m0.2965\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.5026\u001b[0m  0.0003  0.3924\n",
      "     21            \u001b[36m0.9917\u001b[0m        \u001b[32m0.2399\u001b[0m       0.3785            0.3785        \u001b[94m1.4874\u001b[0m  0.0003  0.3934\n",
      "     22            0.9917        0.2861       0.3785            0.3785        \u001b[94m1.4737\u001b[0m  0.0003  0.3896\n",
      "     23            0.9917        0.2608       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.4650\u001b[0m  0.0002  0.3968\n",
      "     24            0.9917        0.2697       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.4641\u001b[0m  0.0002  0.3711\n",
      "     25            \u001b[36m1.0000\u001b[0m        0.2580       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        1.4695  0.0002  0.3878\n",
      "     26            1.0000        0.2400       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        1.4702  0.0002  0.3951\n",
      "     27            1.0000        0.2736       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        1.4714  0.0002  0.3896\n",
      "     28            1.0000        0.2974       0.3958            0.3958        1.4732  0.0001  0.3918\n",
      "     29            1.0000        0.2563       0.3958            0.3958        1.4738  0.0001  0.3958\n",
      "     30            1.0000        \u001b[32m0.2113\u001b[0m       0.3958            0.3958        1.4734  0.0001  0.4077\n",
      "     31            1.0000        \u001b[32m0.2039\u001b[0m       0.3993            0.3993        1.4710  0.0001  0.4349\n",
      "     32            1.0000        \u001b[32m0.1904\u001b[0m       0.3993            0.3993        1.4690  0.0001  0.4552\n",
      "     33            1.0000        0.2729       0.3889            0.3889        1.4669  0.0000  0.4865\n",
      "     34            1.0000        \u001b[32m0.1740\u001b[0m       0.3924            0.3924        1.4646  0.0000  0.4627\n",
      "     35            1.0000        0.1914       0.3958            0.3958        \u001b[94m1.4623\u001b[0m  0.0000  0.4756\n",
      "     36            1.0000        0.2308       0.3924            0.3924        \u001b[94m1.4609\u001b[0m  0.0000  0.4879\n",
      "     37            1.0000        0.2271       0.3924            0.3924        \u001b[94m1.4595\u001b[0m  0.0000  0.4438\n",
      "     38            1.0000        0.2205       0.3924            0.3924        \u001b[94m1.4583\u001b[0m  0.0000  0.4390\n",
      "     39            1.0000        0.1987       0.3958            0.3958        \u001b[94m1.4573\u001b[0m  0.0000  0.3995\n",
      "     40            1.0000        0.2182       0.3958            0.3958        \u001b[94m1.4565\u001b[0m  0.0000  0.3834\n",
      "Training model for subject 7 with 130 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3231\u001b[0m        \u001b[32m1.9071\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m2.0557\u001b[0m  0.0006  0.4137\n",
      "      2            0.2615        \u001b[32m1.4567\u001b[0m       0.2500            0.2500        2.8518  0.0006  0.3862\n",
      "      3            0.2615        \u001b[32m1.2948\u001b[0m       0.2500            0.2500        3.0667  0.0006  0.3988\n",
      "      4            0.2615        \u001b[32m1.1611\u001b[0m       0.2500            0.2500        3.0003  0.0006  0.4111\n",
      "      5            0.2769        \u001b[32m0.9920\u001b[0m       0.2500            0.2500        2.8156  0.0006  0.3965\n",
      "      6            0.3077        \u001b[32m0.9591\u001b[0m       0.2535            0.2535        2.4837  0.0006  0.3871\n",
      "      7            \u001b[36m0.3846\u001b[0m        \u001b[32m0.8814\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        2.2797  0.0006  0.3946\n",
      "      8            \u001b[36m0.4769\u001b[0m        \u001b[32m0.7898\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        2.2050  0.0006  0.4009\n",
      "      9            \u001b[36m0.4923\u001b[0m        \u001b[32m0.7840\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        2.1511  0.0006  0.4172\n",
      "     10            0.4769        \u001b[32m0.7119\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        2.1912  0.0005  0.4138\n",
      "     11            0.4923        \u001b[32m0.6825\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        2.1569  0.0005  0.4107\n",
      "     12            \u001b[36m0.5000\u001b[0m        \u001b[32m0.5845\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m2.0528\u001b[0m  0.0005  0.4046\n",
      "     13            \u001b[36m0.5385\u001b[0m        0.5966       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.8613\u001b[0m  0.0005  0.5499\n",
      "     14            \u001b[36m0.6846\u001b[0m        \u001b[32m0.5352\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.6762\u001b[0m  0.0005  0.3937\n",
      "     15            \u001b[36m0.7923\u001b[0m        \u001b[32m0.4800\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.5498\u001b[0m  0.0004  0.4026\n",
      "     16            \u001b[36m0.8769\u001b[0m        \u001b[32m0.4546\u001b[0m       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.4697\u001b[0m  0.0004  0.4112\n",
      "     17            \u001b[36m0.9077\u001b[0m        \u001b[32m0.4365\u001b[0m       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.4252\u001b[0m  0.0004  0.4146\n",
      "     18            \u001b[36m0.9462\u001b[0m        \u001b[32m0.4156\u001b[0m       0.4340            0.4340        \u001b[94m1.3976\u001b[0m  0.0004  0.4166\n",
      "     19            \u001b[36m0.9615\u001b[0m        0.4463       0.4340            0.4340        \u001b[94m1.3852\u001b[0m  0.0004  0.3875\n",
      "     20            \u001b[36m0.9692\u001b[0m        0.4326       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.3816\u001b[0m  0.0003  0.4015\n",
      "     21            0.9615        \u001b[32m0.3785\u001b[0m       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.3763\u001b[0m  0.0003  0.3947\n",
      "     22            0.9615        \u001b[32m0.3529\u001b[0m       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.3712\u001b[0m  0.0003  0.4415\n",
      "     23            0.9538        0.3904       0.4583            0.4583        \u001b[94m1.3676\u001b[0m  0.0002  0.4652\n",
      "     24            0.9692        0.4079       0.4618            0.4618        \u001b[94m1.3638\u001b[0m  0.0002  0.4720\n",
      "     25            \u001b[36m0.9769\u001b[0m        \u001b[32m0.3176\u001b[0m       0.4653            0.4653        \u001b[94m1.3596\u001b[0m  0.0002  0.4496\n",
      "     26            \u001b[36m0.9846\u001b[0m        0.3502       \u001b[35m0.4722\u001b[0m            \u001b[31m0.4722\u001b[0m        \u001b[94m1.3578\u001b[0m  0.0002  0.4664\n",
      "     27            0.9846        0.3381       0.4653            0.4653        \u001b[94m1.3562\u001b[0m  0.0002  0.4814\n",
      "     28            \u001b[36m0.9923\u001b[0m        0.3310       0.4653            0.4653        \u001b[94m1.3561\u001b[0m  0.0001  0.4822\n",
      "     29            0.9923        \u001b[32m0.3017\u001b[0m       0.4514            0.4514        1.3579  0.0001  0.3928\n",
      "     30            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2794\u001b[0m       0.4514            0.4514        1.3600  0.0001  0.4042\n",
      "     31            1.0000        0.3070       0.4479            0.4479        1.3618  0.0001  0.4139\n",
      "     32            1.0000        0.3233       0.4410            0.4410        1.3623  0.0001  0.4142\n",
      "     33            1.0000        0.2844       0.4410            0.4410        1.3625  0.0000  0.4096\n",
      "     34            0.9923        0.3037       0.4340            0.4340        1.3625  0.0000  0.4007\n",
      "     35            0.9923        0.2927       0.4340            0.4340        1.3623  0.0000  0.3982\n",
      "     36            0.9923        \u001b[32m0.2649\u001b[0m       0.4306            0.4306        1.3620  0.0000  0.3974\n",
      "     37            0.9923        0.3043       0.4306            0.4306        1.3616  0.0000  0.4026\n",
      "     38            0.9923        0.3122       0.4271            0.4271        1.3613  0.0000  0.4107\n",
      "     39            0.9923        0.3133       0.4271            0.4271        1.3611  0.0000  0.4045\n",
      "     40            0.9923        0.3257       0.4340            0.4340        1.3611  0.0000  0.4578\n",
      "Training model for subject 7 with 140 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.6534\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m4.4832\u001b[0m  0.0006  0.4075\n",
      "      2            \u001b[36m0.4000\u001b[0m        \u001b[32m1.3653\u001b[0m       0.2569            0.2569        \u001b[94m3.7748\u001b[0m  0.0006  0.4096\n",
      "      3            0.2929        \u001b[32m1.2522\u001b[0m       0.2500            0.2500        \u001b[94m3.6988\u001b[0m  0.0006  0.3792\n",
      "      4            0.2929        \u001b[32m0.9910\u001b[0m       0.2569            0.2569        \u001b[94m3.2101\u001b[0m  0.0006  0.3855\n",
      "      5            0.3429        1.0012       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m2.8972\u001b[0m  0.0006  0.3754\n",
      "      6            0.4000        \u001b[32m0.9085\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m2.6043\u001b[0m  0.0006  0.3996\n",
      "      7            \u001b[36m0.4143\u001b[0m        \u001b[32m0.8311\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m2.4849\u001b[0m  0.0006  0.3778\n",
      "      8            \u001b[36m0.4643\u001b[0m        \u001b[32m0.6927\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m2.3424\u001b[0m  0.0006  0.4024\n",
      "      9            \u001b[36m0.4714\u001b[0m        0.7631       0.3368            0.3368        \u001b[94m2.3112\u001b[0m  0.0006  0.4165\n",
      "     10            0.4571        \u001b[32m0.5740\u001b[0m       0.3403            0.3403        \u001b[94m2.2633\u001b[0m  0.0005  0.4069\n",
      "     11            \u001b[36m0.5000\u001b[0m        0.5945       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m2.1480\u001b[0m  0.0005  0.3961\n",
      "     12            \u001b[36m0.5786\u001b[0m        0.6625       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.9841\u001b[0m  0.0005  0.4077\n",
      "     13            \u001b[36m0.6714\u001b[0m        \u001b[32m0.5623\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.8310\u001b[0m  0.0005  0.4316\n",
      "     14            \u001b[36m0.7643\u001b[0m        \u001b[32m0.4786\u001b[0m       0.3785            0.3785        \u001b[94m1.7220\u001b[0m  0.0005  0.4423\n",
      "     15            \u001b[36m0.8571\u001b[0m        0.4958       0.3750            0.3750        \u001b[94m1.6418\u001b[0m  0.0004  0.4828\n",
      "     16            \u001b[36m0.9286\u001b[0m        \u001b[32m0.4713\u001b[0m       0.3750            0.3750        \u001b[94m1.5884\u001b[0m  0.0004  0.4629\n",
      "     17            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4671\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.5491\u001b[0m  0.0004  0.4686\n",
      "     18            \u001b[36m0.9571\u001b[0m        \u001b[32m0.4017\u001b[0m       0.3854            0.3854        \u001b[94m1.5251\u001b[0m  0.0004  0.4907\n",
      "     19            \u001b[36m0.9643\u001b[0m        \u001b[32m0.4008\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.5047\u001b[0m  0.0004  0.5016\n",
      "     20            \u001b[36m0.9786\u001b[0m        \u001b[32m0.3989\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.4855\u001b[0m  0.0003  0.3830\n",
      "     21            \u001b[36m0.9857\u001b[0m        \u001b[32m0.3801\u001b[0m       0.3889            0.3889        \u001b[94m1.4639\u001b[0m  0.0003  0.4155\n",
      "     22            \u001b[36m0.9929\u001b[0m        \u001b[32m0.3358\u001b[0m       0.3715            0.3715        \u001b[94m1.4475\u001b[0m  0.0003  0.3901\n",
      "     23            0.9929        0.3662       0.3785            0.3785        \u001b[94m1.4385\u001b[0m  0.0002  0.3875\n",
      "     24            0.9929        0.3444       0.3785            0.3785        \u001b[94m1.4345\u001b[0m  0.0002  0.4068\n",
      "     25            0.9929        \u001b[32m0.3120\u001b[0m       0.3819            0.3819        1.4350  0.0002  0.4006\n",
      "     26            0.9929        0.3304       0.3889            0.3889        1.4355  0.0002  0.3926\n",
      "     27            0.9929        0.3326       0.3958            0.3958        1.4366  0.0002  0.4026\n",
      "     28            0.9929        \u001b[32m0.2950\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        1.4348  0.0001  0.4112\n",
      "     29            0.9929        0.3174       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.4329\u001b[0m  0.0001  0.5115\n",
      "     30            0.9929        \u001b[32m0.2374\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.4301\u001b[0m  0.0001  0.3862\n",
      "     31            0.9929        0.2685       0.4097            0.4097        \u001b[94m1.4256\u001b[0m  0.0001  0.4141\n",
      "     32            0.9929        0.2633       0.4062            0.4062        \u001b[94m1.4215\u001b[0m  0.0001  0.4025\n",
      "     33            0.9929        0.2871       0.4097            0.4097        \u001b[94m1.4171\u001b[0m  0.0000  0.3943\n",
      "     34            0.9929        0.3086       0.4097            0.4097        \u001b[94m1.4137\u001b[0m  0.0000  0.3988\n",
      "     35            \u001b[36m1.0000\u001b[0m        0.3164       0.4097            0.4097        \u001b[94m1.4113\u001b[0m  0.0000  0.4033\n",
      "     36            1.0000        0.2937       0.4097            0.4097        \u001b[94m1.4087\u001b[0m  0.0000  0.4001\n",
      "     37            1.0000        0.2941       0.4097            0.4097        \u001b[94m1.4075\u001b[0m  0.0000  0.4016\n",
      "     38            1.0000        0.3280       0.4097            0.4097        \u001b[94m1.4068\u001b[0m  0.0000  0.4086\n",
      "     39            1.0000        0.2798       0.4097            0.4097        \u001b[94m1.4058\u001b[0m  0.0000  0.3927\n",
      "     40            1.0000        0.3046       0.4097            0.4097        1.4060  0.0000  0.3955\n",
      "Training model for subject 7 with 150 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3733\u001b[0m        \u001b[32m1.5364\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m1.9067\u001b[0m  0.0006  0.3993\n",
      "      2            \u001b[36m0.4533\u001b[0m        \u001b[32m1.2769\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        2.0560  0.0006  0.3944\n",
      "      3            0.3267        \u001b[32m1.1994\u001b[0m       0.2396            0.2396        2.2412  0.0006  0.4007\n",
      "      4            0.2467        \u001b[32m1.0966\u001b[0m       0.2465            0.2465        2.5123  0.0006  0.4324\n",
      "      5            0.2667        \u001b[32m0.9481\u001b[0m       0.2569            0.2569        2.7533  0.0006  0.5248\n",
      "      6            0.3333        \u001b[32m0.9465\u001b[0m       0.2604            0.2604        3.0059  0.0006  0.4653\n",
      "      7            0.3867        \u001b[32m0.8373\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        3.0865  0.0006  0.4526\n",
      "      8            0.3933        \u001b[32m0.7532\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        2.8844  0.0006  0.4541\n",
      "      9            0.4267        0.7593       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        2.5350  0.0006  0.4672\n",
      "     10            \u001b[36m0.4667\u001b[0m        \u001b[32m0.7380\u001b[0m       0.3403            0.3403        2.1670  0.0005  0.4267\n",
      "     11            \u001b[36m0.5400\u001b[0m        \u001b[32m0.5989\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        1.9960  0.0005  0.3940\n",
      "     12            \u001b[36m0.5733\u001b[0m        0.6073       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        1.9560  0.0005  0.4198\n",
      "     13            \u001b[36m0.5867\u001b[0m        \u001b[32m0.5594\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        1.9538  0.0005  0.4000\n",
      "     14            \u001b[36m0.6133\u001b[0m        \u001b[32m0.5426\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        1.9502  0.0005  0.4006\n",
      "     15            \u001b[36m0.6533\u001b[0m        \u001b[32m0.5327\u001b[0m       0.3611            0.3611        \u001b[94m1.8540\u001b[0m  0.0004  0.4063\n",
      "     16            \u001b[36m0.7267\u001b[0m        \u001b[32m0.4920\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.7632\u001b[0m  0.0004  0.4006\n",
      "     17            \u001b[36m0.8000\u001b[0m        0.5365       0.3785            0.3785        \u001b[94m1.6526\u001b[0m  0.0004  0.4011\n",
      "     18            \u001b[36m0.8667\u001b[0m        \u001b[32m0.4823\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.5660\u001b[0m  0.0004  0.4003\n",
      "     19            \u001b[36m0.9200\u001b[0m        \u001b[32m0.4540\u001b[0m       0.3819            0.3819        \u001b[94m1.5031\u001b[0m  0.0004  0.4194\n",
      "     20            \u001b[36m0.9533\u001b[0m        \u001b[32m0.4283\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.4623\u001b[0m  0.0003  0.4000\n",
      "     21            \u001b[36m0.9733\u001b[0m        0.4384       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.4376\u001b[0m  0.0003  0.4046\n",
      "     22            0.9733        \u001b[32m0.4237\u001b[0m       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.4233\u001b[0m  0.0003  0.4176\n",
      "     23            0.9733        0.5060       0.4097            0.4097        \u001b[94m1.4113\u001b[0m  0.0002  0.4020\n",
      "     24            0.9733        \u001b[32m0.3838\u001b[0m       0.4167            0.4167        \u001b[94m1.4006\u001b[0m  0.0002  0.4092\n",
      "     25            0.9733        0.4082       0.4167            0.4167        \u001b[94m1.3936\u001b[0m  0.0002  0.3929\n",
      "     26            \u001b[36m0.9867\u001b[0m        0.4283       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.3902\u001b[0m  0.0002  0.3875\n",
      "     27            0.9867        \u001b[32m0.3234\u001b[0m       0.4201            0.4201        \u001b[94m1.3892\u001b[0m  0.0002  0.4199\n",
      "     28            0.9800        0.3794       0.4132            0.4132        \u001b[94m1.3890\u001b[0m  0.0001  0.3978\n",
      "     29            0.9800        0.3720       0.4132            0.4132        \u001b[94m1.3886\u001b[0m  0.0001  0.4169\n",
      "     30            0.9867        0.3574       0.4132            0.4132        \u001b[94m1.3870\u001b[0m  0.0001  0.4125\n",
      "     31            0.9867        0.3791       0.4132            0.4132        \u001b[94m1.3858\u001b[0m  0.0001  0.4004\n",
      "     32            0.9867        0.3801       0.4132            0.4132        \u001b[94m1.3840\u001b[0m  0.0001  0.3943\n",
      "     33            0.9867        0.3664       0.4132            0.4132        \u001b[94m1.3817\u001b[0m  0.0000  0.4078\n",
      "     34            0.9867        \u001b[32m0.3044\u001b[0m       0.4132            0.4132        \u001b[94m1.3806\u001b[0m  0.0000  0.3983\n",
      "     35            0.9867        0.3393       0.4132            0.4132        \u001b[94m1.3796\u001b[0m  0.0000  0.4542\n",
      "     36            0.9867        0.3146       0.4132            0.4132        \u001b[94m1.3790\u001b[0m  0.0000  0.6204\n",
      "     37            0.9867        0.3218       0.4132            0.4132        \u001b[94m1.3789\u001b[0m  0.0000  0.4544\n",
      "     38            0.9867        \u001b[32m0.2884\u001b[0m       0.4132            0.4132        \u001b[94m1.3788\u001b[0m  0.0000  0.4490\n",
      "     39            0.9867        0.3175       0.4132            0.4132        1.3788  0.0000  0.4776\n",
      "     40            0.9867        0.3715       0.4132            0.4132        1.3791  0.0000  0.4707\n",
      "Training model for subject 7 with 160 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.6001\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.7993\u001b[0m  0.0006  0.3951\n",
      "      2            0.2500        \u001b[32m1.4305\u001b[0m       0.2500            0.2500        4.8550  0.0006  0.4104\n",
      "      3            0.2500        \u001b[32m1.1454\u001b[0m       0.2500            0.2500        5.2493  0.0006  0.4052\n",
      "      4            0.2500        \u001b[32m1.0190\u001b[0m       0.2500            0.2500        5.0426  0.0006  0.4065\n",
      "      5            0.2500        1.0631       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        4.6358  0.0006  0.4003\n",
      "      6            \u001b[36m0.2625\u001b[0m        \u001b[32m0.9533\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        4.3226  0.0006  0.3814\n",
      "      7            0.2625        \u001b[32m0.8345\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m3.7420\u001b[0m  0.0006  0.3868\n",
      "      8            \u001b[36m0.2812\u001b[0m        \u001b[32m0.7072\u001b[0m       0.2604            0.2604        \u001b[94m3.4293\u001b[0m  0.0006  0.3902\n",
      "      9            \u001b[36m0.3125\u001b[0m        0.7279       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m3.1498\u001b[0m  0.0006  0.3985\n",
      "     10            \u001b[36m0.3312\u001b[0m        0.7770       0.2743            0.2743        \u001b[94m2.8678\u001b[0m  0.0005  0.4033\n",
      "     11            0.3312        \u001b[32m0.6576\u001b[0m       0.2743            0.2743        \u001b[94m2.7759\u001b[0m  0.0005  0.3934\n",
      "     12            \u001b[36m0.3625\u001b[0m        \u001b[32m0.6328\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m2.6606\u001b[0m  0.0005  0.3877\n",
      "     13            \u001b[36m0.4062\u001b[0m        \u001b[32m0.6018\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m2.5548\u001b[0m  0.0005  0.3950\n",
      "     14            \u001b[36m0.4437\u001b[0m        \u001b[32m0.5482\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m2.4875\u001b[0m  0.0005  0.3851\n",
      "     15            \u001b[36m0.4750\u001b[0m        0.5987       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m2.4021\u001b[0m  0.0004  0.4015\n",
      "     16            \u001b[36m0.5500\u001b[0m        \u001b[32m0.4994\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m2.2327\u001b[0m  0.0004  0.3970\n",
      "     17            \u001b[36m0.6312\u001b[0m        \u001b[32m0.4879\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m2.0185\u001b[0m  0.0004  0.3987\n",
      "     18            \u001b[36m0.7250\u001b[0m        \u001b[32m0.4500\u001b[0m       0.3646            0.3646        \u001b[94m1.8114\u001b[0m  0.0004  0.3982\n",
      "     19            \u001b[36m0.8625\u001b[0m        0.4549       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.6380\u001b[0m  0.0004  0.4033\n",
      "     20            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3881\u001b[0m       0.3958            0.3958        \u001b[94m1.5447\u001b[0m  0.0003  0.3934\n",
      "     21            \u001b[36m0.9437\u001b[0m        0.3992       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.4957\u001b[0m  0.0003  0.4076\n",
      "     22            0.9437        \u001b[32m0.3412\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.4682\u001b[0m  0.0003  0.4240\n",
      "     23            0.9437        0.3817       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.4542\u001b[0m  0.0002  0.4156\n",
      "     24            \u001b[36m0.9563\u001b[0m        0.3791       0.4132            0.4132        \u001b[94m1.4439\u001b[0m  0.0002  0.4013\n",
      "     25            0.9563        \u001b[32m0.3360\u001b[0m       0.4062            0.4062        \u001b[94m1.4337\u001b[0m  0.0002  0.4605\n",
      "     26            \u001b[36m0.9625\u001b[0m        \u001b[32m0.3260\u001b[0m       0.3924            0.3924        \u001b[94m1.4268\u001b[0m  0.0002  0.4414\n",
      "     27            0.9625        0.3269       0.3958            0.3958        \u001b[94m1.4184\u001b[0m  0.0002  0.4671\n",
      "     28            \u001b[36m0.9812\u001b[0m        0.3384       0.3993            0.3993        \u001b[94m1.4125\u001b[0m  0.0001  0.4550\n",
      "     29            0.9812        0.3423       0.3993            0.3993        \u001b[94m1.4105\u001b[0m  0.0001  0.4710\n",
      "     30            0.9812        0.3433       0.3958            0.3958        \u001b[94m1.4091\u001b[0m  0.0001  0.4610\n",
      "     31            0.9812        0.3665       0.3924            0.3924        \u001b[94m1.4060\u001b[0m  0.0001  0.3907\n",
      "     32            0.9812        0.3596       0.3924            0.3924        \u001b[94m1.4054\u001b[0m  0.0001  0.3918\n",
      "     33            \u001b[36m0.9875\u001b[0m        \u001b[32m0.3201\u001b[0m       0.3924            0.3924        \u001b[94m1.4032\u001b[0m  0.0000  0.3946\n",
      "     34            0.9875        0.3237       0.3958            0.3958        \u001b[94m1.4013\u001b[0m  0.0000  0.4025\n",
      "     35            0.9875        \u001b[32m0.3041\u001b[0m       0.4028            0.4028        \u001b[94m1.4002\u001b[0m  0.0000  0.3970\n",
      "     36            0.9875        0.3577       0.3993            0.3993        \u001b[94m1.3988\u001b[0m  0.0000  0.3969\n",
      "     37            0.9875        0.3320       0.3993            0.3993        \u001b[94m1.3966\u001b[0m  0.0000  0.3961\n",
      "     38            0.9875        0.3308       0.3993            0.3993        \u001b[94m1.3943\u001b[0m  0.0000  0.4040\n",
      "     39            0.9875        0.3222       0.3993            0.3993        \u001b[94m1.3926\u001b[0m  0.0000  0.3971\n",
      "     40            \u001b[36m0.9938\u001b[0m        0.3095       0.3958            0.3958        \u001b[94m1.3911\u001b[0m  0.0000  0.4001\n",
      "Training model for subject 7 with 170 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3471\u001b[0m        \u001b[32m1.6368\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m4.0373\u001b[0m  0.0006  0.3959\n",
      "      2            0.2529        \u001b[32m1.3846\u001b[0m       0.2500            0.2500        4.3893  0.0006  0.4046\n",
      "      3            0.2588        \u001b[32m1.3147\u001b[0m       0.2500            0.2500        \u001b[94m3.6670\u001b[0m  0.0006  0.3925\n",
      "      4            0.2824        \u001b[32m1.1851\u001b[0m       0.2431            0.2431        \u001b[94m2.7530\u001b[0m  0.0006  0.4183\n",
      "      5            0.3353        \u001b[32m0.9677\u001b[0m       0.2465            0.2465        \u001b[94m2.3909\u001b[0m  0.0006  0.3890\n",
      "      6            \u001b[36m0.4235\u001b[0m        \u001b[32m0.9496\u001b[0m       0.2708            0.2708        \u001b[94m1.9596\u001b[0m  0.0006  0.3967\n",
      "      7            \u001b[36m0.5000\u001b[0m        \u001b[32m0.9457\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m1.8248\u001b[0m  0.0006  0.3923\n",
      "      8            0.5000        \u001b[32m0.8369\u001b[0m       0.2743            0.2743        1.8534  0.0006  0.5042\n",
      "      9            \u001b[36m0.5235\u001b[0m        \u001b[32m0.8218\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        \u001b[94m1.7980\u001b[0m  0.0006  0.3986\n",
      "     10            \u001b[36m0.5647\u001b[0m        \u001b[32m0.7903\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m1.7808\u001b[0m  0.0005  0.4076\n",
      "     11            \u001b[36m0.6294\u001b[0m        \u001b[32m0.7263\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m1.7129\u001b[0m  0.0005  0.4006\n",
      "     12            \u001b[36m0.6824\u001b[0m        \u001b[32m0.6836\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.6557\u001b[0m  0.0005  0.3984\n",
      "     13            \u001b[36m0.7294\u001b[0m        0.7105       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.6125\u001b[0m  0.0005  0.3903\n",
      "     14            \u001b[36m0.7706\u001b[0m        \u001b[32m0.6345\u001b[0m       0.3576            0.3576        \u001b[94m1.5687\u001b[0m  0.0005  0.3926\n",
      "     15            \u001b[36m0.8176\u001b[0m        0.6358       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.5216\u001b[0m  0.0004  0.4384\n",
      "     16            \u001b[36m0.8765\u001b[0m        \u001b[32m0.6145\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.4806\u001b[0m  0.0004  0.4568\n",
      "     17            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5449\u001b[0m       0.3924            0.3924        \u001b[94m1.4452\u001b[0m  0.0004  0.4544\n",
      "     18            \u001b[36m0.9176\u001b[0m        \u001b[32m0.4821\u001b[0m       0.3889            0.3889        \u001b[94m1.4239\u001b[0m  0.0004  0.4754\n",
      "     19            \u001b[36m0.9294\u001b[0m        0.5428       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4066\u001b[0m  0.0004  0.4611\n",
      "     20            \u001b[36m0.9353\u001b[0m        \u001b[32m0.4508\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.3964\u001b[0m  0.0003  0.4796\n",
      "     21            \u001b[36m0.9529\u001b[0m        0.4985       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.3830\u001b[0m  0.0003  0.4320\n",
      "     22            0.9471        \u001b[32m0.4329\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.3753\u001b[0m  0.0003  0.4214\n",
      "     23            0.9471        0.5092       0.4097            0.4097        \u001b[94m1.3752\u001b[0m  0.0002  0.3986\n",
      "     24            0.9471        \u001b[32m0.4219\u001b[0m       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.3751\u001b[0m  0.0002  0.3889\n",
      "     25            0.9471        0.4620       0.4097            0.4097        \u001b[94m1.3713\u001b[0m  0.0002  0.3843\n",
      "     26            0.9471        0.4486       0.4097            0.4097        \u001b[94m1.3661\u001b[0m  0.0002  0.3858\n",
      "     27            \u001b[36m0.9588\u001b[0m        \u001b[32m0.3738\u001b[0m       0.4028            0.4028        \u001b[94m1.3557\u001b[0m  0.0002  0.3959\n",
      "     28            0.9588        0.4333       0.4028            0.4028        \u001b[94m1.3499\u001b[0m  0.0001  0.4012\n",
      "     29            0.9588        0.3903       0.3889            0.3889        \u001b[94m1.3442\u001b[0m  0.0001  0.4101\n",
      "     30            0.9588        0.4021       0.3889            0.3889        \u001b[94m1.3402\u001b[0m  0.0001  0.3996\n",
      "     31            0.9588        \u001b[32m0.3727\u001b[0m       0.3924            0.3924        \u001b[94m1.3372\u001b[0m  0.0001  0.3959\n",
      "     32            0.9588        0.3821       0.3958            0.3958        \u001b[94m1.3344\u001b[0m  0.0001  0.3894\n",
      "     33            0.9588        0.3735       0.3958            0.3958        \u001b[94m1.3328\u001b[0m  0.0000  0.4094\n",
      "     34            0.9588        0.4102       0.4028            0.4028        \u001b[94m1.3310\u001b[0m  0.0000  0.4132\n",
      "     35            0.9588        \u001b[32m0.3691\u001b[0m       0.4028            0.4028        \u001b[94m1.3300\u001b[0m  0.0000  0.4126\n",
      "     36            0.9588        0.3972       0.4062            0.4062        \u001b[94m1.3294\u001b[0m  0.0000  0.4001\n",
      "     37            0.9588        \u001b[32m0.3166\u001b[0m       0.4062            0.4062        \u001b[94m1.3281\u001b[0m  0.0000  0.4033\n",
      "     38            0.9588        0.3522       0.4062            0.4062        \u001b[94m1.3274\u001b[0m  0.0000  0.4016\n",
      "     39            0.9588        0.4003       0.4028            0.4028        \u001b[94m1.3272\u001b[0m  0.0000  0.4063\n",
      "     40            0.9588        0.3857       0.4062            0.4062        \u001b[94m1.3272\u001b[0m  0.0000  0.3952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for subject 7 with 180 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.5730\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m5.2666\u001b[0m  0.0006  0.3944\n",
      "      2            0.2500        \u001b[32m1.3284\u001b[0m       0.2500            0.2500        5.6114  0.0006  0.3997\n",
      "      3            0.2500        \u001b[32m1.2007\u001b[0m       0.2500            0.2500        5.3902  0.0006  0.3998\n",
      "      4            0.2500        \u001b[32m1.1208\u001b[0m       0.2500            0.2500        \u001b[94m4.6539\u001b[0m  0.0006  0.3968\n",
      "      5            \u001b[36m0.2944\u001b[0m        \u001b[32m1.0158\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m3.9293\u001b[0m  0.0006  0.4660\n",
      "      6            \u001b[36m0.3111\u001b[0m        \u001b[32m0.8789\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m3.7392\u001b[0m  0.0006  0.4742\n",
      "      7            0.3111        \u001b[32m0.8580\u001b[0m       0.2569            0.2569        \u001b[94m3.6199\u001b[0m  0.0006  0.4919\n",
      "      8            0.3111        \u001b[32m0.8290\u001b[0m       0.2535            0.2535        \u001b[94m3.5669\u001b[0m  0.0006  0.4562\n",
      "      9            0.2889        \u001b[32m0.8019\u001b[0m       0.2535            0.2535        \u001b[94m3.4950\u001b[0m  0.0006  0.4500\n",
      "     10            \u001b[36m0.3222\u001b[0m        0.8454       0.2604            0.2604        \u001b[94m3.1854\u001b[0m  0.0005  0.4880\n",
      "     11            \u001b[36m0.3556\u001b[0m        \u001b[32m0.7267\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m2.8205\u001b[0m  0.0005  0.3947\n",
      "     12            \u001b[36m0.4333\u001b[0m        \u001b[32m0.5954\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m2.4511\u001b[0m  0.0005  0.4053\n",
      "     13            \u001b[36m0.5111\u001b[0m        \u001b[32m0.5613\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m2.1760\u001b[0m  0.0005  0.3827\n",
      "     14            \u001b[36m0.6111\u001b[0m        0.6001       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.9371\u001b[0m  0.0005  0.3977\n",
      "     15            \u001b[36m0.6556\u001b[0m        \u001b[32m0.5588\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.7768\u001b[0m  0.0004  0.3854\n",
      "     16            \u001b[36m0.7278\u001b[0m        \u001b[32m0.5365\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.6669\u001b[0m  0.0004  0.3932\n",
      "     17            \u001b[36m0.7667\u001b[0m        \u001b[32m0.5143\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.5959\u001b[0m  0.0004  0.4002\n",
      "     18            \u001b[36m0.8167\u001b[0m        0.5204       0.4028            0.4028        \u001b[94m1.5372\u001b[0m  0.0004  0.4042\n",
      "     19            \u001b[36m0.8611\u001b[0m        \u001b[32m0.4599\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.4571\u001b[0m  0.0004  0.3912\n",
      "     20            \u001b[36m0.9167\u001b[0m        \u001b[32m0.4177\u001b[0m       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.3864\u001b[0m  0.0003  0.4267\n",
      "     21            \u001b[36m0.9222\u001b[0m        0.4959       0.4410            0.4410        \u001b[94m1.3369\u001b[0m  0.0003  0.4061\n",
      "     22            0.9222        \u001b[32m0.3939\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.3032\u001b[0m  0.0003  0.4058\n",
      "     23            \u001b[36m0.9444\u001b[0m        0.4285       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.2781\u001b[0m  0.0002  0.5047\n",
      "     24            \u001b[36m0.9556\u001b[0m        \u001b[32m0.3897\u001b[0m       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2600\u001b[0m  0.0002  0.4067\n",
      "     25            \u001b[36m0.9611\u001b[0m        \u001b[32m0.3821\u001b[0m       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        \u001b[94m1.2468\u001b[0m  0.0002  0.4214\n",
      "     26            0.9611        0.4117       \u001b[35m0.4931\u001b[0m            \u001b[31m0.4931\u001b[0m        \u001b[94m1.2378\u001b[0m  0.0002  0.4089\n",
      "     27            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3800\u001b[0m       0.4896            0.4896        \u001b[94m1.2317\u001b[0m  0.0002  0.4380\n",
      "     28            0.9667        0.3850       0.4931            0.4931        \u001b[94m1.2276\u001b[0m  0.0001  0.3927\n",
      "     29            0.9667        \u001b[32m0.3300\u001b[0m       0.4931            0.4931        \u001b[94m1.2237\u001b[0m  0.0001  0.4136\n",
      "     30            \u001b[36m0.9722\u001b[0m        0.3472       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        \u001b[94m1.2211\u001b[0m  0.0001  0.4135\n",
      "     31            0.9722        0.3890       0.4965            0.4965        \u001b[94m1.2207\u001b[0m  0.0001  0.4014\n",
      "     32            0.9722        0.3405       0.4965            0.4965        \u001b[94m1.2206\u001b[0m  0.0001  0.3950\n",
      "     33            \u001b[36m0.9833\u001b[0m        \u001b[32m0.3239\u001b[0m       0.4896            0.4896        \u001b[94m1.2190\u001b[0m  0.0000  0.4259\n",
      "     34            0.9833        0.3939       0.4931            0.4931        \u001b[94m1.2186\u001b[0m  0.0000  0.4412\n",
      "     35            0.9833        0.3664       0.4896            0.4896        \u001b[94m1.2167\u001b[0m  0.0000  0.4786\n",
      "     36            0.9833        0.3656       0.4931            0.4931        \u001b[94m1.2162\u001b[0m  0.0000  0.4796\n",
      "     37            0.9833        0.3846       0.4931            0.4931        \u001b[94m1.2157\u001b[0m  0.0000  0.4522\n",
      "     38            0.9833        0.3472       0.4896            0.4896        \u001b[94m1.2152\u001b[0m  0.0000  0.4782\n",
      "     39            0.9833        0.3336       0.4931            0.4931        \u001b[94m1.2143\u001b[0m  0.0000  0.4696\n",
      "     40            0.9833        0.3620       0.4861            0.4861        \u001b[94m1.2132\u001b[0m  0.0000  0.5021\n",
      "Training model for subject 7 with 190 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2947\u001b[0m        \u001b[32m1.6236\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        \u001b[94m3.7778\u001b[0m  0.0006  0.4432\n",
      "      2            0.2474        \u001b[32m1.3915\u001b[0m       0.2500            0.2500        \u001b[94m3.6425\u001b[0m  0.0006  0.4047\n",
      "      3            0.2684        \u001b[32m1.2504\u001b[0m       0.2431            0.2431        3.7456  0.0006  0.3929\n",
      "      4            \u001b[36m0.3105\u001b[0m        \u001b[32m1.1644\u001b[0m       0.2292            0.2292        \u001b[94m3.5723\u001b[0m  0.0006  0.3820\n",
      "      5            \u001b[36m0.4000\u001b[0m        \u001b[32m1.0911\u001b[0m       0.2500            0.2500        \u001b[94m3.1306\u001b[0m  0.0006  0.4052\n",
      "      6            0.3737        \u001b[32m1.0426\u001b[0m       0.2604            0.2604        \u001b[94m2.5306\u001b[0m  0.0006  0.3964\n",
      "      7            0.3947        \u001b[32m0.9468\u001b[0m       0.2500            0.2500        \u001b[94m2.2265\u001b[0m  0.0006  0.3860\n",
      "      8            0.3684        \u001b[32m0.8995\u001b[0m       0.2535            0.2535        \u001b[94m2.1039\u001b[0m  0.0006  0.4021\n",
      "      9            0.3474        0.9456       0.2639            0.2639        \u001b[94m2.0899\u001b[0m  0.0006  0.3949\n",
      "     10            0.3526        \u001b[32m0.8357\u001b[0m       0.2674            0.2674        2.0932  0.0005  0.3969\n",
      "     11            0.3737        \u001b[32m0.8072\u001b[0m       0.2674            0.2674        \u001b[94m2.0401\u001b[0m  0.0005  0.3932\n",
      "     12            \u001b[36m0.4421\u001b[0m        \u001b[32m0.7753\u001b[0m       0.2812            0.2812        \u001b[94m1.9621\u001b[0m  0.0005  0.4014\n",
      "     13            \u001b[36m0.4789\u001b[0m        \u001b[32m0.7500\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        \u001b[94m1.9021\u001b[0m  0.0005  0.4461\n",
      "     14            \u001b[36m0.5158\u001b[0m        \u001b[32m0.7419\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m1.8313\u001b[0m  0.0005  0.4185\n",
      "     15            \u001b[36m0.6158\u001b[0m        \u001b[32m0.6948\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m1.7249\u001b[0m  0.0004  0.4005\n",
      "     16            \u001b[36m0.6789\u001b[0m        \u001b[32m0.6124\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.6318\u001b[0m  0.0004  0.3956\n",
      "     17            \u001b[36m0.7263\u001b[0m        \u001b[32m0.5503\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.5536\u001b[0m  0.0004  0.4240\n",
      "     18            \u001b[36m0.7632\u001b[0m        0.6353       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.4784\u001b[0m  0.0004  0.3926\n",
      "     19            \u001b[36m0.8316\u001b[0m        \u001b[32m0.5269\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.4207\u001b[0m  0.0004  0.4076\n",
      "     20            \u001b[36m0.8632\u001b[0m        0.6419       0.3819            0.3819        \u001b[94m1.3855\u001b[0m  0.0003  0.4076\n",
      "     21            \u001b[36m0.8895\u001b[0m        0.5517       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.3626\u001b[0m  0.0003  0.4103\n",
      "     22            \u001b[36m0.9158\u001b[0m        0.5598       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.3512\u001b[0m  0.0003  0.4079\n",
      "     23            \u001b[36m0.9211\u001b[0m        0.5487       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.3404\u001b[0m  0.0002  0.3823\n",
      "     24            0.9211        0.5285       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.3330\u001b[0m  0.0002  0.4797\n",
      "     25            0.9211        \u001b[32m0.4963\u001b[0m       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.3266\u001b[0m  0.0002  0.5109\n",
      "     26            \u001b[36m0.9263\u001b[0m        0.5314       0.4201            0.4201        \u001b[94m1.3214\u001b[0m  0.0002  0.4909\n",
      "     27            \u001b[36m0.9316\u001b[0m        0.5573       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.3157\u001b[0m  0.0002  0.5654\n",
      "     28            \u001b[36m0.9368\u001b[0m        \u001b[32m0.4836\u001b[0m       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.3115\u001b[0m  0.0001  0.4793\n",
      "     29            \u001b[36m0.9421\u001b[0m        \u001b[32m0.4623\u001b[0m       0.4375            0.4375        \u001b[94m1.3081\u001b[0m  0.0001  0.4214\n",
      "     30            0.9421        \u001b[32m0.4422\u001b[0m       0.4340            0.4340        \u001b[94m1.3057\u001b[0m  0.0001  0.4194\n",
      "     31            \u001b[36m0.9474\u001b[0m        0.4967       0.4375            0.4375        \u001b[94m1.3043\u001b[0m  0.0001  0.4127\n",
      "     32            0.9474        0.4514       0.4340            0.4340        \u001b[94m1.3027\u001b[0m  0.0001  0.3957\n",
      "     33            0.9474        0.4467       0.4375            0.4375        \u001b[94m1.3017\u001b[0m  0.0000  0.3980\n",
      "     34            \u001b[36m0.9526\u001b[0m        0.4428       0.4375            0.4375        \u001b[94m1.3012\u001b[0m  0.0000  0.3957\n",
      "     35            \u001b[36m0.9579\u001b[0m        0.4908       0.4375            0.4375        \u001b[94m1.3008\u001b[0m  0.0000  0.4115\n",
      "     36            0.9579        0.5130       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.3006\u001b[0m  0.0000  0.4167\n",
      "     37            \u001b[36m0.9632\u001b[0m        0.4881       0.4410            0.4410        \u001b[94m1.3003\u001b[0m  0.0000  0.4035\n",
      "     38            0.9632        0.5361       0.4410            0.4410        \u001b[94m1.3003\u001b[0m  0.0000  0.4143\n",
      "     39            0.9632        0.4806       0.4340            0.4340        \u001b[94m1.3001\u001b[0m  0.0000  0.4005\n",
      "     40            0.9579        0.5423       0.4340            0.4340        \u001b[94m1.3001\u001b[0m  0.0000  0.4213\n",
      "Training model for subject 7 with 200 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2600\u001b[0m        \u001b[32m1.5893\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m2.4170\u001b[0m  0.0006  0.5063\n",
      "      2            \u001b[36m0.3400\u001b[0m        \u001b[32m1.2554\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m1.7125\u001b[0m  0.0006  0.5064\n",
      "      3            \u001b[36m0.5150\u001b[0m        \u001b[32m1.1274\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m1.6570\u001b[0m  0.0006  0.5024\n",
      "      4            \u001b[36m0.5900\u001b[0m        \u001b[32m1.0539\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        1.8094  0.0006  0.4950\n",
      "      5            0.5200        \u001b[32m0.9577\u001b[0m       0.3056            0.3056        1.9857  0.0006  0.4912\n",
      "      6            0.4950        \u001b[32m0.8329\u001b[0m       0.2917            0.2917        2.0427  0.0006  0.5048\n",
      "      7            0.5100        \u001b[32m0.7793\u001b[0m       0.3021            0.3021        1.9439  0.0006  0.5067\n",
      "      8            \u001b[36m0.6350\u001b[0m        0.7977       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.6450\u001b[0m  0.0006  0.5143\n",
      "      9            \u001b[36m0.6950\u001b[0m        \u001b[32m0.6328\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.4451\u001b[0m  0.0006  0.4992\n",
      "     10            \u001b[36m0.7400\u001b[0m        0.6553       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.3153\u001b[0m  0.0005  0.5197\n",
      "     11            \u001b[36m0.8850\u001b[0m        \u001b[32m0.6190\u001b[0m       0.4653            0.4653        \u001b[94m1.2423\u001b[0m  0.0005  0.6083\n",
      "     12            \u001b[36m0.9350\u001b[0m        \u001b[32m0.5288\u001b[0m       0.4653            0.4653        \u001b[94m1.2053\u001b[0m  0.0005  0.6187\n",
      "     13            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5030\u001b[0m       0.4618            0.4618        \u001b[94m1.1784\u001b[0m  0.0005  0.5727\n",
      "     14            0.9450        0.5137       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        \u001b[94m1.1533\u001b[0m  0.0005  0.6323\n",
      "     15            \u001b[36m0.9750\u001b[0m        \u001b[32m0.4358\u001b[0m       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        \u001b[94m1.1422\u001b[0m  0.0004  0.5349\n",
      "     16            0.9700        \u001b[32m0.4237\u001b[0m       0.4931            0.4931        \u001b[94m1.1330\u001b[0m  0.0004  0.5028\n",
      "     17            0.9750        0.4412       0.4931            0.4931        \u001b[94m1.1255\u001b[0m  0.0004  0.5151\n",
      "     18            0.9750        \u001b[32m0.3733\u001b[0m       \u001b[35m0.5000\u001b[0m            \u001b[31m0.5000\u001b[0m        1.1312  0.0004  0.5036\n",
      "     19            \u001b[36m0.9800\u001b[0m        \u001b[32m0.3621\u001b[0m       0.4965            0.4965        1.1309  0.0004  0.5079\n",
      "     20            0.9800        \u001b[32m0.3479\u001b[0m       \u001b[35m0.5069\u001b[0m            \u001b[31m0.5069\u001b[0m        1.1321  0.0003  0.5062\n",
      "     21            0.9800        0.3796       \u001b[35m0.5174\u001b[0m            \u001b[31m0.5174\u001b[0m        1.1340  0.0003  0.5110\n",
      "     22            0.9800        0.3742       0.5174            0.5174        1.1313  0.0003  0.5178\n",
      "     23            0.9800        \u001b[32m0.3121\u001b[0m       \u001b[35m0.5382\u001b[0m            \u001b[31m0.5382\u001b[0m        \u001b[94m1.1230\u001b[0m  0.0002  0.5013\n",
      "     24            \u001b[36m0.9850\u001b[0m        0.3133       0.5312            0.5312        \u001b[94m1.1215\u001b[0m  0.0002  0.4960\n",
      "     25            0.9850        0.3220       0.5278            0.5278        \u001b[94m1.1212\u001b[0m  0.0002  0.4979\n",
      "     26            0.9850        0.3413       0.5382            0.5382        \u001b[94m1.1195\u001b[0m  0.0002  0.5177\n",
      "     27            \u001b[36m0.9900\u001b[0m        \u001b[32m0.2851\u001b[0m       0.5347            0.5347        1.1198  0.0002  0.5124\n",
      "     28            \u001b[36m0.9950\u001b[0m        0.2963       0.5347            0.5347        1.1199  0.0001  0.4990\n",
      "     29            0.9950        \u001b[32m0.2796\u001b[0m       0.5382            0.5382        \u001b[94m1.1192\u001b[0m  0.0001  0.5111\n",
      "     30            0.9950        0.2836       \u001b[35m0.5451\u001b[0m            \u001b[31m0.5451\u001b[0m        \u001b[94m1.1182\u001b[0m  0.0001  0.4944\n",
      "     31            0.9900        \u001b[32m0.2462\u001b[0m       0.5417            0.5417        \u001b[94m1.1172\u001b[0m  0.0001  0.4929\n",
      "     32            0.9900        \u001b[32m0.2370\u001b[0m       0.5451            0.5451        \u001b[94m1.1170\u001b[0m  0.0001  0.5062\n",
      "     33            0.9900        0.2446       \u001b[35m0.5486\u001b[0m            \u001b[31m0.5486\u001b[0m        \u001b[94m1.1167\u001b[0m  0.0000  0.5063\n",
      "     34            0.9900        0.2458       0.5417            0.5417        \u001b[94m1.1161\u001b[0m  0.0000  0.6469\n",
      "     35            0.9900        \u001b[32m0.2342\u001b[0m       0.5417            0.5417        \u001b[94m1.1153\u001b[0m  0.0000  0.6137\n",
      "     36            0.9900        0.2569       0.5451            0.5451        \u001b[94m1.1149\u001b[0m  0.0000  0.5943\n",
      "     37            0.9900        \u001b[32m0.2277\u001b[0m       0.5417            0.5417        \u001b[94m1.1148\u001b[0m  0.0000  0.5844\n",
      "     38            0.9900        0.2579       0.5382            0.5382        \u001b[94m1.1147\u001b[0m  0.0000  0.5811\n",
      "     39            0.9900        0.2373       0.5451            0.5451        \u001b[94m1.1146\u001b[0m  0.0000  0.5671\n",
      "     40            0.9900        0.2297       0.5486            0.5486        1.1146  0.0000  0.5103\n",
      "Training model for subject 7 with 210 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2476\u001b[0m        \u001b[32m1.5386\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m5.7196\u001b[0m  0.0006  0.5243\n",
      "      2            0.2476        \u001b[32m1.3129\u001b[0m       0.2500            0.2500        \u001b[94m4.3533\u001b[0m  0.0006  0.4996\n",
      "      3            \u001b[36m0.3333\u001b[0m        \u001b[32m1.0703\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m2.9362\u001b[0m  0.0006  0.4824\n",
      "      4            \u001b[36m0.4333\u001b[0m        \u001b[32m0.9975\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m2.4797\u001b[0m  0.0006  0.5020\n",
      "      5            \u001b[36m0.4714\u001b[0m        1.0027       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        \u001b[94m2.2938\u001b[0m  0.0006  0.5095\n",
      "      6            \u001b[36m0.5238\u001b[0m        \u001b[32m0.9272\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m1.9987\u001b[0m  0.0006  0.5059\n",
      "      7            \u001b[36m0.6143\u001b[0m        \u001b[32m0.8808\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m1.8280\u001b[0m  0.0006  0.4900\n",
      "      8            \u001b[36m0.6667\u001b[0m        \u001b[32m0.7637\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.6391\u001b[0m  0.0006  0.5094\n",
      "      9            \u001b[36m0.7381\u001b[0m        \u001b[32m0.7450\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4817\u001b[0m  0.0006  0.4953\n",
      "     10            \u001b[36m0.7762\u001b[0m        \u001b[32m0.6888\u001b[0m       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.4197\u001b[0m  0.0005  0.5005\n",
      "     11            \u001b[36m0.8143\u001b[0m        \u001b[32m0.6212\u001b[0m       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.3772\u001b[0m  0.0005  0.4949\n",
      "     12            \u001b[36m0.8619\u001b[0m        \u001b[32m0.6087\u001b[0m       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.3399\u001b[0m  0.0005  0.4928\n",
      "     13            \u001b[36m0.8857\u001b[0m        \u001b[32m0.5498\u001b[0m       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        \u001b[94m1.3258\u001b[0m  0.0005  0.5026\n",
      "     14            \u001b[36m0.9000\u001b[0m        0.5907       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        \u001b[94m1.3060\u001b[0m  0.0005  0.4946\n",
      "     15            \u001b[36m0.9238\u001b[0m        \u001b[32m0.4611\u001b[0m       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        \u001b[94m1.2725\u001b[0m  0.0004  0.5008\n",
      "     16            \u001b[36m0.9524\u001b[0m        0.5193       0.4861            0.4861        \u001b[94m1.2591\u001b[0m  0.0004  0.5244\n",
      "     17            \u001b[36m0.9571\u001b[0m        0.4856       0.4757            0.4757        \u001b[94m1.2499\u001b[0m  0.0004  0.5255\n",
      "     18            0.9571        0.4874       0.4722            0.4722        \u001b[94m1.2482\u001b[0m  0.0004  0.5057\n",
      "     19            \u001b[36m0.9667\u001b[0m        0.4897       0.4861            0.4861        \u001b[94m1.2361\u001b[0m  0.0004  0.5969\n",
      "     20            \u001b[36m0.9714\u001b[0m        \u001b[32m0.4108\u001b[0m       \u001b[35m0.5104\u001b[0m            \u001b[31m0.5104\u001b[0m        \u001b[94m1.2297\u001b[0m  0.0003  0.5853\n",
      "     21            \u001b[36m0.9762\u001b[0m        \u001b[32m0.3848\u001b[0m       0.5104            0.5104        \u001b[94m1.2236\u001b[0m  0.0003  0.5871\n",
      "     22            \u001b[36m0.9810\u001b[0m        \u001b[32m0.3738\u001b[0m       0.5104            0.5104        \u001b[94m1.2212\u001b[0m  0.0003  0.6126\n",
      "     23            0.9762        0.4253       0.5069            0.5069        \u001b[94m1.2176\u001b[0m  0.0002  0.5375\n",
      "     24            0.9810        \u001b[32m0.3693\u001b[0m       0.5000            0.5000        \u001b[94m1.2142\u001b[0m  0.0002  0.5022\n",
      "     25            \u001b[36m0.9857\u001b[0m        \u001b[32m0.3311\u001b[0m       0.5104            0.5104        \u001b[94m1.2046\u001b[0m  0.0002  0.5102\n",
      "     26            0.9857        0.3632       \u001b[35m0.5174\u001b[0m            \u001b[31m0.5174\u001b[0m        \u001b[94m1.1980\u001b[0m  0.0002  0.5161\n",
      "     27            0.9857        0.3370       \u001b[35m0.5243\u001b[0m            \u001b[31m0.5243\u001b[0m        \u001b[94m1.1939\u001b[0m  0.0002  0.5103\n",
      "     28            0.9857        0.3574       \u001b[35m0.5278\u001b[0m            \u001b[31m0.5278\u001b[0m        \u001b[94m1.1893\u001b[0m  0.0001  0.5116\n",
      "     29            0.9857        \u001b[32m0.3187\u001b[0m       0.5278            0.5278        \u001b[94m1.1846\u001b[0m  0.0001  0.5165\n",
      "     30            0.9857        0.3247       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        \u001b[94m1.1819\u001b[0m  0.0001  0.5021\n",
      "     31            0.9857        \u001b[32m0.2985\u001b[0m       \u001b[35m0.5347\u001b[0m            \u001b[31m0.5347\u001b[0m        \u001b[94m1.1802\u001b[0m  0.0001  0.5055\n",
      "     32            0.9857        0.3185       0.5347            0.5347        \u001b[94m1.1788\u001b[0m  0.0001  0.5103\n",
      "     33            0.9857        0.3229       0.5347            0.5347        \u001b[94m1.1783\u001b[0m  0.0000  0.5166\n",
      "     34            0.9857        0.3077       0.5347            0.5347        \u001b[94m1.1781\u001b[0m  0.0000  0.5058\n",
      "     35            0.9857        0.3251       \u001b[35m0.5382\u001b[0m            \u001b[31m0.5382\u001b[0m        1.1781  0.0000  0.5094\n",
      "     36            0.9857        0.3781       0.5382            0.5382        1.1783  0.0000  0.6334\n",
      "     37            0.9857        0.3658       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        1.1781  0.0000  0.5024\n",
      "     38            0.9857        0.3130       0.5417            0.5417        \u001b[94m1.1780\u001b[0m  0.0000  0.5087\n",
      "     39            0.9857        0.3072       0.5417            0.5417        \u001b[94m1.1778\u001b[0m  0.0000  0.5172\n",
      "     40            0.9857        0.3282       0.5417            0.5417        \u001b[94m1.1775\u001b[0m  0.0000  0.5044\n",
      "Training model for subject 7 with 220 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2455\u001b[0m        \u001b[32m1.7308\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m6.7164\u001b[0m  0.0006  0.5094\n",
      "      2            0.2455        \u001b[32m1.3842\u001b[0m       0.2500            0.2500        \u001b[94m6.0252\u001b[0m  0.0006  0.5509\n",
      "      3            0.2455        \u001b[32m1.1967\u001b[0m       0.2500            0.2500        \u001b[94m4.3083\u001b[0m  0.0006  0.5844\n",
      "      4            0.2455        \u001b[32m1.0946\u001b[0m       0.2500            0.2500        \u001b[94m3.9321\u001b[0m  0.0006  0.5988\n",
      "      5            0.2455        \u001b[32m0.9627\u001b[0m       0.2500            0.2500        \u001b[94m3.6055\u001b[0m  0.0006  0.5834\n",
      "      6            \u001b[36m0.2500\u001b[0m        \u001b[32m0.8713\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m3.2138\u001b[0m  0.0006  0.6237\n",
      "      7            \u001b[36m0.2773\u001b[0m        \u001b[32m0.8494\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m2.5374\u001b[0m  0.0006  0.5128\n",
      "      8            \u001b[36m0.5409\u001b[0m        \u001b[32m0.7777\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m1.7997\u001b[0m  0.0006  0.5262\n",
      "      9            \u001b[36m0.7955\u001b[0m        0.7948       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.4775\u001b[0m  0.0006  0.4967\n",
      "     10            \u001b[36m0.8182\u001b[0m        \u001b[32m0.7550\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.3614\u001b[0m  0.0005  0.5322\n",
      "     11            \u001b[36m0.8500\u001b[0m        \u001b[32m0.5884\u001b[0m       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.3155\u001b[0m  0.0005  0.5134\n",
      "     12            \u001b[36m0.8818\u001b[0m        \u001b[32m0.5605\u001b[0m       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.3015\u001b[0m  0.0005  0.5036\n",
      "     13            \u001b[36m0.9045\u001b[0m        0.6444       0.4375            0.4375        \u001b[94m1.2897\u001b[0m  0.0005  0.5034\n",
      "     14            \u001b[36m0.9182\u001b[0m        \u001b[32m0.5556\u001b[0m       0.4306            0.4306        1.3134  0.0005  0.4935\n",
      "     15            \u001b[36m0.9227\u001b[0m        0.5714       0.4167            0.4167        1.3248  0.0004  0.5080\n",
      "     16            \u001b[36m0.9409\u001b[0m        0.5643       \u001b[35m0.4722\u001b[0m            \u001b[31m0.4722\u001b[0m        \u001b[94m1.2878\u001b[0m  0.0004  0.5076\n",
      "     17            \u001b[36m0.9545\u001b[0m        \u001b[32m0.4807\u001b[0m       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        \u001b[94m1.2602\u001b[0m  0.0004  0.5079\n",
      "     18            0.9545        \u001b[32m0.4517\u001b[0m       0.4722            0.4722        \u001b[94m1.2494\u001b[0m  0.0004  0.5053\n",
      "     19            \u001b[36m0.9682\u001b[0m        0.4528       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        \u001b[94m1.2397\u001b[0m  0.0004  0.5016\n",
      "     20            0.9636        0.4849       0.4931            0.4931        \u001b[94m1.2341\u001b[0m  0.0003  0.5130\n",
      "     21            0.9682        \u001b[32m0.4060\u001b[0m       0.4896            0.4896        \u001b[94m1.2276\u001b[0m  0.0003  0.5049\n",
      "     22            0.9682        0.4236       \u001b[35m0.5035\u001b[0m            \u001b[31m0.5035\u001b[0m        \u001b[94m1.2182\u001b[0m  0.0003  0.5031\n",
      "     23            \u001b[36m0.9818\u001b[0m        \u001b[32m0.3936\u001b[0m       \u001b[35m0.5139\u001b[0m            \u001b[31m0.5139\u001b[0m        \u001b[94m1.2046\u001b[0m  0.0002  0.5019\n",
      "     24            0.9818        0.4275       0.5069            0.5069        \u001b[94m1.1997\u001b[0m  0.0002  0.5083\n",
      "     25            0.9818        0.4036       0.4931            0.4931        1.2054  0.0002  0.5014\n",
      "     26            0.9818        \u001b[32m0.3697\u001b[0m       0.4861            0.4861        1.2085  0.0002  0.5436\n",
      "     27            \u001b[36m0.9864\u001b[0m        \u001b[32m0.3632\u001b[0m       0.4826            0.4826        1.2091  0.0002  0.6140\n",
      "     28            0.9864        \u001b[32m0.3277\u001b[0m       0.4861            0.4861        1.2084  0.0001  0.6050\n",
      "     29            0.9864        0.3320       0.4896            0.4896        1.2071  0.0001  0.6009\n",
      "     30            0.9864        0.3452       0.4931            0.4931        1.2076  0.0001  0.6350\n",
      "     31            0.9864        \u001b[32m0.3210\u001b[0m       0.4965            0.4965        1.2071  0.0001  0.5002\n",
      "     32            0.9864        \u001b[32m0.3074\u001b[0m       0.5000            0.5000        1.2065  0.0001  0.5100\n",
      "     33            0.9864        0.3626       0.4965            0.4965        1.2056  0.0000  0.5177\n",
      "     34            0.9864        0.3340       0.5000            0.5000        1.2058  0.0000  0.5069\n",
      "     35            0.9864        0.3354       0.5000            0.5000        1.2057  0.0000  0.5216\n",
      "     36            0.9864        \u001b[32m0.3047\u001b[0m       0.5000            0.5000        1.2048  0.0000  0.5181\n",
      "     37            0.9864        \u001b[32m0.2949\u001b[0m       0.5035            0.5035        1.2039  0.0000  0.5090\n",
      "     38            \u001b[36m0.9909\u001b[0m        \u001b[32m0.2947\u001b[0m       0.5035            0.5035        1.2033  0.0000  0.4929\n",
      "     39            0.9909        0.3538       0.5035            0.5035        1.2031  0.0000  0.4949\n",
      "     40            0.9909        0.3000       0.5035            0.5035        1.2031  0.0000  0.5161\n",
      "Training model for subject 7 with 230 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2826\u001b[0m        \u001b[32m1.6074\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        \u001b[94m3.2756\u001b[0m  0.0006  0.5219\n",
      "      2            \u001b[36m0.3652\u001b[0m        \u001b[32m1.3287\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m2.1259\u001b[0m  0.0006  0.5935\n",
      "      3            0.3565        \u001b[32m1.2748\u001b[0m       0.2986            0.2986        \u001b[94m1.6513\u001b[0m  0.0006  0.4980\n",
      "      4            \u001b[36m0.3957\u001b[0m        \u001b[32m1.1072\u001b[0m       0.2708            0.2708        1.8389  0.0006  0.5022\n",
      "      5            \u001b[36m0.4522\u001b[0m        \u001b[32m1.0651\u001b[0m       0.2986            0.2986        1.9115  0.0006  0.5020\n",
      "      6            \u001b[36m0.4826\u001b[0m        \u001b[32m1.0468\u001b[0m       0.2986            0.2986        1.7571  0.0006  0.5035\n",
      "      7            \u001b[36m0.6478\u001b[0m        \u001b[32m0.8753\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.4797\u001b[0m  0.0006  0.5042\n",
      "      8            \u001b[36m0.7261\u001b[0m        0.9039       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.3544\u001b[0m  0.0006  0.4835\n",
      "      9            \u001b[36m0.7783\u001b[0m        \u001b[32m0.8075\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.3154\u001b[0m  0.0006  0.5255\n",
      "     10            \u001b[36m0.8174\u001b[0m        \u001b[32m0.7381\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.2958\u001b[0m  0.0005  0.5843\n",
      "     11            0.8130        \u001b[32m0.7251\u001b[0m       0.3958            0.3958        1.3021  0.0005  0.6163\n",
      "     12            0.8000        \u001b[32m0.6673\u001b[0m       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        1.3094  0.0005  0.6121\n",
      "     13            0.8043        \u001b[32m0.6430\u001b[0m       0.4201            0.4201        1.3157  0.0005  0.6926\n",
      "     14            \u001b[36m0.8348\u001b[0m        \u001b[32m0.6113\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        1.2998  0.0005  0.5135\n",
      "     15            \u001b[36m0.8783\u001b[0m        \u001b[32m0.5881\u001b[0m       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.2670\u001b[0m  0.0004  0.5323\n",
      "     16            \u001b[36m0.9174\u001b[0m        \u001b[32m0.5514\u001b[0m       0.4583            0.4583        \u001b[94m1.2466\u001b[0m  0.0004  0.5231\n",
      "     17            \u001b[36m0.9304\u001b[0m        0.5818       0.4444            0.4444        \u001b[94m1.2367\u001b[0m  0.0004  0.4971\n",
      "     18            \u001b[36m0.9348\u001b[0m        \u001b[32m0.5138\u001b[0m       0.4549            0.4549        \u001b[94m1.2329\u001b[0m  0.0004  0.5037\n",
      "     19            0.9304        0.5222       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        1.2337  0.0004  0.5035\n",
      "     20            \u001b[36m0.9435\u001b[0m        0.5264       \u001b[35m0.4722\u001b[0m            \u001b[31m0.4722\u001b[0m        \u001b[94m1.2281\u001b[0m  0.0003  0.5216\n",
      "     21            0.9304        \u001b[32m0.4709\u001b[0m       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        \u001b[94m1.2231\u001b[0m  0.0003  0.5079\n",
      "     22            0.9348        \u001b[32m0.4201\u001b[0m       \u001b[35m0.4931\u001b[0m            \u001b[31m0.4931\u001b[0m        \u001b[94m1.2180\u001b[0m  0.0003  0.5053\n",
      "     23            \u001b[36m0.9565\u001b[0m        0.5105       0.4861            0.4861        \u001b[94m1.2130\u001b[0m  0.0002  0.5167\n",
      "     24            \u001b[36m0.9609\u001b[0m        0.4577       0.4826            0.4826        \u001b[94m1.2099\u001b[0m  0.0002  0.5219\n",
      "     25            \u001b[36m0.9739\u001b[0m        \u001b[32m0.4012\u001b[0m       0.4757            0.4757        1.2113  0.0002  0.4958\n",
      "     26            \u001b[36m0.9783\u001b[0m        0.4529       0.4722            0.4722        1.2129  0.0002  0.5103\n",
      "     27            \u001b[36m0.9826\u001b[0m        \u001b[32m0.3859\u001b[0m       0.4653            0.4653        1.2125  0.0002  0.5194\n",
      "     28            \u001b[36m0.9913\u001b[0m        0.4089       0.4722            0.4722        1.2126  0.0001  0.5140\n",
      "     29            0.9913        0.4320       0.4722            0.4722        1.2122  0.0001  0.5062\n",
      "     30            0.9913        \u001b[32m0.3736\u001b[0m       0.4757            0.4757        1.2111  0.0001  0.5079\n",
      "     31            0.9870        0.4076       0.4757            0.4757        1.2104  0.0001  0.5174\n",
      "     32            0.9870        0.3748       0.4792            0.4792        \u001b[94m1.2089\u001b[0m  0.0001  0.5356\n",
      "     33            0.9870        \u001b[32m0.3403\u001b[0m       0.4757            0.4757        \u001b[94m1.2081\u001b[0m  0.0000  0.6227\n",
      "     34            0.9913        0.3884       0.4861            0.4861        \u001b[94m1.2078\u001b[0m  0.0000  0.6152\n",
      "     35            0.9913        0.3787       0.4861            0.4861        \u001b[94m1.2073\u001b[0m  0.0000  0.6071\n",
      "     36            0.9913        0.3415       0.4861            0.4861        \u001b[94m1.2070\u001b[0m  0.0000  0.6784\n",
      "     37            0.9913        0.3763       0.4861            0.4861        \u001b[94m1.2067\u001b[0m  0.0000  0.5332\n",
      "     38            0.9913        0.4001       0.4861            0.4861        1.2068  0.0000  0.5183\n",
      "     39            0.9913        0.3913       0.4861            0.4861        1.2071  0.0000  0.5094\n",
      "     40            0.9913        0.3837       0.4826            0.4826        1.2072  0.0000  0.5280\n",
      "Training model for subject 7 with 240 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.5247\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m5.0623\u001b[0m  0.0006  0.5113\n",
      "      2            \u001b[36m0.3708\u001b[0m        \u001b[32m1.3334\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m2.6840\u001b[0m  0.0006  0.5129\n",
      "      3            0.3292        \u001b[32m1.2151\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        2.8204  0.0006  0.4899\n",
      "      4            0.3583        \u001b[32m1.1278\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m2.6123\u001b[0m  0.0006  0.4866\n",
      "      5            0.3417        \u001b[32m1.0887\u001b[0m       0.2743            0.2743        \u001b[94m2.1895\u001b[0m  0.0006  0.4979\n",
      "      6            0.3667        \u001b[32m1.0220\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m1.8360\u001b[0m  0.0006  0.5066\n",
      "      7            \u001b[36m0.4500\u001b[0m        \u001b[32m0.9303\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m1.6691\u001b[0m  0.0006  0.4922\n",
      "      8            \u001b[36m0.5833\u001b[0m        \u001b[32m0.9073\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.4892\u001b[0m  0.0006  0.4990\n",
      "      9            \u001b[36m0.6708\u001b[0m        \u001b[32m0.8068\u001b[0m       0.3646            0.3646        \u001b[94m1.3956\u001b[0m  0.0006  0.5052\n",
      "     10            \u001b[36m0.7708\u001b[0m        \u001b[32m0.8033\u001b[0m       0.3576            0.3576        \u001b[94m1.3604\u001b[0m  0.0005  0.5047\n",
      "     11            \u001b[36m0.8042\u001b[0m        \u001b[32m0.7396\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.3248\u001b[0m  0.0005  0.5021\n",
      "     12            \u001b[36m0.8542\u001b[0m        \u001b[32m0.7110\u001b[0m       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.3178\u001b[0m  0.0005  0.4995\n",
      "     13            0.8458        0.7141       0.4236            0.4236        \u001b[94m1.3153\u001b[0m  0.0005  0.5016\n",
      "     14            \u001b[36m0.8792\u001b[0m        \u001b[32m0.6816\u001b[0m       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.2997\u001b[0m  0.0005  0.4981\n",
      "     15            \u001b[36m0.8833\u001b[0m        \u001b[32m0.6410\u001b[0m       0.4236            0.4236        \u001b[94m1.2937\u001b[0m  0.0004  0.5509\n",
      "     16            \u001b[36m0.8958\u001b[0m        \u001b[32m0.6015\u001b[0m       0.4201            0.4201        \u001b[94m1.2891\u001b[0m  0.0004  0.6916\n",
      "     17            \u001b[36m0.9167\u001b[0m        \u001b[32m0.5106\u001b[0m       0.4132            0.4132        \u001b[94m1.2761\u001b[0m  0.0004  0.6301\n",
      "     18            \u001b[36m0.9250\u001b[0m        0.5399       0.4097            0.4097        \u001b[94m1.2725\u001b[0m  0.0004  0.5999\n",
      "     19            \u001b[36m0.9458\u001b[0m        0.5874       0.4271            0.4271        1.2771  0.0004  0.6157\n",
      "     20            0.9417        0.5294       0.4271            0.4271        1.2727  0.0003  0.5753\n",
      "     21            0.9375        0.5526       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.2649\u001b[0m  0.0003  0.5078\n",
      "     22            0.9375        \u001b[32m0.4523\u001b[0m       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.2615\u001b[0m  0.0003  0.5011\n",
      "     23            \u001b[36m0.9583\u001b[0m        0.4611       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.2557\u001b[0m  0.0002  0.5043\n",
      "     24            \u001b[36m0.9625\u001b[0m        \u001b[32m0.3957\u001b[0m       0.4410            0.4410        \u001b[94m1.2513\u001b[0m  0.0002  0.5015\n",
      "     25            0.9583        0.4305       0.4479            0.4479        \u001b[94m1.2488\u001b[0m  0.0002  0.5099\n",
      "     26            0.9583        0.4225       0.4444            0.4444        1.2511  0.0002  0.5070\n",
      "     27            \u001b[36m0.9667\u001b[0m        0.4127       0.4479            0.4479        1.2524  0.0002  0.5225\n",
      "     28            \u001b[36m0.9708\u001b[0m        0.4408       0.4444            0.4444        1.2566  0.0001  0.5152\n",
      "     29            0.9708        0.4287       0.4410            0.4410        1.2585  0.0001  0.5388\n",
      "     30            \u001b[36m0.9750\u001b[0m        0.4118       0.4444            0.4444        1.2596  0.0001  0.5053\n",
      "     31            0.9750        \u001b[32m0.3866\u001b[0m       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        1.2611  0.0001  0.4972\n",
      "     32            0.9708        0.4375       0.4549            0.4549        1.2622  0.0001  0.5211\n",
      "     33            0.9750        0.4146       0.4549            0.4549        1.2616  0.0000  0.5083\n",
      "     34            0.9750        0.4088       0.4549            0.4549        1.2611  0.0000  0.5090\n",
      "     35            \u001b[36m0.9792\u001b[0m        \u001b[32m0.3834\u001b[0m       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        1.2607  0.0000  0.4957\n",
      "     36            0.9792        0.3868       0.4549            0.4549        1.2602  0.0000  0.5040\n",
      "     37            0.9792        \u001b[32m0.3660\u001b[0m       0.4549            0.4549        1.2598  0.0000  0.5063\n",
      "     38            0.9792        0.4274       0.4583            0.4583        1.2597  0.0000  0.5111\n",
      "     39            0.9750        0.4052       0.4549            0.4549        1.2593  0.0000  0.6676\n",
      "     40            0.9750        0.4109       0.4549            0.4549        1.2591  0.0000  0.6550\n",
      "Training model for subject 7 with 250 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2880\u001b[0m        \u001b[32m1.6903\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m2.7556\u001b[0m  0.0006  0.6795\n",
      "      2            0.2560        \u001b[32m1.3641\u001b[0m       0.2535            0.2535        \u001b[94m2.3933\u001b[0m  0.0006  0.6726\n",
      "      3            \u001b[36m0.3720\u001b[0m        \u001b[32m1.2371\u001b[0m       0.2396            0.2396        2.7842  0.0006  0.5375\n",
      "      4            0.3720        \u001b[32m1.1279\u001b[0m       0.2465            0.2465        2.8428  0.0006  0.5050\n",
      "      5            0.3440        \u001b[32m1.0412\u001b[0m       0.2708            0.2708        2.6196  0.0006  0.4996\n",
      "      6            \u001b[36m0.4200\u001b[0m        \u001b[32m1.0230\u001b[0m       0.2708            0.2708        \u001b[94m2.3505\u001b[0m  0.0006  0.5719\n",
      "      7            \u001b[36m0.4280\u001b[0m        \u001b[32m0.8973\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m2.1379\u001b[0m  0.0006  0.6060\n",
      "      8            \u001b[36m0.5320\u001b[0m        \u001b[32m0.8637\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m1.7927\u001b[0m  0.0006  0.5645\n",
      "      9            \u001b[36m0.6320\u001b[0m        \u001b[32m0.8345\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.5602\u001b[0m  0.0006  0.5341\n",
      "     10            \u001b[36m0.7080\u001b[0m        0.8402       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.4710\u001b[0m  0.0005  0.7907\n",
      "     11            \u001b[36m0.7400\u001b[0m        \u001b[32m0.7202\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4150\u001b[0m  0.0005  0.5702\n",
      "     12            \u001b[36m0.7640\u001b[0m        \u001b[32m0.6619\u001b[0m       0.3924            0.3924        \u001b[94m1.3780\u001b[0m  0.0005  0.5103\n",
      "     13            \u001b[36m0.8000\u001b[0m        \u001b[32m0.6175\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.3404\u001b[0m  0.0005  0.6611\n",
      "     14            \u001b[36m0.8560\u001b[0m        0.6368       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.3102\u001b[0m  0.0005  0.6086\n",
      "     15            \u001b[36m0.8920\u001b[0m        \u001b[32m0.5864\u001b[0m       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.2895\u001b[0m  0.0004  0.5377\n",
      "     16            \u001b[36m0.9040\u001b[0m        0.5916       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.2751\u001b[0m  0.0004  0.5577\n",
      "     17            \u001b[36m0.9080\u001b[0m        \u001b[32m0.5309\u001b[0m       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.2576\u001b[0m  0.0004  0.5326\n",
      "     18            \u001b[36m0.9280\u001b[0m        \u001b[32m0.5252\u001b[0m       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        \u001b[94m1.2455\u001b[0m  0.0004  0.9044\n",
      "     19            0.9280        \u001b[32m0.5002\u001b[0m       0.4722            0.4722        \u001b[94m1.2411\u001b[0m  0.0004  0.8223\n",
      "     20            \u001b[36m0.9360\u001b[0m        0.5712       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        \u001b[94m1.2409\u001b[0m  0.0003  0.6587\n",
      "     21            \u001b[36m0.9560\u001b[0m        \u001b[32m0.4779\u001b[0m       0.4757            0.4757        1.2411  0.0003  0.6173\n",
      "     22            0.9560        \u001b[32m0.4552\u001b[0m       0.4826            0.4826        1.2434  0.0003  0.6416\n",
      "     23            \u001b[36m0.9600\u001b[0m        0.4822       0.4722            0.4722        1.2440  0.0002  0.5223\n",
      "     24            \u001b[36m0.9640\u001b[0m        \u001b[32m0.4214\u001b[0m       0.4826            0.4826        \u001b[94m1.2342\u001b[0m  0.0002  0.5244\n",
      "     25            \u001b[36m0.9680\u001b[0m        0.4385       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        \u001b[94m1.2174\u001b[0m  0.0002  0.5172\n",
      "     26            \u001b[36m0.9720\u001b[0m        \u001b[32m0.4057\u001b[0m       0.4861            0.4861        \u001b[94m1.2066\u001b[0m  0.0002  0.5099\n",
      "     27            0.9720        \u001b[32m0.3983\u001b[0m       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        \u001b[94m1.2020\u001b[0m  0.0002  0.5077\n",
      "     28            0.9640        \u001b[32m0.3880\u001b[0m       \u001b[35m0.5069\u001b[0m            \u001b[31m0.5069\u001b[0m        \u001b[94m1.1980\u001b[0m  0.0001  0.5137\n",
      "     29            0.9680        \u001b[32m0.3743\u001b[0m       0.5000            0.5000        \u001b[94m1.1965\u001b[0m  0.0001  0.5283\n",
      "     30            0.9720        \u001b[32m0.3654\u001b[0m       0.5000            0.5000        \u001b[94m1.1943\u001b[0m  0.0001  0.5071\n",
      "     31            0.9720        0.3852       0.5035            0.5035        \u001b[94m1.1930\u001b[0m  0.0001  0.5118\n",
      "     32            \u001b[36m0.9760\u001b[0m        0.3657       0.5000            0.5000        \u001b[94m1.1923\u001b[0m  0.0001  0.5151\n",
      "     33            0.9760        0.3819       0.5035            0.5035        \u001b[94m1.1918\u001b[0m  0.0000  0.5046\n",
      "     34            0.9760        0.4114       0.5000            0.5000        1.1924  0.0000  0.5019\n",
      "     35            \u001b[36m0.9800\u001b[0m        0.4078       0.5000            0.5000        1.1924  0.0000  0.4998\n",
      "     36            0.9800        \u001b[32m0.3416\u001b[0m       0.4965            0.4965        1.1929  0.0000  0.5023\n",
      "     37            0.9800        0.3523       0.4965            0.4965        1.1931  0.0000  0.5021\n",
      "     38            0.9800        0.4050       0.4931            0.4931        1.1929  0.0000  0.5152\n",
      "     39            0.9800        0.3517       0.4931            0.4931        1.1927  0.0000  0.5335\n",
      "     40            0.9800        0.3581       0.4931            0.4931        1.1925  0.0000  0.5302\n",
      "Training model for subject 7 with 260 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2731\u001b[0m        \u001b[32m1.6451\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m1.7883\u001b[0m  0.0006  0.6684\n",
      "      2            0.2615        \u001b[32m1.3434\u001b[0m       0.2604            0.2604        1.9409  0.0006  0.7132\n",
      "      3            \u001b[36m0.3731\u001b[0m        \u001b[32m1.1190\u001b[0m       0.2604            0.2604        1.8209  0.0006  0.7325\n",
      "      4            \u001b[36m0.4308\u001b[0m        \u001b[32m1.0024\u001b[0m       0.2708            0.2708        1.9043  0.0006  0.6988\n",
      "      5            \u001b[36m0.4692\u001b[0m        \u001b[32m0.9254\u001b[0m       0.2743            0.2743        1.7993  0.0006  0.7164\n",
      "      6            \u001b[36m0.6038\u001b[0m        \u001b[32m0.8573\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m1.5360\u001b[0m  0.0006  0.6175\n",
      "      7            \u001b[36m0.7231\u001b[0m        0.8578       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.3982\u001b[0m  0.0006  0.5915\n",
      "      8            \u001b[36m0.7654\u001b[0m        \u001b[32m0.7582\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.3429\u001b[0m  0.0006  0.6061\n",
      "      9            \u001b[36m0.8115\u001b[0m        \u001b[32m0.7200\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.2956\u001b[0m  0.0006  0.6064\n",
      "     10            \u001b[36m0.8769\u001b[0m        \u001b[32m0.6265\u001b[0m       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.2695\u001b[0m  0.0005  0.5990\n",
      "     11            \u001b[36m0.8885\u001b[0m        \u001b[32m0.6171\u001b[0m       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.2347\u001b[0m  0.0005  0.6088\n",
      "     12            \u001b[36m0.9077\u001b[0m        \u001b[32m0.5476\u001b[0m       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        \u001b[94m1.2103\u001b[0m  0.0005  0.6397\n",
      "     13            \u001b[36m0.9385\u001b[0m        0.5732       0.4653            0.4653        1.2123  0.0005  0.6283\n",
      "     14            \u001b[36m0.9423\u001b[0m        \u001b[32m0.4614\u001b[0m       0.4653            0.4653        1.2217  0.0005  0.6219\n",
      "     15            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4506\u001b[0m       0.4792            0.4792        1.2177  0.0004  0.6142\n",
      "     16            \u001b[36m0.9615\u001b[0m        \u001b[32m0.3852\u001b[0m       \u001b[35m0.5069\u001b[0m            \u001b[31m0.5069\u001b[0m        \u001b[94m1.2084\u001b[0m  0.0004  0.5957\n",
      "     17            \u001b[36m0.9692\u001b[0m        0.4112       0.5069            0.5069        \u001b[94m1.2050\u001b[0m  0.0004  0.6034\n",
      "     18            \u001b[36m0.9769\u001b[0m        0.4098       0.5069            0.5069        \u001b[94m1.1986\u001b[0m  0.0004  0.6075\n",
      "     19            0.9769        \u001b[32m0.3627\u001b[0m       \u001b[35m0.5104\u001b[0m            \u001b[31m0.5104\u001b[0m        \u001b[94m1.1863\u001b[0m  0.0004  0.6188\n",
      "     20            \u001b[36m0.9846\u001b[0m        \u001b[32m0.3452\u001b[0m       0.5000            0.5000        \u001b[94m1.1786\u001b[0m  0.0003  0.6011\n",
      "     21            \u001b[36m0.9885\u001b[0m        0.3493       \u001b[35m0.5243\u001b[0m            \u001b[31m0.5243\u001b[0m        \u001b[94m1.1630\u001b[0m  0.0003  0.7166\n",
      "     22            0.9846        \u001b[32m0.2998\u001b[0m       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        \u001b[94m1.1527\u001b[0m  0.0003  0.7356\n",
      "     23            \u001b[36m0.9923\u001b[0m        0.3260       \u001b[35m0.5451\u001b[0m            \u001b[31m0.5451\u001b[0m        \u001b[94m1.1512\u001b[0m  0.0002  0.7182\n",
      "     24            0.9923        \u001b[32m0.2733\u001b[0m       0.5417            0.5417        \u001b[94m1.1498\u001b[0m  0.0002  0.8749\n",
      "     25            \u001b[36m0.9962\u001b[0m        \u001b[32m0.2609\u001b[0m       0.5417            0.5417        1.1548  0.0002  0.6045\n",
      "     26            0.9962        0.2774       0.5382            0.5382        1.1596  0.0002  0.6044\n",
      "     27            0.9962        0.3037       0.5312            0.5312        1.1603  0.0002  0.6182\n",
      "     28            0.9962        0.3106       0.5312            0.5312        1.1570  0.0001  0.6221\n",
      "     29            0.9962        \u001b[32m0.2361\u001b[0m       0.5347            0.5347        1.1539  0.0001  0.6217\n",
      "     30            0.9962        0.2510       0.5382            0.5382        1.1512  0.0001  0.5984\n",
      "     31            0.9962        0.2866       0.5347            0.5347        1.1504  0.0001  0.5928\n",
      "     32            \u001b[36m1.0000\u001b[0m        0.2935       0.5312            0.5312        1.1507  0.0001  0.6128\n",
      "     33            1.0000        0.2686       0.5347            0.5347        \u001b[94m1.1496\u001b[0m  0.0000  0.6080\n",
      "     34            1.0000        0.2810       0.5347            0.5347        \u001b[94m1.1478\u001b[0m  0.0000  0.6058\n",
      "     35            1.0000        \u001b[32m0.2279\u001b[0m       0.5347            0.5347        \u001b[94m1.1462\u001b[0m  0.0000  0.6066\n",
      "     36            1.0000        0.2467       0.5312            0.5312        \u001b[94m1.1451\u001b[0m  0.0000  0.6041\n",
      "     37            1.0000        0.2348       0.5312            0.5312        \u001b[94m1.1448\u001b[0m  0.0000  0.6030\n",
      "     38            1.0000        0.2642       0.5347            0.5347        \u001b[94m1.1446\u001b[0m  0.0000  0.6089\n",
      "     39            1.0000        0.2578       0.5347            0.5347        \u001b[94m1.1445\u001b[0m  0.0000  0.6131\n",
      "     40            1.0000        0.2446       0.5347            0.5347        \u001b[94m1.1445\u001b[0m  0.0000  0.6376\n",
      "Training model for subject 7 with 270 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2519\u001b[0m        \u001b[32m1.5383\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m2.4567\u001b[0m  0.0006  0.7418\n",
      "      2            \u001b[36m0.3704\u001b[0m        \u001b[32m1.2240\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        2.4646  0.0006  0.7160\n",
      "      3            \u001b[36m0.3815\u001b[0m        \u001b[32m1.1220\u001b[0m       0.2569            0.2569        2.5931  0.0006  0.7273\n",
      "      4            \u001b[36m0.4037\u001b[0m        \u001b[32m1.0535\u001b[0m       0.2535            0.2535        \u001b[94m2.3379\u001b[0m  0.0006  0.7489\n",
      "      5            \u001b[36m0.4556\u001b[0m        \u001b[32m0.9054\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m2.1916\u001b[0m  0.0006  0.6205\n",
      "      6            \u001b[36m0.4815\u001b[0m        \u001b[32m0.8061\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m1.9102\u001b[0m  0.0006  0.6141\n",
      "      7            \u001b[36m0.6185\u001b[0m        0.8315       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.5334\u001b[0m  0.0006  0.6133\n",
      "      8            \u001b[36m0.7000\u001b[0m        \u001b[32m0.7470\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.3675\u001b[0m  0.0006  0.6081\n",
      "      9            \u001b[36m0.8148\u001b[0m        \u001b[32m0.7378\u001b[0m       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.2510\u001b[0m  0.0006  0.5964\n",
      "     10            \u001b[36m0.9037\u001b[0m        \u001b[32m0.6589\u001b[0m       \u001b[35m0.4722\u001b[0m            \u001b[31m0.4722\u001b[0m        \u001b[94m1.1792\u001b[0m  0.0005  0.5944\n",
      "     11            \u001b[36m0.9185\u001b[0m        \u001b[32m0.5977\u001b[0m       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        \u001b[94m1.1535\u001b[0m  0.0005  0.6025\n",
      "     12            0.9185        \u001b[32m0.5835\u001b[0m       \u001b[35m0.5139\u001b[0m            \u001b[31m0.5139\u001b[0m        \u001b[94m1.1364\u001b[0m  0.0005  0.6211\n",
      "     13            0.9111        \u001b[32m0.5623\u001b[0m       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        \u001b[94m1.1325\u001b[0m  0.0005  0.6214\n",
      "     14            \u001b[36m0.9296\u001b[0m        \u001b[32m0.5075\u001b[0m       \u001b[35m0.5556\u001b[0m            \u001b[31m0.5556\u001b[0m        \u001b[94m1.1132\u001b[0m  0.0005  0.6003\n",
      "     15            \u001b[36m0.9444\u001b[0m        \u001b[32m0.4709\u001b[0m       0.5556            0.5556        \u001b[94m1.0978\u001b[0m  0.0004  0.6071\n",
      "     16            \u001b[36m0.9556\u001b[0m        \u001b[32m0.4399\u001b[0m       0.5451            0.5451        1.1005  0.0004  0.6380\n",
      "     17            \u001b[36m0.9593\u001b[0m        0.4544       0.5521            0.5521        1.1080  0.0004  0.5944\n",
      "     18            \u001b[36m0.9741\u001b[0m        \u001b[32m0.3913\u001b[0m       \u001b[35m0.5660\u001b[0m            \u001b[31m0.5660\u001b[0m        \u001b[94m1.0974\u001b[0m  0.0004  0.6181\n",
      "     19            0.9741        0.4017       \u001b[35m0.5694\u001b[0m            \u001b[31m0.5694\u001b[0m        \u001b[94m1.0906\u001b[0m  0.0004  0.6154\n",
      "     20            \u001b[36m0.9815\u001b[0m        \u001b[32m0.3475\u001b[0m       \u001b[35m0.5799\u001b[0m            \u001b[31m0.5799\u001b[0m        \u001b[94m1.0770\u001b[0m  0.0003  0.6903\n",
      "     21            0.9815        \u001b[32m0.3404\u001b[0m       0.5729            0.5729        \u001b[94m1.0695\u001b[0m  0.0003  0.7320\n",
      "     22            0.9778        \u001b[32m0.3202\u001b[0m       0.5625            0.5625        \u001b[94m1.0675\u001b[0m  0.0003  0.7175\n",
      "     23            0.9778        0.3316       0.5694            0.5694        \u001b[94m1.0642\u001b[0m  0.0002  0.7616\n",
      "     24            \u001b[36m0.9852\u001b[0m        \u001b[32m0.3122\u001b[0m       0.5729            0.5729        \u001b[94m1.0571\u001b[0m  0.0002  0.6353\n",
      "     25            \u001b[36m0.9889\u001b[0m        \u001b[32m0.3004\u001b[0m       0.5694            0.5694        1.0578  0.0002  0.6564\n",
      "     26            0.9889        \u001b[32m0.2887\u001b[0m       0.5694            0.5694        1.0618  0.0002  0.6194\n",
      "     27            \u001b[36m0.9926\u001b[0m        \u001b[32m0.2864\u001b[0m       0.5799            0.5799        1.0630  0.0002  0.6300\n",
      "     28            0.9926        0.3058       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.0582  0.0001  0.6255\n",
      "     29            0.9889        \u001b[32m0.2857\u001b[0m       \u001b[35m0.5868\u001b[0m            \u001b[31m0.5868\u001b[0m        \u001b[94m1.0565\u001b[0m  0.0001  0.7669\n",
      "     30            0.9889        0.3009       \u001b[35m0.5903\u001b[0m            \u001b[31m0.5903\u001b[0m        \u001b[94m1.0532\u001b[0m  0.0001  0.7012\n",
      "     31            0.9926        \u001b[32m0.2826\u001b[0m       0.5903            0.5903        \u001b[94m1.0531\u001b[0m  0.0001  0.6433\n",
      "     32            0.9926        \u001b[32m0.2721\u001b[0m       0.5903            0.5903        \u001b[94m1.0525\u001b[0m  0.0001  0.6829\n",
      "     33            0.9926        0.2730       0.5903            0.5903        \u001b[94m1.0523\u001b[0m  0.0000  0.6218\n",
      "     34            0.9926        0.2953       0.5903            0.5903        1.0532  0.0000  0.6198\n",
      "     35            0.9889        0.2737       0.5903            0.5903        1.0540  0.0000  0.6214\n",
      "     36            0.9889        \u001b[32m0.2577\u001b[0m       0.5903            0.5903        1.0546  0.0000  0.6388\n",
      "     37            0.9926        0.2645       0.5903            0.5903        1.0548  0.0000  0.6249\n",
      "     38            0.9926        0.2583       0.5903            0.5903        1.0543  0.0000  0.6484\n",
      "     39            0.9926        0.2621       0.5903            0.5903        1.0541  0.0000  0.7563\n",
      "     40            0.9926        0.2965       0.5903            0.5903        1.0540  0.0000  0.6724\n",
      "Training model for subject 8 with 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.5000\u001b[0m        \u001b[32m1.6806\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m5.9093\u001b[0m  0.0006  0.4276\n",
      "      2            0.5000        \u001b[32m1.2620\u001b[0m       0.2500            0.2500        7.3857  0.0006  0.4453\n",
      "      3            0.5000        \u001b[32m0.5672\u001b[0m       0.2500            0.2500        8.1025  0.0006  0.3442\n",
      "      4            0.5000        \u001b[32m0.1921\u001b[0m       0.2500            0.2500        8.2307  0.0006  0.3373\n",
      "      5            0.5000        0.3038       0.2500            0.2500        8.0044  0.0006  0.3133\n",
      "      6            0.5000        \u001b[32m0.1313\u001b[0m       0.2500            0.2500        7.4243  0.0006  0.3336\n",
      "      7            0.5000        0.1358       0.2500            0.2500        6.7295  0.0006  0.3231\n",
      "      8            0.5000        \u001b[32m0.0732\u001b[0m       0.2500            0.2500        6.1249  0.0006  0.3285\n",
      "      9            \u001b[36m0.6000\u001b[0m        \u001b[32m0.0331\u001b[0m       0.2500            0.2500        \u001b[94m5.5609\u001b[0m  0.0006  0.3328\n",
      "     10            \u001b[36m0.7000\u001b[0m        0.0715       0.2500            0.2500        \u001b[94m5.0042\u001b[0m  0.0005  0.3230\n",
      "     11            \u001b[36m0.9000\u001b[0m        \u001b[32m0.0199\u001b[0m       0.2500            0.2500        \u001b[94m4.5361\u001b[0m  0.0005  0.3305\n",
      "     12            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0137\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m4.1260\u001b[0m  0.0005  0.3308\n",
      "     13            1.0000        0.0228       0.2535            0.2535        \u001b[94m3.7750\u001b[0m  0.0005  0.3231\n",
      "     14            1.0000        0.0223       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m3.4905\u001b[0m  0.0005  0.3170\n",
      "     15            1.0000        0.0169       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m3.2582\u001b[0m  0.0004  0.3453\n",
      "     16            1.0000        0.0499       0.2604            0.2604        \u001b[94m3.0870\u001b[0m  0.0004  0.3229\n",
      "     17            1.0000        \u001b[32m0.0100\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m2.9489\u001b[0m  0.0004  0.3182\n",
      "     18            1.0000        0.0185       0.2535            0.2535        \u001b[94m2.8406\u001b[0m  0.0004  0.3433\n",
      "     19            1.0000        0.0158       0.2604            0.2604        \u001b[94m2.7517\u001b[0m  0.0004  0.3261\n",
      "     20            1.0000        0.0211       0.2604            0.2604        \u001b[94m2.6816\u001b[0m  0.0003  0.3168\n",
      "     21            1.0000        \u001b[32m0.0089\u001b[0m       0.2604            0.2604        \u001b[94m2.6313\u001b[0m  0.0003  0.3422\n",
      "     22            1.0000        0.0327       0.2604            0.2604        \u001b[94m2.5934\u001b[0m  0.0003  0.3107\n",
      "     23            1.0000        0.0190       0.2604            0.2604        \u001b[94m2.5585\u001b[0m  0.0002  0.3319\n",
      "     24            1.0000        0.0517       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m2.5323\u001b[0m  0.0002  0.3280\n",
      "     25            1.0000        0.0109       0.2639            0.2639        \u001b[94m2.5087\u001b[0m  0.0002  0.3271\n",
      "     26            1.0000        0.0110       0.2674            0.2674        \u001b[94m2.4843\u001b[0m  0.0002  0.3318\n",
      "     27            1.0000        0.0196       0.2639            0.2639        \u001b[94m2.4657\u001b[0m  0.0002  0.3276\n",
      "     28            1.0000        0.0289       0.2639            0.2639        \u001b[94m2.4512\u001b[0m  0.0001  0.3232\n",
      "     29            1.0000        \u001b[32m0.0082\u001b[0m       0.2604            0.2604        \u001b[94m2.4327\u001b[0m  0.0001  0.3177\n",
      "     30            1.0000        0.0099       0.2604            0.2604        \u001b[94m2.4148\u001b[0m  0.0001  0.3258\n",
      "     31            1.0000        0.0103       0.2604            0.2604        \u001b[94m2.4029\u001b[0m  0.0001  0.3271\n",
      "     32            1.0000        \u001b[32m0.0060\u001b[0m       0.2569            0.2569        \u001b[94m2.3882\u001b[0m  0.0001  0.3335\n",
      "     33            1.0000        0.0120       0.2569            0.2569        \u001b[94m2.3797\u001b[0m  0.0000  0.3257\n",
      "     34            1.0000        0.0071       0.2535            0.2535        \u001b[94m2.3687\u001b[0m  0.0000  0.3257\n",
      "     35            1.0000        0.0108       0.2569            0.2569        \u001b[94m2.3638\u001b[0m  0.0000  0.3334\n",
      "     36            1.0000        0.0146       0.2604            0.2604        \u001b[94m2.3546\u001b[0m  0.0000  0.3272\n",
      "     37            1.0000        0.0190       0.2604            0.2604        \u001b[94m2.3463\u001b[0m  0.0000  0.3272\n",
      "     38            1.0000        0.0099       0.2604            0.2604        \u001b[94m2.3425\u001b[0m  0.0000  0.3367\n",
      "     39            1.0000        0.0075       0.2604            0.2604        \u001b[94m2.3412\u001b[0m  0.0000  0.3221\n",
      "     40            1.0000        \u001b[32m0.0059\u001b[0m       0.2604            0.2604        \u001b[94m2.3359\u001b[0m  0.0000  0.4223\n",
      "Training model for subject 8 with 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.5000\u001b[0m        \u001b[32m1.6187\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m4.6519\u001b[0m  0.0006  0.3602\n",
      "      2            0.4000        \u001b[32m0.9385\u001b[0m       0.2535            0.2535        4.7752  0.0006  0.4918\n",
      "      3            0.4000        \u001b[32m0.6793\u001b[0m       0.2465            0.2465        4.8073  0.0006  0.3800\n",
      "      4            0.4500        \u001b[32m0.4124\u001b[0m       0.2465            0.2465        4.7094  0.0006  0.3753\n",
      "      5            \u001b[36m0.5500\u001b[0m        \u001b[32m0.3073\u001b[0m       0.2465            0.2465        \u001b[94m4.5381\u001b[0m  0.0006  0.3999\n",
      "      6            \u001b[36m0.7000\u001b[0m        \u001b[32m0.2101\u001b[0m       0.2535            0.2535        \u001b[94m4.1901\u001b[0m  0.0006  0.3414\n",
      "      7            \u001b[36m0.8000\u001b[0m        \u001b[32m0.1453\u001b[0m       0.2639            0.2639        \u001b[94m3.8452\u001b[0m  0.0006  0.2776\n",
      "      8            \u001b[36m0.9000\u001b[0m        \u001b[32m0.1120\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m3.4939\u001b[0m  0.0006  0.3001\n",
      "      9            0.9000        0.1506       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m3.1761\u001b[0m  0.0006  0.3179\n",
      "     10            0.9000        \u001b[32m0.1093\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m2.9265\u001b[0m  0.0005  0.2895\n",
      "     11            \u001b[36m0.9500\u001b[0m        \u001b[32m0.0451\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m2.7354\u001b[0m  0.0005  0.2978\n",
      "     12            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0448\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m2.5767\u001b[0m  0.0005  0.2915\n",
      "     13            1.0000        \u001b[32m0.0412\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m2.4382\u001b[0m  0.0005  0.2938\n",
      "     14            1.0000        \u001b[32m0.0382\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m2.3234\u001b[0m  0.0005  0.2850\n",
      "     15            1.0000        \u001b[32m0.0219\u001b[0m       0.3750            0.3750        \u001b[94m2.2283\u001b[0m  0.0004  0.3885\n",
      "     16            1.0000        0.0317       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m2.1484\u001b[0m  0.0004  0.2985\n",
      "     17            1.0000        0.0419       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m2.0841\u001b[0m  0.0004  0.3477\n",
      "     18            1.0000        0.0378       0.3889            0.3889        \u001b[94m2.0323\u001b[0m  0.0004  0.2805\n",
      "     19            1.0000        0.0314       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.9901\u001b[0m  0.0004  0.3079\n",
      "     20            1.0000        0.0335       0.3889            0.3889        \u001b[94m1.9525\u001b[0m  0.0003  0.2879\n",
      "     21            1.0000        0.0391       0.3889            0.3889        \u001b[94m1.9217\u001b[0m  0.0003  0.2774\n",
      "     22            1.0000        \u001b[32m0.0181\u001b[0m       0.3854            0.3854        \u001b[94m1.8960\u001b[0m  0.0003  0.2764\n",
      "     23            1.0000        \u001b[32m0.0173\u001b[0m       0.3819            0.3819        \u001b[94m1.8750\u001b[0m  0.0002  0.3294\n",
      "     24            1.0000        0.0180       0.3854            0.3854        \u001b[94m1.8576\u001b[0m  0.0002  0.3399\n",
      "     25            1.0000        0.0182       0.3854            0.3854        \u001b[94m1.8431\u001b[0m  0.0002  0.2971\n",
      "     26            1.0000        \u001b[32m0.0160\u001b[0m       0.3854            0.3854        \u001b[94m1.8317\u001b[0m  0.0002  0.3196\n",
      "     27            1.0000        0.0549       0.3854            0.3854        \u001b[94m1.8242\u001b[0m  0.0002  0.3603\n",
      "     28            1.0000        0.0258       0.3889            0.3889        \u001b[94m1.8168\u001b[0m  0.0001  0.3349\n",
      "     29            1.0000        \u001b[32m0.0089\u001b[0m       0.3889            0.3889        \u001b[94m1.8109\u001b[0m  0.0001  0.3262\n",
      "     30            1.0000        0.0124       0.3889            0.3889        \u001b[94m1.8053\u001b[0m  0.0001  0.3471\n",
      "     31            1.0000        0.0188       0.3889            0.3889        \u001b[94m1.8006\u001b[0m  0.0001  0.3025\n",
      "     32            1.0000        0.0178       0.3854            0.3854        \u001b[94m1.7968\u001b[0m  0.0001  0.2867\n",
      "     33            1.0000        0.0141       0.3854            0.3854        \u001b[94m1.7932\u001b[0m  0.0000  0.3145\n",
      "     34            1.0000        0.0270       0.3854            0.3854        \u001b[94m1.7904\u001b[0m  0.0000  0.2990\n",
      "     35            1.0000        0.0253       0.3854            0.3854        \u001b[94m1.7886\u001b[0m  0.0000  0.3013\n",
      "     36            1.0000        0.0162       0.3854            0.3854        \u001b[94m1.7870\u001b[0m  0.0000  0.3721\n",
      "     37            1.0000        0.0182       0.3854            0.3854        \u001b[94m1.7854\u001b[0m  0.0000  0.3515\n",
      "     38            1.0000        0.0176       0.3854            0.3854        \u001b[94m1.7844\u001b[0m  0.0000  0.3475\n",
      "     39            1.0000        0.0176       0.3854            0.3854        \u001b[94m1.7832\u001b[0m  0.0000  0.3939\n",
      "     40            1.0000        0.0164       0.3854            0.3854        \u001b[94m1.7823\u001b[0m  0.0000  0.2971\n",
      "Training model for subject 8 with 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2333\u001b[0m        \u001b[32m1.6945\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m6.6175\u001b[0m  0.0006  0.3126\n",
      "      2            0.2333        \u001b[32m1.2066\u001b[0m       0.2500            0.2500        \u001b[94m5.2017\u001b[0m  0.0006  0.3837\n",
      "      3            \u001b[36m0.2667\u001b[0m        \u001b[32m0.7212\u001b[0m       0.2500            0.2500        \u001b[94m3.9076\u001b[0m  0.0006  0.2632\n",
      "      4            \u001b[36m0.3000\u001b[0m        \u001b[32m0.5987\u001b[0m       0.2500            0.2500        \u001b[94m3.0502\u001b[0m  0.0006  0.2813\n",
      "      5            \u001b[36m0.5333\u001b[0m        \u001b[32m0.4657\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m2.5971\u001b[0m  0.0006  0.2967\n",
      "      6            \u001b[36m0.7000\u001b[0m        \u001b[32m0.3324\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m2.3134\u001b[0m  0.0006  0.3438\n",
      "      7            \u001b[36m0.9333\u001b[0m        \u001b[32m0.2672\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m2.1413\u001b[0m  0.0006  0.3492\n",
      "      8            0.9333        \u001b[32m0.2256\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m2.0142\u001b[0m  0.0006  0.4155\n",
      "      9            0.9333        \u001b[32m0.1807\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.9228\u001b[0m  0.0006  0.3438\n",
      "     10            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1797\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.8610\u001b[0m  0.0005  0.3281\n",
      "     11            1.0000        \u001b[32m0.1250\u001b[0m       0.3681            0.3681        \u001b[94m1.8201\u001b[0m  0.0005  0.3550\n",
      "     12            1.0000        \u001b[32m0.1117\u001b[0m       0.3611            0.3611        \u001b[94m1.7861\u001b[0m  0.0005  0.3692\n",
      "     13            1.0000        0.1413       0.3715            0.3715        \u001b[94m1.7510\u001b[0m  0.0005  0.4062\n",
      "     14            1.0000        0.1323       0.3576            0.3576        \u001b[94m1.7229\u001b[0m  0.0005  0.3051\n",
      "     15            1.0000        \u001b[32m0.0583\u001b[0m       0.3576            0.3576        \u001b[94m1.7072\u001b[0m  0.0004  0.3031\n",
      "     16            1.0000        0.0684       0.3368            0.3368        \u001b[94m1.7009\u001b[0m  0.0004  0.2661\n",
      "     17            1.0000        0.0841       0.3403            0.3403        1.7014  0.0004  0.2668\n",
      "     18            1.0000        \u001b[32m0.0564\u001b[0m       0.3403            0.3403        1.7067  0.0004  0.2656\n",
      "     19            1.0000        \u001b[32m0.0470\u001b[0m       0.3438            0.3438        1.7148  0.0004  0.2506\n",
      "     20            1.0000        0.0502       0.3264            0.3264        1.7230  0.0003  0.2658\n",
      "     21            1.0000        0.0655       0.3299            0.3299        1.7304  0.0003  0.2761\n",
      "     22            1.0000        0.0547       0.3194            0.3194        1.7385  0.0003  0.2991\n",
      "     23            1.0000        0.0562       0.3160            0.3160        1.7474  0.0002  0.2974\n",
      "     24            1.0000        \u001b[32m0.0386\u001b[0m       0.3160            0.3160        1.7551  0.0002  0.3187\n",
      "     25            1.0000        0.0483       0.3229            0.3229        1.7618  0.0002  0.3145\n",
      "     26            1.0000        0.0442       0.3194            0.3194        1.7671  0.0002  0.2995\n",
      "     27            1.0000        0.0438       0.3194            0.3194        1.7709  0.0002  0.3649\n",
      "     28            1.0000        0.0447       0.3194            0.3194        1.7736  0.0001  0.2996\n",
      "     29            1.0000        0.0429       0.3229            0.3229        1.7759  0.0001  0.2999\n",
      "     30            1.0000        \u001b[32m0.0351\u001b[0m       0.3229            0.3229        1.7776  0.0001  0.3534\n",
      "     31            1.0000        0.0439       0.3229            0.3229        1.7787  0.0001  0.2900\n",
      "     32            1.0000        0.0384       0.3229            0.3229        1.7793  0.0001  0.2609\n",
      "     33            1.0000        \u001b[32m0.0305\u001b[0m       0.3194            0.3194        1.7793  0.0000  0.2704\n",
      "     34            1.0000        0.0375       0.3194            0.3194        1.7794  0.0000  0.2656\n",
      "     35            1.0000        \u001b[32m0.0299\u001b[0m       0.3194            0.3194        1.7795  0.0000  0.2503\n",
      "     36            1.0000        0.0336       0.3194            0.3194        1.7792  0.0000  0.2666\n",
      "     37            1.0000        0.0340       0.3194            0.3194        1.7789  0.0000  0.2666\n",
      "     38            1.0000        0.0419       0.3194            0.3194        1.7790  0.0000  0.2505\n",
      "     39            1.0000        0.0355       0.3194            0.3194        1.7789  0.0000  0.2819\n",
      "     40            1.0000        \u001b[32m0.0222\u001b[0m       0.3194            0.3194        1.7788  0.0000  0.4219\n",
      "Training model for subject 8 with 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.4250\u001b[0m        \u001b[32m1.8665\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m3.6959\u001b[0m  0.0006  0.2660\n",
      "      2            \u001b[36m0.4750\u001b[0m        \u001b[32m1.2327\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m2.5035\u001b[0m  0.0006  0.2658\n",
      "      3            \u001b[36m0.6000\u001b[0m        \u001b[32m0.8950\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m1.8488\u001b[0m  0.0006  0.2825\n",
      "      4            \u001b[36m0.7750\u001b[0m        \u001b[32m0.7011\u001b[0m       0.3056            0.3056        \u001b[94m1.7314\u001b[0m  0.0006  0.2743\n",
      "      5            \u001b[36m0.8250\u001b[0m        \u001b[32m0.5500\u001b[0m       0.3160            0.3160        \u001b[94m1.7219\u001b[0m  0.0006  0.2999\n",
      "      6            0.8250        \u001b[32m0.5054\u001b[0m       0.3125            0.3125        1.7526  0.0006  0.2820\n",
      "      7            0.8250        \u001b[32m0.4890\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        1.7686  0.0006  0.3871\n",
      "      8            \u001b[36m0.8500\u001b[0m        \u001b[32m0.3815\u001b[0m       0.3125            0.3125        1.8004  0.0006  0.2832\n",
      "      9            \u001b[36m0.8750\u001b[0m        \u001b[32m0.2772\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        1.7772  0.0006  0.3524\n",
      "     10            \u001b[36m0.9250\u001b[0m        \u001b[32m0.2167\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        1.7476  0.0005  0.2938\n",
      "     11            \u001b[36m0.9750\u001b[0m        0.2392       0.3438            0.3438        \u001b[94m1.7143\u001b[0m  0.0005  0.2824\n",
      "     12            0.9750        \u001b[32m0.2019\u001b[0m       0.3438            0.3438        \u001b[94m1.6846\u001b[0m  0.0005  0.2661\n",
      "     13            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1429\u001b[0m       0.3403            0.3403        \u001b[94m1.6661\u001b[0m  0.0005  0.2969\n",
      "     14            1.0000        0.1793       0.3333            0.3333        \u001b[94m1.6584\u001b[0m  0.0005  0.3438\n",
      "     15            1.0000        \u001b[32m0.1356\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.6494\u001b[0m  0.0004  0.3281\n",
      "     16            1.0000        0.1494       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.6409\u001b[0m  0.0004  0.4442\n",
      "     17            1.0000        \u001b[32m0.1299\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.6380\u001b[0m  0.0004  0.3313\n",
      "     18            1.0000        \u001b[32m0.0868\u001b[0m       0.3715            0.3715        1.6394  0.0004  0.3284\n",
      "     19            1.0000        0.0883       0.3715            0.3715        1.6429  0.0004  0.4887\n",
      "     20            1.0000        0.1121       0.3681            0.3681        1.6468  0.0003  0.3609\n",
      "     21            1.0000        0.0897       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        1.6506  0.0003  0.3281\n",
      "     22            1.0000        0.0889       0.3785            0.3785        1.6555  0.0003  0.2672\n",
      "     23            1.0000        0.1085       0.3785            0.3785        1.6608  0.0002  0.2664\n",
      "     24            1.0000        0.0879       0.3819            0.3819        1.6665  0.0002  0.4062\n",
      "     25            1.0000        \u001b[32m0.0585\u001b[0m       0.3819            0.3819        1.6709  0.0002  0.4401\n",
      "     26            1.0000        0.0984       0.3819            0.3819        1.6746  0.0002  0.4016\n",
      "     27            1.0000        0.0791       0.3819            0.3819        1.6773  0.0002  0.3001\n",
      "     28            1.0000        0.0922       0.3819            0.3819        1.6796  0.0001  0.3027\n",
      "     29            1.0000        \u001b[32m0.0541\u001b[0m       0.3785            0.3785        1.6818  0.0001  0.2656\n",
      "     30            1.0000        0.0601       0.3785            0.3785        1.6830  0.0001  0.3231\n",
      "     31            1.0000        0.0643       0.3785            0.3785        1.6838  0.0001  0.2816\n",
      "     32            1.0000        0.0619       0.3785            0.3785        1.6842  0.0001  0.3267\n",
      "     33            1.0000        0.0644       0.3819            0.3819        1.6844  0.0000  0.2806\n",
      "     34            1.0000        0.0626       0.3819            0.3819        1.6846  0.0000  0.2727\n",
      "     35            1.0000        0.0884       0.3819            0.3819        1.6848  0.0000  0.2782\n",
      "     36            1.0000        0.0633       0.3819            0.3819        1.6848  0.0000  0.2905\n",
      "     37            1.0000        0.0550       0.3819            0.3819        1.6849  0.0000  0.2837\n",
      "     38            1.0000        0.0757       0.3819            0.3819        1.6848  0.0000  0.2660\n",
      "     39            1.0000        0.0599       0.3819            0.3819        1.6847  0.0000  0.2710\n",
      "     40            1.0000        0.0857       0.3819            0.3819        1.6846  0.0000  0.2806\n",
      "Training model for subject 8 with 50 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2400\u001b[0m        \u001b[32m1.7929\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m5.1808\u001b[0m  0.0006  0.2969\n",
      "      2            \u001b[36m0.2600\u001b[0m        \u001b[32m1.3392\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m3.9806\u001b[0m  0.0006  0.2761\n",
      "      3            \u001b[36m0.4000\u001b[0m        \u001b[32m1.0449\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m3.4767\u001b[0m  0.0006  0.3048\n",
      "      4            \u001b[36m0.5000\u001b[0m        \u001b[32m0.8753\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m3.3112\u001b[0m  0.0006  0.3315\n",
      "      5            0.4800        \u001b[32m0.7478\u001b[0m       0.2743            0.2743        3.4146  0.0006  0.3069\n",
      "      6            0.4400        \u001b[32m0.5939\u001b[0m       0.2639            0.2639        3.3747  0.0006  0.2969\n",
      "      7            0.4600        \u001b[32m0.4720\u001b[0m       0.2604            0.2604        \u001b[94m3.2990\u001b[0m  0.0006  0.2813\n",
      "      8            0.4600        0.4766       0.2569            0.2569        \u001b[94m3.2490\u001b[0m  0.0006  0.2656\n",
      "      9            0.4800        \u001b[32m0.3944\u001b[0m       0.2569            0.2569        \u001b[94m3.1555\u001b[0m  0.0006  0.2924\n",
      "     10            0.5000        \u001b[32m0.3869\u001b[0m       0.2569            0.2569        \u001b[94m3.0234\u001b[0m  0.0005  0.2813\n",
      "     11            \u001b[36m0.6000\u001b[0m        \u001b[32m0.2778\u001b[0m       0.2639            0.2639        \u001b[94m2.7585\u001b[0m  0.0005  0.2817\n",
      "     12            \u001b[36m0.6600\u001b[0m        0.2988       0.2743            0.2743        \u001b[94m2.5586\u001b[0m  0.0005  0.2969\n",
      "     13            \u001b[36m0.8000\u001b[0m        0.2902       0.3021            0.3021        \u001b[94m2.3873\u001b[0m  0.0005  0.2814\n",
      "     14            \u001b[36m0.8600\u001b[0m        \u001b[32m0.2077\u001b[0m       0.3021            0.3021        \u001b[94m2.2297\u001b[0m  0.0005  0.2819\n",
      "     15            \u001b[36m0.9400\u001b[0m        0.2465       0.2882            0.2882        \u001b[94m2.1202\u001b[0m  0.0004  0.2813\n",
      "     16            \u001b[36m0.9800\u001b[0m        \u001b[32m0.1595\u001b[0m       0.2986            0.2986        \u001b[94m2.0337\u001b[0m  0.0004  0.2813\n",
      "     17            0.9800        0.2276       0.2986            0.2986        \u001b[94m1.9748\u001b[0m  0.0004  0.2813\n",
      "     18            0.9800        \u001b[32m0.1457\u001b[0m       0.3125            0.3125        \u001b[94m1.9269\u001b[0m  0.0004  0.2816\n",
      "     19            0.9800        \u001b[32m0.1416\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m1.8856\u001b[0m  0.0004  0.2969\n",
      "     20            \u001b[36m1.0000\u001b[0m        0.1625       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.8449\u001b[0m  0.0003  0.3594\n",
      "     21            1.0000        0.1494       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.8098\u001b[0m  0.0003  0.3539\n",
      "     22            1.0000        \u001b[32m0.1250\u001b[0m       0.3403            0.3403        \u001b[94m1.7828\u001b[0m  0.0003  0.3594\n",
      "     23            1.0000        0.1287       0.3472            0.3472        \u001b[94m1.7606\u001b[0m  0.0002  0.3594\n",
      "     24            1.0000        \u001b[32m0.1150\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.7432\u001b[0m  0.0002  0.3438\n",
      "     25            1.0000        0.1166       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.7312\u001b[0m  0.0002  0.3831\n",
      "     26            1.0000        \u001b[32m0.0931\u001b[0m       0.3542            0.3542        \u001b[94m1.7210\u001b[0m  0.0002  0.3730\n",
      "     27            1.0000        0.1084       0.3542            0.3542        \u001b[94m1.7127\u001b[0m  0.0002  0.3286\n",
      "     28            1.0000        \u001b[32m0.0785\u001b[0m       0.3611            0.3611        \u001b[94m1.7063\u001b[0m  0.0001  0.2813\n",
      "     29            1.0000        \u001b[32m0.0697\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.7016\u001b[0m  0.0001  0.2969\n",
      "     30            1.0000        0.0935       0.3646            0.3646        \u001b[94m1.6984\u001b[0m  0.0001  0.2972\n",
      "     31            1.0000        0.0859       0.3611            0.3611        \u001b[94m1.6958\u001b[0m  0.0001  0.2818\n",
      "     32            1.0000        0.1006       0.3611            0.3611        \u001b[94m1.6930\u001b[0m  0.0001  0.2969\n",
      "     33            1.0000        0.1071       0.3611            0.3611        \u001b[94m1.6908\u001b[0m  0.0000  0.2979\n",
      "     34            1.0000        0.1024       0.3542            0.3542        \u001b[94m1.6894\u001b[0m  0.0000  0.2817\n",
      "     35            1.0000        \u001b[32m0.0681\u001b[0m       0.3542            0.3542        \u001b[94m1.6880\u001b[0m  0.0000  0.2813\n",
      "     36            1.0000        0.1074       0.3542            0.3542        \u001b[94m1.6869\u001b[0m  0.0000  0.2813\n",
      "     37            1.0000        0.1066       0.3542            0.3542        \u001b[94m1.6859\u001b[0m  0.0000  0.2969\n",
      "     38            1.0000        0.1443       0.3576            0.3576        \u001b[94m1.6851\u001b[0m  0.0000  0.2973\n",
      "     39            1.0000        0.1535       0.3611            0.3611        \u001b[94m1.6843\u001b[0m  0.0000  0.2813\n",
      "     40            1.0000        0.1020       0.3576            0.3576        \u001b[94m1.6835\u001b[0m  0.0000  0.2813\n",
      "Training model for subject 8 with 60 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2333\u001b[0m        \u001b[32m1.6460\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m3.7687\u001b[0m  0.0006  0.2969\n",
      "      2            0.2333        \u001b[32m1.2631\u001b[0m       0.2500            0.2500        4.0485  0.0006  0.2815\n",
      "      3            0.2333        \u001b[32m1.0077\u001b[0m       0.2500            0.2500        3.9196  0.0006  0.2973\n",
      "      4            \u001b[36m0.2833\u001b[0m        \u001b[32m0.8700\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m3.6009\u001b[0m  0.0006  0.2817\n",
      "      5            \u001b[36m0.4167\u001b[0m        \u001b[32m0.7231\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m3.3007\u001b[0m  0.0006  0.2969\n",
      "      6            \u001b[36m0.4833\u001b[0m        \u001b[32m0.6803\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m3.0501\u001b[0m  0.0006  0.2969\n",
      "      7            \u001b[36m0.5000\u001b[0m        \u001b[32m0.6629\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m2.8657\u001b[0m  0.0006  0.2903\n",
      "      8            \u001b[36m0.6333\u001b[0m        \u001b[32m0.5445\u001b[0m       0.3438            0.3438        \u001b[94m2.6332\u001b[0m  0.0006  0.2969\n",
      "      9            \u001b[36m0.7000\u001b[0m        \u001b[32m0.4487\u001b[0m       0.3472            0.3472        \u001b[94m2.4749\u001b[0m  0.0006  0.2815\n",
      "     10            \u001b[36m0.7333\u001b[0m        \u001b[32m0.3551\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m2.3900\u001b[0m  0.0005  0.2969\n",
      "     11            0.7333        0.3934       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m2.3221\u001b[0m  0.0005  0.2973\n",
      "     12            \u001b[36m0.7833\u001b[0m        0.3568       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m2.2213\u001b[0m  0.0005  0.2813\n",
      "     13            \u001b[36m0.8500\u001b[0m        \u001b[32m0.2367\u001b[0m       0.3750            0.3750        \u001b[94m2.1187\u001b[0m  0.0005  0.2969\n",
      "     14            \u001b[36m0.9000\u001b[0m        0.3025       0.3646            0.3646        \u001b[94m2.0419\u001b[0m  0.0005  0.2814\n",
      "     15            \u001b[36m0.9333\u001b[0m        \u001b[32m0.2364\u001b[0m       0.3611            0.3611        \u001b[94m1.9750\u001b[0m  0.0004  0.2970\n",
      "     16            \u001b[36m0.9667\u001b[0m        \u001b[32m0.2200\u001b[0m       0.3681            0.3681        \u001b[94m1.9255\u001b[0m  0.0004  0.2978\n",
      "     17            0.9667        \u001b[32m0.1976\u001b[0m       0.3646            0.3646        \u001b[94m1.8744\u001b[0m  0.0004  0.2969\n",
      "     18            \u001b[36m0.9833\u001b[0m        0.2168       0.3576            0.3576        \u001b[94m1.8261\u001b[0m  0.0004  0.2969\n",
      "     19            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1621\u001b[0m       0.3576            0.3576        \u001b[94m1.7878\u001b[0m  0.0004  0.2864\n",
      "     20            1.0000        0.1718       0.3507            0.3507        \u001b[94m1.7549\u001b[0m  0.0003  0.2974\n",
      "     21            1.0000        0.1927       0.3576            0.3576        \u001b[94m1.7275\u001b[0m  0.0003  0.2813\n",
      "     22            1.0000        0.1665       0.3611            0.3611        \u001b[94m1.7008\u001b[0m  0.0003  0.2986\n",
      "     23            1.0000        \u001b[32m0.1347\u001b[0m       0.3715            0.3715        \u001b[94m1.6794\u001b[0m  0.0002  0.2817\n",
      "     24            1.0000        0.1372       0.3681            0.3681        \u001b[94m1.6618\u001b[0m  0.0002  0.2976\n",
      "     25            1.0000        0.1388       0.3681            0.3681        \u001b[94m1.6491\u001b[0m  0.0002  0.2813\n",
      "     26            1.0000        \u001b[32m0.1138\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.6414\u001b[0m  0.0002  0.3438\n",
      "     27            1.0000        0.1213       0.3681            0.3681        \u001b[94m1.6386\u001b[0m  0.0002  0.3594\n",
      "     28            1.0000        0.1201       0.3715            0.3715        \u001b[94m1.6354\u001b[0m  0.0001  0.3594\n",
      "     29            1.0000        0.1324       0.3750            0.3750        \u001b[94m1.6328\u001b[0m  0.0001  0.3750\n",
      "     30            1.0000        0.1246       0.3750            0.3750        \u001b[94m1.6307\u001b[0m  0.0001  0.3590\n",
      "     31            1.0000        0.1329       0.3785            0.3785        \u001b[94m1.6279\u001b[0m  0.0001  0.3957\n",
      "     32            1.0000        0.1293       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.6249\u001b[0m  0.0001  0.4063\n",
      "     33            1.0000        0.1217       0.3819            0.3819        \u001b[94m1.6229\u001b[0m  0.0000  0.3907\n",
      "     34            1.0000        0.1322       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.6214\u001b[0m  0.0000  0.2969\n",
      "     35            1.0000        \u001b[32m0.0880\u001b[0m       0.3854            0.3854        \u001b[94m1.6202\u001b[0m  0.0000  0.3130\n",
      "     36            1.0000        0.1052       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.6192\u001b[0m  0.0000  0.2969\n",
      "     37            1.0000        0.0928       0.3889            0.3889        \u001b[94m1.6181\u001b[0m  0.0000  0.3206\n",
      "     38            1.0000        0.1250       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.6173\u001b[0m  0.0000  0.3547\n",
      "     39            1.0000        0.1058       0.3889            0.3889        \u001b[94m1.6165\u001b[0m  0.0000  0.3166\n",
      "     40            1.0000        0.1021       0.3889            0.3889        \u001b[94m1.6160\u001b[0m  0.0000  0.2848\n",
      "Training model for subject 8 with 70 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2571\u001b[0m        \u001b[32m1.5748\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m7.7436\u001b[0m  0.0006  0.3125\n",
      "      2            0.2571        \u001b[32m1.1114\u001b[0m       0.2500            0.2500        \u001b[94m6.5974\u001b[0m  0.0006  0.4266\n",
      "      3            0.2286        \u001b[32m1.0836\u001b[0m       0.2500            0.2500        \u001b[94m6.2364\u001b[0m  0.0006  0.3360\n",
      "      4            0.2286        \u001b[32m0.8610\u001b[0m       0.2500            0.2500        6.3462  0.0006  0.3132\n",
      "      5            0.2286        \u001b[32m0.8097\u001b[0m       0.2500            0.2500        \u001b[94m6.1293\u001b[0m  0.0006  0.3125\n",
      "      6            0.2286        \u001b[32m0.6010\u001b[0m       0.2500            0.2500        \u001b[94m5.8653\u001b[0m  0.0006  0.3129\n",
      "      7            0.2571        \u001b[32m0.5280\u001b[0m       0.2535            0.2535        \u001b[94m5.3120\u001b[0m  0.0006  0.3125\n",
      "      8            0.2571        0.5384       0.2535            0.2535        \u001b[94m4.7602\u001b[0m  0.0006  0.3125\n",
      "      9            \u001b[36m0.3000\u001b[0m        \u001b[32m0.5105\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m4.1664\u001b[0m  0.0006  0.3125\n",
      "     10            \u001b[36m0.3714\u001b[0m        \u001b[32m0.4328\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m3.5839\u001b[0m  0.0005  0.3125\n",
      "     11            \u001b[36m0.4714\u001b[0m        0.4664       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m3.1149\u001b[0m  0.0005  0.3125\n",
      "     12            \u001b[36m0.6143\u001b[0m        \u001b[32m0.3550\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        \u001b[94m2.7511\u001b[0m  0.0005  0.3125\n",
      "     13            \u001b[36m0.7429\u001b[0m        \u001b[32m0.3266\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m2.4420\u001b[0m  0.0005  0.3125\n",
      "     14            \u001b[36m0.8286\u001b[0m        \u001b[32m0.2930\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        \u001b[94m2.1802\u001b[0m  0.0005  0.3131\n",
      "     15            \u001b[36m0.9143\u001b[0m        0.3141       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.9824\u001b[0m  0.0004  0.2969\n",
      "     16            \u001b[36m0.9571\u001b[0m        \u001b[32m0.2609\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.8595\u001b[0m  0.0004  0.3545\n",
      "     17            \u001b[36m0.9714\u001b[0m        \u001b[32m0.1921\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.7767\u001b[0m  0.0004  0.3280\n",
      "     18            \u001b[36m0.9857\u001b[0m        0.2135       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.7118\u001b[0m  0.0004  0.3129\n",
      "     19            \u001b[36m1.0000\u001b[0m        0.1929       0.3854            0.3854        \u001b[94m1.6728\u001b[0m  0.0004  0.4361\n",
      "     20            1.0000        \u001b[32m0.1821\u001b[0m       0.3854            0.3854        \u001b[94m1.6500\u001b[0m  0.0003  0.3685\n",
      "     21            1.0000        0.2355       0.3854            0.3854        \u001b[94m1.6354\u001b[0m  0.0003  0.3889\n",
      "     22            1.0000        0.2200       0.3819            0.3819        \u001b[94m1.6250\u001b[0m  0.0003  0.3286\n",
      "     23            1.0000        \u001b[32m0.1589\u001b[0m       0.3924            0.3924        \u001b[94m1.6182\u001b[0m  0.0002  0.3125\n",
      "     24            1.0000        0.1673       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.6157\u001b[0m  0.0002  0.3125\n",
      "     25            1.0000        0.1607       0.3958            0.3958        \u001b[94m1.6113\u001b[0m  0.0002  0.3125\n",
      "     26            1.0000        \u001b[32m0.1495\u001b[0m       0.3958            0.3958        \u001b[94m1.6058\u001b[0m  0.0002  0.3129\n",
      "     27            1.0000        0.1506       0.3889            0.3889        \u001b[94m1.6011\u001b[0m  0.0002  0.3443\n",
      "     28            1.0000        \u001b[32m0.1425\u001b[0m       0.3889            0.3889        \u001b[94m1.5992\u001b[0m  0.0001  0.3750\n",
      "     29            1.0000        0.1466       0.3924            0.3924        \u001b[94m1.5963\u001b[0m  0.0001  0.4063\n",
      "     30            1.0000        \u001b[32m0.1335\u001b[0m       0.3924            0.3924        \u001b[94m1.5934\u001b[0m  0.0001  0.3750\n",
      "     31            1.0000        0.1545       0.3889            0.3889        \u001b[94m1.5919\u001b[0m  0.0001  0.3750\n",
      "     32            1.0000        0.1378       0.3854            0.3854        \u001b[94m1.5913\u001b[0m  0.0001  0.3750\n",
      "     33            1.0000        0.1573       0.3889            0.3889        \u001b[94m1.5905\u001b[0m  0.0000  0.3906\n",
      "     34            1.0000        \u001b[32m0.0871\u001b[0m       0.3924            0.3924        \u001b[94m1.5897\u001b[0m  0.0000  0.4063\n",
      "     35            1.0000        0.1327       0.3889            0.3889        \u001b[94m1.5888\u001b[0m  0.0000  0.3282\n",
      "     36            1.0000        0.1172       0.3889            0.3889        \u001b[94m1.5877\u001b[0m  0.0000  0.3125\n",
      "     37            1.0000        0.1332       0.3889            0.3889        \u001b[94m1.5870\u001b[0m  0.0000  0.3125\n",
      "     38            1.0000        0.1353       0.3889            0.3889        \u001b[94m1.5862\u001b[0m  0.0000  0.3125\n",
      "     39            1.0000        0.1100       0.3889            0.3889        \u001b[94m1.5857\u001b[0m  0.0000  0.3282\n",
      "     40            1.0000        0.1417       0.3889            0.3889        \u001b[94m1.5851\u001b[0m  0.0000  0.4844\n",
      "Training model for subject 8 with 80 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2625\u001b[0m        \u001b[32m1.6309\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m6.5227\u001b[0m  0.0006  0.3594\n",
      "      2            0.2625        \u001b[32m1.2898\u001b[0m       0.2500            0.2500        \u001b[94m5.7831\u001b[0m  0.0006  0.3441\n",
      "      3            0.2625        \u001b[32m1.0388\u001b[0m       0.2500            0.2500        6.0216  0.0006  0.3438\n",
      "      4            0.2625        1.0571       0.2500            0.2500        5.9656  0.0006  0.3286\n",
      "      5            0.2625        \u001b[32m0.8818\u001b[0m       0.2500            0.2500        \u001b[94m5.7722\u001b[0m  0.0006  0.3128\n",
      "      6            0.2625        \u001b[32m0.8217\u001b[0m       0.2500            0.2500        \u001b[94m5.6460\u001b[0m  0.0006  0.3125\n",
      "      7            0.2625        \u001b[32m0.6697\u001b[0m       0.2500            0.2500        \u001b[94m5.4956\u001b[0m  0.0006  0.3281\n",
      "      8            \u001b[36m0.2750\u001b[0m        0.6944       0.2500            0.2500        \u001b[94m5.2181\u001b[0m  0.0006  0.3281\n",
      "      9            0.2750        \u001b[32m0.5897\u001b[0m       0.2500            0.2500        \u001b[94m4.9709\u001b[0m  0.0006  0.3128\n",
      "     10            \u001b[36m0.2875\u001b[0m        \u001b[32m0.5457\u001b[0m       0.2500            0.2500        \u001b[94m4.6068\u001b[0m  0.0005  0.3282\n",
      "     11            \u001b[36m0.3125\u001b[0m        \u001b[32m0.4899\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m4.2008\u001b[0m  0.0005  0.3281\n",
      "     12            \u001b[36m0.3875\u001b[0m        \u001b[32m0.3800\u001b[0m       0.2535            0.2535        \u001b[94m3.7929\u001b[0m  0.0005  0.3125\n",
      "     13            \u001b[36m0.4250\u001b[0m        \u001b[32m0.3442\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m3.3910\u001b[0m  0.0005  0.3281\n",
      "     14            \u001b[36m0.5250\u001b[0m        \u001b[32m0.3374\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        \u001b[94m3.0491\u001b[0m  0.0005  0.3438\n",
      "     15            \u001b[36m0.6625\u001b[0m        0.3537       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m2.7304\u001b[0m  0.0004  0.3283\n",
      "     16            \u001b[36m0.7875\u001b[0m        \u001b[32m0.2808\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m2.4666\u001b[0m  0.0004  0.3594\n",
      "     17            \u001b[36m0.8375\u001b[0m        \u001b[32m0.2663\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m2.2765\u001b[0m  0.0004  0.4532\n",
      "     18            \u001b[36m0.9250\u001b[0m        0.2684       0.3333            0.3333        \u001b[94m2.1432\u001b[0m  0.0004  0.3442\n",
      "     19            \u001b[36m0.9625\u001b[0m        \u001b[32m0.2552\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m2.0364\u001b[0m  0.0004  0.3441\n",
      "     20            0.9625        0.2560       0.3368            0.3368        \u001b[94m1.9586\u001b[0m  0.0003  0.3281\n",
      "     21            0.9625        0.2605       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.9002\u001b[0m  0.0003  0.3437\n",
      "     22            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2002\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.8460\u001b[0m  0.0003  0.3282\n",
      "     23            \u001b[36m0.9875\u001b[0m        \u001b[32m0.1590\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.7974\u001b[0m  0.0002  0.3281\n",
      "     24            0.9875        0.2102       0.3507            0.3507        \u001b[94m1.7575\u001b[0m  0.0002  0.3281\n",
      "     25            0.9875        0.1733       0.3403            0.3403        \u001b[94m1.7240\u001b[0m  0.0002  0.3142\n",
      "     26            \u001b[36m1.0000\u001b[0m        0.1676       0.3403            0.3403        \u001b[94m1.6985\u001b[0m  0.0002  0.3438\n",
      "     27            1.0000        0.1656       0.3438            0.3438        \u001b[94m1.6772\u001b[0m  0.0002  0.3750\n",
      "     28            1.0000        0.1693       0.3472            0.3472        \u001b[94m1.6588\u001b[0m  0.0001  0.3751\n",
      "     29            1.0000        \u001b[32m0.1515\u001b[0m       0.3472            0.3472        \u001b[94m1.6448\u001b[0m  0.0001  0.4223\n",
      "     30            1.0000        0.1755       0.3542            0.3542        \u001b[94m1.6342\u001b[0m  0.0001  0.3750\n",
      "     31            1.0000        0.1719       0.3542            0.3542        \u001b[94m1.6257\u001b[0m  0.0001  0.4063\n",
      "     32            1.0000        \u001b[32m0.1398\u001b[0m       0.3576            0.3576        \u001b[94m1.6191\u001b[0m  0.0001  0.4219\n",
      "     33            1.0000        \u001b[32m0.1362\u001b[0m       0.3576            0.3576        \u001b[94m1.6135\u001b[0m  0.0000  0.4163\n",
      "     34            1.0000        0.1517       0.3542            0.3542        \u001b[94m1.6090\u001b[0m  0.0000  0.3441\n",
      "     35            1.0000        0.1739       0.3576            0.3576        \u001b[94m1.6053\u001b[0m  0.0000  0.3134\n",
      "     36            1.0000        0.1478       0.3542            0.3542        \u001b[94m1.6021\u001b[0m  0.0000  0.3281\n",
      "     37            1.0000        0.1363       0.3576            0.3576        \u001b[94m1.5995\u001b[0m  0.0000  0.3286\n",
      "     38            1.0000        0.1479       0.3576            0.3576        \u001b[94m1.5969\u001b[0m  0.0000  0.3130\n",
      "     39            1.0000        \u001b[32m0.1233\u001b[0m       0.3542            0.3542        \u001b[94m1.5950\u001b[0m  0.0000  0.3596\n",
      "     40            1.0000        0.1721       0.3507            0.3507        \u001b[94m1.5935\u001b[0m  0.0000  0.3289\n",
      "Training model for subject 8 with 90 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2556\u001b[0m        \u001b[32m1.6258\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m2.3159\u001b[0m  0.0006  0.3438\n",
      "      2            0.2556        \u001b[32m1.4258\u001b[0m       0.2500            0.2500        3.2568  0.0006  0.3281\n",
      "      3            0.2556        \u001b[32m1.2289\u001b[0m       0.2500            0.2500        3.7123  0.0006  0.3438\n",
      "      4            0.2556        \u001b[32m0.9431\u001b[0m       0.2500            0.2500        3.8166  0.0006  0.3438\n",
      "      5            0.2556        \u001b[32m0.9092\u001b[0m       0.2500            0.2500        3.7025  0.0006  0.3438\n",
      "      6            \u001b[36m0.2667\u001b[0m        \u001b[32m0.7612\u001b[0m       0.2500            0.2500        3.5680  0.0006  0.3555\n",
      "      7            \u001b[36m0.2889\u001b[0m        \u001b[32m0.7279\u001b[0m       0.2535            0.2535        3.2538  0.0006  0.3510\n",
      "      8            \u001b[36m0.3667\u001b[0m        \u001b[32m0.6828\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        3.0688  0.0006  0.3532\n",
      "      9            \u001b[36m0.4000\u001b[0m        \u001b[32m0.5880\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        2.9048  0.0006  0.3384\n",
      "     10            \u001b[36m0.4667\u001b[0m        \u001b[32m0.5557\u001b[0m       0.2674            0.2674        2.7552  0.0005  0.4497\n",
      "     11            \u001b[36m0.5556\u001b[0m        0.5587       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        2.5316  0.0005  0.3463\n",
      "     12            \u001b[36m0.6444\u001b[0m        \u001b[32m0.4648\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        2.3486  0.0005  0.3393\n",
      "     13            \u001b[36m0.7000\u001b[0m        0.4811       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m2.1438\u001b[0m  0.0005  0.3281\n",
      "     14            \u001b[36m0.7444\u001b[0m        \u001b[32m0.4035\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m1.9483\u001b[0m  0.0005  0.3438\n",
      "     15            \u001b[36m0.8333\u001b[0m        0.4061       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.8124\u001b[0m  0.0004  0.3283\n",
      "     16            \u001b[36m0.9111\u001b[0m        0.4176       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.7036\u001b[0m  0.0004  0.5781\n",
      "     17            \u001b[36m0.9889\u001b[0m        \u001b[32m0.3174\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.6279\u001b[0m  0.0004  0.4047\n",
      "     18            0.9889        0.3389       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.5805\u001b[0m  0.0004  0.4303\n",
      "     19            \u001b[36m1.0000\u001b[0m        0.3456       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.5518\u001b[0m  0.0004  0.4233\n",
      "     20            1.0000        \u001b[32m0.3062\u001b[0m       0.3819            0.3819        \u001b[94m1.5401\u001b[0m  0.0003  0.3494\n",
      "     21            1.0000        \u001b[32m0.2809\u001b[0m       0.3785            0.3785        \u001b[94m1.5369\u001b[0m  0.0003  0.3669\n",
      "     22            1.0000        0.2898       0.3819            0.3819        \u001b[94m1.5285\u001b[0m  0.0003  0.3669\n",
      "     23            1.0000        0.2914       0.3924            0.3924        \u001b[94m1.5190\u001b[0m  0.0002  0.3812\n",
      "     24            1.0000        \u001b[32m0.2405\u001b[0m       0.3889            0.3889        \u001b[94m1.5117\u001b[0m  0.0002  0.3999\n",
      "     25            1.0000        \u001b[32m0.1999\u001b[0m       0.3889            0.3889        \u001b[94m1.5078\u001b[0m  0.0002  0.4166\n",
      "     26            1.0000        0.2318       0.3889            0.3889        \u001b[94m1.5053\u001b[0m  0.0002  0.4000\n",
      "     27            1.0000        0.2475       0.3889            0.3889        \u001b[94m1.5034\u001b[0m  0.0002  0.3994\n",
      "     28            1.0000        0.2265       0.3924            0.3924        \u001b[94m1.5014\u001b[0m  0.0001  0.4198\n",
      "     29            1.0000        \u001b[32m0.1903\u001b[0m       0.3924            0.3924        \u001b[94m1.4993\u001b[0m  0.0001  0.5154\n",
      "     30            1.0000        0.1970       0.3924            0.3924        \u001b[94m1.4971\u001b[0m  0.0001  0.4619\n",
      "     31            1.0000        0.2120       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4949\u001b[0m  0.0001  0.4023\n",
      "     32            1.0000        0.2021       0.3958            0.3958        \u001b[94m1.4942\u001b[0m  0.0001  0.3792\n",
      "     33            1.0000        0.2244       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.4933\u001b[0m  0.0000  0.3246\n",
      "     34            1.0000        0.2188       0.3993            0.3993        \u001b[94m1.4922\u001b[0m  0.0000  0.3661\n",
      "     35            1.0000        0.2009       0.3993            0.3993        \u001b[94m1.4911\u001b[0m  0.0000  0.3653\n",
      "     36            1.0000        0.2172       0.3958            0.3958        \u001b[94m1.4901\u001b[0m  0.0000  0.4169\n",
      "     37            1.0000        0.1960       0.3958            0.3958        \u001b[94m1.4891\u001b[0m  0.0000  0.3988\n",
      "     38            1.0000        0.1975       0.3958            0.3958        \u001b[94m1.4881\u001b[0m  0.0000  0.3502\n",
      "     39            1.0000        \u001b[32m0.1870\u001b[0m       0.3958            0.3958        \u001b[94m1.4874\u001b[0m  0.0000  0.3482\n",
      "     40            1.0000        0.1872       0.3958            0.3958        \u001b[94m1.4868\u001b[0m  0.0000  0.3726\n",
      "Training model for subject 8 with 100 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3800\u001b[0m        \u001b[32m1.5871\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m2.6181\u001b[0m  0.0006  0.3652\n",
      "      2            0.2700        \u001b[32m1.2405\u001b[0m       0.2535            0.2535        2.7337  0.0006  0.3627\n",
      "      3            0.2600        \u001b[32m1.0282\u001b[0m       0.2535            0.2535        3.0224  0.0006  0.3750\n",
      "      4            0.2500        \u001b[32m0.8957\u001b[0m       0.2500            0.2500        3.4135  0.0006  0.3593\n",
      "      5            0.2600        \u001b[32m0.7378\u001b[0m       0.2535            0.2535        3.4361  0.0006  0.3594\n",
      "      6            0.3000        0.7622       0.2535            0.2535        3.0977  0.0006  0.3444\n",
      "      7            \u001b[36m0.4300\u001b[0m        \u001b[32m0.6653\u001b[0m       0.2743            0.2743        2.6487  0.0006  0.3489\n",
      "      8            \u001b[36m0.5800\u001b[0m        \u001b[32m0.5993\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m2.3395\u001b[0m  0.0006  0.4566\n",
      "      9            \u001b[36m0.7000\u001b[0m        \u001b[32m0.5686\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m2.0754\u001b[0m  0.0006  0.4622\n",
      "     10            \u001b[36m0.7800\u001b[0m        \u001b[32m0.5169\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.9276\u001b[0m  0.0005  0.3691\n",
      "     11            \u001b[36m0.8400\u001b[0m        0.5279       0.3333            0.3333        \u001b[94m1.8086\u001b[0m  0.0005  0.3750\n",
      "     12            \u001b[36m0.8900\u001b[0m        \u001b[32m0.4814\u001b[0m       0.3333            0.3333        \u001b[94m1.7242\u001b[0m  0.0005  0.3438\n",
      "     13            \u001b[36m0.9300\u001b[0m        \u001b[32m0.4713\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.6759\u001b[0m  0.0005  0.3438\n",
      "     14            \u001b[36m0.9500\u001b[0m        \u001b[32m0.3748\u001b[0m       0.3438            0.3438        \u001b[94m1.6386\u001b[0m  0.0005  0.3594\n",
      "     15            0.9500        0.3921       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.6102\u001b[0m  0.0004  0.3594\n",
      "     16            \u001b[36m0.9800\u001b[0m        \u001b[32m0.3513\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.5910\u001b[0m  0.0004  0.3594\n",
      "     17            \u001b[36m0.9900\u001b[0m        \u001b[32m0.3025\u001b[0m       0.3611            0.3611        \u001b[94m1.5831\u001b[0m  0.0004  0.3594\n",
      "     18            \u001b[36m1.0000\u001b[0m        0.3029       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.5638\u001b[0m  0.0004  0.3754\n",
      "     19            1.0000        \u001b[32m0.2963\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.5422\u001b[0m  0.0004  0.4219\n",
      "     20            1.0000        \u001b[32m0.2724\u001b[0m       0.3819            0.3819        \u001b[94m1.5338\u001b[0m  0.0003  0.4222\n",
      "     21            1.0000        0.2889       0.3854            0.3854        \u001b[94m1.5243\u001b[0m  0.0003  0.4375\n",
      "     22            1.0000        0.2858       0.3785            0.3785        \u001b[94m1.5197\u001b[0m  0.0003  0.4063\n",
      "     23            1.0000        0.2785       0.3715            0.3715        1.5232  0.0002  0.4375\n",
      "     24            1.0000        \u001b[32m0.2353\u001b[0m       0.3715            0.3715        1.5277  0.0002  0.4375\n",
      "     25            1.0000        \u001b[32m0.1979\u001b[0m       0.3715            0.3715        1.5320  0.0002  0.4219\n",
      "     26            1.0000        0.2100       0.3681            0.3681        1.5333  0.0002  0.3597\n",
      "     27            1.0000        0.2170       0.3750            0.3750        1.5351  0.0002  0.3438\n",
      "     28            1.0000        0.2822       0.3750            0.3750        1.5364  0.0001  0.3439\n",
      "     29            1.0000        0.2277       0.3750            0.3750        1.5363  0.0001  0.3385\n",
      "     30            1.0000        \u001b[32m0.1882\u001b[0m       0.3785            0.3785        1.5362  0.0001  0.4653\n",
      "     31            1.0000        0.2147       0.3785            0.3785        1.5349  0.0001  0.3594\n",
      "     32            1.0000        0.2310       0.3785            0.3785        1.5337  0.0001  0.3594\n",
      "     33            1.0000        \u001b[32m0.1571\u001b[0m       0.3785            0.3785        1.5326  0.0000  0.4765\n",
      "     34            1.0000        0.1857       0.3785            0.3785        1.5318  0.0000  0.3629\n",
      "     35            1.0000        \u001b[32m0.1476\u001b[0m       0.3785            0.3785        1.5310  0.0000  0.3928\n",
      "     36            1.0000        0.1960       0.3785            0.3785        1.5307  0.0000  0.3600\n",
      "     37            1.0000        0.1769       0.3785            0.3785        1.5305  0.0000  0.4211\n",
      "     38            1.0000        0.2012       0.3785            0.3785        1.5301  0.0000  0.3535\n",
      "     39            1.0000        \u001b[32m0.1360\u001b[0m       0.3785            0.3785        1.5299  0.0000  0.4696\n",
      "     40            1.0000        0.1766       0.3785            0.3785        1.5297  0.0000  0.3640\n",
      "Training model for subject 8 with 110 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2909\u001b[0m        \u001b[32m1.6438\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m2.6266\u001b[0m  0.0006  0.3907\n",
      "      2            \u001b[36m0.3364\u001b[0m        \u001b[32m1.3075\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        2.6644  0.0006  0.3907\n",
      "      3            \u001b[36m0.3909\u001b[0m        \u001b[32m1.1771\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        \u001b[94m2.2335\u001b[0m  0.0006  0.3598\n",
      "      4            \u001b[36m0.4818\u001b[0m        \u001b[32m0.9441\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m1.9184\u001b[0m  0.0006  0.3594\n",
      "      5            \u001b[36m0.6091\u001b[0m        0.9602       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.7446\u001b[0m  0.0006  0.3594\n",
      "      6            \u001b[36m0.7091\u001b[0m        \u001b[32m0.8079\u001b[0m       0.3403            0.3403        \u001b[94m1.7270\u001b[0m  0.0006  0.5001\n",
      "      7            \u001b[36m0.7455\u001b[0m        \u001b[32m0.7225\u001b[0m       0.3403            0.3403        \u001b[94m1.6758\u001b[0m  0.0006  0.3596\n",
      "      8            \u001b[36m0.7909\u001b[0m        \u001b[32m0.6841\u001b[0m       0.3507            0.3507        1.7034  0.0006  0.4069\n",
      "      9            \u001b[36m0.8273\u001b[0m        \u001b[32m0.6729\u001b[0m       0.3403            0.3403        1.7100  0.0006  0.4849\n",
      "     10            \u001b[36m0.8364\u001b[0m        \u001b[32m0.6079\u001b[0m       0.3472            0.3472        1.7177  0.0005  0.3934\n",
      "     11            \u001b[36m0.8455\u001b[0m        \u001b[32m0.5452\u001b[0m       0.3542            0.3542        1.7420  0.0005  0.5861\n",
      "     12            0.8455        \u001b[32m0.5164\u001b[0m       0.3542            0.3542        1.7429  0.0005  0.4052\n",
      "     13            \u001b[36m0.8818\u001b[0m        \u001b[32m0.4488\u001b[0m       0.3542            0.3542        1.7115  0.0005  0.5958\n",
      "     14            \u001b[36m0.9091\u001b[0m        0.4497       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.6718\u001b[0m  0.0005  0.4558\n",
      "     15            \u001b[36m0.9182\u001b[0m        \u001b[32m0.4061\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.6429\u001b[0m  0.0004  0.4739\n",
      "     16            \u001b[36m0.9545\u001b[0m        \u001b[32m0.3909\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.6171\u001b[0m  0.0004  0.4532\n",
      "     17            \u001b[36m0.9818\u001b[0m        \u001b[32m0.3818\u001b[0m       0.3646            0.3646        \u001b[94m1.5872\u001b[0m  0.0004  0.5901\n",
      "     18            \u001b[36m0.9909\u001b[0m        \u001b[32m0.3798\u001b[0m       0.3715            0.3715        \u001b[94m1.5667\u001b[0m  0.0004  0.4219\n",
      "     19            0.9909        \u001b[32m0.3754\u001b[0m       0.3681            0.3681        \u001b[94m1.5444\u001b[0m  0.0004  0.3595\n",
      "     20            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2855\u001b[0m       0.3681            0.3681        \u001b[94m1.5337\u001b[0m  0.0003  0.3594\n",
      "     21            1.0000        0.3003       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.5245\u001b[0m  0.0003  0.3594\n",
      "     22            1.0000        0.3320       0.3854            0.3854        1.5279  0.0003  0.3570\n",
      "     23            1.0000        0.3605       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        1.5281  0.0002  0.3594\n",
      "     24            1.0000        0.2864       0.3889            0.3889        1.5267  0.0002  0.3600\n",
      "     25            1.0000        \u001b[32m0.2434\u001b[0m       0.3785            0.3785        \u001b[94m1.5215\u001b[0m  0.0002  0.3594\n",
      "     26            1.0000        \u001b[32m0.2379\u001b[0m       0.3854            0.3854        \u001b[94m1.5168\u001b[0m  0.0002  0.3594\n",
      "     27            1.0000        0.2753       0.3854            0.3854        \u001b[94m1.5120\u001b[0m  0.0002  0.3629\n",
      "     28            1.0000        0.2406       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.5088\u001b[0m  0.0001  0.3984\n",
      "     29            1.0000        0.2633       0.3924            0.3924        \u001b[94m1.5069\u001b[0m  0.0001  0.3962\n",
      "     30            1.0000        0.2478       0.3924            0.3924        \u001b[94m1.5050\u001b[0m  0.0001  0.3600\n",
      "     31            1.0000        \u001b[32m0.2340\u001b[0m       0.3889            0.3889        \u001b[94m1.5012\u001b[0m  0.0001  0.3594\n",
      "     32            1.0000        \u001b[32m0.1887\u001b[0m       0.3958            0.3958        \u001b[94m1.4993\u001b[0m  0.0001  0.4914\n",
      "     33            1.0000        0.1955       0.3993            0.3993        \u001b[94m1.4977\u001b[0m  0.0000  0.3927\n",
      "     34            1.0000        0.2358       0.3993            0.3993        \u001b[94m1.4966\u001b[0m  0.0000  0.3750\n",
      "     35            1.0000        0.2075       0.3993            0.3993        \u001b[94m1.4955\u001b[0m  0.0000  0.3590\n",
      "     36            1.0000        0.2044       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.4945\u001b[0m  0.0000  0.3598\n",
      "     37            1.0000        0.2363       0.4028            0.4028        \u001b[94m1.4937\u001b[0m  0.0000  0.3596\n",
      "     38            1.0000        0.2335       0.4028            0.4028        \u001b[94m1.4932\u001b[0m  0.0000  0.3594\n",
      "     39            1.0000        0.2370       0.4028            0.4028        \u001b[94m1.4927\u001b[0m  0.0000  0.3757\n",
      "     40            1.0000        \u001b[32m0.1873\u001b[0m       0.4028            0.4028        \u001b[94m1.4922\u001b[0m  0.0000  0.3598\n",
      "Training model for subject 8 with 120 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2583\u001b[0m        \u001b[32m1.6004\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.3351\u001b[0m  0.0006  0.3907\n",
      "      2            0.2583        \u001b[32m1.2402\u001b[0m       0.2500            0.2500        \u001b[94m3.1359\u001b[0m  0.0006  0.3750\n",
      "      3            \u001b[36m0.3083\u001b[0m        \u001b[32m1.0537\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m2.4254\u001b[0m  0.0006  0.4024\n",
      "      4            \u001b[36m0.4667\u001b[0m        \u001b[32m0.9979\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m2.2542\u001b[0m  0.0006  0.3887\n",
      "      5            \u001b[36m0.5083\u001b[0m        \u001b[32m0.8624\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        2.3968  0.0006  0.4589\n",
      "      6            0.5000        \u001b[32m0.7966\u001b[0m       0.3819            0.3819        2.4732  0.0006  0.4542\n",
      "      7            0.5000        \u001b[32m0.7413\u001b[0m       0.3646            0.3646        2.4335  0.0006  0.4649\n",
      "      8            0.5083        \u001b[32m0.6878\u001b[0m       0.3542            0.3542        2.2881  0.0006  0.5315\n",
      "      9            \u001b[36m0.5250\u001b[0m        \u001b[32m0.6425\u001b[0m       0.3438            0.3438        2.2728  0.0006  0.4957\n",
      "     10            \u001b[36m0.5750\u001b[0m        \u001b[32m0.5585\u001b[0m       0.3472            0.3472        \u001b[94m2.1671\u001b[0m  0.0005  0.4746\n",
      "     11            \u001b[36m0.6250\u001b[0m        \u001b[32m0.5259\u001b[0m       0.3542            0.3542        \u001b[94m2.0431\u001b[0m  0.0005  0.4219\n",
      "     12            \u001b[36m0.7000\u001b[0m        \u001b[32m0.4772\u001b[0m       0.3611            0.3611        \u001b[94m1.9376\u001b[0m  0.0005  0.4000\n",
      "     13            \u001b[36m0.7917\u001b[0m        0.5072       0.3750            0.3750        \u001b[94m1.8526\u001b[0m  0.0005  0.3857\n",
      "     14            \u001b[36m0.8917\u001b[0m        \u001b[32m0.4436\u001b[0m       0.3715            0.3715        \u001b[94m1.7616\u001b[0m  0.0005  0.3822\n",
      "     15            \u001b[36m0.9500\u001b[0m        0.4834       0.3785            0.3785        \u001b[94m1.7048\u001b[0m  0.0004  0.3875\n",
      "     16            \u001b[36m0.9583\u001b[0m        \u001b[32m0.3904\u001b[0m       0.3819            0.3819        \u001b[94m1.6795\u001b[0m  0.0004  0.3704\n",
      "     17            \u001b[36m0.9667\u001b[0m        0.3993       0.3646            0.3646        \u001b[94m1.6685\u001b[0m  0.0004  0.3764\n",
      "     18            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3308\u001b[0m       0.3611            0.3611        \u001b[94m1.6624\u001b[0m  0.0004  0.3907\n",
      "     19            \u001b[36m0.9917\u001b[0m        0.3599       0.3681            0.3681        \u001b[94m1.6610\u001b[0m  0.0004  0.5100\n",
      "     20            \u001b[36m1.0000\u001b[0m        0.4212       0.3646            0.3646        \u001b[94m1.6476\u001b[0m  0.0003  0.3779\n",
      "     21            1.0000        0.3341       0.3715            0.3715        \u001b[94m1.6392\u001b[0m  0.0003  0.3752\n",
      "     22            1.0000        \u001b[32m0.3243\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.6299\u001b[0m  0.0003  0.3594\n",
      "     23            1.0000        \u001b[32m0.3032\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.6176\u001b[0m  0.0002  0.3750\n",
      "     24            1.0000        \u001b[32m0.2931\u001b[0m       0.3924            0.3924        \u001b[94m1.6125\u001b[0m  0.0002  0.3752\n",
      "     25            1.0000        \u001b[32m0.2871\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.6028\u001b[0m  0.0002  0.4542\n",
      "     26            1.0000        \u001b[32m0.2341\u001b[0m       0.3993            0.3993        \u001b[94m1.5957\u001b[0m  0.0002  0.3944\n",
      "     27            1.0000        0.2430       0.3993            0.3993        \u001b[94m1.5940\u001b[0m  0.0002  0.3943\n",
      "     28            1.0000        0.2602       0.3958            0.3958        1.5949  0.0001  0.3822\n",
      "     29            1.0000        0.2445       0.3958            0.3958        1.5966  0.0001  0.3877\n",
      "     30            1.0000        0.3395       0.3924            0.3924        1.5981  0.0001  0.4991\n",
      "     31            1.0000        0.2900       0.3958            0.3958        1.5984  0.0001  0.3769\n",
      "     32            1.0000        \u001b[32m0.2030\u001b[0m       0.3958            0.3958        1.5965  0.0001  0.5504\n",
      "     33            1.0000        0.2675       0.3924            0.3924        \u001b[94m1.5930\u001b[0m  0.0000  0.3907\n",
      "     34            1.0000        0.2453       0.3924            0.3924        \u001b[94m1.5900\u001b[0m  0.0000  0.3755\n",
      "     35            1.0000        0.2564       0.3889            0.3889        \u001b[94m1.5871\u001b[0m  0.0000  0.3909\n",
      "     36            1.0000        0.2822       0.3924            0.3924        \u001b[94m1.5849\u001b[0m  0.0000  0.4067\n",
      "     37            1.0000        0.2170       0.3924            0.3924        \u001b[94m1.5827\u001b[0m  0.0000  0.4375\n",
      "     38            1.0000        \u001b[32m0.1987\u001b[0m       0.3924            0.3924        \u001b[94m1.5814\u001b[0m  0.0000  0.4531\n",
      "     39            1.0000        0.2541       0.3924            0.3924        \u001b[94m1.5802\u001b[0m  0.0000  0.4532\n",
      "     40            1.0000        0.2876       0.3924            0.3924        \u001b[94m1.5794\u001b[0m  0.0000  0.5550\n",
      "Training model for subject 8 with 130 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2462\u001b[0m        \u001b[32m1.7633\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m2.9285\u001b[0m  0.0006  0.5023\n",
      "      2            0.2462        \u001b[32m1.4391\u001b[0m       0.2500            0.2500        3.5217  0.0006  0.5000\n",
      "      3            0.2462        \u001b[32m1.1062\u001b[0m       0.2500            0.2500        3.6337  0.0006  0.5452\n",
      "      4            0.2462        1.1485       0.2500            0.2500        3.6461  0.0006  0.4082\n",
      "      5            \u001b[36m0.2538\u001b[0m        \u001b[32m0.9526\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        3.6870  0.0006  0.4244\n",
      "      6            \u001b[36m0.3154\u001b[0m        \u001b[32m0.9124\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        3.5076  0.0006  0.3953\n",
      "      7            \u001b[36m0.3615\u001b[0m        \u001b[32m0.8820\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        3.2123  0.0006  0.3952\n",
      "      8            \u001b[36m0.4462\u001b[0m        \u001b[32m0.8244\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        2.9834  0.0006  0.3906\n",
      "      9            \u001b[36m0.5154\u001b[0m        \u001b[32m0.7163\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m2.7513\u001b[0m  0.0006  0.3750\n",
      "     10            \u001b[36m0.5385\u001b[0m        \u001b[32m0.7127\u001b[0m       0.3403            0.3403        \u001b[94m2.5507\u001b[0m  0.0005  0.3754\n",
      "     11            \u001b[36m0.5769\u001b[0m        \u001b[32m0.5540\u001b[0m       0.3368            0.3368        \u001b[94m2.3688\u001b[0m  0.0005  0.3907\n",
      "     12            \u001b[36m0.6077\u001b[0m        0.5979       0.3264            0.3264        \u001b[94m2.2350\u001b[0m  0.0005  0.3906\n",
      "     13            \u001b[36m0.6462\u001b[0m        0.6054       0.3264            0.3264        \u001b[94m2.1216\u001b[0m  0.0005  0.3906\n",
      "     14            \u001b[36m0.7231\u001b[0m        0.5696       0.3368            0.3368        \u001b[94m2.0193\u001b[0m  0.0005  0.3906\n",
      "     15            \u001b[36m0.7769\u001b[0m        \u001b[32m0.4922\u001b[0m       0.3368            0.3368        \u001b[94m1.9418\u001b[0m  0.0004  0.3820\n",
      "     16            \u001b[36m0.8231\u001b[0m        \u001b[32m0.4598\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.8781\u001b[0m  0.0004  0.6238\n",
      "     17            \u001b[36m0.8615\u001b[0m        \u001b[32m0.4435\u001b[0m       0.3576            0.3576        \u001b[94m1.8064\u001b[0m  0.0004  0.6255\n",
      "     18            \u001b[36m0.9077\u001b[0m        \u001b[32m0.4084\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.7414\u001b[0m  0.0004  0.6504\n",
      "     19            \u001b[36m0.9462\u001b[0m        \u001b[32m0.3926\u001b[0m       0.3611            0.3611        \u001b[94m1.6735\u001b[0m  0.0004  0.5167\n",
      "     20            \u001b[36m0.9769\u001b[0m        0.3965       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.6182\u001b[0m  0.0003  0.4989\n",
      "     21            \u001b[36m0.9846\u001b[0m        \u001b[32m0.3840\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.5753\u001b[0m  0.0003  0.4883\n",
      "     22            0.9846        \u001b[32m0.3837\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.5440\u001b[0m  0.0003  0.3906\n",
      "     23            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3347\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.5230\u001b[0m  0.0002  0.3981\n",
      "     24            1.0000        0.3931       0.3750            0.3750        \u001b[94m1.5084\u001b[0m  0.0002  0.4063\n",
      "     25            1.0000        \u001b[32m0.3074\u001b[0m       0.3750            0.3750        \u001b[94m1.4961\u001b[0m  0.0002  0.4605\n",
      "     26            1.0000        0.3249       0.3785            0.3785        \u001b[94m1.4837\u001b[0m  0.0002  0.4688\n",
      "     27            1.0000        \u001b[32m0.2949\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.4737\u001b[0m  0.0002  0.5002\n",
      "     28            1.0000        \u001b[32m0.2832\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.4672\u001b[0m  0.0001  0.4532\n",
      "     29            1.0000        0.2854       0.3889            0.3889        \u001b[94m1.4631\u001b[0m  0.0001  0.4689\n",
      "     30            1.0000        0.3045       0.3889            0.3889        \u001b[94m1.4594\u001b[0m  0.0001  0.4844\n",
      "     31            1.0000        0.3321       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4570\u001b[0m  0.0001  0.4380\n",
      "     32            1.0000        0.2889       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.4550\u001b[0m  0.0001  0.3907\n",
      "     33            1.0000        0.2997       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.4539\u001b[0m  0.0000  0.3755\n",
      "     34            1.0000        0.3284       0.4028            0.4028        \u001b[94m1.4526\u001b[0m  0.0000  0.4003\n",
      "     35            1.0000        \u001b[32m0.2783\u001b[0m       0.4028            0.4028        \u001b[94m1.4514\u001b[0m  0.0000  0.3863\n",
      "     36            1.0000        \u001b[32m0.2618\u001b[0m       0.4028            0.4028        \u001b[94m1.4503\u001b[0m  0.0000  0.4474\n",
      "     37            1.0000        0.2645       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.4489\u001b[0m  0.0000  0.5140\n",
      "     38            1.0000        0.2952       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.4478\u001b[0m  0.0000  0.4063\n",
      "     39            1.0000        0.2680       0.4062            0.4062        \u001b[94m1.4468\u001b[0m  0.0000  0.3906\n",
      "     40            1.0000        0.2853       0.4062            0.4062        \u001b[94m1.4461\u001b[0m  0.0000  0.4228\n",
      "Training model for subject 8 with 140 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2786\u001b[0m        \u001b[32m1.7297\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m1.8125\u001b[0m  0.0006  0.3933\n",
      "      2            \u001b[36m0.3857\u001b[0m        \u001b[32m1.3807\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m1.6781\u001b[0m  0.0006  0.4466\n",
      "      3            \u001b[36m0.4000\u001b[0m        \u001b[32m1.2059\u001b[0m       0.2847            0.2847        1.7109  0.0006  0.4665\n",
      "      4            0.3786        \u001b[32m1.0873\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        2.0419  0.0006  0.3906\n",
      "      5            0.3857        \u001b[32m0.9149\u001b[0m       0.3021            0.3021        2.4590  0.0006  0.3779\n",
      "      6            0.3929        \u001b[32m0.8451\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        2.6421  0.0006  0.3925\n",
      "      7            \u001b[36m0.4357\u001b[0m        0.8625       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        2.7574  0.0006  0.4033\n",
      "      8            \u001b[36m0.4500\u001b[0m        \u001b[32m0.7614\u001b[0m       0.3472            0.3472        2.6903  0.0006  0.3907\n",
      "      9            \u001b[36m0.4857\u001b[0m        \u001b[32m0.7405\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        2.5048  0.0006  0.4063\n",
      "     10            \u001b[36m0.5500\u001b[0m        \u001b[32m0.6933\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        2.2608  0.0005  0.3751\n",
      "     11            \u001b[36m0.6357\u001b[0m        \u001b[32m0.6281\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        2.0594  0.0005  0.3926\n",
      "     12            \u001b[36m0.7071\u001b[0m        \u001b[32m0.5760\u001b[0m       0.3646            0.3646        1.9133  0.0005  0.3937\n",
      "     13            \u001b[36m0.7571\u001b[0m        \u001b[32m0.5120\u001b[0m       0.3715            0.3715        1.7653  0.0005  0.4397\n",
      "     14            \u001b[36m0.8571\u001b[0m        \u001b[32m0.5090\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.6424\u001b[0m  0.0005  0.4229\n",
      "     15            \u001b[36m0.9143\u001b[0m        \u001b[32m0.4957\u001b[0m       0.3819            0.3819        \u001b[94m1.5755\u001b[0m  0.0004  0.3925\n",
      "     16            \u001b[36m0.9357\u001b[0m        \u001b[32m0.4606\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.5474\u001b[0m  0.0004  0.4554\n",
      "     17            0.9286        \u001b[32m0.4204\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.5455\u001b[0m  0.0004  0.4862\n",
      "     18            \u001b[36m0.9429\u001b[0m        0.4432       0.3924            0.3924        \u001b[94m1.5366\u001b[0m  0.0004  0.4561\n",
      "     19            0.9429        \u001b[32m0.3795\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        1.5371  0.0004  0.4545\n",
      "     20            \u001b[36m0.9500\u001b[0m        \u001b[32m0.3762\u001b[0m       0.3958            0.3958        \u001b[94m1.5364\u001b[0m  0.0003  0.4310\n",
      "     21            \u001b[36m0.9643\u001b[0m        \u001b[32m0.3358\u001b[0m       0.3854            0.3854        1.5409  0.0003  0.4532\n",
      "     22            \u001b[36m0.9714\u001b[0m        0.3818       0.3889            0.3889        \u001b[94m1.5312\u001b[0m  0.0003  0.4421\n",
      "     23            \u001b[36m0.9786\u001b[0m        \u001b[32m0.3314\u001b[0m       0.3889            0.3889        \u001b[94m1.5138\u001b[0m  0.0002  0.4402\n",
      "     24            \u001b[36m0.9929\u001b[0m        0.3321       0.3889            0.3889        \u001b[94m1.4967\u001b[0m  0.0002  0.4067\n",
      "     25            0.9929        \u001b[32m0.2980\u001b[0m       0.3924            0.3924        \u001b[94m1.4826\u001b[0m  0.0002  0.3911\n",
      "     26            0.9929        \u001b[32m0.2610\u001b[0m       0.3993            0.3993        \u001b[94m1.4723\u001b[0m  0.0002  0.3775\n",
      "     27            0.9929        0.3025       0.3993            0.3993        \u001b[94m1.4602\u001b[0m  0.0002  0.3929\n",
      "     28            0.9929        0.3249       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.4487\u001b[0m  0.0001  0.4079\n",
      "     29            0.9929        0.2649       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.4400\u001b[0m  0.0001  0.4326\n",
      "     30            0.9929        0.3106       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.4329\u001b[0m  0.0001  0.4172\n",
      "     31            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2510\u001b[0m       0.4201            0.4201        \u001b[94m1.4261\u001b[0m  0.0001  0.4998\n",
      "     32            1.0000        0.2953       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.4208\u001b[0m  0.0001  0.4174\n",
      "     33            1.0000        0.2771       0.4306            0.4306        \u001b[94m1.4171\u001b[0m  0.0000  0.4095\n",
      "     34            1.0000        0.3094       0.4271            0.4271        \u001b[94m1.4148\u001b[0m  0.0000  0.3863\n",
      "     35            1.0000        0.2523       0.4271            0.4271        \u001b[94m1.4139\u001b[0m  0.0000  0.4082\n",
      "     36            1.0000        0.3106       0.4271            0.4271        \u001b[94m1.4116\u001b[0m  0.0000  0.3923\n",
      "     37            1.0000        \u001b[32m0.2480\u001b[0m       0.4271            0.4271        \u001b[94m1.4096\u001b[0m  0.0000  0.4083\n",
      "     38            1.0000        0.2933       0.4306            0.4306        \u001b[94m1.4083\u001b[0m  0.0000  0.3923\n",
      "     39            1.0000        0.2593       0.4306            0.4306        \u001b[94m1.4072\u001b[0m  0.0000  0.3908\n",
      "     40            1.0000        0.3232       0.4306            0.4306        \u001b[94m1.4063\u001b[0m  0.0000  0.3906\n",
      "Training model for subject 8 with 150 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2800\u001b[0m        \u001b[32m1.7612\u001b[0m       \u001b[35m0.2465\u001b[0m            \u001b[31m0.2465\u001b[0m        \u001b[94m3.2391\u001b[0m  0.0006  0.5648\n",
      "      2            \u001b[36m0.3467\u001b[0m        \u001b[32m1.4443\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m2.5391\u001b[0m  0.0006  0.3938\n",
      "      3            \u001b[36m0.4267\u001b[0m        \u001b[32m1.1975\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m2.3747\u001b[0m  0.0006  0.4162\n",
      "      4            0.3933        \u001b[32m1.0692\u001b[0m       0.3229            0.3229        2.7353  0.0006  0.4180\n",
      "      5            0.3133        \u001b[32m0.9530\u001b[0m       0.2639            0.2639        3.3077  0.0006  0.4335\n",
      "      6            0.2867        1.0101       0.2604            0.2604        3.6874  0.0006  0.4182\n",
      "      7            0.2933        \u001b[32m0.9028\u001b[0m       0.2674            0.2674        3.4122  0.0006  0.4715\n",
      "      8            0.3067        \u001b[32m0.7625\u001b[0m       0.2778            0.2778        3.0884  0.0006  0.4862\n",
      "      9            0.3467        0.7776       0.2812            0.2812        2.8690  0.0006  0.4710\n",
      "     10            0.3867        0.7659       0.2951            0.2951        2.5445  0.0005  0.4710\n",
      "     11            0.4267        \u001b[32m0.7087\u001b[0m       0.2917            0.2917        \u001b[94m2.2589\u001b[0m  0.0005  0.5024\n",
      "     12            \u001b[36m0.4533\u001b[0m        \u001b[32m0.6640\u001b[0m       0.3056            0.3056        \u001b[94m2.0915\u001b[0m  0.0005  0.5112\n",
      "     13            \u001b[36m0.5400\u001b[0m        \u001b[32m0.5963\u001b[0m       0.3125            0.3125        \u001b[94m1.9508\u001b[0m  0.0005  0.4483\n",
      "     14            \u001b[36m0.6200\u001b[0m        \u001b[32m0.4720\u001b[0m       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.8157\u001b[0m  0.0005  0.5355\n",
      "     15            \u001b[36m0.7267\u001b[0m        0.4991       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.7311\u001b[0m  0.0004  0.3979\n",
      "     16            \u001b[36m0.8067\u001b[0m        0.5195       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.6541\u001b[0m  0.0004  0.5263\n",
      "     17            \u001b[36m0.8933\u001b[0m        0.4936       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.5618\u001b[0m  0.0004  0.3766\n",
      "     18            \u001b[36m0.9400\u001b[0m        \u001b[32m0.4609\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.5095\u001b[0m  0.0004  0.3765\n",
      "     19            \u001b[36m0.9533\u001b[0m        \u001b[32m0.4459\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.4724\u001b[0m  0.0004  0.4276\n",
      "     20            \u001b[36m0.9600\u001b[0m        \u001b[32m0.4392\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.4441\u001b[0m  0.0003  0.4340\n",
      "     21            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3928\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.4195\u001b[0m  0.0003  0.4374\n",
      "     22            \u001b[36m0.9800\u001b[0m        \u001b[32m0.3797\u001b[0m       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.3985\u001b[0m  0.0003  0.4670\n",
      "     23            \u001b[36m0.9933\u001b[0m        0.3905       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.3844\u001b[0m  0.0002  0.4083\n",
      "     24            0.9933        0.4514       0.4271            0.4271        \u001b[94m1.3721\u001b[0m  0.0002  0.3965\n",
      "     25            0.9933        \u001b[32m0.3604\u001b[0m       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.3668\u001b[0m  0.0002  0.3877\n",
      "     26            0.9933        \u001b[32m0.3274\u001b[0m       0.4306            0.4306        \u001b[94m1.3647\u001b[0m  0.0002  0.4250\n",
      "     27            0.9933        0.3593       0.4375            0.4375        \u001b[94m1.3594\u001b[0m  0.0002  0.4268\n",
      "     28            0.9933        \u001b[32m0.2837\u001b[0m       0.4375            0.4375        \u001b[94m1.3527\u001b[0m  0.0001  0.3920\n",
      "     29            0.9933        0.3205       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.3485\u001b[0m  0.0001  0.3906\n",
      "     30            0.9933        0.3247       0.4410            0.4410        \u001b[94m1.3460\u001b[0m  0.0001  0.3938\n",
      "     31            0.9933        0.3178       0.4375            0.4375        1.3470  0.0001  0.3772\n",
      "     32            0.9933        0.3064       0.4375            0.4375        1.3483  0.0001  0.3770\n",
      "     33            0.9933        \u001b[32m0.2761\u001b[0m       0.4410            0.4410        1.3492  0.0000  0.3925\n",
      "     34            0.9933        0.2882       0.4410            0.4410        1.3487  0.0000  0.3926\n",
      "     35            0.9933        0.3728       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        1.3477  0.0000  0.3946\n",
      "     36            0.9933        0.3686       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        1.3473  0.0000  0.5012\n",
      "     37            0.9933        0.3155       0.4444            0.4444        1.3473  0.0000  0.5159\n",
      "     38            0.9933        0.2917       0.4479            0.4479        1.3467  0.0000  0.4618\n",
      "     39            0.9933        0.3339       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.3460\u001b[0m  0.0000  0.4397\n",
      "     40            0.9933        0.2900       0.4479            0.4479        \u001b[94m1.3455\u001b[0m  0.0000  0.4562\n",
      "Training model for subject 8 with 160 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3250\u001b[0m        \u001b[32m1.5984\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m2.6704\u001b[0m  0.0006  0.5030\n",
      "      2            \u001b[36m0.4313\u001b[0m        \u001b[32m1.3319\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        2.7891  0.0006  0.4063\n",
      "      3            \u001b[36m0.4562\u001b[0m        \u001b[32m1.1717\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        2.6729  0.0006  0.3755\n",
      "      4            0.4437        \u001b[32m1.0981\u001b[0m       0.3576            0.3576        \u001b[94m2.5518\u001b[0m  0.0006  0.3915\n",
      "      5            \u001b[36m0.4625\u001b[0m        \u001b[32m0.9443\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m2.4965\u001b[0m  0.0006  0.3747\n",
      "      6            0.4313        0.9444       0.3403            0.3403        2.5996  0.0006  0.3786\n",
      "      7            0.4188        \u001b[32m0.8979\u001b[0m       0.3299            0.3299        2.6932  0.0006  0.3915\n",
      "      8            0.3937        \u001b[32m0.8778\u001b[0m       0.3264            0.3264        2.8807  0.0006  0.3767\n",
      "      9            0.3875        \u001b[32m0.8506\u001b[0m       0.3229            0.3229        2.9407  0.0006  0.3923\n",
      "     10            0.3875        \u001b[32m0.6442\u001b[0m       0.3229            0.3229        2.9082  0.0005  0.3922\n",
      "     11            0.4062        0.6788       0.3299            0.3299        2.7894  0.0005  0.3835\n",
      "     12            0.4125        0.6643       0.3333            0.3333        2.5878  0.0005  0.4074\n",
      "     13            \u001b[36m0.4875\u001b[0m        \u001b[32m0.6389\u001b[0m       0.3472            0.3472        \u001b[94m2.2654\u001b[0m  0.0005  0.3993\n",
      "     14            \u001b[36m0.5563\u001b[0m        \u001b[32m0.6090\u001b[0m       0.3507            0.3507        \u001b[94m2.0066\u001b[0m  0.0005  0.3797\n",
      "     15            \u001b[36m0.7063\u001b[0m        \u001b[32m0.5446\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.8204\u001b[0m  0.0004  0.3938\n",
      "     16            \u001b[36m0.8000\u001b[0m        \u001b[32m0.5031\u001b[0m       0.3715            0.3715        \u001b[94m1.7228\u001b[0m  0.0004  0.3967\n",
      "     17            \u001b[36m0.8375\u001b[0m        0.5207       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.6522\u001b[0m  0.0004  0.3890\n",
      "     18            \u001b[36m0.8938\u001b[0m        \u001b[32m0.4953\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.6029\u001b[0m  0.0004  0.3915\n",
      "     19            \u001b[36m0.9187\u001b[0m        0.5000       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.5649\u001b[0m  0.0004  0.4802\n",
      "     20            \u001b[36m0.9313\u001b[0m        \u001b[32m0.4731\u001b[0m       0.3924            0.3924        \u001b[94m1.5458\u001b[0m  0.0003  0.3886\n",
      "     21            \u001b[36m0.9437\u001b[0m        \u001b[32m0.3981\u001b[0m       0.3785            0.3785        \u001b[94m1.5239\u001b[0m  0.0003  0.3755\n",
      "     22            0.9437        0.4284       0.3889            0.3889        \u001b[94m1.5001\u001b[0m  0.0003  0.3940\n",
      "     23            \u001b[36m0.9563\u001b[0m        0.4308       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.4850\u001b[0m  0.0002  0.3924\n",
      "     24            \u001b[36m0.9625\u001b[0m        0.4374       0.4028            0.4028        \u001b[94m1.4783\u001b[0m  0.0002  0.3770\n",
      "     25            \u001b[36m0.9688\u001b[0m        0.4402       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.4716\u001b[0m  0.0002  0.3934\n",
      "     26            0.9688        0.4203       0.4028            0.4028        \u001b[94m1.4630\u001b[0m  0.0002  0.3918\n",
      "     27            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3167\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.4586\u001b[0m  0.0002  0.4864\n",
      "     28            0.9750        0.3812       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.4543\u001b[0m  0.0001  0.5340\n",
      "     29            0.9750        0.3789       0.4097            0.4097        \u001b[94m1.4518\u001b[0m  0.0001  0.6764\n",
      "     30            \u001b[36m0.9812\u001b[0m        0.3250       0.4097            0.4097        \u001b[94m1.4484\u001b[0m  0.0001  0.4936\n",
      "     31            0.9750        0.4035       0.3993            0.3993        \u001b[94m1.4439\u001b[0m  0.0001  0.5677\n",
      "     32            0.9750        0.3912       0.3993            0.3993        \u001b[94m1.4414\u001b[0m  0.0001  0.4532\n",
      "     33            0.9812        0.3403       0.4028            0.4028        \u001b[94m1.4392\u001b[0m  0.0000  0.3750\n",
      "     34            0.9812        0.3380       0.4062            0.4062        \u001b[94m1.4367\u001b[0m  0.0000  0.3804\n",
      "     35            0.9812        0.3751       0.4132            0.4132        \u001b[94m1.4339\u001b[0m  0.0000  0.3766\n",
      "     36            0.9812        0.3210       0.4132            0.4132        \u001b[94m1.4323\u001b[0m  0.0000  0.3936\n",
      "     37            0.9812        0.3610       0.4097            0.4097        \u001b[94m1.4305\u001b[0m  0.0000  0.3764\n",
      "     38            0.9812        0.3245       0.4062            0.4062        \u001b[94m1.4280\u001b[0m  0.0000  0.4134\n",
      "     39            0.9812        0.3697       0.4062            0.4062        \u001b[94m1.4271\u001b[0m  0.0000  0.4261\n",
      "     40            0.9812        0.3468       0.4097            0.4097        \u001b[94m1.4259\u001b[0m  0.0000  0.4121\n",
      "Training model for subject 8 with 170 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2294\u001b[0m        \u001b[32m1.7537\u001b[0m       \u001b[35m0.2431\u001b[0m            \u001b[31m0.2431\u001b[0m        \u001b[94m1.8917\u001b[0m  0.0006  0.4167\n",
      "      2            \u001b[36m0.3647\u001b[0m        \u001b[32m1.3594\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        2.2696  0.0006  0.4168\n",
      "      3            \u001b[36m0.3824\u001b[0m        \u001b[32m1.2993\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        3.0708  0.0006  0.3857\n",
      "      4            0.2706        \u001b[32m1.2116\u001b[0m       0.2465            0.2465        3.4837  0.0006  0.3903\n",
      "      5            0.2471        \u001b[32m1.0981\u001b[0m       0.2500            0.2500        3.9665  0.0006  0.3791\n",
      "      6            0.2471        \u001b[32m1.0230\u001b[0m       0.2500            0.2500        4.1402  0.0006  0.5685\n",
      "      7            0.2471        \u001b[32m0.9476\u001b[0m       0.2500            0.2500        4.0430  0.0006  0.4626\n",
      "      8            0.2471        \u001b[32m0.7983\u001b[0m       0.2535            0.2535        3.7426  0.0006  0.3907\n",
      "      9            0.2529        \u001b[32m0.6800\u001b[0m       0.2569            0.2569        3.4574  0.0006  0.3752\n",
      "     10            0.2765        0.7044       0.2569            0.2569        2.9258  0.0005  0.4219\n",
      "     11            0.3294        0.7057       0.2778            0.2778        2.4434  0.0005  0.3906\n",
      "     12            \u001b[36m0.4882\u001b[0m        0.7191       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        2.0433  0.0005  0.3750\n",
      "     13            \u001b[36m0.6176\u001b[0m        0.6937       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m1.8402\u001b[0m  0.0005  0.4058\n",
      "     14            \u001b[36m0.7000\u001b[0m        \u001b[32m0.6626\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.6917\u001b[0m  0.0005  0.4190\n",
      "     15            \u001b[36m0.7471\u001b[0m        \u001b[32m0.5730\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.5980\u001b[0m  0.0004  0.5066\n",
      "     16            \u001b[36m0.7882\u001b[0m        \u001b[32m0.5444\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.5169\u001b[0m  0.0004  0.4606\n",
      "     17            \u001b[36m0.8647\u001b[0m        0.5680       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.4665\u001b[0m  0.0004  0.5007\n",
      "     18            \u001b[36m0.8941\u001b[0m        \u001b[32m0.4667\u001b[0m       0.3889            0.3889        \u001b[94m1.4296\u001b[0m  0.0004  0.4839\n",
      "     19            \u001b[36m0.9118\u001b[0m        0.4944       0.3854            0.3854        \u001b[94m1.3948\u001b[0m  0.0004  0.4667\n",
      "     20            0.9118        \u001b[32m0.4634\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.3775\u001b[0m  0.0003  0.5273\n",
      "     21            \u001b[36m0.9235\u001b[0m        0.4638       0.4028            0.4028        \u001b[94m1.3661\u001b[0m  0.0003  0.4406\n",
      "     22            0.9235        \u001b[32m0.3769\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.3599\u001b[0m  0.0003  0.4664\n",
      "     23            \u001b[36m0.9294\u001b[0m        0.4128       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.3559\u001b[0m  0.0002  0.4855\n",
      "     24            \u001b[36m0.9353\u001b[0m        0.4185       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.3543\u001b[0m  0.0002  0.4167\n",
      "     25            \u001b[36m0.9412\u001b[0m        0.3843       0.4271            0.4271        \u001b[94m1.3436\u001b[0m  0.0002  0.4404\n",
      "     26            \u001b[36m0.9529\u001b[0m        0.4237       0.4236            0.4236        \u001b[94m1.3289\u001b[0m  0.0002  0.4144\n",
      "     27            \u001b[36m0.9588\u001b[0m        0.4278       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.3172\u001b[0m  0.0002  0.4177\n",
      "     28            \u001b[36m0.9647\u001b[0m        0.4039       0.4410            0.4410        \u001b[94m1.3094\u001b[0m  0.0001  0.4001\n",
      "     29            0.9647        0.4061       0.4444            0.4444        \u001b[94m1.3036\u001b[0m  0.0001  0.4001\n",
      "     30            0.9647        0.3788       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.2974\u001b[0m  0.0001  0.3827\n",
      "     31            0.9647        0.3879       0.4444            0.4444        \u001b[94m1.2925\u001b[0m  0.0001  0.3982\n",
      "     32            \u001b[36m0.9765\u001b[0m        \u001b[32m0.3751\u001b[0m       0.4444            0.4444        \u001b[94m1.2897\u001b[0m  0.0001  0.4508\n",
      "     33            \u001b[36m0.9824\u001b[0m        \u001b[32m0.3547\u001b[0m       0.4444            0.4444        \u001b[94m1.2877\u001b[0m  0.0000  0.4047\n",
      "     34            0.9824        0.4619       0.4479            0.4479        \u001b[94m1.2852\u001b[0m  0.0000  0.3970\n",
      "     35            0.9824        \u001b[32m0.2991\u001b[0m       0.4514            0.4514        \u001b[94m1.2834\u001b[0m  0.0000  0.4886\n",
      "     36            0.9824        0.3988       0.4514            0.4514        \u001b[94m1.2814\u001b[0m  0.0000  0.4466\n",
      "     37            0.9824        0.3351       0.4514            0.4514        \u001b[94m1.2803\u001b[0m  0.0000  0.4253\n",
      "     38            0.9824        0.3635       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.2797\u001b[0m  0.0000  0.4356\n",
      "     39            0.9824        0.3546       0.4549            0.4549        \u001b[94m1.2792\u001b[0m  0.0000  0.4155\n",
      "     40            0.9824        0.3261       0.4549            0.4549        \u001b[94m1.2791\u001b[0m  0.0000  0.4159\n",
      "Training model for subject 8 with 180 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.7654\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m9.8046\u001b[0m  0.0006  0.4036\n",
      "      2            0.2500        \u001b[32m1.4663\u001b[0m       0.2500            0.2500        \u001b[94m7.1239\u001b[0m  0.0006  0.4682\n",
      "      3            0.2500        \u001b[32m1.3090\u001b[0m       0.2500            0.2500        \u001b[94m5.3218\u001b[0m  0.0006  0.4679\n",
      "      4            \u001b[36m0.3889\u001b[0m        \u001b[32m1.1667\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m3.8668\u001b[0m  0.0006  0.4890\n",
      "      5            \u001b[36m0.4000\u001b[0m        \u001b[32m1.0241\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m3.4709\u001b[0m  0.0006  0.4326\n",
      "      6            0.4000        \u001b[32m1.0091\u001b[0m       0.2951            0.2951        \u001b[94m3.1726\u001b[0m  0.0006  0.5277\n",
      "      7            \u001b[36m0.4222\u001b[0m        \u001b[32m0.9128\u001b[0m       0.2951            0.2951        \u001b[94m2.7994\u001b[0m  0.0006  0.4795\n",
      "      8            0.4111        \u001b[32m0.8494\u001b[0m       0.2778            0.2778        \u001b[94m2.6143\u001b[0m  0.0006  0.5212\n",
      "      9            0.3944        \u001b[32m0.7610\u001b[0m       0.2743            0.2743        \u001b[94m2.4887\u001b[0m  0.0006  0.4330\n",
      "     10            \u001b[36m0.4444\u001b[0m        0.7986       0.2847            0.2847        \u001b[94m2.3038\u001b[0m  0.0005  0.3999\n",
      "     11            \u001b[36m0.4889\u001b[0m        \u001b[32m0.6923\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m2.1202\u001b[0m  0.0005  0.4172\n",
      "     12            \u001b[36m0.5667\u001b[0m        0.7254       0.3056            0.3056        \u001b[94m1.9852\u001b[0m  0.0005  0.4159\n",
      "     13            \u001b[36m0.6333\u001b[0m        0.7943       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m1.7995\u001b[0m  0.0005  0.4366\n",
      "     14            \u001b[36m0.6944\u001b[0m        \u001b[32m0.6307\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.6419\u001b[0m  0.0005  0.4170\n",
      "     15            \u001b[36m0.7611\u001b[0m        \u001b[32m0.6010\u001b[0m       0.3507            0.3507        \u001b[94m1.5174\u001b[0m  0.0004  0.4146\n",
      "     16            \u001b[36m0.8111\u001b[0m        \u001b[32m0.5792\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.4354\u001b[0m  0.0004  0.4158\n",
      "     17            \u001b[36m0.8444\u001b[0m        \u001b[32m0.5751\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.3772\u001b[0m  0.0004  0.4171\n",
      "     18            \u001b[36m0.8722\u001b[0m        0.6024       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.3402\u001b[0m  0.0004  0.4310\n",
      "     19            \u001b[36m0.8889\u001b[0m        \u001b[32m0.5589\u001b[0m       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.3146\u001b[0m  0.0004  0.4175\n",
      "     20            \u001b[36m0.9056\u001b[0m        0.5832       0.4167            0.4167        \u001b[94m1.2935\u001b[0m  0.0003  0.4176\n",
      "     21            \u001b[36m0.9278\u001b[0m        \u001b[32m0.5316\u001b[0m       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.2843\u001b[0m  0.0003  0.3869\n",
      "     22            \u001b[36m0.9389\u001b[0m        \u001b[32m0.4806\u001b[0m       0.4306            0.4306        \u001b[94m1.2803\u001b[0m  0.0003  0.4317\n",
      "     23            0.9333        \u001b[32m0.4591\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.2717\u001b[0m  0.0002  0.4181\n",
      "     24            0.9389        0.4872       0.4479            0.4479        \u001b[94m1.2663\u001b[0m  0.0002  0.4169\n",
      "     25            \u001b[36m0.9444\u001b[0m        0.5093       0.4410            0.4410        \u001b[94m1.2615\u001b[0m  0.0002  0.4170\n",
      "     26            0.9444        0.4737       0.4479            0.4479        \u001b[94m1.2569\u001b[0m  0.0002  0.4175\n",
      "     27            \u001b[36m0.9500\u001b[0m        0.4853       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.2521\u001b[0m  0.0002  0.4148\n",
      "     28            0.9500        0.5151       0.4514            0.4514        \u001b[94m1.2487\u001b[0m  0.0001  0.4129\n",
      "     29            \u001b[36m0.9556\u001b[0m        \u001b[32m0.4086\u001b[0m       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.2451\u001b[0m  0.0001  0.4160\n",
      "     30            0.9556        0.4764       0.4514            0.4514        \u001b[94m1.2396\u001b[0m  0.0001  0.3998\n",
      "     31            \u001b[36m0.9611\u001b[0m        0.4365       0.4549            0.4549        \u001b[94m1.2350\u001b[0m  0.0001  0.4706\n",
      "     32            \u001b[36m0.9667\u001b[0m        0.4215       0.4514            0.4514        \u001b[94m1.2305\u001b[0m  0.0001  0.5331\n",
      "     33            0.9667        0.4341       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2269\u001b[0m  0.0000  0.5308\n",
      "     34            0.9667        0.4331       0.4618            0.4618        \u001b[94m1.2245\u001b[0m  0.0000  0.6967\n",
      "     35            0.9611        \u001b[32m0.3863\u001b[0m       0.4618            0.4618        \u001b[94m1.2229\u001b[0m  0.0000  0.5688\n",
      "     36            0.9611        0.5014       0.4583            0.4583        \u001b[94m1.2218\u001b[0m  0.0000  0.4741\n",
      "     37            0.9611        0.4475       0.4583            0.4583        \u001b[94m1.2211\u001b[0m  0.0000  0.4157\n",
      "     38            0.9611        0.4167       0.4618            0.4618        \u001b[94m1.2204\u001b[0m  0.0000  0.4362\n",
      "     39            0.9611        0.4214       0.4618            0.4618        \u001b[94m1.2203\u001b[0m  0.0000  0.4246\n",
      "     40            0.9611        0.3934       0.4618            0.4618        \u001b[94m1.2202\u001b[0m  0.0000  0.4037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for subject 8 with 190 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2632\u001b[0m        \u001b[32m1.5548\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m1.6824\u001b[0m  0.0006  0.4201\n",
      "      2            0.2474        \u001b[32m1.2202\u001b[0m       0.2500            0.2500        3.1760  0.0006  0.4265\n",
      "      3            0.2474        1.2363       0.2500            0.2500        3.8755  0.0006  0.4317\n",
      "      4            0.2474        \u001b[32m1.0998\u001b[0m       0.2500            0.2500        4.3286  0.0006  0.4103\n",
      "      5            0.2526        \u001b[32m1.0825\u001b[0m       0.2500            0.2500        4.1345  0.0006  0.4592\n",
      "      6            \u001b[36m0.2789\u001b[0m        \u001b[32m1.0085\u001b[0m       0.2569            0.2569        3.8219  0.0006  0.4118\n",
      "      7            \u001b[36m0.3000\u001b[0m        \u001b[32m0.9735\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        3.3711  0.0006  0.4300\n",
      "      8            0.2895        \u001b[32m0.8390\u001b[0m       0.2639            0.2639        3.1685  0.0006  0.4444\n",
      "      9            0.2947        \u001b[32m0.8374\u001b[0m       0.2708            0.2708        2.7881  0.0006  0.4166\n",
      "     10            0.3000        \u001b[32m0.8234\u001b[0m       0.2708            0.2708        2.4878  0.0005  0.4071\n",
      "     11            \u001b[36m0.3316\u001b[0m        0.8408       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        2.2103  0.0005  0.4811\n",
      "     12            \u001b[36m0.4737\u001b[0m        \u001b[32m0.7355\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        1.9090  0.0005  0.4156\n",
      "     13            \u001b[36m0.5842\u001b[0m        \u001b[32m0.6581\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        1.7212  0.0005  0.4031\n",
      "     14            \u001b[36m0.6421\u001b[0m        0.6958       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.6205\u001b[0m  0.0005  0.4125\n",
      "     15            \u001b[36m0.6895\u001b[0m        0.7410       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.5402\u001b[0m  0.0004  0.4219\n",
      "     16            \u001b[36m0.7579\u001b[0m        0.6988       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.4740\u001b[0m  0.0004  0.3906\n",
      "     17            \u001b[36m0.7947\u001b[0m        \u001b[32m0.6139\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.4385\u001b[0m  0.0004  0.4068\n",
      "     18            \u001b[36m0.8000\u001b[0m        \u001b[32m0.6006\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.4262\u001b[0m  0.0004  0.4066\n",
      "     19            \u001b[36m0.8105\u001b[0m        0.6280       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.4222\u001b[0m  0.0004  0.5450\n",
      "     20            \u001b[36m0.8421\u001b[0m        \u001b[32m0.5607\u001b[0m       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.4021\u001b[0m  0.0003  0.5454\n",
      "     21            \u001b[36m0.8632\u001b[0m        \u001b[32m0.5347\u001b[0m       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.3790\u001b[0m  0.0003  0.4803\n",
      "     22            \u001b[36m0.8842\u001b[0m        \u001b[32m0.5063\u001b[0m       0.4167            0.4167        \u001b[94m1.3625\u001b[0m  0.0003  0.4821\n",
      "     23            \u001b[36m0.9053\u001b[0m        \u001b[32m0.4669\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.3401\u001b[0m  0.0002  0.5707\n",
      "     24            \u001b[36m0.9263\u001b[0m        \u001b[32m0.4141\u001b[0m       0.4236            0.4236        \u001b[94m1.3149\u001b[0m  0.0002  0.6067\n",
      "     25            \u001b[36m0.9368\u001b[0m        0.5336       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.2960\u001b[0m  0.0002  0.4467\n",
      "     26            \u001b[36m0.9474\u001b[0m        0.5254       0.4271            0.4271        \u001b[94m1.2794\u001b[0m  0.0002  0.4072\n",
      "     27            \u001b[36m0.9526\u001b[0m        0.4597       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.2686\u001b[0m  0.0002  0.5228\n",
      "     28            0.9526        \u001b[32m0.4012\u001b[0m       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.2634\u001b[0m  0.0001  0.3905\n",
      "     29            \u001b[36m0.9579\u001b[0m        0.4982       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.2589\u001b[0m  0.0001  0.4174\n",
      "     30            \u001b[36m0.9632\u001b[0m        \u001b[32m0.3995\u001b[0m       0.4549            0.4549        \u001b[94m1.2560\u001b[0m  0.0001  0.4572\n",
      "     31            \u001b[36m0.9684\u001b[0m        0.4039       0.4514            0.4514        \u001b[94m1.2544\u001b[0m  0.0001  0.4064\n",
      "     32            0.9684        0.4396       0.4479            0.4479        \u001b[94m1.2526\u001b[0m  0.0001  0.4063\n",
      "     33            0.9684        0.5041       0.4479            0.4479        \u001b[94m1.2505\u001b[0m  0.0000  0.4640\n",
      "     34            0.9684        0.4442       0.4479            0.4479        \u001b[94m1.2488\u001b[0m  0.0000  0.4375\n",
      "     35            0.9684        0.4618       0.4549            0.4549        \u001b[94m1.2473\u001b[0m  0.0000  0.4183\n",
      "     36            0.9632        0.4575       0.4549            0.4549        \u001b[94m1.2461\u001b[0m  0.0000  0.4540\n",
      "     37            0.9632        0.4076       0.4583            0.4583        \u001b[94m1.2454\u001b[0m  0.0000  0.4063\n",
      "     38            0.9632        0.4859       0.4583            0.4583        \u001b[94m1.2448\u001b[0m  0.0000  0.4532\n",
      "     39            0.9632        0.4521       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2445\u001b[0m  0.0000  0.4063\n",
      "     40            0.9579        0.4409       0.4688            0.4688        \u001b[94m1.2442\u001b[0m  0.0000  0.5938\n",
      "Training model for subject 8 with 200 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.6047\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m5.3429\u001b[0m  0.0006  0.5157\n",
      "      2            0.2500        \u001b[32m1.2852\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        5.4208  0.0006  0.5317\n",
      "      3            \u001b[36m0.2800\u001b[0m        \u001b[32m1.1775\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m4.2911\u001b[0m  0.0006  0.5471\n",
      "      4            \u001b[36m0.3050\u001b[0m        \u001b[32m1.0421\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m3.3176\u001b[0m  0.0006  0.6563\n",
      "      5            \u001b[36m0.4200\u001b[0m        \u001b[32m0.9035\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        \u001b[94m2.4500\u001b[0m  0.0006  0.7347\n",
      "      6            \u001b[36m0.4800\u001b[0m        0.9237       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m2.0575\u001b[0m  0.0006  0.5625\n",
      "      7            \u001b[36m0.5550\u001b[0m        \u001b[32m0.8356\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m1.8243\u001b[0m  0.0006  0.6407\n",
      "      8            \u001b[36m0.6000\u001b[0m        \u001b[32m0.7651\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.6379\u001b[0m  0.0006  0.6486\n",
      "      9            \u001b[36m0.6700\u001b[0m        \u001b[32m0.6909\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.5192\u001b[0m  0.0006  0.5313\n",
      "     10            \u001b[36m0.7750\u001b[0m        \u001b[32m0.6755\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.3966\u001b[0m  0.0005  0.5618\n",
      "     11            \u001b[36m0.8250\u001b[0m        \u001b[32m0.6152\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.3409\u001b[0m  0.0005  0.5161\n",
      "     12            \u001b[36m0.8650\u001b[0m        \u001b[32m0.6068\u001b[0m       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.3204\u001b[0m  0.0005  0.5313\n",
      "     13            \u001b[36m0.8800\u001b[0m        \u001b[32m0.5156\u001b[0m       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.2956\u001b[0m  0.0005  0.5313\n",
      "     14            \u001b[36m0.9050\u001b[0m        \u001b[32m0.4902\u001b[0m       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.2670\u001b[0m  0.0005  0.5313\n",
      "     15            \u001b[36m0.9300\u001b[0m        \u001b[32m0.4530\u001b[0m       0.4583            0.4583        \u001b[94m1.2357\u001b[0m  0.0004  0.5471\n",
      "     16            \u001b[36m0.9450\u001b[0m        \u001b[32m0.4081\u001b[0m       0.4549            0.4549        \u001b[94m1.2188\u001b[0m  0.0004  0.5157\n",
      "     17            \u001b[36m0.9550\u001b[0m        \u001b[32m0.3849\u001b[0m       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2131\u001b[0m  0.0004  0.5000\n",
      "     18            \u001b[36m0.9600\u001b[0m        0.4576       \u001b[35m0.4722\u001b[0m            \u001b[31m0.4722\u001b[0m        \u001b[94m1.2122\u001b[0m  0.0004  0.4849\n",
      "     19            \u001b[36m0.9850\u001b[0m        0.4184       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        \u001b[94m1.2080\u001b[0m  0.0004  0.4844\n",
      "     20            0.9850        \u001b[32m0.3622\u001b[0m       0.4583            0.4583        \u001b[94m1.2064\u001b[0m  0.0003  0.5000\n",
      "     21            0.9850        0.3839       0.4618            0.4618        1.2069  0.0003  0.5314\n",
      "     22            \u001b[36m0.9900\u001b[0m        0.3841       0.4757            0.4757        \u001b[94m1.2041\u001b[0m  0.0003  0.5157\n",
      "     23            0.9900        \u001b[32m0.3239\u001b[0m       0.4722            0.4722        \u001b[94m1.1999\u001b[0m  0.0002  0.5000\n",
      "     24            0.9900        \u001b[32m0.2944\u001b[0m       \u001b[35m0.4931\u001b[0m            \u001b[31m0.4931\u001b[0m        \u001b[94m1.1951\u001b[0m  0.0002  0.5165\n",
      "     25            0.9900        0.3254       0.4757            0.4757        \u001b[94m1.1926\u001b[0m  0.0002  0.5303\n",
      "     26            \u001b[36m0.9950\u001b[0m        0.3289       0.4757            0.4757        \u001b[94m1.1913\u001b[0m  0.0002  0.5443\n",
      "     27            0.9950        0.3106       0.4688            0.4688        \u001b[94m1.1893\u001b[0m  0.0002  0.6319\n",
      "     28            0.9900        \u001b[32m0.2814\u001b[0m       0.4826            0.4826        \u001b[94m1.1877\u001b[0m  0.0001  0.6717\n",
      "     29            0.9950        \u001b[32m0.2732\u001b[0m       0.4792            0.4792        \u001b[94m1.1851\u001b[0m  0.0001  0.5656\n",
      "     30            0.9950        \u001b[32m0.2687\u001b[0m       0.4792            0.4792        \u001b[94m1.1831\u001b[0m  0.0001  0.6407\n",
      "     31            0.9950        \u001b[32m0.2582\u001b[0m       0.4792            0.4792        \u001b[94m1.1809\u001b[0m  0.0001  0.8116\n",
      "     32            0.9950        \u001b[32m0.2525\u001b[0m       0.4826            0.4826        \u001b[94m1.1806\u001b[0m  0.0001  0.5469\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.2722       0.4931            0.4931        1.1810  0.0000  0.5313\n",
      "     34            1.0000        0.2918       0.4931            0.4931        1.1809  0.0000  0.5000\n",
      "     35            1.0000        0.2957       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        1.1807  0.0000  0.5782\n",
      "     36            1.0000        0.2920       0.4965            0.4965        \u001b[94m1.1801\u001b[0m  0.0000  0.5157\n",
      "     37            1.0000        0.2832       0.4965            0.4965        \u001b[94m1.1798\u001b[0m  0.0000  0.5474\n",
      "     38            1.0000        0.2905       0.4965            0.4965        \u001b[94m1.1797\u001b[0m  0.0000  0.5317\n",
      "     39            1.0000        0.2646       0.4931            0.4931        \u001b[94m1.1795\u001b[0m  0.0000  0.5000\n",
      "     40            1.0000        0.2897       0.4931            0.4931        \u001b[94m1.1794\u001b[0m  0.0000  0.5004\n",
      "Training model for subject 8 with 210 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2476\u001b[0m        \u001b[32m1.5584\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.9729\u001b[0m  0.0006  0.4844\n",
      "      2            \u001b[36m0.3524\u001b[0m        \u001b[32m1.3129\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        4.1006  0.0006  0.5000\n",
      "      3            \u001b[36m0.3762\u001b[0m        \u001b[32m1.2026\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        \u001b[94m3.2308\u001b[0m  0.0006  0.6094\n",
      "      4            \u001b[36m0.4286\u001b[0m        \u001b[32m1.0614\u001b[0m       0.2847            0.2847        \u001b[94m2.7891\u001b[0m  0.0006  0.5157\n",
      "      5            \u001b[36m0.4619\u001b[0m        \u001b[32m1.0530\u001b[0m       0.2639            0.2639        \u001b[94m2.5262\u001b[0m  0.0006  0.5207\n",
      "      6            0.4524        \u001b[32m0.8990\u001b[0m       0.2604            0.2604        \u001b[94m2.3047\u001b[0m  0.0006  0.6722\n",
      "      7            \u001b[36m0.5143\u001b[0m        \u001b[32m0.8489\u001b[0m       0.2986            0.2986        \u001b[94m2.0316\u001b[0m  0.0006  0.5005\n",
      "      8            \u001b[36m0.6095\u001b[0m        \u001b[32m0.7907\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m1.7461\u001b[0m  0.0006  0.7838\n",
      "      9            \u001b[36m0.7333\u001b[0m        \u001b[32m0.7614\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.5366\u001b[0m  0.0006  0.6355\n",
      "     10            \u001b[36m0.8143\u001b[0m        \u001b[32m0.6966\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.4283\u001b[0m  0.0005  0.6540\n",
      "     11            \u001b[36m0.8762\u001b[0m        \u001b[32m0.6748\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.3772\u001b[0m  0.0005  0.6204\n",
      "     12            \u001b[36m0.9048\u001b[0m        \u001b[32m0.6574\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.3347\u001b[0m  0.0005  0.6285\n",
      "     13            0.9048        \u001b[32m0.5967\u001b[0m       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.2984\u001b[0m  0.0005  0.7964\n",
      "     14            \u001b[36m0.9238\u001b[0m        \u001b[32m0.5847\u001b[0m       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.2761\u001b[0m  0.0005  0.6495\n",
      "     15            \u001b[36m0.9381\u001b[0m        \u001b[32m0.4922\u001b[0m       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2623\u001b[0m  0.0004  0.6080\n",
      "     16            0.9381        0.5137       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        \u001b[94m1.2460\u001b[0m  0.0004  0.5857\n",
      "     17            \u001b[36m0.9524\u001b[0m        \u001b[32m0.4558\u001b[0m       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        1.2475  0.0004  0.5293\n",
      "     18            0.9524        \u001b[32m0.4404\u001b[0m       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        1.2506  0.0004  0.5992\n",
      "     19            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3996\u001b[0m       \u001b[35m0.5000\u001b[0m            \u001b[31m0.5000\u001b[0m        \u001b[94m1.2457\u001b[0m  0.0004  0.5169\n",
      "     20            \u001b[36m0.9762\u001b[0m        0.4507       0.5000            0.5000        \u001b[94m1.2404\u001b[0m  0.0003  0.5490\n",
      "     21            0.9714        0.4036       \u001b[35m0.5139\u001b[0m            \u001b[31m0.5139\u001b[0m        \u001b[94m1.2261\u001b[0m  0.0003  0.5523\n",
      "     22            0.9714        \u001b[32m0.3860\u001b[0m       \u001b[35m0.5174\u001b[0m            \u001b[31m0.5174\u001b[0m        \u001b[94m1.2197\u001b[0m  0.0003  0.5150\n",
      "     23            \u001b[36m0.9810\u001b[0m        \u001b[32m0.3623\u001b[0m       0.5174            0.5174        \u001b[94m1.2177\u001b[0m  0.0002  0.5292\n",
      "     24            0.9810        0.3741       0.5069            0.5069        \u001b[94m1.2173\u001b[0m  0.0002  0.4890\n",
      "     25            \u001b[36m0.9857\u001b[0m        \u001b[32m0.3615\u001b[0m       0.5104            0.5104        \u001b[94m1.2169\u001b[0m  0.0002  0.5642\n",
      "     26            0.9857        \u001b[32m0.3597\u001b[0m       0.5104            0.5104        \u001b[94m1.2156\u001b[0m  0.0002  0.5129\n",
      "     27            0.9857        \u001b[32m0.3526\u001b[0m       0.5174            0.5174        \u001b[94m1.2112\u001b[0m  0.0002  0.4867\n",
      "     28            \u001b[36m0.9905\u001b[0m        \u001b[32m0.3348\u001b[0m       \u001b[35m0.5243\u001b[0m            \u001b[31m0.5243\u001b[0m        \u001b[94m1.2066\u001b[0m  0.0001  0.4866\n",
      "     29            0.9905        0.3510       \u001b[35m0.5278\u001b[0m            \u001b[31m0.5278\u001b[0m        \u001b[94m1.2025\u001b[0m  0.0001  0.4932\n",
      "     30            0.9905        0.3550       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        \u001b[94m1.1979\u001b[0m  0.0001  0.5228\n",
      "     31            \u001b[36m0.9952\u001b[0m        0.3433       \u001b[35m0.5382\u001b[0m            \u001b[31m0.5382\u001b[0m        \u001b[94m1.1946\u001b[0m  0.0001  0.5601\n",
      "     32            0.9952        \u001b[32m0.2904\u001b[0m       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        \u001b[94m1.1919\u001b[0m  0.0001  0.6123\n",
      "     33            0.9952        \u001b[32m0.2884\u001b[0m       \u001b[35m0.5486\u001b[0m            \u001b[31m0.5486\u001b[0m        \u001b[94m1.1903\u001b[0m  0.0000  0.5645\n",
      "     34            0.9952        0.3262       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        \u001b[94m1.1895\u001b[0m  0.0000  0.5811\n",
      "     35            0.9952        \u001b[32m0.2636\u001b[0m       0.5521            0.5521        \u001b[94m1.1892\u001b[0m  0.0000  0.6112\n",
      "     36            0.9905        0.3064       0.5521            0.5521        \u001b[94m1.1891\u001b[0m  0.0000  0.5500\n",
      "     37            0.9905        0.2941       0.5521            0.5521        \u001b[94m1.1890\u001b[0m  0.0000  0.6312\n",
      "     38            0.9905        0.3006       0.5521            0.5521        \u001b[94m1.1889\u001b[0m  0.0000  0.4982\n",
      "     39            0.9905        0.3289       0.5521            0.5521        \u001b[94m1.1887\u001b[0m  0.0000  0.4872\n",
      "     40            0.9905        0.2776       0.5521            0.5521        1.1889  0.0000  0.4858\n",
      "Training model for subject 8 with 220 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2455\u001b[0m        \u001b[32m1.6571\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.8387\u001b[0m  0.0006  0.4901\n",
      "      2            \u001b[36m0.2864\u001b[0m        \u001b[32m1.3944\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m2.2979\u001b[0m  0.0006  0.4822\n",
      "      3            \u001b[36m0.4409\u001b[0m        \u001b[32m1.1791\u001b[0m       0.2465            0.2465        \u001b[94m1.8902\u001b[0m  0.0006  0.5136\n",
      "      4            \u001b[36m0.5455\u001b[0m        \u001b[32m1.1164\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m1.6750\u001b[0m  0.0006  0.5434\n",
      "      5            \u001b[36m0.5636\u001b[0m        \u001b[32m0.9858\u001b[0m       0.2812            0.2812        \u001b[94m1.6293\u001b[0m  0.0006  0.5208\n",
      "      6            0.5409        \u001b[32m0.8984\u001b[0m       0.2778            0.2778        1.6567  0.0006  0.5284\n",
      "      7            \u001b[36m0.6091\u001b[0m        0.8991       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m1.6117\u001b[0m  0.0006  0.5512\n",
      "      8            \u001b[36m0.6455\u001b[0m        \u001b[32m0.7792\u001b[0m       0.3021            0.3021        \u001b[94m1.5650\u001b[0m  0.0006  0.5509\n",
      "      9            \u001b[36m0.6864\u001b[0m        \u001b[32m0.7630\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m1.5127\u001b[0m  0.0006  0.5051\n",
      "     10            \u001b[36m0.7045\u001b[0m        \u001b[32m0.6788\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.4672\u001b[0m  0.0005  0.4858\n",
      "     11            \u001b[36m0.7727\u001b[0m        \u001b[32m0.6678\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.4202\u001b[0m  0.0005  0.4718\n",
      "     12            \u001b[36m0.8409\u001b[0m        0.6781       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.3833\u001b[0m  0.0005  0.5014\n",
      "     13            \u001b[36m0.8773\u001b[0m        \u001b[32m0.5916\u001b[0m       0.3993            0.3993        \u001b[94m1.3619\u001b[0m  0.0005  0.5029\n",
      "     14            \u001b[36m0.9227\u001b[0m        0.5955       0.4132            0.4132        \u001b[94m1.3454\u001b[0m  0.0005  0.5862\n",
      "     15            \u001b[36m0.9409\u001b[0m        \u001b[32m0.5605\u001b[0m       0.4062            0.4062        \u001b[94m1.3281\u001b[0m  0.0004  0.6394\n",
      "     16            0.9364        \u001b[32m0.5451\u001b[0m       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.3149\u001b[0m  0.0004  0.5618\n",
      "     17            0.9409        \u001b[32m0.4741\u001b[0m       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.3078\u001b[0m  0.0004  0.7915\n",
      "     18            \u001b[36m0.9500\u001b[0m        0.5588       0.4514            0.4514        \u001b[94m1.3007\u001b[0m  0.0004  0.6290\n",
      "     19            \u001b[36m0.9591\u001b[0m        0.4922       0.4549            0.4549        \u001b[94m1.2971\u001b[0m  0.0004  0.5898\n",
      "     20            \u001b[36m0.9864\u001b[0m        \u001b[32m0.4228\u001b[0m       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2914\u001b[0m  0.0003  0.5314\n",
      "     21            \u001b[36m0.9909\u001b[0m        \u001b[32m0.4004\u001b[0m       0.4583            0.4583        1.2920  0.0003  0.5115\n",
      "     22            0.9909        0.4476       0.4514            0.4514        1.2923  0.0003  0.4998\n",
      "     23            0.9909        0.4053       0.4688            0.4688        1.2925  0.0002  0.5150\n",
      "     24            0.9818        \u001b[32m0.3875\u001b[0m       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        \u001b[94m1.2868\u001b[0m  0.0002  0.5150\n",
      "     25            0.9818        \u001b[32m0.3593\u001b[0m       0.4757            0.4757        \u001b[94m1.2803\u001b[0m  0.0002  0.5026\n",
      "     26            0.9864        \u001b[32m0.3526\u001b[0m       0.4722            0.4722        \u001b[94m1.2728\u001b[0m  0.0002  0.6330\n",
      "     27            0.9864        0.3806       0.4722            0.4722        \u001b[94m1.2671\u001b[0m  0.0002  0.6881\n",
      "     28            0.9864        0.3562       0.4757            0.4757        \u001b[94m1.2623\u001b[0m  0.0001  0.5246\n",
      "     29            0.9909        0.3864       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        \u001b[94m1.2575\u001b[0m  0.0001  0.4885\n",
      "     30            \u001b[36m0.9955\u001b[0m        \u001b[32m0.3120\u001b[0m       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        \u001b[94m1.2546\u001b[0m  0.0001  0.5322\n",
      "     31            0.9955        0.3468       0.4896            0.4896        \u001b[94m1.2537\u001b[0m  0.0001  0.4989\n",
      "     32            0.9955        0.3399       0.4861            0.4861        \u001b[94m1.2532\u001b[0m  0.0001  0.5333\n",
      "     33            0.9955        \u001b[32m0.3096\u001b[0m       0.4896            0.4896        \u001b[94m1.2523\u001b[0m  0.0000  0.5111\n",
      "     34            0.9955        0.3137       0.4861            0.4861        \u001b[94m1.2522\u001b[0m  0.0000  0.4844\n",
      "     35            0.9955        0.3368       0.4896            0.4896        \u001b[94m1.2515\u001b[0m  0.0000  0.4858\n",
      "     36            0.9955        0.3259       0.4896            0.4896        \u001b[94m1.2507\u001b[0m  0.0000  0.4861\n",
      "     37            0.9955        0.3819       0.4896            0.4896        1.2509  0.0000  0.5617\n",
      "     38            0.9955        0.3536       0.4861            0.4861        1.2508  0.0000  0.5982\n",
      "     39            0.9955        0.3255       0.4896            0.4896        1.2508  0.0000  0.5976\n",
      "     40            0.9955        \u001b[32m0.2926\u001b[0m       0.4896            0.4896        \u001b[94m1.2505\u001b[0m  0.0000  0.5797\n",
      "Training model for subject 8 with 230 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2522\u001b[0m        \u001b[32m1.6319\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m2.6342\u001b[0m  0.0006  0.6112\n",
      "      2            \u001b[36m0.3130\u001b[0m        \u001b[32m1.2867\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m2.1020\u001b[0m  0.0006  0.5043\n",
      "      3            0.3087        \u001b[32m1.2683\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m1.8019\u001b[0m  0.0006  0.4843\n",
      "      4            \u001b[36m0.4130\u001b[0m        \u001b[32m1.0602\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m1.7994\u001b[0m  0.0006  0.4867\n",
      "      5            \u001b[36m0.4783\u001b[0m        \u001b[32m1.0142\u001b[0m       0.2986            0.2986        \u001b[94m1.7426\u001b[0m  0.0006  0.5018\n",
      "      6            \u001b[36m0.5087\u001b[0m        \u001b[32m0.9628\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        1.7756  0.0006  0.4862\n",
      "      7            0.5000        \u001b[32m0.8686\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        1.8181  0.0006  0.4864\n",
      "      8            \u001b[36m0.5348\u001b[0m        \u001b[32m0.8653\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        1.7988  0.0006  0.5021\n",
      "      9            \u001b[36m0.5957\u001b[0m        \u001b[32m0.7894\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m1.6391\u001b[0m  0.0006  0.4874\n",
      "     10            \u001b[36m0.6783\u001b[0m        \u001b[32m0.7464\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.4362\u001b[0m  0.0005  0.4811\n",
      "     11            \u001b[36m0.8478\u001b[0m        \u001b[32m0.7393\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.3210\u001b[0m  0.0005  0.4908\n",
      "     12            \u001b[36m0.8870\u001b[0m        \u001b[32m0.7219\u001b[0m       0.4028            0.4028        \u001b[94m1.2939\u001b[0m  0.0005  0.4884\n",
      "     13            \u001b[36m0.8913\u001b[0m        \u001b[32m0.5848\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.2830\u001b[0m  0.0005  0.4860\n",
      "     14            \u001b[36m0.8957\u001b[0m        0.6103       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.2740\u001b[0m  0.0005  0.4970\n",
      "     15            0.8913        \u001b[32m0.5613\u001b[0m       0.4167            0.4167        1.2771  0.0004  0.4864\n",
      "     16            0.8870        \u001b[32m0.4996\u001b[0m       0.4201            0.4201        1.2784  0.0004  0.5492\n",
      "     17            0.8870        0.5376       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.2694\u001b[0m  0.0004  0.5140\n",
      "     18            \u001b[36m0.9000\u001b[0m        0.5457       0.4375            0.4375        \u001b[94m1.2591\u001b[0m  0.0004  0.6615\n",
      "     19            \u001b[36m0.9217\u001b[0m        0.5241       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.2445\u001b[0m  0.0004  0.4834\n",
      "     20            \u001b[36m0.9522\u001b[0m        \u001b[32m0.4817\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.2254\u001b[0m  0.0003  0.5032\n",
      "     21            \u001b[36m0.9609\u001b[0m        \u001b[32m0.4764\u001b[0m       0.4410            0.4410        \u001b[94m1.2105\u001b[0m  0.0003  0.5959\n",
      "     22            0.9565        \u001b[32m0.4033\u001b[0m       0.4410            0.4410        \u001b[94m1.1990\u001b[0m  0.0003  0.5803\n",
      "     23            0.9609        0.4254       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.1949\u001b[0m  0.0002  0.5820\n",
      "     24            \u001b[36m0.9739\u001b[0m        0.4442       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.1937\u001b[0m  0.0002  0.5960\n",
      "     25            0.9739        0.4134       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.1926\u001b[0m  0.0002  0.5500\n",
      "     26            \u001b[36m0.9783\u001b[0m        0.4602       0.4583            0.4583        \u001b[94m1.1916\u001b[0m  0.0002  0.4868\n",
      "     27            \u001b[36m0.9826\u001b[0m        \u001b[32m0.3278\u001b[0m       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.1901\u001b[0m  0.0002  0.5019\n",
      "     28            0.9826        0.3634       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.1893\u001b[0m  0.0001  0.5019\n",
      "     29            0.9826        0.3727       0.4618            0.4618        \u001b[94m1.1889\u001b[0m  0.0001  0.5020\n",
      "     30            0.9826        0.3671       0.4618            0.4618        \u001b[94m1.1868\u001b[0m  0.0001  0.4784\n",
      "     31            0.9826        0.3625       0.4618            0.4618        \u001b[94m1.1830\u001b[0m  0.0001  0.4856\n",
      "     32            \u001b[36m0.9870\u001b[0m        \u001b[32m0.3262\u001b[0m       0.4618            0.4618        \u001b[94m1.1797\u001b[0m  0.0001  0.5025\n",
      "     33            0.9870        0.4017       0.4618            0.4618        \u001b[94m1.1768\u001b[0m  0.0000  0.4862\n",
      "     34            0.9870        0.3985       0.4618            0.4618        \u001b[94m1.1747\u001b[0m  0.0000  0.5027\n",
      "     35            0.9870        0.3636       0.4618            0.4618        \u001b[94m1.1729\u001b[0m  0.0000  0.5084\n",
      "     36            0.9870        0.3701       0.4653            0.4653        \u001b[94m1.1719\u001b[0m  0.0000  0.4864\n",
      "     37            0.9870        0.3663       \u001b[35m0.4722\u001b[0m            \u001b[31m0.4722\u001b[0m        \u001b[94m1.1711\u001b[0m  0.0000  0.4865\n",
      "     38            0.9870        \u001b[32m0.3096\u001b[0m       0.4688            0.4688        1.1712  0.0000  0.5385\n",
      "     39            0.9870        0.3728       0.4722            0.4722        \u001b[94m1.1708\u001b[0m  0.0000  0.5622\n",
      "     40            0.9870        0.3233       0.4722            0.4722        \u001b[94m1.1706\u001b[0m  0.0000  0.5113\n",
      "Training model for subject 8 with 240 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2542\u001b[0m        \u001b[32m1.5958\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m2.5465\u001b[0m  0.0006  0.4768\n",
      "      2            \u001b[36m0.3792\u001b[0m        \u001b[32m1.3190\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m2.3735\u001b[0m  0.0006  0.4853\n",
      "      3            0.2542        \u001b[32m1.2047\u001b[0m       0.2535            0.2535        2.9731  0.0006  0.5005\n",
      "      4            0.2583        \u001b[32m1.0711\u001b[0m       0.2569            0.2569        2.8464  0.0006  0.5469\n",
      "      5            0.2792        \u001b[32m1.0105\u001b[0m       0.2535            0.2535        2.5272  0.0006  0.6563\n",
      "      6            0.3083        \u001b[32m0.8665\u001b[0m       0.2500            0.2500        \u001b[94m2.2800\u001b[0m  0.0006  0.5625\n",
      "      7            \u001b[36m0.3917\u001b[0m        0.8785       0.2604            0.2604        \u001b[94m2.0513\u001b[0m  0.0006  0.5782\n",
      "      8            \u001b[36m0.4542\u001b[0m        \u001b[32m0.7842\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m1.8570\u001b[0m  0.0006  0.6407\n",
      "      9            \u001b[36m0.5500\u001b[0m        \u001b[32m0.7273\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m1.7028\u001b[0m  0.0006  0.6719\n",
      "     10            \u001b[36m0.6542\u001b[0m        0.7303       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.5780\u001b[0m  0.0005  0.5000\n",
      "     11            \u001b[36m0.7458\u001b[0m        \u001b[32m0.6796\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.4845\u001b[0m  0.0005  0.5000\n",
      "     12            \u001b[36m0.8125\u001b[0m        \u001b[32m0.6090\u001b[0m       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.4247\u001b[0m  0.0005  0.5005\n",
      "     13            \u001b[36m0.8625\u001b[0m        0.6561       0.4236            0.4236        \u001b[94m1.3795\u001b[0m  0.0005  0.5164\n",
      "     14            \u001b[36m0.8833\u001b[0m        0.6305       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.3526\u001b[0m  0.0005  0.4860\n",
      "     15            \u001b[36m0.9000\u001b[0m        0.6181       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.3273\u001b[0m  0.0004  0.5008\n",
      "     16            \u001b[36m0.9250\u001b[0m        \u001b[32m0.5359\u001b[0m       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        \u001b[94m1.3143\u001b[0m  0.0004  0.5157\n",
      "     17            \u001b[36m0.9375\u001b[0m        \u001b[32m0.5313\u001b[0m       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        \u001b[94m1.3051\u001b[0m  0.0004  0.5001\n",
      "     18            0.9375        \u001b[32m0.4991\u001b[0m       0.4549            0.4549        \u001b[94m1.2970\u001b[0m  0.0004  0.4844\n",
      "     19            0.9375        0.5043       0.4514            0.4514        \u001b[94m1.2899\u001b[0m  0.0004  0.5000\n",
      "     20            0.9292        \u001b[32m0.4453\u001b[0m       0.4618            0.4618        \u001b[94m1.2816\u001b[0m  0.0003  0.5120\n",
      "     21            \u001b[36m0.9417\u001b[0m        0.4861       0.4688            0.4688        \u001b[94m1.2752\u001b[0m  0.0003  0.5005\n",
      "     22            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4227\u001b[0m       0.4757            0.4757        \u001b[94m1.2723\u001b[0m  0.0003  0.5000\n",
      "     23            \u001b[36m0.9542\u001b[0m        \u001b[32m0.4008\u001b[0m       0.4722            0.4722        1.2747  0.0002  0.4850\n",
      "     24            \u001b[36m0.9625\u001b[0m        0.4020       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        1.2792  0.0002  0.4844\n",
      "     25            \u001b[36m0.9667\u001b[0m        0.4357       0.4826            0.4826        1.2833  0.0002  0.4849\n",
      "     26            0.9625        \u001b[32m0.3863\u001b[0m       0.4722            0.4722        1.2866  0.0002  0.4854\n",
      "     27            0.9583        \u001b[32m0.3445\u001b[0m       0.4688            0.4688        1.2896  0.0002  0.5163\n",
      "     28            0.9625        0.3645       0.4757            0.4757        1.2852  0.0001  0.5785\n",
      "     29            \u001b[36m0.9708\u001b[0m        0.3843       0.4757            0.4757        1.2799  0.0001  0.6407\n",
      "     30            0.9708        \u001b[32m0.3357\u001b[0m       0.4757            0.4757        1.2750  0.0001  0.6100\n",
      "     31            0.9708        0.3665       0.4792            0.4792        \u001b[94m1.2718\u001b[0m  0.0001  0.6251\n",
      "     32            0.9708        0.3830       0.4757            0.4757        \u001b[94m1.2699\u001b[0m  0.0001  0.5000\n",
      "     33            0.9708        0.3540       0.4792            0.4792        \u001b[94m1.2697\u001b[0m  0.0000  0.4844\n",
      "     34            0.9708        \u001b[32m0.3149\u001b[0m       0.4792            0.4792        \u001b[94m1.2692\u001b[0m  0.0000  0.4853\n",
      "     35            \u001b[36m0.9750\u001b[0m        0.3548       0.4722            0.4722        \u001b[94m1.2689\u001b[0m  0.0000  0.4844\n",
      "     36            0.9708        0.3544       0.4653            0.4653        \u001b[94m1.2689\u001b[0m  0.0000  0.4844\n",
      "     37            0.9708        0.3803       0.4653            0.4653        1.2689  0.0000  0.4844\n",
      "     38            0.9750        0.3465       0.4688            0.4688        1.2689  0.0000  0.5035\n",
      "     39            0.9750        0.3291       0.4653            0.4653        1.2689  0.0000  0.5001\n",
      "     40            0.9750        0.3317       0.4653            0.4653        1.2690  0.0000  0.4844\n",
      "Training model for subject 8 with 250 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2520\u001b[0m        \u001b[32m1.8367\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.3495\u001b[0m  0.0006  0.5000\n",
      "      2            \u001b[36m0.3400\u001b[0m        \u001b[32m1.4595\u001b[0m       0.2292            0.2292        \u001b[94m2.6608\u001b[0m  0.0006  0.4844\n",
      "      3            \u001b[36m0.3640\u001b[0m        \u001b[32m1.1982\u001b[0m       0.2361            0.2361        \u001b[94m2.6464\u001b[0m  0.0006  0.4844\n",
      "      4            0.3320        \u001b[32m1.1396\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m2.5394\u001b[0m  0.0006  0.4844\n",
      "      5            0.3120        \u001b[32m1.0968\u001b[0m       0.2535            0.2535        2.6126  0.0006  0.5000\n",
      "      6            0.3160        \u001b[32m0.9851\u001b[0m       0.2535            0.2535        \u001b[94m2.5122\u001b[0m  0.0006  0.5005\n",
      "      7            0.3360        0.9899       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m2.3092\u001b[0m  0.0006  0.4844\n",
      "      8            \u001b[36m0.4040\u001b[0m        \u001b[32m0.8937\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m1.9771\u001b[0m  0.0006  0.4849\n",
      "      9            \u001b[36m0.5560\u001b[0m        0.9030       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m1.6957\u001b[0m  0.0006  0.4844\n",
      "     10            \u001b[36m0.7160\u001b[0m        \u001b[32m0.7559\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.5288\u001b[0m  0.0005  0.4850\n",
      "     11            \u001b[36m0.7640\u001b[0m        \u001b[32m0.7423\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.4797\u001b[0m  0.0005  0.5471\n",
      "     12            \u001b[36m0.7920\u001b[0m        \u001b[32m0.7002\u001b[0m       0.3542            0.3542        \u001b[94m1.4340\u001b[0m  0.0005  0.6094\n",
      "     13            \u001b[36m0.8320\u001b[0m        \u001b[32m0.6981\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.3793\u001b[0m  0.0005  0.5941\n",
      "     14            \u001b[36m0.8520\u001b[0m        \u001b[32m0.6175\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.3421\u001b[0m  0.0005  0.6094\n",
      "     15            \u001b[36m0.8800\u001b[0m        \u001b[32m0.5239\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.3087\u001b[0m  0.0004  0.6250\n",
      "     16            \u001b[36m0.8920\u001b[0m        0.5905       0.4062            0.4062        \u001b[94m1.2903\u001b[0m  0.0004  0.5941\n",
      "     17            \u001b[36m0.8960\u001b[0m        0.5711       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.2774\u001b[0m  0.0004  0.4847\n",
      "     18            \u001b[36m0.9120\u001b[0m        0.5474       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.2659\u001b[0m  0.0004  0.4844\n",
      "     19            0.9120        \u001b[32m0.4983\u001b[0m       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        \u001b[94m1.2534\u001b[0m  0.0004  0.4844\n",
      "     20            \u001b[36m0.9200\u001b[0m        \u001b[32m0.4955\u001b[0m       \u001b[35m0.4931\u001b[0m            \u001b[31m0.4931\u001b[0m        \u001b[94m1.2393\u001b[0m  0.0003  0.4848\n",
      "     21            \u001b[36m0.9400\u001b[0m        \u001b[32m0.4501\u001b[0m       \u001b[35m0.5104\u001b[0m            \u001b[31m0.5104\u001b[0m        \u001b[94m1.2217\u001b[0m  0.0003  0.4844\n",
      "     22            \u001b[36m0.9480\u001b[0m        0.5366       0.5000            0.5000        \u001b[94m1.2134\u001b[0m  0.0003  0.4844\n",
      "     23            \u001b[36m0.9520\u001b[0m        0.5062       0.5104            0.5104        \u001b[94m1.2105\u001b[0m  0.0002  0.4844\n",
      "     24            \u001b[36m0.9560\u001b[0m        0.5012       0.5104            0.5104        \u001b[94m1.2076\u001b[0m  0.0002  0.4848\n",
      "     25            0.9520        \u001b[32m0.3990\u001b[0m       0.5104            0.5104        \u001b[94m1.2058\u001b[0m  0.0002  0.4849\n",
      "     26            \u001b[36m0.9600\u001b[0m        0.4189       \u001b[35m0.5174\u001b[0m            \u001b[31m0.5174\u001b[0m        \u001b[94m1.1990\u001b[0m  0.0002  0.4850\n",
      "     27            \u001b[36m0.9640\u001b[0m        0.4052       \u001b[35m0.5243\u001b[0m            \u001b[31m0.5243\u001b[0m        \u001b[94m1.1920\u001b[0m  0.0002  0.4847\n",
      "     28            0.9640        \u001b[32m0.3750\u001b[0m       0.5208            0.5208        \u001b[94m1.1880\u001b[0m  0.0001  0.4844\n",
      "     29            \u001b[36m0.9680\u001b[0m        0.3802       0.5174            0.5174        \u001b[94m1.1842\u001b[0m  0.0001  0.4844\n",
      "     30            \u001b[36m0.9720\u001b[0m        0.4315       0.5208            0.5208        \u001b[94m1.1801\u001b[0m  0.0001  0.4945\n",
      "     31            \u001b[36m0.9760\u001b[0m        \u001b[32m0.3356\u001b[0m       0.5243            0.5243        \u001b[94m1.1773\u001b[0m  0.0001  0.4851\n",
      "     32            \u001b[36m0.9800\u001b[0m        0.3892       \u001b[35m0.5278\u001b[0m            \u001b[31m0.5278\u001b[0m        \u001b[94m1.1747\u001b[0m  0.0001  0.4844\n",
      "     33            0.9800        0.3727       0.5278            0.5278        \u001b[94m1.1724\u001b[0m  0.0000  0.5001\n",
      "     34            0.9800        0.3687       0.5243            0.5243        \u001b[94m1.1709\u001b[0m  0.0000  0.5156\n",
      "     35            \u001b[36m0.9840\u001b[0m        0.4279       0.5243            0.5243        \u001b[94m1.1703\u001b[0m  0.0000  0.5625\n",
      "     36            0.9840        \u001b[32m0.3052\u001b[0m       0.5278            0.5278        \u001b[94m1.1701\u001b[0m  0.0000  0.6568\n",
      "     37            0.9840        0.4265       0.5243            0.5243        1.1703  0.0000  0.5625\n",
      "     38            0.9840        0.3922       0.5278            0.5278        1.1701  0.0000  0.6097\n",
      "     39            0.9840        0.4579       0.5243            0.5243        1.1701  0.0000  0.5157\n",
      "     40            0.9840        0.3648       0.5243            0.5243        1.1704  0.0000  0.5005\n",
      "Training model for subject 8 with 260 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3192\u001b[0m        \u001b[32m1.5304\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m3.6180\u001b[0m  0.0006  0.5782\n",
      "      2            \u001b[36m0.4385\u001b[0m        \u001b[32m1.2435\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m2.0009\u001b[0m  0.0006  0.5786\n",
      "      3            0.4231        \u001b[32m1.0892\u001b[0m       0.3090            0.3090        \u001b[94m1.8854\u001b[0m  0.0006  0.5782\n",
      "      4            \u001b[36m0.4538\u001b[0m        \u001b[32m1.0120\u001b[0m       0.2882            0.2882        1.9979  0.0006  0.5941\n",
      "      5            \u001b[36m0.5538\u001b[0m        \u001b[32m1.0055\u001b[0m       0.3229            0.3229        \u001b[94m1.7109\u001b[0m  0.0006  0.5939\n",
      "      6            \u001b[36m0.6769\u001b[0m        \u001b[32m0.8488\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.5111\u001b[0m  0.0006  0.5782\n",
      "      7            \u001b[36m0.7423\u001b[0m        \u001b[32m0.8023\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.4467\u001b[0m  0.0006  0.5943\n",
      "      8            0.7308        \u001b[32m0.7301\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.4245\u001b[0m  0.0006  0.5939\n",
      "      9            \u001b[36m0.8154\u001b[0m        \u001b[32m0.7163\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.3894\u001b[0m  0.0006  0.5782\n",
      "     10            0.8154        \u001b[32m0.7050\u001b[0m       0.3924            0.3924        \u001b[94m1.3882\u001b[0m  0.0005  0.5942\n",
      "     11            \u001b[36m0.8692\u001b[0m        \u001b[32m0.6513\u001b[0m       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.3669\u001b[0m  0.0005  0.5938\n",
      "     12            \u001b[36m0.9038\u001b[0m        \u001b[32m0.6077\u001b[0m       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.3479\u001b[0m  0.0005  0.5938\n",
      "     13            \u001b[36m0.9115\u001b[0m        \u001b[32m0.5022\u001b[0m       0.4167            0.4167        1.3501  0.0005  0.5791\n",
      "     14            \u001b[36m0.9192\u001b[0m        0.5366       0.4167            0.4167        1.3608  0.0005  0.5791\n",
      "     15            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4637\u001b[0m       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        1.3510  0.0004  0.5941\n",
      "     16            0.9500        \u001b[32m0.4118\u001b[0m       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.3278\u001b[0m  0.0004  0.7188\n",
      "     17            \u001b[36m0.9654\u001b[0m        0.4288       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        \u001b[94m1.2913\u001b[0m  0.0004  0.6875\n",
      "     18            \u001b[36m0.9808\u001b[0m        \u001b[32m0.4113\u001b[0m       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2758\u001b[0m  0.0004  0.6876\n",
      "     19            \u001b[36m0.9885\u001b[0m        0.4223       \u001b[35m0.4722\u001b[0m            \u001b[31m0.4722\u001b[0m        \u001b[94m1.2539\u001b[0m  0.0004  0.7205\n",
      "     20            0.9846        \u001b[32m0.3772\u001b[0m       \u001b[35m0.5035\u001b[0m            \u001b[31m0.5035\u001b[0m        \u001b[94m1.2391\u001b[0m  0.0003  0.5942\n",
      "     21            0.9885        \u001b[32m0.3408\u001b[0m       \u001b[35m0.5069\u001b[0m            \u001b[31m0.5069\u001b[0m        \u001b[94m1.2294\u001b[0m  0.0003  0.5784\n",
      "     22            0.9885        \u001b[32m0.3159\u001b[0m       0.5035            0.5035        \u001b[94m1.2222\u001b[0m  0.0003  0.5939\n",
      "     23            \u001b[36m0.9923\u001b[0m        \u001b[32m0.3095\u001b[0m       \u001b[35m0.5174\u001b[0m            \u001b[31m0.5174\u001b[0m        \u001b[94m1.2161\u001b[0m  0.0002  0.6108\n",
      "     24            0.9923        \u001b[32m0.2915\u001b[0m       0.5035            0.5035        1.2193  0.0002  0.6040\n",
      "     25            0.9923        0.3101       0.5139            0.5139        1.2289  0.0002  0.5938\n",
      "     26            \u001b[36m0.9962\u001b[0m        \u001b[32m0.2766\u001b[0m       0.5139            0.5139        1.2257  0.0002  0.5939\n",
      "     27            0.9962        \u001b[32m0.2765\u001b[0m       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        1.2260  0.0002  0.5942\n",
      "     28            0.9962        \u001b[32m0.2530\u001b[0m       0.5104            0.5104        1.2277  0.0001  0.5782\n",
      "     29            0.9962        0.2716       0.5208            0.5208        1.2249  0.0001  0.5785\n",
      "     30            0.9962        0.2614       0.5208            0.5208        1.2236  0.0001  0.5942\n",
      "     31            0.9962        \u001b[32m0.2489\u001b[0m       0.5208            0.5208        1.2199  0.0001  0.5938\n",
      "     32            \u001b[36m1.0000\u001b[0m        0.2696       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        1.2168  0.0001  0.5938\n",
      "     33            1.0000        0.2510       0.5312            0.5312        \u001b[94m1.2156\u001b[0m  0.0000  0.5786\n",
      "     34            1.0000        \u001b[32m0.2479\u001b[0m       0.5312            0.5312        1.2157  0.0000  0.5782\n",
      "     35            1.0000        \u001b[32m0.2338\u001b[0m       0.5312            0.5312        \u001b[94m1.2143\u001b[0m  0.0000  0.5939\n",
      "     36            1.0000        0.2632       \u001b[35m0.5347\u001b[0m            \u001b[31m0.5347\u001b[0m        \u001b[94m1.2138\u001b[0m  0.0000  0.6875\n",
      "     37            1.0000        0.2617       0.5347            0.5347        \u001b[94m1.2133\u001b[0m  0.0000  0.7344\n",
      "     38            1.0000        0.2546       0.5347            0.5347        \u001b[94m1.2130\u001b[0m  0.0000  0.7188\n",
      "     39            1.0000        0.2370       0.5347            0.5347        \u001b[94m1.2127\u001b[0m  0.0000  0.7188\n",
      "     40            1.0000        0.2770       0.5347            0.5347        \u001b[94m1.2125\u001b[0m  0.0000  0.5947\n",
      "Training model for subject 8 with 270 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3296\u001b[0m        \u001b[32m1.5565\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m3.7262\u001b[0m  0.0006  0.5942\n",
      "      2            0.2444        \u001b[32m1.2823\u001b[0m       0.2500            0.2500        3.9452  0.0006  0.5782\n",
      "      3            0.2444        \u001b[32m1.1535\u001b[0m       0.2500            0.2500        3.7817  0.0006  0.5938\n",
      "      4            0.3111        \u001b[32m1.0411\u001b[0m       0.2535            0.2535        \u001b[94m2.5354\u001b[0m  0.0006  0.5785\n",
      "      5            \u001b[36m0.4630\u001b[0m        \u001b[32m0.9686\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m1.8866\u001b[0m  0.0006  0.5942\n",
      "      6            \u001b[36m0.6704\u001b[0m        \u001b[32m0.8885\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m1.5422\u001b[0m  0.0006  0.5938\n",
      "      7            \u001b[36m0.7000\u001b[0m        \u001b[32m0.7938\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.4641\u001b[0m  0.0006  0.5782\n",
      "      8            \u001b[36m0.7185\u001b[0m        0.8035       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.4161\u001b[0m  0.0006  0.5786\n",
      "      9            \u001b[36m0.7444\u001b[0m        \u001b[32m0.7167\u001b[0m       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.3879\u001b[0m  0.0006  0.5938\n",
      "     10            \u001b[36m0.8296\u001b[0m        \u001b[32m0.6306\u001b[0m       0.4306            0.4306        \u001b[94m1.3337\u001b[0m  0.0005  0.5782\n",
      "     11            \u001b[36m0.8593\u001b[0m        0.6413       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.3007\u001b[0m  0.0005  0.5938\n",
      "     12            \u001b[36m0.8778\u001b[0m        0.6316       0.4306            0.4306        \u001b[94m1.2596\u001b[0m  0.0005  0.5788\n",
      "     13            \u001b[36m0.9259\u001b[0m        \u001b[32m0.5567\u001b[0m       0.4271            0.4271        \u001b[94m1.2441\u001b[0m  0.0005  0.5938\n",
      "     14            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5228\u001b[0m       0.4375            0.4375        \u001b[94m1.2386\u001b[0m  0.0005  0.5786\n",
      "     15            \u001b[36m0.9370\u001b[0m        \u001b[32m0.5036\u001b[0m       0.4340            0.4340        1.2454  0.0004  0.5941\n",
      "     16            \u001b[36m0.9556\u001b[0m        \u001b[32m0.4527\u001b[0m       0.4444            0.4444        \u001b[94m1.2366\u001b[0m  0.0004  0.6567\n",
      "     17            \u001b[36m0.9630\u001b[0m        \u001b[32m0.3932\u001b[0m       0.4375            0.4375        \u001b[94m1.2311\u001b[0m  0.0004  0.7032\n",
      "     18            \u001b[36m0.9667\u001b[0m        0.5070       \u001b[35m0.4722\u001b[0m            \u001b[31m0.4722\u001b[0m        \u001b[94m1.2046\u001b[0m  0.0004  0.6875\n",
      "     19            0.9630        \u001b[32m0.3913\u001b[0m       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        \u001b[94m1.1911\u001b[0m  0.0004  0.7344\n",
      "     20            \u001b[36m0.9704\u001b[0m        0.4033       0.4722            0.4722        1.1956  0.0003  0.6094\n",
      "     21            \u001b[36m0.9741\u001b[0m        \u001b[32m0.3656\u001b[0m       0.4583            0.4583        1.2137  0.0003  0.5938\n",
      "     22            \u001b[36m0.9815\u001b[0m        \u001b[32m0.3513\u001b[0m       0.4479            0.4479        1.2059  0.0003  0.5944\n",
      "     23            \u001b[36m0.9889\u001b[0m        \u001b[32m0.3430\u001b[0m       0.4583            0.4583        1.2075  0.0002  0.5938\n",
      "     24            \u001b[36m0.9963\u001b[0m        \u001b[32m0.3153\u001b[0m       0.4514            0.4514        1.2058  0.0002  0.5941\n",
      "     25            0.9926        0.3417       0.4583            0.4583        1.1982  0.0002  0.5790\n",
      "     26            0.9889        0.3492       0.4688            0.4688        1.2023  0.0002  0.6102\n",
      "     27            0.9963        0.3297       0.4757            0.4757        1.1968  0.0002  0.5782\n",
      "     28            0.9963        \u001b[32m0.2843\u001b[0m       0.4757            0.4757        1.1915  0.0001  0.5785\n",
      "     29            0.9963        0.3216       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        \u001b[94m1.1794\u001b[0m  0.0001  0.5942\n",
      "     30            0.9926        0.2851       0.4896            0.4896        \u001b[94m1.1681\u001b[0m  0.0001  0.5938\n",
      "     31            0.9926        0.3026       \u001b[35m0.4931\u001b[0m            \u001b[31m0.4931\u001b[0m        \u001b[94m1.1633\u001b[0m  0.0001  0.5943\n",
      "     32            0.9926        0.2901       \u001b[35m0.5000\u001b[0m            \u001b[31m0.5000\u001b[0m        \u001b[94m1.1605\u001b[0m  0.0001  0.5950\n",
      "     33            0.9926        0.2852       0.4965            0.4965        \u001b[94m1.1591\u001b[0m  0.0000  0.5942\n",
      "     34            0.9926        0.3052       0.4965            0.4965        1.1596  0.0000  0.6097\n",
      "     35            0.9926        \u001b[32m0.2495\u001b[0m       0.5000            0.5000        1.1594  0.0000  0.5782\n",
      "     36            0.9926        0.3226       0.5000            0.5000        \u001b[94m1.1584\u001b[0m  0.0000  0.6407\n",
      "     37            0.9926        0.2978       0.5000            0.5000        \u001b[94m1.1580\u001b[0m  0.0000  0.7344\n",
      "     38            0.9926        0.3008       0.4965            0.4965        \u001b[94m1.1580\u001b[0m  0.0000  0.6875\n",
      "     39            0.9926        0.2708       0.4965            0.4965        \u001b[94m1.1579\u001b[0m  0.0000  0.7344\n",
      "     40            0.9926        0.2616       0.4965            0.4965        \u001b[94m1.1576\u001b[0m  0.0000  0.6563\n",
      "Training model for subject 9 with 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.7000\u001b[0m        \u001b[32m1.1241\u001b[0m       \u001b[35m0.2361\u001b[0m            \u001b[31m0.2361\u001b[0m        \u001b[94m4.1604\u001b[0m  0.0006  0.3125\n",
      "      2            0.5000        \u001b[32m0.8196\u001b[0m       \u001b[35m0.2431\u001b[0m            \u001b[31m0.2431\u001b[0m        5.3482  0.0006  0.3281\n",
      "      3            0.5000        \u001b[32m0.3293\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        6.3561  0.0006  0.3125\n",
      "      4            0.5000        \u001b[32m0.3209\u001b[0m       0.2500            0.2500        7.1507  0.0006  0.3125\n",
      "      5            0.5000        \u001b[32m0.2652\u001b[0m       0.2500            0.2500        7.1069  0.0006  0.3125\n",
      "      6            0.6000        \u001b[32m0.1601\u001b[0m       0.2500            0.2500        6.7548  0.0006  0.3125\n",
      "      7            \u001b[36m0.8000\u001b[0m        \u001b[32m0.0710\u001b[0m       0.2500            0.2500        6.2695  0.0006  0.3125\n",
      "      8            \u001b[36m0.9000\u001b[0m        \u001b[32m0.0367\u001b[0m       0.2500            0.2500        5.7573  0.0006  0.3125\n",
      "      9            0.9000        0.0450       0.2465            0.2465        5.2342  0.0006  0.3125\n",
      "     10            0.9000        \u001b[32m0.0246\u001b[0m       0.2465            0.2465        4.8075  0.0005  0.2969\n",
      "     11            \u001b[36m1.0000\u001b[0m        0.0501       0.2465            0.2465        4.4852  0.0005  0.3282\n",
      "     12            1.0000        \u001b[32m0.0206\u001b[0m       0.2396            0.2396        4.2211  0.0005  0.3125\n",
      "     13            1.0000        \u001b[32m0.0179\u001b[0m       0.2396            0.2396        \u001b[94m3.9933\u001b[0m  0.0005  0.3129\n",
      "     14            1.0000        \u001b[32m0.0134\u001b[0m       0.2431            0.2431        \u001b[94m3.7990\u001b[0m  0.0005  0.3125\n",
      "     15            1.0000        0.0333       0.2465            0.2465        \u001b[94m3.6137\u001b[0m  0.0004  0.3125\n",
      "     16            1.0000        \u001b[32m0.0055\u001b[0m       0.2465            0.2465        \u001b[94m3.4600\u001b[0m  0.0004  0.4375\n",
      "     17            1.0000        0.0066       0.2465            0.2465        \u001b[94m3.3294\u001b[0m  0.0004  0.3594\n",
      "     18            1.0000        0.0086       0.2431            0.2431        \u001b[94m3.2188\u001b[0m  0.0004  0.3281\n",
      "     19            1.0000        0.0102       0.2431            0.2431        \u001b[94m3.1369\u001b[0m  0.0004  0.3281\n",
      "     20            1.0000        \u001b[32m0.0038\u001b[0m       0.2431            0.2431        \u001b[94m3.0654\u001b[0m  0.0003  0.3125\n",
      "     21            1.0000        0.0126       0.2465            0.2465        \u001b[94m3.0109\u001b[0m  0.0003  0.3125\n",
      "     22            1.0000        0.0050       0.2500            0.2500        \u001b[94m2.9628\u001b[0m  0.0003  0.3125\n",
      "     23            1.0000        0.0044       0.2500            0.2500        \u001b[94m2.9272\u001b[0m  0.0002  0.3129\n",
      "     24            1.0000        0.0145       0.2500            0.2500        \u001b[94m2.8995\u001b[0m  0.0002  0.3125\n",
      "     25            1.0000        0.0057       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m2.8805\u001b[0m  0.0002  0.3286\n",
      "     26            1.0000        0.0074       0.2535            0.2535        \u001b[94m2.8669\u001b[0m  0.0002  0.3125\n",
      "     27            1.0000        0.0046       0.2535            0.2535        \u001b[94m2.8485\u001b[0m  0.0002  0.3125\n",
      "     28            1.0000        0.0063       0.2535            0.2535        \u001b[94m2.8403\u001b[0m  0.0001  0.3125\n",
      "     29            1.0000        0.0079       0.2535            0.2535        \u001b[94m2.8266\u001b[0m  0.0001  0.3125\n",
      "     30            1.0000        0.0072       0.2535            0.2535        \u001b[94m2.8150\u001b[0m  0.0001  0.3125\n",
      "     31            1.0000        0.0042       0.2535            0.2535        \u001b[94m2.8057\u001b[0m  0.0001  0.3281\n",
      "     32            1.0000        0.0103       0.2535            0.2535        \u001b[94m2.8008\u001b[0m  0.0001  0.3281\n",
      "     33            1.0000        0.0056       0.2535            0.2535        \u001b[94m2.7948\u001b[0m  0.0000  0.3281\n",
      "     34            1.0000        0.0049       0.2535            0.2535        \u001b[94m2.7924\u001b[0m  0.0000  0.3297\n",
      "     35            1.0000        0.0079       0.2535            0.2535        \u001b[94m2.7862\u001b[0m  0.0000  0.3594\n",
      "     36            1.0000        0.0117       0.2535            0.2535        \u001b[94m2.7860\u001b[0m  0.0000  0.4219\n",
      "     37            1.0000        0.0066       0.2535            0.2535        \u001b[94m2.7825\u001b[0m  0.0000  0.4375\n",
      "     38            1.0000        0.0067       0.2535            0.2535        2.7851  0.0000  0.4219\n",
      "     39            1.0000        0.0083       0.2535            0.2535        2.7883  0.0000  0.4375\n",
      "     40            1.0000        \u001b[32m0.0031\u001b[0m       0.2535            0.2535        2.7827  0.0000  0.4375\n",
      "Training model for subject 9 with 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2000\u001b[0m        \u001b[32m2.1823\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m2.0335\u001b[0m  0.0006  0.2969\n",
      "      2            \u001b[36m0.3000\u001b[0m        \u001b[32m1.2280\u001b[0m       0.2500            0.2500        2.2612  0.0006  0.2656\n",
      "      3            \u001b[36m0.4500\u001b[0m        \u001b[32m0.7327\u001b[0m       0.2535            0.2535        2.4563  0.0006  0.2741\n",
      "      4            \u001b[36m0.5500\u001b[0m        \u001b[32m0.6756\u001b[0m       0.2882            0.2882        2.6180  0.0006  0.2813\n",
      "      5            \u001b[36m0.6000\u001b[0m        \u001b[32m0.6265\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        2.7120  0.0006  0.2656\n",
      "      6            0.6000        \u001b[32m0.3470\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        2.7539  0.0006  0.2656\n",
      "      7            0.6000        0.3719       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        2.7133  0.0006  0.2656\n",
      "      8            0.6000        \u001b[32m0.1887\u001b[0m       0.3542            0.3542        2.5533  0.0006  0.2656\n",
      "      9            \u001b[36m0.7000\u001b[0m        0.1970       0.3472            0.3472        2.4002  0.0006  0.2656\n",
      "     10            \u001b[36m0.9000\u001b[0m        \u001b[32m0.1415\u001b[0m       0.3507            0.3507        2.2359  0.0005  0.2656\n",
      "     11            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0828\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        2.0965  0.0005  0.2656\n",
      "     12            1.0000        0.1050       0.3576            0.3576        \u001b[94m2.0040\u001b[0m  0.0005  0.2656\n",
      "     13            1.0000        0.1106       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.9491\u001b[0m  0.0005  0.2656\n",
      "     14            1.0000        \u001b[32m0.0679\u001b[0m       0.3611            0.3611        \u001b[94m1.9025\u001b[0m  0.0005  0.2656\n",
      "     15            1.0000        \u001b[32m0.0436\u001b[0m       0.3576            0.3576        \u001b[94m1.8642\u001b[0m  0.0004  0.2500\n",
      "     16            1.0000        0.0517       0.3611            0.3611        \u001b[94m1.8224\u001b[0m  0.0004  0.2500\n",
      "     17            1.0000        \u001b[32m0.0414\u001b[0m       0.3542            0.3542        \u001b[94m1.7905\u001b[0m  0.0004  0.2656\n",
      "     18            1.0000        \u001b[32m0.0384\u001b[0m       0.3507            0.3507        \u001b[94m1.7584\u001b[0m  0.0004  0.2500\n",
      "     19            1.0000        \u001b[32m0.0363\u001b[0m       0.3542            0.3542        \u001b[94m1.7353\u001b[0m  0.0004  0.2656\n",
      "     20            1.0000        0.0588       0.3576            0.3576        \u001b[94m1.7270\u001b[0m  0.0003  0.2500\n",
      "     21            1.0000        0.0465       0.3542            0.3542        \u001b[94m1.7169\u001b[0m  0.0003  0.2657\n",
      "     22            1.0000        \u001b[32m0.0314\u001b[0m       0.3611            0.3611        \u001b[94m1.7076\u001b[0m  0.0003  0.2500\n",
      "     23            1.0000        0.0365       0.3646            0.3646        \u001b[94m1.7005\u001b[0m  0.0002  0.2500\n",
      "     24            1.0000        0.0339       0.3646            0.3646        \u001b[94m1.6955\u001b[0m  0.0002  0.2656\n",
      "     25            1.0000        0.0380       0.3646            0.3646        \u001b[94m1.6918\u001b[0m  0.0002  0.2657\n",
      "     26            1.0000        0.0361       0.3611            0.3611        \u001b[94m1.6872\u001b[0m  0.0002  0.2656\n",
      "     27            1.0000        0.0342       0.3681            0.3681        \u001b[94m1.6831\u001b[0m  0.0002  0.2656\n",
      "     28            1.0000        0.0391       0.3681            0.3681        \u001b[94m1.6813\u001b[0m  0.0001  0.2656\n",
      "     29            1.0000        \u001b[32m0.0253\u001b[0m       0.3646            0.3646        \u001b[94m1.6785\u001b[0m  0.0001  0.2656\n",
      "     30            1.0000        0.0340       0.3611            0.3611        \u001b[94m1.6758\u001b[0m  0.0001  0.2656\n",
      "     31            1.0000        \u001b[32m0.0220\u001b[0m       0.3576            0.3576        \u001b[94m1.6730\u001b[0m  0.0001  0.2656\n",
      "     32            1.0000        \u001b[32m0.0173\u001b[0m       0.3576            0.3576        \u001b[94m1.6704\u001b[0m  0.0001  0.2656\n",
      "     33            1.0000        0.0332       0.3507            0.3507        \u001b[94m1.6681\u001b[0m  0.0000  0.2656\n",
      "     34            1.0000        0.0287       0.3542            0.3542        \u001b[94m1.6663\u001b[0m  0.0000  0.2656\n",
      "     35            1.0000        0.0325       0.3542            0.3542        \u001b[94m1.6649\u001b[0m  0.0000  0.2656\n",
      "     36            1.0000        0.0298       0.3542            0.3542        \u001b[94m1.6634\u001b[0m  0.0000  0.2500\n",
      "     37            1.0000        0.0282       0.3507            0.3507        \u001b[94m1.6620\u001b[0m  0.0000  0.2656\n",
      "     38            1.0000        0.0288       0.3507            0.3507        \u001b[94m1.6607\u001b[0m  0.0000  0.2656\n",
      "     39            1.0000        0.0375       0.3507            0.3507        \u001b[94m1.6598\u001b[0m  0.0000  0.2656\n",
      "     40            1.0000        0.0187       0.3507            0.3507        \u001b[94m1.6588\u001b[0m  0.0000  0.2656\n",
      "Training model for subject 9 with 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.4000\u001b[0m        \u001b[32m1.6760\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m2.7759\u001b[0m  0.0006  0.2511\n",
      "      2            \u001b[36m0.5333\u001b[0m        \u001b[32m1.1500\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m2.3381\u001b[0m  0.0006  0.2500\n",
      "      3            0.5333        \u001b[32m0.9821\u001b[0m       0.2604            0.2604        \u001b[94m2.0806\u001b[0m  0.0006  0.2656\n",
      "      4            \u001b[36m0.7333\u001b[0m        \u001b[32m0.6082\u001b[0m       0.2569            0.2569        \u001b[94m1.9735\u001b[0m  0.0006  0.2500\n",
      "      5            \u001b[36m0.8000\u001b[0m        \u001b[32m0.4596\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m1.8166\u001b[0m  0.0006  0.2500\n",
      "      6            \u001b[36m0.8333\u001b[0m        \u001b[32m0.2802\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m1.8004\u001b[0m  0.0006  0.2500\n",
      "      7            \u001b[36m0.8667\u001b[0m        \u001b[32m0.2730\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.7646\u001b[0m  0.0006  0.2504\n",
      "      8            \u001b[36m0.9000\u001b[0m        \u001b[32m0.2598\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.7516\u001b[0m  0.0006  0.2659\n",
      "      9            0.9000        \u001b[32m0.2126\u001b[0m       0.3646            0.3646        1.7667  0.0006  0.3130\n",
      "     10            \u001b[36m0.9667\u001b[0m        \u001b[32m0.1883\u001b[0m       0.3576            0.3576        \u001b[94m1.7303\u001b[0m  0.0005  0.3281\n",
      "     11            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1202\u001b[0m       0.3438            0.3438        \u001b[94m1.6856\u001b[0m  0.0005  0.3594\n",
      "     12            1.0000        0.1739       0.3194            0.3194        \u001b[94m1.6515\u001b[0m  0.0005  0.3130\n",
      "     13            1.0000        0.1485       0.3403            0.3403        \u001b[94m1.6422\u001b[0m  0.0005  0.3281\n",
      "     14            1.0000        0.1206       0.3299            0.3299        1.6442  0.0005  0.3594\n",
      "     15            1.0000        \u001b[32m0.0870\u001b[0m       0.3299            0.3299        1.6526  0.0004  0.3438\n",
      "     16            1.0000        \u001b[32m0.0789\u001b[0m       0.3438            0.3438        1.6664  0.0004  0.2813\n",
      "     17            1.0000        0.0871       0.3368            0.3368        1.6774  0.0004  0.2504\n",
      "     18            1.0000        \u001b[32m0.0732\u001b[0m       0.3333            0.3333        1.6854  0.0004  0.2656\n",
      "     19            1.0000        \u001b[32m0.0554\u001b[0m       0.3264            0.3264        1.6916  0.0004  0.2500\n",
      "     20            1.0000        0.0598       0.3056            0.3056        1.6952  0.0003  0.2503\n",
      "     21            1.0000        \u001b[32m0.0379\u001b[0m       0.3056            0.3056        1.6952  0.0003  0.2500\n",
      "     22            1.0000        0.0529       0.3021            0.3021        1.6933  0.0003  0.2500\n",
      "     23            1.0000        0.0391       0.2986            0.2986        1.6903  0.0002  0.2500\n",
      "     24            1.0000        \u001b[32m0.0370\u001b[0m       0.2986            0.2986        1.6893  0.0002  0.2502\n",
      "     25            1.0000        0.0628       0.2951            0.2951        1.6902  0.0002  0.2656\n",
      "     26            1.0000        0.0467       0.2917            0.2917        1.6916  0.0002  0.2500\n",
      "     27            1.0000        0.0494       0.2882            0.2882        1.6942  0.0002  0.2506\n",
      "     28            1.0000        \u001b[32m0.0352\u001b[0m       0.2986            0.2986        1.6956  0.0001  0.2499\n",
      "     29            1.0000        0.0387       0.3021            0.3021        1.6965  0.0001  0.2501\n",
      "     30            1.0000        0.0355       0.3056            0.3056        1.6976  0.0001  0.2500\n",
      "     31            1.0000        \u001b[32m0.0272\u001b[0m       0.3056            0.3056        1.6979  0.0001  0.2500\n",
      "     32            1.0000        0.0529       0.3056            0.3056        1.6981  0.0001  0.2500\n",
      "     33            1.0000        0.0474       0.3056            0.3056        1.6982  0.0000  0.2656\n",
      "     34            1.0000        \u001b[32m0.0261\u001b[0m       0.3056            0.3056        1.6982  0.0000  0.2656\n",
      "     35            1.0000        0.0273       0.3056            0.3056        1.6981  0.0000  0.2500\n",
      "     36            1.0000        \u001b[32m0.0256\u001b[0m       0.3056            0.3056        1.6983  0.0000  0.2500\n",
      "     37            1.0000        \u001b[32m0.0193\u001b[0m       0.3056            0.3056        1.6982  0.0000  0.2656\n",
      "     38            1.0000        0.0312       0.3021            0.3021        1.6980  0.0000  0.2511\n",
      "     39            1.0000        0.0492       0.3021            0.3021        1.6981  0.0000  0.2505\n",
      "     40            1.0000        0.0254       0.3021            0.3021        1.6981  0.0000  0.2500\n",
      "Training model for subject 9 with 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3250\u001b[0m        \u001b[32m1.7738\u001b[0m       \u001b[35m0.2465\u001b[0m            \u001b[31m0.2465\u001b[0m        \u001b[94m2.8291\u001b[0m  0.0006  0.2656\n",
      "      2            \u001b[36m0.4250\u001b[0m        \u001b[32m1.2303\u001b[0m       0.2292            0.2292        \u001b[94m2.4319\u001b[0m  0.0006  0.2660\n",
      "      3            0.3500        \u001b[32m0.9558\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        2.4845  0.0006  0.2500\n",
      "      4            0.4250        \u001b[32m0.7183\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        2.6124  0.0006  0.2662\n",
      "      5            0.4250        \u001b[32m0.5740\u001b[0m       0.2535            0.2535        2.7157  0.0006  0.2501\n",
      "      6            \u001b[36m0.4500\u001b[0m        \u001b[32m0.4545\u001b[0m       0.2535            0.2535        2.7963  0.0006  0.2656\n",
      "      7            \u001b[36m0.5000\u001b[0m        \u001b[32m0.4439\u001b[0m       0.2569            0.2569        2.8002  0.0006  0.2503\n",
      "      8            \u001b[36m0.5500\u001b[0m        \u001b[32m0.3375\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        2.6998  0.0006  0.2660\n",
      "      9            \u001b[36m0.5750\u001b[0m        \u001b[32m0.2606\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        2.5847  0.0006  0.2656\n",
      "     10            \u001b[36m0.6250\u001b[0m        0.3304       0.2778            0.2778        2.5643  0.0005  0.2656\n",
      "     11            \u001b[36m0.7500\u001b[0m        \u001b[32m0.2160\u001b[0m       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m2.3917\u001b[0m  0.0005  0.2659\n",
      "     12            \u001b[36m0.8750\u001b[0m        0.2220       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m2.2404\u001b[0m  0.0005  0.2656\n",
      "     13            \u001b[36m0.9500\u001b[0m        \u001b[32m0.1870\u001b[0m       0.2951            0.2951        \u001b[94m2.1024\u001b[0m  0.0005  0.2661\n",
      "     14            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1577\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m1.9744\u001b[0m  0.0005  0.2656\n",
      "     15            1.0000        \u001b[32m0.1388\u001b[0m       0.3090            0.3090        \u001b[94m1.8997\u001b[0m  0.0004  0.2657\n",
      "     16            1.0000        0.1403       0.3090            0.3090        \u001b[94m1.8530\u001b[0m  0.0004  0.2500\n",
      "     17            1.0000        \u001b[32m0.0983\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m1.8150\u001b[0m  0.0004  0.2656\n",
      "     18            1.0000        0.1165       0.3264            0.3264        \u001b[94m1.7943\u001b[0m  0.0004  0.2656\n",
      "     19            1.0000        \u001b[32m0.0946\u001b[0m       0.3160            0.3160        \u001b[94m1.7773\u001b[0m  0.0004  0.2500\n",
      "     20            1.0000        \u001b[32m0.0899\u001b[0m       0.3264            0.3264        \u001b[94m1.7641\u001b[0m  0.0003  0.2656\n",
      "     21            1.0000        0.0934       0.3264            0.3264        \u001b[94m1.7538\u001b[0m  0.0003  0.2656\n",
      "     22            1.0000        \u001b[32m0.0745\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m1.7466\u001b[0m  0.0003  0.2813\n",
      "     23            1.0000        0.0787       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m1.7421\u001b[0m  0.0002  0.3281\n",
      "     24            1.0000        \u001b[32m0.0570\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.7383\u001b[0m  0.0002  0.3281\n",
      "     25            1.0000        0.1153       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.7361\u001b[0m  0.0002  0.3281\n",
      "     26            1.0000        0.0652       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        1.7363  0.0002  0.3125\n",
      "     27            1.0000        0.1022       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        1.7371  0.0002  0.3438\n",
      "     28            1.0000        \u001b[32m0.0557\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        1.7378  0.0001  0.3440\n",
      "     29            1.0000        0.0611       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        1.7388  0.0001  0.3594\n",
      "     30            1.0000        0.0678       0.3646            0.3646        1.7387  0.0001  0.2656\n",
      "     31            1.0000        0.0586       0.3646            0.3646        1.7389  0.0001  0.2656\n",
      "     32            1.0000        0.0625       0.3646            0.3646        1.7392  0.0001  0.2656\n",
      "     33            1.0000        0.0777       0.3611            0.3611        1.7393  0.0000  0.2500\n",
      "     34            1.0000        0.0774       0.3542            0.3542        1.7393  0.0000  0.2656\n",
      "     35            1.0000        0.0717       0.3542            0.3542        1.7392  0.0000  0.2659\n",
      "     36            1.0000        0.0705       0.3542            0.3542        1.7390  0.0000  0.3910\n",
      "     37            1.0000        0.0727       0.3576            0.3576        1.7387  0.0000  0.2500\n",
      "     38            1.0000        \u001b[32m0.0532\u001b[0m       0.3542            0.3542        1.7383  0.0000  0.2664\n",
      "     39            1.0000        0.0629       0.3576            0.3576        1.7381  0.0000  0.2661\n",
      "     40            1.0000        0.0566       0.3576            0.3576        1.7378  0.0000  0.2608\n",
      "Training model for subject 9 with 50 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3000\u001b[0m        \u001b[32m1.5841\u001b[0m       \u001b[35m0.2257\u001b[0m            \u001b[31m0.2257\u001b[0m        \u001b[94m2.3889\u001b[0m  0.0006  0.2813\n",
      "      2            \u001b[36m0.4800\u001b[0m        \u001b[32m1.3267\u001b[0m       \u001b[35m0.2326\u001b[0m            \u001b[31m0.2326\u001b[0m        \u001b[94m1.8440\u001b[0m  0.0006  0.2813\n",
      "      3            \u001b[36m0.6800\u001b[0m        \u001b[32m0.9183\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m1.8314\u001b[0m  0.0006  0.2815\n",
      "      4            \u001b[36m0.7000\u001b[0m        \u001b[32m0.8478\u001b[0m       0.2778            0.2778        2.1477  0.0006  0.2816\n",
      "      5            0.5800        \u001b[32m0.7407\u001b[0m       0.2674            0.2674        2.4964  0.0006  0.2813\n",
      "      6            0.5000        \u001b[32m0.5068\u001b[0m       0.2674            0.2674        2.7393  0.0006  0.2813\n",
      "      7            0.5000        \u001b[32m0.4851\u001b[0m       0.2639            0.2639        2.9311  0.0006  0.2813\n",
      "      8            0.5000        \u001b[32m0.4181\u001b[0m       0.2639            0.2639        2.9967  0.0006  0.2813\n",
      "      9            0.5600        \u001b[32m0.3654\u001b[0m       0.2674            0.2674        2.9643  0.0006  0.2971\n",
      "     10            0.7000        \u001b[32m0.3099\u001b[0m       0.2743            0.2743        2.8342  0.0005  0.2815\n",
      "     11            \u001b[36m0.7400\u001b[0m        0.3887       0.2743            0.2743        2.7107  0.0005  0.2820\n",
      "     12            \u001b[36m0.7800\u001b[0m        \u001b[32m0.2805\u001b[0m       0.2743            0.2743        2.5526  0.0005  0.2813\n",
      "     13            \u001b[36m0.8400\u001b[0m        0.3090       0.2743            0.2743        2.3973  0.0005  0.2813\n",
      "     14            \u001b[36m0.9400\u001b[0m        \u001b[32m0.2309\u001b[0m       0.2778            0.2778        2.2879  0.0005  0.2813\n",
      "     15            \u001b[36m0.9600\u001b[0m        0.2382       0.2778            0.2778        2.2155  0.0004  0.2813\n",
      "     16            \u001b[36m0.9800\u001b[0m        0.2749       0.2778            0.2778        2.1610  0.0004  0.2972\n",
      "     17            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1504\u001b[0m       0.2778            0.2778        2.1085  0.0004  0.2813\n",
      "     18            1.0000        0.1584       0.2778            0.2778        2.0618  0.0004  0.2813\n",
      "     19            1.0000        \u001b[32m0.1085\u001b[0m       0.2847            0.2847        2.0248  0.0004  0.2815\n",
      "     20            1.0000        0.1857       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        1.9944  0.0003  0.2813\n",
      "     21            1.0000        0.1151       0.3056            0.3056        1.9663  0.0003  0.2818\n",
      "     22            1.0000        \u001b[32m0.1076\u001b[0m       0.3021            0.3021        1.9410  0.0003  0.2814\n",
      "     23            1.0000        0.1154       0.3021            0.3021        1.9172  0.0002  0.2813\n",
      "     24            1.0000        \u001b[32m0.0967\u001b[0m       0.3021            0.3021        1.8952  0.0002  0.2816\n",
      "     25            1.0000        \u001b[32m0.0851\u001b[0m       0.2882            0.2882        1.8793  0.0002  0.2813\n",
      "     26            1.0000        0.1291       0.2986            0.2986        1.8670  0.0002  0.2816\n",
      "     27            1.0000        0.1387       0.3021            0.3021        1.8587  0.0002  0.2817\n",
      "     28            1.0000        0.1149       0.2986            0.2986        1.8526  0.0001  0.2813\n",
      "     29            1.0000        0.1239       0.2986            0.2986        1.8469  0.0001  0.2813\n",
      "     30            1.0000        0.1093       0.3021            0.3021        1.8417  0.0001  0.2969\n",
      "     31            1.0000        0.0891       0.2951            0.2951        1.8378  0.0001  0.2813\n",
      "     32            1.0000        0.1066       0.2917            0.2917        1.8348  0.0001  0.3128\n",
      "     33            1.0000        \u001b[32m0.0774\u001b[0m       0.2986            0.2986        1.8328  0.0000  0.3438\n",
      "     34            1.0000        0.0841       0.2951            0.2951        \u001b[94m1.8313\u001b[0m  0.0000  0.3594\n",
      "     35            1.0000        0.1191       0.2951            0.2951        \u001b[94m1.8310\u001b[0m  0.0000  0.3438\n",
      "     36            1.0000        \u001b[32m0.0639\u001b[0m       0.2951            0.2951        \u001b[94m1.8307\u001b[0m  0.0000  0.3438\n",
      "     37            1.0000        0.0940       0.2951            0.2951        \u001b[94m1.8305\u001b[0m  0.0000  0.3441\n",
      "     38            1.0000        0.1061       0.2951            0.2951        \u001b[94m1.8301\u001b[0m  0.0000  0.3594\n",
      "     39            1.0000        0.0958       0.2951            0.2951        \u001b[94m1.8298\u001b[0m  0.0000  0.3906\n",
      "     40            1.0000        0.1204       0.2951            0.2951        \u001b[94m1.8297\u001b[0m  0.0000  0.2813\n",
      "Training model for subject 9 with 60 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2333\u001b[0m        \u001b[32m1.6678\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m5.4788\u001b[0m  0.0006  0.2972\n",
      "      2            0.2333        \u001b[32m1.2420\u001b[0m       0.2500            0.2500        5.6709  0.0006  0.2969\n",
      "      3            0.2333        \u001b[32m1.0819\u001b[0m       0.2500            0.2500        5.6676  0.0006  0.2969\n",
      "      4            0.2333        \u001b[32m0.8840\u001b[0m       0.2500            0.2500        \u001b[94m5.4515\u001b[0m  0.0006  0.2972\n",
      "      5            0.2333        \u001b[32m0.7612\u001b[0m       0.2500            0.2500        \u001b[94m5.2894\u001b[0m  0.0006  0.3003\n",
      "      6            0.2333        \u001b[32m0.5578\u001b[0m       0.2500            0.2500        \u001b[94m4.7754\u001b[0m  0.0006  0.2813\n",
      "      7            0.2333        0.5969       0.2500            0.2500        \u001b[94m4.0806\u001b[0m  0.0006  0.2813\n",
      "      8            \u001b[36m0.3167\u001b[0m        0.6178       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m3.4358\u001b[0m  0.0006  0.2812\n",
      "      9            \u001b[36m0.4667\u001b[0m        \u001b[32m0.3953\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m3.0082\u001b[0m  0.0006  0.2813\n",
      "     10            \u001b[36m0.5167\u001b[0m        \u001b[32m0.3849\u001b[0m       0.2743            0.2743        \u001b[94m2.8335\u001b[0m  0.0005  0.2973\n",
      "     11            \u001b[36m0.6000\u001b[0m        \u001b[32m0.2726\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m2.7253\u001b[0m  0.0005  0.2970\n",
      "     12            \u001b[36m0.6667\u001b[0m        0.3097       0.2812            0.2812        \u001b[94m2.5951\u001b[0m  0.0005  0.2813\n",
      "     13            \u001b[36m0.7500\u001b[0m        0.2848       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        \u001b[94m2.4522\u001b[0m  0.0005  0.2813\n",
      "     14            \u001b[36m0.8333\u001b[0m        0.2881       0.2882            0.2882        \u001b[94m2.2912\u001b[0m  0.0005  0.2814\n",
      "     15            \u001b[36m0.8833\u001b[0m        \u001b[32m0.2441\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        \u001b[94m2.1398\u001b[0m  0.0004  0.2813\n",
      "     16            \u001b[36m0.9167\u001b[0m        \u001b[32m0.2095\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m2.0329\u001b[0m  0.0004  0.2813\n",
      "     17            \u001b[36m0.9667\u001b[0m        \u001b[32m0.1948\u001b[0m       0.3229            0.3229        \u001b[94m1.9378\u001b[0m  0.0004  0.2969\n",
      "     18            \u001b[36m1.0000\u001b[0m        0.2310       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.8642\u001b[0m  0.0004  0.2969\n",
      "     19            1.0000        0.2412       0.3472            0.3472        \u001b[94m1.8054\u001b[0m  0.0004  0.2969\n",
      "     20            1.0000        0.2066       0.3403            0.3403        \u001b[94m1.7582\u001b[0m  0.0003  0.2814\n",
      "     21            1.0000        \u001b[32m0.1720\u001b[0m       0.3403            0.3403        \u001b[94m1.7275\u001b[0m  0.0003  0.2817\n",
      "     22            1.0000        \u001b[32m0.1188\u001b[0m       0.3472            0.3472        \u001b[94m1.7047\u001b[0m  0.0003  0.2817\n",
      "     23            1.0000        0.1283       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.6896\u001b[0m  0.0002  0.2813\n",
      "     24            1.0000        0.1388       0.3507            0.3507        \u001b[94m1.6782\u001b[0m  0.0002  0.2813\n",
      "     25            1.0000        0.1323       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.6700\u001b[0m  0.0002  0.2832\n",
      "     26            1.0000        0.1351       0.3611            0.3611        \u001b[94m1.6659\u001b[0m  0.0002  0.2816\n",
      "     27            1.0000        0.1591       0.3611            0.3611        \u001b[94m1.6632\u001b[0m  0.0002  0.2813\n",
      "     28            1.0000        0.1296       0.3542            0.3542        \u001b[94m1.6623\u001b[0m  0.0001  0.2818\n",
      "     29            1.0000        0.1365       0.3507            0.3507        1.6631  0.0001  0.2969\n",
      "     30            1.0000        \u001b[32m0.0983\u001b[0m       0.3438            0.3438        1.6634  0.0001  0.2969\n",
      "     31            1.0000        0.1303       0.3472            0.3472        1.6647  0.0001  0.2969\n",
      "     32            1.0000        0.1140       0.3472            0.3472        1.6659  0.0001  0.2972\n",
      "     33            1.0000        0.1435       0.3507            0.3507        1.6671  0.0000  0.2821\n",
      "     34            1.0000        0.1207       0.3542            0.3542        1.6673  0.0000  0.2969\n",
      "     35            1.0000        0.1562       0.3542            0.3542        1.6676  0.0000  0.2973\n",
      "     36            1.0000        0.1257       0.3542            0.3542        1.6677  0.0000  0.2867\n",
      "     37            1.0000        0.1164       0.3542            0.3542        1.6677  0.0000  0.2973\n",
      "     38            1.0000        0.1106       0.3576            0.3576        1.6675  0.0000  0.2969\n",
      "     39            1.0000        0.1129       0.3576            0.3576        1.6675  0.0000  0.3594\n",
      "     40            1.0000        0.1004       0.3576            0.3576        1.6674  0.0000  0.3438\n",
      "Training model for subject 9 with 70 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3143\u001b[0m        \u001b[32m1.5902\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m2.5371\u001b[0m  0.0006  0.4219\n",
      "      2            \u001b[36m0.3714\u001b[0m        \u001b[32m1.2666\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        2.6720  0.0006  0.3594\n",
      "      3            0.3000        \u001b[32m1.0607\u001b[0m       0.2569            0.2569        3.0605  0.0006  0.3594\n",
      "      4            0.2429        \u001b[32m0.8409\u001b[0m       0.2500            0.2500        3.7012  0.0006  0.3907\n",
      "      5            0.2429        \u001b[32m0.7923\u001b[0m       0.2500            0.2500        4.0651  0.0006  0.4063\n",
      "      6            0.2714        \u001b[32m0.7487\u001b[0m       0.2500            0.2500        4.0915  0.0006  0.3125\n",
      "      7            0.2857        \u001b[32m0.5964\u001b[0m       0.2500            0.2500        3.8106  0.0006  0.3130\n",
      "      8            0.3571        \u001b[32m0.4536\u001b[0m       0.2604            0.2604        3.5062  0.0006  0.3126\n",
      "      9            \u001b[36m0.4429\u001b[0m        0.4988       0.2639            0.2639        3.1110  0.0006  0.2969\n",
      "     10            \u001b[36m0.5000\u001b[0m        \u001b[32m0.4336\u001b[0m       0.2743            0.2743        2.7477  0.0005  0.3125\n",
      "     11            \u001b[36m0.6286\u001b[0m        \u001b[32m0.3882\u001b[0m       0.2951            0.2951        \u001b[94m2.5073\u001b[0m  0.0005  0.3125\n",
      "     12            \u001b[36m0.7143\u001b[0m        \u001b[32m0.3759\u001b[0m       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m2.3100\u001b[0m  0.0005  0.2969\n",
      "     13            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3134\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m2.1571\u001b[0m  0.0005  0.2969\n",
      "     14            0.9000        0.3337       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m2.0751\u001b[0m  0.0005  0.3125\n",
      "     15            \u001b[36m0.9286\u001b[0m        \u001b[32m0.2677\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m2.0282\u001b[0m  0.0004  0.2969\n",
      "     16            \u001b[36m0.9429\u001b[0m        \u001b[32m0.2395\u001b[0m       0.3299            0.3299        \u001b[94m1.9834\u001b[0m  0.0004  0.3125\n",
      "     17            \u001b[36m0.9571\u001b[0m        \u001b[32m0.2042\u001b[0m       0.3264            0.3264        \u001b[94m1.9259\u001b[0m  0.0004  0.2973\n",
      "     18            \u001b[36m0.9714\u001b[0m        0.2250       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.8650\u001b[0m  0.0004  0.3004\n",
      "     19            \u001b[36m0.9857\u001b[0m        \u001b[32m0.1938\u001b[0m       0.3368            0.3368        \u001b[94m1.8068\u001b[0m  0.0004  0.2969\n",
      "     20            0.9857        \u001b[32m0.1654\u001b[0m       0.3333            0.3333        \u001b[94m1.7613\u001b[0m  0.0003  0.2969\n",
      "     21            \u001b[36m1.0000\u001b[0m        0.1673       0.3403            0.3403        \u001b[94m1.7191\u001b[0m  0.0003  0.3130\n",
      "     22            1.0000        0.1717       0.3403            0.3403        \u001b[94m1.6838\u001b[0m  0.0003  0.2969\n",
      "     23            1.0000        0.1818       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.6568\u001b[0m  0.0002  0.3125\n",
      "     24            1.0000        \u001b[32m0.1321\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.6328\u001b[0m  0.0002  0.2969\n",
      "     25            1.0000        0.1407       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.6165\u001b[0m  0.0002  0.2969\n",
      "     26            1.0000        0.1578       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.6066\u001b[0m  0.0002  0.3129\n",
      "     27            1.0000        0.1572       0.3715            0.3715        \u001b[94m1.6044\u001b[0m  0.0002  0.2969\n",
      "     28            1.0000        0.1336       0.3750            0.3750        \u001b[94m1.6014\u001b[0m  0.0001  0.3125\n",
      "     29            1.0000        \u001b[32m0.1116\u001b[0m       0.3750            0.3750        \u001b[94m1.5992\u001b[0m  0.0001  0.3125\n",
      "     30            1.0000        0.1224       0.3750            0.3750        \u001b[94m1.5984\u001b[0m  0.0001  0.3126\n",
      "     31            1.0000        \u001b[32m0.1057\u001b[0m       0.3750            0.3750        \u001b[94m1.5970\u001b[0m  0.0001  0.2971\n",
      "     32            1.0000        0.1460       0.3715            0.3715        \u001b[94m1.5965\u001b[0m  0.0001  0.3125\n",
      "     33            1.0000        0.1340       0.3750            0.3750        \u001b[94m1.5959\u001b[0m  0.0000  0.3125\n",
      "     34            1.0000        0.1253       0.3715            0.3715        \u001b[94m1.5950\u001b[0m  0.0000  0.3125\n",
      "     35            1.0000        0.1369       0.3715            0.3715        \u001b[94m1.5942\u001b[0m  0.0000  0.2969\n",
      "     36            1.0000        0.1240       0.3715            0.3715        \u001b[94m1.5933\u001b[0m  0.0000  0.3129\n",
      "     37            1.0000        0.1344       0.3681            0.3681        \u001b[94m1.5925\u001b[0m  0.0000  0.2969\n",
      "     38            1.0000        0.1245       0.3681            0.3681        \u001b[94m1.5915\u001b[0m  0.0000  0.2969\n",
      "     39            1.0000        0.1449       0.3681            0.3681        \u001b[94m1.5907\u001b[0m  0.0000  0.3125\n",
      "     40            1.0000        0.1377       0.3681            0.3681        \u001b[94m1.5900\u001b[0m  0.0000  0.3125\n",
      "Training model for subject 9 with 80 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3625\u001b[0m        \u001b[32m1.6864\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m2.1855\u001b[0m  0.0006  0.3281\n",
      "      2            \u001b[36m0.3875\u001b[0m        \u001b[32m1.2330\u001b[0m       0.2465            0.2465        3.1325  0.0006  0.3594\n",
      "      3            0.3625        \u001b[32m1.0092\u001b[0m       0.2569            0.2569        3.7051  0.0006  0.3594\n",
      "      4            0.3250        \u001b[32m0.8256\u001b[0m       0.2535            0.2535        3.8913  0.0006  0.4063\n",
      "      5            0.3250        0.9486       0.2535            0.2535        3.8367  0.0006  0.3750\n",
      "      6            0.3375        \u001b[32m0.7857\u001b[0m       0.2535            0.2535        3.7458  0.0006  0.3907\n",
      "      7            0.3125        \u001b[32m0.6630\u001b[0m       0.2569            0.2569        3.7474  0.0006  0.3750\n",
      "      8            0.3500        \u001b[32m0.5695\u001b[0m       0.2535            0.2535        3.6082  0.0006  0.4063\n",
      "      9            0.3750        0.5713       0.2604            0.2604        3.2549  0.0006  0.3911\n",
      "     10            \u001b[36m0.4750\u001b[0m        \u001b[32m0.5096\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        2.9423  0.0005  0.3281\n",
      "     11            \u001b[36m0.5750\u001b[0m        \u001b[32m0.4828\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        2.6111  0.0005  0.3126\n",
      "     12            \u001b[36m0.6250\u001b[0m        \u001b[32m0.4714\u001b[0m       \u001b[35m0.3021\u001b[0m            \u001b[31m0.3021\u001b[0m        2.3312  0.0005  0.3129\n",
      "     13            \u001b[36m0.7250\u001b[0m        \u001b[32m0.2965\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        \u001b[94m2.1044\u001b[0m  0.0005  0.3281\n",
      "     14            \u001b[36m0.7625\u001b[0m        0.3514       \u001b[35m0.3403\u001b[0m            \u001b[31m0.3403\u001b[0m        \u001b[94m1.9308\u001b[0m  0.0005  0.3283\n",
      "     15            \u001b[36m0.8250\u001b[0m        0.3253       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.8010\u001b[0m  0.0004  0.3282\n",
      "     16            \u001b[36m0.8750\u001b[0m        0.3229       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.7115\u001b[0m  0.0004  0.3125\n",
      "     17            \u001b[36m0.9125\u001b[0m        \u001b[32m0.2963\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.6512\u001b[0m  0.0004  0.3125\n",
      "     18            \u001b[36m0.9375\u001b[0m        \u001b[32m0.2716\u001b[0m       0.3993            0.3993        \u001b[94m1.5869\u001b[0m  0.0004  0.3125\n",
      "     19            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2685\u001b[0m       0.3854            0.3854        \u001b[94m1.5337\u001b[0m  0.0004  0.3126\n",
      "     20            \u001b[36m0.9875\u001b[0m        \u001b[32m0.2131\u001b[0m       0.3819            0.3819        \u001b[94m1.4945\u001b[0m  0.0003  0.3281\n",
      "     21            \u001b[36m1.0000\u001b[0m        0.2398       0.3958            0.3958        \u001b[94m1.4742\u001b[0m  0.0003  0.3281\n",
      "     22            1.0000        \u001b[32m0.1968\u001b[0m       0.3993            0.3993        \u001b[94m1.4632\u001b[0m  0.0003  0.3282\n",
      "     23            1.0000        0.2299       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.4593\u001b[0m  0.0002  0.3281\n",
      "     24            1.0000        0.2316       0.4028            0.4028        \u001b[94m1.4520\u001b[0m  0.0002  0.3283\n",
      "     25            1.0000        \u001b[32m0.1824\u001b[0m       0.4028            0.4028        \u001b[94m1.4447\u001b[0m  0.0002  0.3281\n",
      "     26            1.0000        \u001b[32m0.1626\u001b[0m       0.4028            0.4028        \u001b[94m1.4371\u001b[0m  0.0002  0.3281\n",
      "     27            1.0000        0.2264       0.3924            0.3924        \u001b[94m1.4344\u001b[0m  0.0002  0.3281\n",
      "     28            1.0000        \u001b[32m0.1618\u001b[0m       0.3958            0.3958        \u001b[94m1.4333\u001b[0m  0.0001  0.3281\n",
      "     29            1.0000        0.1818       0.3958            0.3958        \u001b[94m1.4329\u001b[0m  0.0001  0.3129\n",
      "     30            1.0000        0.2124       0.3993            0.3993        \u001b[94m1.4328\u001b[0m  0.0001  0.3125\n",
      "     31            1.0000        0.1645       0.4028            0.4028        1.4331  0.0001  0.3130\n",
      "     32            1.0000        \u001b[32m0.1579\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.4323\u001b[0m  0.0001  0.3281\n",
      "     33            1.0000        0.2031       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.4311\u001b[0m  0.0000  0.3125\n",
      "     34            1.0000        0.1587       0.4097            0.4097        \u001b[94m1.4297\u001b[0m  0.0000  0.3281\n",
      "     35            1.0000        \u001b[32m0.1538\u001b[0m       0.4062            0.4062        \u001b[94m1.4283\u001b[0m  0.0000  0.3281\n",
      "     36            1.0000        0.1785       0.4028            0.4028        \u001b[94m1.4276\u001b[0m  0.0000  0.3281\n",
      "     37            1.0000        \u001b[32m0.1338\u001b[0m       0.3993            0.3993        \u001b[94m1.4272\u001b[0m  0.0000  0.3281\n",
      "     38            1.0000        \u001b[32m0.1291\u001b[0m       0.3993            0.3993        \u001b[94m1.4271\u001b[0m  0.0000  0.3283\n",
      "     39            1.0000        0.1871       0.3993            0.3993        \u001b[94m1.4271\u001b[0m  0.0000  0.3281\n",
      "     40            1.0000        0.1436       0.3958            0.3958        1.4272  0.0000  0.3127\n",
      "Training model for subject 9 with 90 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2444\u001b[0m        \u001b[32m1.7455\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.4671\u001b[0m  0.0006  0.4063\n",
      "      2            \u001b[36m0.2556\u001b[0m        \u001b[32m1.4145\u001b[0m       0.2500            0.2500        \u001b[94m2.2380\u001b[0m  0.0006  0.3594\n",
      "      3            \u001b[36m0.3778\u001b[0m        \u001b[32m1.1498\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m1.9445\u001b[0m  0.0006  0.4063\n",
      "      4            \u001b[36m0.4778\u001b[0m        \u001b[32m1.0071\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        1.9761  0.0006  0.3906\n",
      "      5            0.4000        \u001b[32m0.9856\u001b[0m       0.2465            0.2465        2.2781  0.0006  0.3907\n",
      "      6            0.4222        \u001b[32m0.8520\u001b[0m       0.2535            0.2535        2.4293  0.0006  0.4063\n",
      "      7            0.4778        \u001b[32m0.7816\u001b[0m       0.2708            0.2708        2.2462  0.0006  0.3907\n",
      "      8            \u001b[36m0.5556\u001b[0m        0.8000       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        2.0809  0.0006  0.4688\n",
      "      9            \u001b[36m0.5889\u001b[0m        \u001b[32m0.5775\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        1.9770  0.0006  0.3907\n",
      "     10            \u001b[36m0.6444\u001b[0m        0.5789       0.3229            0.3229        \u001b[94m1.8904\u001b[0m  0.0005  0.3438\n",
      "     11            \u001b[36m0.7667\u001b[0m        \u001b[32m0.5058\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.7926\u001b[0m  0.0005  0.3286\n",
      "     12            \u001b[36m0.8111\u001b[0m        0.5445       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.6904\u001b[0m  0.0005  0.3281\n",
      "     13            \u001b[36m0.9222\u001b[0m        \u001b[32m0.3853\u001b[0m       0.3403            0.3403        \u001b[94m1.6161\u001b[0m  0.0005  0.3442\n",
      "     14            \u001b[36m0.9333\u001b[0m        0.4487       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.5716\u001b[0m  0.0005  0.3282\n",
      "     15            \u001b[36m0.9667\u001b[0m        0.4282       0.3646            0.3646        \u001b[94m1.5275\u001b[0m  0.0004  0.3289\n",
      "     16            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3295\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.5011\u001b[0m  0.0004  0.3289\n",
      "     17            \u001b[36m0.9889\u001b[0m        0.3770       0.3750            0.3750        \u001b[94m1.4834\u001b[0m  0.0004  0.3438\n",
      "     18            0.9889        \u001b[32m0.2900\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4674\u001b[0m  0.0004  0.3438\n",
      "     19            0.9889        0.2987       0.3958            0.3958        \u001b[94m1.4579\u001b[0m  0.0004  0.3281\n",
      "     20            0.9889        0.3084       0.3889            0.3889        \u001b[94m1.4499\u001b[0m  0.0003  0.3285\n",
      "     21            0.9889        \u001b[32m0.2846\u001b[0m       0.3924            0.3924        \u001b[94m1.4383\u001b[0m  0.0003  0.3281\n",
      "     22            0.9889        \u001b[32m0.2794\u001b[0m       0.3958            0.3958        \u001b[94m1.4310\u001b[0m  0.0003  0.3438\n",
      "     23            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2398\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.4260\u001b[0m  0.0002  0.3438\n",
      "     24            1.0000        0.2636       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.4236\u001b[0m  0.0002  0.3285\n",
      "     25            1.0000        \u001b[32m0.2375\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.4201\u001b[0m  0.0002  0.3287\n",
      "     26            1.0000        0.2395       0.4097            0.4097        \u001b[94m1.4169\u001b[0m  0.0002  0.3442\n",
      "     27            1.0000        \u001b[32m0.2309\u001b[0m       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.4165\u001b[0m  0.0002  0.3438\n",
      "     28            1.0000        \u001b[32m0.2145\u001b[0m       0.4167            0.4167        1.4168  0.0001  0.3281\n",
      "     29            1.0000        0.2509       0.4201            0.4201        1.4166  0.0001  0.3281\n",
      "     30            1.0000        0.2379       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        1.4182  0.0001  0.3281\n",
      "     31            1.0000        \u001b[32m0.2108\u001b[0m       0.4201            0.4201        1.4209  0.0001  0.3442\n",
      "     32            1.0000        \u001b[32m0.1949\u001b[0m       0.4236            0.4236        1.4238  0.0001  0.3281\n",
      "     33            1.0000        0.2007       0.4236            0.4236        1.4252  0.0000  0.3286\n",
      "     34            1.0000        0.2219       0.4236            0.4236        1.4254  0.0000  0.3438\n",
      "     35            1.0000        0.2354       0.4236            0.4236        1.4254  0.0000  0.3281\n",
      "     36            1.0000        0.2012       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        1.4250  0.0000  0.3599\n",
      "     37            1.0000        \u001b[32m0.1754\u001b[0m       0.4271            0.4271        1.4241  0.0000  0.4223\n",
      "     38            1.0000        0.1955       0.4271            0.4271        1.4234  0.0000  0.3438\n",
      "     39            1.0000        0.2249       0.4236            0.4236        1.4226  0.0000  0.3438\n",
      "     40            1.0000        0.2137       0.4236            0.4236        1.4220  0.0000  0.3281\n",
      "Training model for subject 9 with 100 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.7598\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m5.5373\u001b[0m  0.0006  0.3750\n",
      "      2            0.2500        \u001b[32m1.3461\u001b[0m       0.2535            0.2535        \u001b[94m4.9889\u001b[0m  0.0006  0.4219\n",
      "      3            0.2500        \u001b[32m1.1157\u001b[0m       0.2500            0.2500        \u001b[94m4.4773\u001b[0m  0.0006  0.4375\n",
      "      4            \u001b[36m0.2700\u001b[0m        \u001b[32m0.9055\u001b[0m       0.2500            0.2500        \u001b[94m3.8142\u001b[0m  0.0006  0.4063\n",
      "      5            \u001b[36m0.2900\u001b[0m        \u001b[32m0.8669\u001b[0m       0.2535            0.2535        \u001b[94m3.3403\u001b[0m  0.0006  0.4378\n",
      "      6            \u001b[36m0.3900\u001b[0m        \u001b[32m0.7039\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m2.8602\u001b[0m  0.0006  0.5625\n",
      "      7            \u001b[36m0.4800\u001b[0m        0.7470       0.2604            0.2604        \u001b[94m2.5539\u001b[0m  0.0006  0.3907\n",
      "      8            \u001b[36m0.5800\u001b[0m        \u001b[32m0.6019\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m2.3503\u001b[0m  0.0006  0.3598\n",
      "      9            \u001b[36m0.6500\u001b[0m        0.6760       0.3056            0.3056        \u001b[94m2.2349\u001b[0m  0.0006  0.3594\n",
      "     10            \u001b[36m0.6900\u001b[0m        \u001b[32m0.5063\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m2.1521\u001b[0m  0.0005  0.3438\n",
      "     11            \u001b[36m0.7400\u001b[0m        0.5426       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m2.0727\u001b[0m  0.0005  0.3438\n",
      "     12            \u001b[36m0.8100\u001b[0m        \u001b[32m0.5043\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m2.0131\u001b[0m  0.0005  0.3438\n",
      "     13            \u001b[36m0.8500\u001b[0m        \u001b[32m0.4574\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m1.9515\u001b[0m  0.0005  0.3750\n",
      "     14            \u001b[36m0.8800\u001b[0m        \u001b[32m0.4206\u001b[0m       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m1.8942\u001b[0m  0.0005  0.3448\n",
      "     15            \u001b[36m0.9300\u001b[0m        \u001b[32m0.3660\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.8388\u001b[0m  0.0004  0.3443\n",
      "     16            \u001b[36m0.9500\u001b[0m        \u001b[32m0.3504\u001b[0m       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.7961\u001b[0m  0.0004  0.3438\n",
      "     17            \u001b[36m0.9700\u001b[0m        \u001b[32m0.3405\u001b[0m       0.3542            0.3542        \u001b[94m1.7540\u001b[0m  0.0004  0.3461\n",
      "     18            \u001b[36m0.9900\u001b[0m        \u001b[32m0.3169\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.7066\u001b[0m  0.0004  0.3603\n",
      "     19            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2685\u001b[0m       0.3611            0.3611        \u001b[94m1.6750\u001b[0m  0.0004  0.3594\n",
      "     20            1.0000        0.2810       0.3611            0.3611        \u001b[94m1.6491\u001b[0m  0.0003  0.3438\n",
      "     21            1.0000        \u001b[32m0.2611\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.6222\u001b[0m  0.0003  0.3594\n",
      "     22            1.0000        \u001b[32m0.2572\u001b[0m       0.3611            0.3611        \u001b[94m1.6024\u001b[0m  0.0003  0.3438\n",
      "     23            1.0000        \u001b[32m0.2228\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.5935\u001b[0m  0.0002  0.3438\n",
      "     24            1.0000        0.2410       0.3854            0.3854        \u001b[94m1.5928\u001b[0m  0.0002  0.4688\n",
      "     25            1.0000        \u001b[32m0.1870\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        1.5960  0.0002  0.3594\n",
      "     26            1.0000        0.2434       0.3854            0.3854        1.5984  0.0002  0.3520\n",
      "     27            1.0000        0.2167       0.3854            0.3854        1.5974  0.0002  0.3594\n",
      "     28            1.0000        0.2069       0.3854            0.3854        1.5940  0.0001  0.3598\n",
      "     29            1.0000        0.2124       0.3854            0.3854        \u001b[94m1.5926\u001b[0m  0.0001  0.3441\n",
      "     30            1.0000        0.2078       0.3889            0.3889        \u001b[94m1.5920\u001b[0m  0.0001  0.3438\n",
      "     31            1.0000        0.2342       0.3889            0.3889        \u001b[94m1.5903\u001b[0m  0.0001  0.3438\n",
      "     32            1.0000        0.1940       0.3889            0.3889        \u001b[94m1.5868\u001b[0m  0.0001  0.3441\n",
      "     33            1.0000        0.2135       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.5841\u001b[0m  0.0000  0.3594\n",
      "     34            1.0000        0.2094       0.3924            0.3924        \u001b[94m1.5822\u001b[0m  0.0000  0.3594\n",
      "     35            1.0000        0.2117       0.3924            0.3924        \u001b[94m1.5810\u001b[0m  0.0000  0.3594\n",
      "     36            1.0000        0.2145       0.3924            0.3924        \u001b[94m1.5799\u001b[0m  0.0000  0.3594\n",
      "     37            1.0000        0.2039       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.5790\u001b[0m  0.0000  0.4063\n",
      "     38            1.0000        \u001b[32m0.1633\u001b[0m       0.3958            0.3958        \u001b[94m1.5785\u001b[0m  0.0000  0.4063\n",
      "     39            1.0000        0.2032       0.3958            0.3958        \u001b[94m1.5781\u001b[0m  0.0000  0.4375\n",
      "     40            1.0000        0.2319       0.3958            0.3958        \u001b[94m1.5778\u001b[0m  0.0000  0.4063\n",
      "Training model for subject 9 with 110 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2727\u001b[0m        \u001b[32m1.7876\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m3.8022\u001b[0m  0.0006  0.4375\n",
      "      2            \u001b[36m0.4000\u001b[0m        \u001b[32m1.1957\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        3.8638  0.0006  0.4532\n",
      "      3            \u001b[36m0.4455\u001b[0m        \u001b[32m1.0093\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m3.7411\u001b[0m  0.0006  0.4532\n",
      "      4            0.4455        \u001b[32m0.8870\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m3.4062\u001b[0m  0.0006  0.3751\n",
      "      5            \u001b[36m0.4727\u001b[0m        0.9484       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m2.8249\u001b[0m  0.0006  0.3594\n",
      "      6            \u001b[36m0.5000\u001b[0m        \u001b[32m0.7481\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m2.3349\u001b[0m  0.0006  0.3594\n",
      "      7            \u001b[36m0.6091\u001b[0m        \u001b[32m0.7086\u001b[0m       0.3681            0.3681        \u001b[94m2.0265\u001b[0m  0.0006  0.3750\n",
      "      8            \u001b[36m0.7091\u001b[0m        \u001b[32m0.6880\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.8891\u001b[0m  0.0006  0.3596\n",
      "      9            \u001b[36m0.7455\u001b[0m        \u001b[32m0.6584\u001b[0m       0.3542            0.3542        \u001b[94m1.8181\u001b[0m  0.0006  0.3598\n",
      "     10            \u001b[36m0.7727\u001b[0m        \u001b[32m0.5746\u001b[0m       0.3715            0.3715        \u001b[94m1.8001\u001b[0m  0.0005  0.3599\n",
      "     11            \u001b[36m0.8000\u001b[0m        \u001b[32m0.4974\u001b[0m       0.3646            0.3646        1.8202  0.0005  0.3441\n",
      "     12            \u001b[36m0.8364\u001b[0m        0.5231       0.3542            0.3542        1.8481  0.0005  0.3594\n",
      "     13            \u001b[36m0.8909\u001b[0m        \u001b[32m0.4673\u001b[0m       0.3403            0.3403        1.8195  0.0005  0.3602\n",
      "     14            \u001b[36m0.9091\u001b[0m        \u001b[32m0.4324\u001b[0m       0.3438            0.3438        \u001b[94m1.7911\u001b[0m  0.0005  0.3438\n",
      "     15            \u001b[36m0.9545\u001b[0m        \u001b[32m0.4016\u001b[0m       0.3646            0.3646        \u001b[94m1.7609\u001b[0m  0.0004  0.3594\n",
      "     16            \u001b[36m0.9727\u001b[0m        \u001b[32m0.3252\u001b[0m       0.3646            0.3646        \u001b[94m1.7493\u001b[0m  0.0004  0.3603\n",
      "     17            0.9727        0.4246       0.3646            0.3646        \u001b[94m1.7392\u001b[0m  0.0004  0.3602\n",
      "     18            0.9727        \u001b[32m0.3113\u001b[0m       0.3576            0.3576        \u001b[94m1.7311\u001b[0m  0.0004  0.3594\n",
      "     19            \u001b[36m0.9818\u001b[0m        0.3400       0.3611            0.3611        \u001b[94m1.7041\u001b[0m  0.0004  0.3594\n",
      "     20            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2973\u001b[0m       0.3681            0.3681        \u001b[94m1.6790\u001b[0m  0.0003  0.3594\n",
      "     21            1.0000        \u001b[32m0.2803\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.6483\u001b[0m  0.0003  0.3438\n",
      "     22            1.0000        0.3012       0.3819            0.3819        \u001b[94m1.6237\u001b[0m  0.0003  0.3594\n",
      "     23            1.0000        \u001b[32m0.2747\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.6044\u001b[0m  0.0002  0.3594\n",
      "     24            1.0000        \u001b[32m0.2552\u001b[0m       0.3854            0.3854        \u001b[94m1.5928\u001b[0m  0.0002  0.3595\n",
      "     25            1.0000        0.2743       0.3854            0.3854        \u001b[94m1.5820\u001b[0m  0.0002  0.3594\n",
      "     26            1.0000        0.2820       0.3889            0.3889        \u001b[94m1.5750\u001b[0m  0.0002  0.3594\n",
      "     27            1.0000        0.2659       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.5701\u001b[0m  0.0002  0.3598\n",
      "     28            1.0000        0.2568       0.3889            0.3889        \u001b[94m1.5631\u001b[0m  0.0001  0.3594\n",
      "     29            1.0000        0.2757       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.5573\u001b[0m  0.0001  0.3598\n",
      "     30            1.0000        \u001b[32m0.2121\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.5549\u001b[0m  0.0001  0.3750\n",
      "     31            1.0000        0.2266       0.4062            0.4062        \u001b[94m1.5534\u001b[0m  0.0001  0.3594\n",
      "     32            1.0000        0.2411       0.4062            0.4062        \u001b[94m1.5509\u001b[0m  0.0001  0.3754\n",
      "     33            1.0000        0.2191       0.4062            0.4062        \u001b[94m1.5475\u001b[0m  0.0000  0.4063\n",
      "     34            1.0000        0.2271       0.4062            0.4062        \u001b[94m1.5455\u001b[0m  0.0000  0.4375\n",
      "     35            1.0000        0.2385       0.4062            0.4062        \u001b[94m1.5440\u001b[0m  0.0000  0.4219\n",
      "     36            1.0000        0.2163       0.4062            0.4062        \u001b[94m1.5431\u001b[0m  0.0000  0.4375\n",
      "     37            1.0000        0.2167       0.4062            0.4062        \u001b[94m1.5422\u001b[0m  0.0000  0.4219\n",
      "     38            1.0000        0.2433       0.4062            0.4062        \u001b[94m1.5419\u001b[0m  0.0000  0.4375\n",
      "     39            1.0000        \u001b[32m0.2061\u001b[0m       0.4062            0.4062        \u001b[94m1.5415\u001b[0m  0.0000  0.3907\n",
      "     40            1.0000        0.2769       0.4062            0.4062        \u001b[94m1.5414\u001b[0m  0.0000  0.3911\n",
      "Training model for subject 9 with 120 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3667\u001b[0m        \u001b[32m1.6662\u001b[0m       \u001b[35m0.2465\u001b[0m            \u001b[31m0.2465\u001b[0m        \u001b[94m2.8313\u001b[0m  0.0006  0.3750\n",
      "      2            \u001b[36m0.4250\u001b[0m        \u001b[32m1.2905\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m2.6649\u001b[0m  0.0006  0.3750\n",
      "      3            0.4250        \u001b[32m1.1182\u001b[0m       0.2917            0.2917        2.7937  0.0006  0.3750\n",
      "      4            0.3833        \u001b[32m1.0357\u001b[0m       0.2847            0.2847        2.7483  0.0006  0.3754\n",
      "      5            0.4083        \u001b[32m0.9472\u001b[0m       0.2986            0.2986        2.8108  0.0006  0.3750\n",
      "      6            \u001b[36m0.4667\u001b[0m        \u001b[32m0.8009\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        2.6791  0.0006  0.3754\n",
      "      7            \u001b[36m0.4833\u001b[0m        0.8024       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m2.6009\u001b[0m  0.0006  0.3750\n",
      "      8            \u001b[36m0.4917\u001b[0m        \u001b[32m0.7063\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m2.4772\u001b[0m  0.0006  0.3750\n",
      "      9            \u001b[36m0.5333\u001b[0m        \u001b[32m0.5702\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m2.3588\u001b[0m  0.0006  0.3753\n",
      "     10            0.5333        0.6089       0.3750            0.3750        \u001b[94m2.2447\u001b[0m  0.0005  0.3750\n",
      "     11            \u001b[36m0.5750\u001b[0m        0.6250       0.3750            0.3750        \u001b[94m2.1545\u001b[0m  0.0005  0.3907\n",
      "     12            \u001b[36m0.6083\u001b[0m        \u001b[32m0.4938\u001b[0m       0.3750            0.3750        \u001b[94m2.0709\u001b[0m  0.0005  0.3750\n",
      "     13            \u001b[36m0.6583\u001b[0m        0.5148       0.3715            0.3715        \u001b[94m1.9811\u001b[0m  0.0005  0.3750\n",
      "     14            \u001b[36m0.6833\u001b[0m        \u001b[32m0.4651\u001b[0m       0.3785            0.3785        \u001b[94m1.9374\u001b[0m  0.0005  0.3752\n",
      "     15            \u001b[36m0.7500\u001b[0m        \u001b[32m0.4260\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.8767\u001b[0m  0.0004  0.3750\n",
      "     16            \u001b[36m0.8417\u001b[0m        \u001b[32m0.4031\u001b[0m       0.3715            0.3715        \u001b[94m1.7951\u001b[0m  0.0004  0.3750\n",
      "     17            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3539\u001b[0m       0.3819            0.3819        \u001b[94m1.7227\u001b[0m  0.0004  0.3755\n",
      "     18            \u001b[36m0.9250\u001b[0m        0.3567       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.6633\u001b[0m  0.0004  0.3750\n",
      "     19            \u001b[36m0.9417\u001b[0m        \u001b[32m0.3434\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.6100\u001b[0m  0.0004  0.3598\n",
      "     20            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3429\u001b[0m       0.3924            0.3924        \u001b[94m1.5622\u001b[0m  0.0003  0.3753\n",
      "     21            \u001b[36m0.9917\u001b[0m        \u001b[32m0.2949\u001b[0m       0.3889            0.3889        \u001b[94m1.5198\u001b[0m  0.0003  0.3750\n",
      "     22            0.9917        \u001b[32m0.2716\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.4806\u001b[0m  0.0003  0.3750\n",
      "     23            \u001b[36m1.0000\u001b[0m        0.2767       0.4028            0.4028        \u001b[94m1.4512\u001b[0m  0.0002  0.3751\n",
      "     24            1.0000        0.2894       0.4028            0.4028        \u001b[94m1.4345\u001b[0m  0.0002  0.3756\n",
      "     25            1.0000        \u001b[32m0.2693\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.4210\u001b[0m  0.0002  0.3750\n",
      "     26            1.0000        0.3123       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.4133\u001b[0m  0.0002  0.3750\n",
      "     27            1.0000        0.2826       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.4104\u001b[0m  0.0002  0.4063\n",
      "     28            1.0000        \u001b[32m0.2630\u001b[0m       0.4167            0.4167        1.4114  0.0001  0.4375\n",
      "     29            1.0000        0.2926       0.4132            0.4132        1.4113  0.0001  0.4844\n",
      "     30            1.0000        \u001b[32m0.2437\u001b[0m       0.4097            0.4097        \u001b[94m1.4100\u001b[0m  0.0001  0.4375\n",
      "     31            1.0000        \u001b[32m0.2118\u001b[0m       0.4062            0.4062        \u001b[94m1.4091\u001b[0m  0.0001  0.4532\n",
      "     32            1.0000        0.2731       0.4097            0.4097        \u001b[94m1.4081\u001b[0m  0.0001  0.4848\n",
      "     33            1.0000        0.2629       0.4132            0.4132        \u001b[94m1.4059\u001b[0m  0.0000  0.4532\n",
      "     34            1.0000        0.2393       0.4167            0.4167        \u001b[94m1.4048\u001b[0m  0.0000  0.3750\n",
      "     35            1.0000        0.2436       0.4167            0.4167        \u001b[94m1.4041\u001b[0m  0.0000  0.3750\n",
      "     36            1.0000        0.2321       0.4167            0.4167        \u001b[94m1.4035\u001b[0m  0.0000  0.3754\n",
      "     37            1.0000        0.2507       0.4167            0.4167        \u001b[94m1.4032\u001b[0m  0.0000  0.3907\n",
      "     38            1.0000        \u001b[32m0.2103\u001b[0m       0.4167            0.4167        \u001b[94m1.4030\u001b[0m  0.0000  0.3757\n",
      "     39            1.0000        0.2179       0.4167            0.4167        \u001b[94m1.4026\u001b[0m  0.0000  0.3594\n",
      "     40            1.0000        0.2647       0.4167            0.4167        \u001b[94m1.4024\u001b[0m  0.0000  0.3750\n",
      "Training model for subject 9 with 130 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2692\u001b[0m        \u001b[32m1.6575\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m2.8369\u001b[0m  0.0006  0.3750\n",
      "      2            0.2462        \u001b[32m1.3485\u001b[0m       0.2569            0.2569        2.9475  0.0006  0.3909\n",
      "      3            0.2538        \u001b[32m1.1909\u001b[0m       0.2604            0.2604        3.0605  0.0006  0.3907\n",
      "      4            \u001b[36m0.3308\u001b[0m        \u001b[32m1.0772\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        2.8647  0.0006  0.3907\n",
      "      5            \u001b[36m0.4154\u001b[0m        \u001b[32m0.9422\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m2.5757\u001b[0m  0.0006  0.3906\n",
      "      6            \u001b[36m0.4615\u001b[0m        \u001b[32m0.8617\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m2.4863\u001b[0m  0.0006  0.3907\n",
      "      7            \u001b[36m0.4692\u001b[0m        \u001b[32m0.8249\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m2.4662\u001b[0m  0.0006  0.3906\n",
      "      8            \u001b[36m0.5000\u001b[0m        \u001b[32m0.7086\u001b[0m       0.3646            0.3646        \u001b[94m2.4146\u001b[0m  0.0006  0.3910\n",
      "      9            \u001b[36m0.5154\u001b[0m        0.7157       0.3611            0.3611        \u001b[94m2.3595\u001b[0m  0.0006  0.3911\n",
      "     10            \u001b[36m0.5538\u001b[0m        \u001b[32m0.6494\u001b[0m       0.3681            0.3681        \u001b[94m2.2580\u001b[0m  0.0005  0.3906\n",
      "     11            \u001b[36m0.6231\u001b[0m        0.6976       0.3542            0.3542        \u001b[94m2.1844\u001b[0m  0.0005  0.3910\n",
      "     12            \u001b[36m0.6308\u001b[0m        \u001b[32m0.5396\u001b[0m       0.3542            0.3542        \u001b[94m2.1018\u001b[0m  0.0005  0.3906\n",
      "     13            \u001b[36m0.6923\u001b[0m        \u001b[32m0.5346\u001b[0m       0.3576            0.3576        \u001b[94m2.0312\u001b[0m  0.0005  0.3750\n",
      "     14            \u001b[36m0.7692\u001b[0m        0.5617       0.3576            0.3576        \u001b[94m1.9513\u001b[0m  0.0005  0.4063\n",
      "     15            \u001b[36m0.8000\u001b[0m        0.5605       0.3611            0.3611        \u001b[94m1.8936\u001b[0m  0.0004  0.3906\n",
      "     16            \u001b[36m0.8308\u001b[0m        \u001b[32m0.4411\u001b[0m       \u001b[35m0.3715\u001b[0m            \u001b[31m0.3715\u001b[0m        \u001b[94m1.8359\u001b[0m  0.0004  0.3906\n",
      "     17            \u001b[36m0.8692\u001b[0m        \u001b[32m0.4115\u001b[0m       0.3715            0.3715        \u001b[94m1.7706\u001b[0m  0.0004  0.3917\n",
      "     18            \u001b[36m0.9000\u001b[0m        0.4130       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.7062\u001b[0m  0.0004  0.3754\n",
      "     19            \u001b[36m0.9308\u001b[0m        \u001b[32m0.3658\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.6491\u001b[0m  0.0004  0.3755\n",
      "     20            \u001b[36m0.9538\u001b[0m        0.3930       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.6042\u001b[0m  0.0003  0.4532\n",
      "     21            \u001b[36m0.9615\u001b[0m        0.3680       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.5703\u001b[0m  0.0003  0.4688\n",
      "     22            0.9615        \u001b[32m0.3651\u001b[0m       0.3889            0.3889        \u001b[94m1.5444\u001b[0m  0.0003  0.4375\n",
      "     23            \u001b[36m0.9692\u001b[0m        \u001b[32m0.3206\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.5265\u001b[0m  0.0002  0.4688\n",
      "     24            0.9692        0.3943       0.3958            0.3958        \u001b[94m1.5082\u001b[0m  0.0002  0.4688\n",
      "     25            \u001b[36m0.9769\u001b[0m        0.3397       0.3958            0.3958        \u001b[94m1.4858\u001b[0m  0.0002  0.5000\n",
      "     26            \u001b[36m0.9846\u001b[0m        \u001b[32m0.2979\u001b[0m       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.4664\u001b[0m  0.0002  0.3923\n",
      "     27            \u001b[36m0.9923\u001b[0m        0.3075       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.4495\u001b[0m  0.0002  0.3750\n",
      "     28            0.9923        0.3379       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.4352\u001b[0m  0.0001  0.3906\n",
      "     29            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2654\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.4227\u001b[0m  0.0001  0.3911\n",
      "     30            1.0000        0.2909       0.4236            0.4236        \u001b[94m1.4134\u001b[0m  0.0001  0.3906\n",
      "     31            1.0000        0.2946       0.4271            0.4271        \u001b[94m1.4054\u001b[0m  0.0001  0.3750\n",
      "     32            1.0000        0.3415       0.4236            0.4236        \u001b[94m1.3985\u001b[0m  0.0001  0.4064\n",
      "     33            1.0000        0.2998       0.4201            0.4201        \u001b[94m1.3924\u001b[0m  0.0000  0.3750\n",
      "     34            1.0000        0.2682       0.4236            0.4236        \u001b[94m1.3866\u001b[0m  0.0000  0.5178\n",
      "     35            1.0000        0.3401       0.4167            0.4167        \u001b[94m1.3817\u001b[0m  0.0000  0.3910\n",
      "     36            1.0000        0.2884       0.4167            0.4167        \u001b[94m1.3778\u001b[0m  0.0000  0.3755\n",
      "     37            1.0000        \u001b[32m0.2525\u001b[0m       0.4167            0.4167        \u001b[94m1.3748\u001b[0m  0.0000  0.3750\n",
      "     38            1.0000        0.2714       0.4201            0.4201        \u001b[94m1.3722\u001b[0m  0.0000  0.3908\n",
      "     39            1.0000        0.2941       0.4236            0.4236        \u001b[94m1.3707\u001b[0m  0.0000  0.3750\n",
      "     40            1.0000        0.3057       0.4236            0.4236        \u001b[94m1.3694\u001b[0m  0.0000  0.3750\n",
      "Training model for subject 9 with 140 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.6064\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.1797\u001b[0m  0.0006  0.3909\n",
      "      2            0.2500        \u001b[32m1.2995\u001b[0m       0.2500            0.2500        3.6247  0.0006  0.3750\n",
      "      3            0.2500        \u001b[32m1.2536\u001b[0m       0.2500            0.2500        3.6687  0.0006  0.3907\n",
      "      4            \u001b[36m0.2857\u001b[0m        \u001b[32m1.1089\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m3.0489\u001b[0m  0.0006  0.3750\n",
      "      5            \u001b[36m0.3643\u001b[0m        \u001b[32m0.9516\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m2.7661\u001b[0m  0.0006  0.3750\n",
      "      6            \u001b[36m0.4500\u001b[0m        \u001b[32m0.8941\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m2.6099\u001b[0m  0.0006  0.3907\n",
      "      7            \u001b[36m0.4929\u001b[0m        \u001b[32m0.8365\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m2.4066\u001b[0m  0.0006  0.3750\n",
      "      8            \u001b[36m0.5214\u001b[0m        \u001b[32m0.7755\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m2.2294\u001b[0m  0.0006  0.3907\n",
      "      9            \u001b[36m0.5286\u001b[0m        \u001b[32m0.7110\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m2.0896\u001b[0m  0.0006  0.3750\n",
      "     10            \u001b[36m0.5786\u001b[0m        \u001b[32m0.6206\u001b[0m       0.3924            0.3924        \u001b[94m1.9753\u001b[0m  0.0005  0.3907\n",
      "     11            \u001b[36m0.6000\u001b[0m        \u001b[32m0.5872\u001b[0m       0.3924            0.3924        \u001b[94m1.9372\u001b[0m  0.0005  0.3907\n",
      "     12            \u001b[36m0.7071\u001b[0m        0.6753       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.8329\u001b[0m  0.0005  0.4219\n",
      "     13            \u001b[36m0.7643\u001b[0m        \u001b[32m0.5032\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.7468\u001b[0m  0.0005  0.4532\n",
      "     14            \u001b[36m0.8000\u001b[0m        0.5131       0.4062            0.4062        \u001b[94m1.6988\u001b[0m  0.0005  0.4688\n",
      "     15            \u001b[36m0.8357\u001b[0m        0.5277       0.3958            0.3958        \u001b[94m1.6598\u001b[0m  0.0004  0.4532\n",
      "     16            \u001b[36m0.8714\u001b[0m        \u001b[32m0.4838\u001b[0m       0.3854            0.3854        \u001b[94m1.6151\u001b[0m  0.0004  0.4844\n",
      "     17            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3991\u001b[0m       0.3785            0.3785        \u001b[94m1.5742\u001b[0m  0.0004  0.5000\n",
      "     18            \u001b[36m0.9357\u001b[0m        0.4123       0.3715            0.3715        \u001b[94m1.5421\u001b[0m  0.0004  0.5173\n",
      "     19            \u001b[36m0.9571\u001b[0m        0.4239       0.3715            0.3715        \u001b[94m1.5135\u001b[0m  0.0004  0.3907\n",
      "     20            \u001b[36m0.9857\u001b[0m        0.4097       0.3889            0.3889        \u001b[94m1.4826\u001b[0m  0.0003  0.3911\n",
      "     21            0.9857        \u001b[32m0.3720\u001b[0m       0.3854            0.3854        \u001b[94m1.4599\u001b[0m  0.0003  0.3754\n",
      "     22            \u001b[36m0.9929\u001b[0m        \u001b[32m0.3292\u001b[0m       0.3854            0.3854        \u001b[94m1.4507\u001b[0m  0.0003  0.3910\n",
      "     23            0.9929        \u001b[32m0.3222\u001b[0m       0.3854            0.3854        \u001b[94m1.4441\u001b[0m  0.0002  0.3907\n",
      "     24            0.9929        \u001b[32m0.3173\u001b[0m       0.3854            0.3854        \u001b[94m1.4365\u001b[0m  0.0002  0.3907\n",
      "     25            0.9929        0.3618       0.3854            0.3854        \u001b[94m1.4336\u001b[0m  0.0002  0.3750\n",
      "     26            0.9929        \u001b[32m0.3160\u001b[0m       0.3993            0.3993        \u001b[94m1.4315\u001b[0m  0.0002  0.3754\n",
      "     27            \u001b[36m1.0000\u001b[0m        0.3259       0.3993            0.3993        \u001b[94m1.4280\u001b[0m  0.0002  0.3750\n",
      "     28            1.0000        \u001b[32m0.3141\u001b[0m       0.4028            0.4028        \u001b[94m1.4263\u001b[0m  0.0001  0.3908\n",
      "     29            1.0000        \u001b[32m0.2925\u001b[0m       0.4062            0.4062        \u001b[94m1.4261\u001b[0m  0.0001  0.3906\n",
      "     30            1.0000        \u001b[32m0.2782\u001b[0m       0.4097            0.4097        1.4268  0.0001  0.3907\n",
      "     31            1.0000        0.3370       0.4132            0.4132        1.4264  0.0001  0.3754\n",
      "     32            1.0000        \u001b[32m0.2687\u001b[0m       0.4097            0.4097        \u001b[94m1.4259\u001b[0m  0.0001  0.3750\n",
      "     33            1.0000        \u001b[32m0.2558\u001b[0m       0.4097            0.4097        \u001b[94m1.4258\u001b[0m  0.0000  0.3906\n",
      "     34            1.0000        0.2734       0.4062            0.4062        1.4261  0.0000  0.3911\n",
      "     35            1.0000        0.2710       0.4097            0.4097        \u001b[94m1.4250\u001b[0m  0.0000  0.3906\n",
      "     36            1.0000        0.2881       0.4097            0.4097        \u001b[94m1.4238\u001b[0m  0.0000  0.3754\n",
      "     37            1.0000        0.2770       0.4062            0.4062        \u001b[94m1.4225\u001b[0m  0.0000  0.3907\n",
      "     38            1.0000        \u001b[32m0.2433\u001b[0m       0.4062            0.4062        \u001b[94m1.4219\u001b[0m  0.0000  0.3752\n",
      "     39            1.0000        0.3069       0.4062            0.4062        \u001b[94m1.4213\u001b[0m  0.0000  0.3910\n",
      "     40            1.0000        0.2670       0.4097            0.4097        \u001b[94m1.4204\u001b[0m  0.0000  0.3750\n",
      "Training model for subject 9 with 150 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2533\u001b[0m        \u001b[32m1.5723\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.9801\u001b[0m  0.0006  0.3750\n",
      "      2            \u001b[36m0.3667\u001b[0m        \u001b[32m1.2914\u001b[0m       \u001b[35m0.2986\u001b[0m            \u001b[31m0.2986\u001b[0m        \u001b[94m2.5622\u001b[0m  0.0006  0.3750\n",
      "      3            \u001b[36m0.4133\u001b[0m        \u001b[32m1.2661\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        2.5920  0.0006  0.3906\n",
      "      4            \u001b[36m0.4467\u001b[0m        \u001b[32m1.0308\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        2.7017  0.0006  0.4219\n",
      "      5            \u001b[36m0.4667\u001b[0m        \u001b[32m0.9801\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        2.6541  0.0006  0.4688\n",
      "      6            \u001b[36m0.4733\u001b[0m        \u001b[32m0.9130\u001b[0m       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        2.6531  0.0006  0.4844\n",
      "      7            \u001b[36m0.4800\u001b[0m        \u001b[32m0.8195\u001b[0m       0.3785            0.3785        \u001b[94m2.5011\u001b[0m  0.0006  0.4688\n",
      "      8            \u001b[36m0.5067\u001b[0m        \u001b[32m0.7758\u001b[0m       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m2.3262\u001b[0m  0.0006  0.4532\n",
      "      9            \u001b[36m0.5267\u001b[0m        \u001b[32m0.7377\u001b[0m       0.3819            0.3819        \u001b[94m2.1541\u001b[0m  0.0006  0.5162\n",
      "     10            \u001b[36m0.5600\u001b[0m        \u001b[32m0.6805\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.9359\u001b[0m  0.0005  0.4063\n",
      "     11            \u001b[36m0.6000\u001b[0m        0.6855       0.3924            0.3924        \u001b[94m1.8265\u001b[0m  0.0005  0.3755\n",
      "     12            \u001b[36m0.6533\u001b[0m        \u001b[32m0.6116\u001b[0m       0.3854            0.3854        \u001b[94m1.7325\u001b[0m  0.0005  0.4219\n",
      "     13            \u001b[36m0.7467\u001b[0m        \u001b[32m0.5848\u001b[0m       0.3924            0.3924        \u001b[94m1.6194\u001b[0m  0.0005  0.3911\n",
      "     14            \u001b[36m0.8800\u001b[0m        \u001b[32m0.5476\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.5539\u001b[0m  0.0005  0.4067\n",
      "     15            \u001b[36m0.9200\u001b[0m        \u001b[32m0.5340\u001b[0m       0.3819            0.3819        \u001b[94m1.5238\u001b[0m  0.0004  0.3907\n",
      "     16            \u001b[36m0.9400\u001b[0m        \u001b[32m0.5239\u001b[0m       0.3924            0.3924        \u001b[94m1.5192\u001b[0m  0.0004  0.3751\n",
      "     17            \u001b[36m0.9467\u001b[0m        0.5418       0.4028            0.4028        \u001b[94m1.4990\u001b[0m  0.0004  0.3760\n",
      "     18            \u001b[36m0.9533\u001b[0m        0.5536       0.3958            0.3958        1.5045  0.0004  0.3911\n",
      "     19            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4738\u001b[0m       0.3819            0.3819        1.5156  0.0004  0.3758\n",
      "     20            0.9667        \u001b[32m0.4618\u001b[0m       0.3819            0.3819        1.5229  0.0003  0.3907\n",
      "     21            0.9533        \u001b[32m0.4044\u001b[0m       0.3785            0.3785        1.5222  0.0003  0.3910\n",
      "     22            0.9600        \u001b[32m0.3703\u001b[0m       0.3854            0.3854        1.5176  0.0003  0.3750\n",
      "     23            0.9667        0.3994       0.3889            0.3889        1.4995  0.0002  0.3755\n",
      "     24            \u001b[36m0.9800\u001b[0m        0.4121       0.3924            0.3924        \u001b[94m1.4828\u001b[0m  0.0002  0.3907\n",
      "     25            \u001b[36m0.9867\u001b[0m        0.3908       0.3924            0.3924        \u001b[94m1.4753\u001b[0m  0.0002  0.3907\n",
      "     26            \u001b[36m0.9933\u001b[0m        0.4165       0.4028            0.4028        \u001b[94m1.4689\u001b[0m  0.0002  0.3907\n",
      "     27            0.9933        \u001b[32m0.3461\u001b[0m       0.4028            0.4028        \u001b[94m1.4664\u001b[0m  0.0002  0.3907\n",
      "     28            0.9933        0.3493       0.4028            0.4028        \u001b[94m1.4649\u001b[0m  0.0001  0.3755\n",
      "     29            0.9933        \u001b[32m0.3373\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.4635\u001b[0m  0.0001  0.3755\n",
      "     30            0.9933        0.3684       0.4132            0.4132        1.4637  0.0001  0.3750\n",
      "     31            0.9933        0.3413       0.4132            0.4132        \u001b[94m1.4629\u001b[0m  0.0001  0.3750\n",
      "     32            0.9933        \u001b[32m0.3359\u001b[0m       0.4097            0.4097        \u001b[94m1.4601\u001b[0m  0.0001  0.3750\n",
      "     33            0.9933        \u001b[32m0.3058\u001b[0m       0.4097            0.4097        \u001b[94m1.4570\u001b[0m  0.0000  0.3750\n",
      "     34            0.9933        0.3319       0.4062            0.4062        \u001b[94m1.4554\u001b[0m  0.0000  0.3759\n",
      "     35            0.9933        \u001b[32m0.3019\u001b[0m       0.4062            0.4062        \u001b[94m1.4527\u001b[0m  0.0000  0.3907\n",
      "     36            0.9933        0.3153       0.4132            0.4132        \u001b[94m1.4489\u001b[0m  0.0000  0.4219\n",
      "     37            0.9933        0.3076       0.4132            0.4132        \u001b[94m1.4463\u001b[0m  0.0000  0.5004\n",
      "     38            0.9933        0.3319       0.4132            0.4132        \u001b[94m1.4439\u001b[0m  0.0000  0.4531\n",
      "     39            0.9933        0.3119       0.4132            0.4132        \u001b[94m1.4435\u001b[0m  0.0000  0.4532\n",
      "     40            0.9933        0.3797       0.4132            0.4132        \u001b[94m1.4429\u001b[0m  0.0000  0.4692\n",
      "Training model for subject 9 with 160 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2437\u001b[0m        \u001b[32m1.5453\u001b[0m       \u001b[35m0.2465\u001b[0m            \u001b[31m0.2465\u001b[0m        \u001b[94m2.2251\u001b[0m  0.0006  0.5000\n",
      "      2            \u001b[36m0.3250\u001b[0m        \u001b[32m1.3533\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m1.7892\u001b[0m  0.0006  0.4063\n",
      "      3            \u001b[36m0.4813\u001b[0m        \u001b[32m1.1527\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        1.8603  0.0006  0.3750\n",
      "      4            0.3625        \u001b[32m1.0332\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        2.4239  0.0006  0.3750\n",
      "      5            0.3500        \u001b[32m0.8746\u001b[0m       0.3021            0.3021        3.0038  0.0006  0.3915\n",
      "      6            0.3312        0.9979       0.2951            0.2951        3.2065  0.0006  0.4071\n",
      "      7            0.3063        \u001b[32m0.8550\u001b[0m       0.2847            0.2847        3.2056  0.0006  0.3912\n",
      "      8            0.3312        \u001b[32m0.7633\u001b[0m       0.2847            0.2847        3.0291  0.0006  0.3909\n",
      "      9            0.3438        \u001b[32m0.7194\u001b[0m       0.3021            0.3021        2.7991  0.0006  0.3914\n",
      "     10            0.3875        0.7406       0.3056            0.3056        2.5962  0.0005  0.3750\n",
      "     11            0.4313        \u001b[32m0.6244\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        2.3865  0.0005  0.3750\n",
      "     12            0.4688        \u001b[32m0.6180\u001b[0m       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        2.2693  0.0005  0.3907\n",
      "     13            \u001b[36m0.5375\u001b[0m        0.6345       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        2.0604  0.0005  0.3911\n",
      "     14            \u001b[36m0.6312\u001b[0m        \u001b[32m0.5722\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        1.8685  0.0005  0.3907\n",
      "     15            \u001b[36m0.6875\u001b[0m        \u001b[32m0.4953\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.7381\u001b[0m  0.0004  0.3750\n",
      "     16            \u001b[36m0.7562\u001b[0m        0.4992       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.6611\u001b[0m  0.0004  0.3906\n",
      "     17            \u001b[36m0.8500\u001b[0m        \u001b[32m0.4348\u001b[0m       0.3854            0.3854        \u001b[94m1.6033\u001b[0m  0.0004  0.3750\n",
      "     18            \u001b[36m0.9000\u001b[0m        0.4736       0.3924            0.3924        \u001b[94m1.5540\u001b[0m  0.0004  0.4375\n",
      "     19            \u001b[36m0.9125\u001b[0m        0.4620       0.3854            0.3854        \u001b[94m1.5168\u001b[0m  0.0004  0.4063\n",
      "     20            \u001b[36m0.9187\u001b[0m        0.4496       0.3785            0.3785        \u001b[94m1.4847\u001b[0m  0.0003  0.5469\n",
      "     21            \u001b[36m0.9437\u001b[0m        \u001b[32m0.4032\u001b[0m       0.3750            0.3750        \u001b[94m1.4559\u001b[0m  0.0003  0.4071\n",
      "     22            \u001b[36m0.9500\u001b[0m        0.4207       0.3854            0.3854        \u001b[94m1.4363\u001b[0m  0.0003  0.4067\n",
      "     23            \u001b[36m0.9563\u001b[0m        0.4040       0.3958            0.3958        \u001b[94m1.4161\u001b[0m  0.0002  0.3907\n",
      "     24            \u001b[36m0.9625\u001b[0m        \u001b[32m0.3879\u001b[0m       0.4028            0.4028        \u001b[94m1.4015\u001b[0m  0.0002  0.3911\n",
      "     25            0.9625        \u001b[32m0.3055\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.3885\u001b[0m  0.0002  0.3936\n",
      "     26            \u001b[36m0.9688\u001b[0m        0.3682       0.4062            0.4062        \u001b[94m1.3756\u001b[0m  0.0002  0.4063\n",
      "     27            \u001b[36m0.9875\u001b[0m        0.3430       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.3661\u001b[0m  0.0002  0.4844\n",
      "     28            \u001b[36m1.0000\u001b[0m        0.3582       0.4167            0.4167        \u001b[94m1.3598\u001b[0m  0.0001  0.4688\n",
      "     29            1.0000        0.3396       0.4132            0.4132        \u001b[94m1.3551\u001b[0m  0.0001  0.4688\n",
      "     30            1.0000        0.3386       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.3505\u001b[0m  0.0001  0.4688\n",
      "     31            1.0000        0.3459       0.4271            0.4271        \u001b[94m1.3450\u001b[0m  0.0001  0.4844\n",
      "     32            1.0000        0.3749       0.4271            0.4271        \u001b[94m1.3430\u001b[0m  0.0001  0.4219\n",
      "     33            1.0000        0.3148       0.4201            0.4201        \u001b[94m1.3417\u001b[0m  0.0000  0.3756\n",
      "     34            1.0000        0.3409       0.4201            0.4201        \u001b[94m1.3388\u001b[0m  0.0000  0.3907\n",
      "     35            1.0000        0.3248       0.4271            0.4271        \u001b[94m1.3364\u001b[0m  0.0000  0.3750\n",
      "     36            1.0000        0.3203       0.4271            0.4271        \u001b[94m1.3352\u001b[0m  0.0000  0.3752\n",
      "     37            1.0000        0.3549       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.3344\u001b[0m  0.0000  0.3911\n",
      "     38            1.0000        0.3143       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.3340\u001b[0m  0.0000  0.4063\n",
      "     39            1.0000        0.3392       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.3331\u001b[0m  0.0000  0.4051\n",
      "     40            1.0000        0.3671       0.4375            0.4375        \u001b[94m1.3327\u001b[0m  0.0000  0.4001\n",
      "Training model for subject 9 with 170 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2824\u001b[0m        \u001b[32m1.6719\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m2.8191\u001b[0m  0.0006  0.4177\n",
      "      2            \u001b[36m0.3588\u001b[0m        \u001b[32m1.3865\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        2.9200  0.0006  0.4145\n",
      "      3            \u001b[36m0.3882\u001b[0m        \u001b[32m1.2920\u001b[0m       0.2847            0.2847        3.4374  0.0006  0.4082\n",
      "      4            0.3353        \u001b[32m1.0707\u001b[0m       0.2847            0.2847        3.5047  0.0006  0.4940\n",
      "      5            0.3412        \u001b[32m1.0542\u001b[0m       0.2847            0.2847        3.2562  0.0006  0.4373\n",
      "      6            0.3235        \u001b[32m1.0359\u001b[0m       0.2812            0.2812        3.1641  0.0006  0.3958\n",
      "      7            0.3000        \u001b[32m0.8901\u001b[0m       0.2708            0.2708        3.1519  0.0006  0.4098\n",
      "      8            0.3000        \u001b[32m0.8897\u001b[0m       0.2674            0.2674        3.0344  0.0006  0.3931\n",
      "      9            0.3118        \u001b[32m0.8090\u001b[0m       0.2743            0.2743        2.8569  0.0006  0.5074\n",
      "     10            0.3471        \u001b[32m0.8046\u001b[0m       0.2847            0.2847        \u001b[94m2.7548\u001b[0m  0.0005  0.4460\n",
      "     11            0.3765        \u001b[32m0.7739\u001b[0m       0.2951            0.2951        \u001b[94m2.6593\u001b[0m  0.0005  0.3943\n",
      "     12            \u001b[36m0.4059\u001b[0m        \u001b[32m0.7551\u001b[0m       0.3056            0.3056        \u001b[94m2.4705\u001b[0m  0.0005  0.3911\n",
      "     13            \u001b[36m0.4118\u001b[0m        \u001b[32m0.7004\u001b[0m       0.3125            0.3125        \u001b[94m2.3472\u001b[0m  0.0005  0.5377\n",
      "     14            \u001b[36m0.4588\u001b[0m        \u001b[32m0.6922\u001b[0m       0.3125            0.3125        \u001b[94m2.2127\u001b[0m  0.0005  0.4106\n",
      "     15            \u001b[36m0.5353\u001b[0m        \u001b[32m0.6345\u001b[0m       0.3194            0.3194        \u001b[94m2.0898\u001b[0m  0.0004  0.3925\n",
      "     16            \u001b[36m0.6176\u001b[0m        0.6445       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m1.9463\u001b[0m  0.0004  0.6656\n",
      "     17            \u001b[36m0.6882\u001b[0m        0.6379       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.8277\u001b[0m  0.0004  0.7127\n",
      "     18            \u001b[36m0.7294\u001b[0m        \u001b[32m0.5076\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.7145\u001b[0m  0.0004  0.6775\n",
      "     19            \u001b[36m0.7941\u001b[0m        0.5760       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.6133\u001b[0m  0.0004  0.6429\n",
      "     20            \u001b[36m0.8529\u001b[0m        0.5582       \u001b[35m0.3819\u001b[0m            \u001b[31m0.3819\u001b[0m        \u001b[94m1.5410\u001b[0m  0.0003  0.5476\n",
      "     21            \u001b[36m0.8824\u001b[0m        \u001b[32m0.5041\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4908\u001b[0m  0.0003  0.4020\n",
      "     22            \u001b[36m0.9294\u001b[0m        \u001b[32m0.4879\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.4544\u001b[0m  0.0003  0.4473\n",
      "     23            \u001b[36m0.9471\u001b[0m        \u001b[32m0.4446\u001b[0m       0.4097            0.4097        \u001b[94m1.4287\u001b[0m  0.0002  0.4313\n",
      "     24            \u001b[36m0.9529\u001b[0m        0.4843       0.4062            0.4062        \u001b[94m1.4177\u001b[0m  0.0002  0.3937\n",
      "     25            0.9471        \u001b[32m0.4202\u001b[0m       0.4097            0.4097        \u001b[94m1.4108\u001b[0m  0.0002  0.4289\n",
      "     26            0.9471        \u001b[32m0.4093\u001b[0m       \u001b[35m0.4167\u001b[0m            \u001b[31m0.4167\u001b[0m        \u001b[94m1.4046\u001b[0m  0.0002  0.4505\n",
      "     27            \u001b[36m0.9588\u001b[0m        0.4845       0.4132            0.4132        \u001b[94m1.3985\u001b[0m  0.0002  0.4040\n",
      "     28            0.9588        0.4652       0.4132            0.4132        \u001b[94m1.3938\u001b[0m  0.0001  0.5803\n",
      "     29            \u001b[36m0.9647\u001b[0m        0.4855       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.3881\u001b[0m  0.0001  0.4180\n",
      "     30            \u001b[36m0.9706\u001b[0m        \u001b[32m0.3598\u001b[0m       0.4132            0.4132        \u001b[94m1.3829\u001b[0m  0.0001  0.4393\n",
      "     31            0.9706        0.4054       0.4097            0.4097        \u001b[94m1.3763\u001b[0m  0.0001  0.4170\n",
      "     32            0.9706        0.4945       0.4028            0.4028        \u001b[94m1.3709\u001b[0m  0.0001  0.4074\n",
      "     33            0.9706        0.4096       0.4028            0.4028        \u001b[94m1.3662\u001b[0m  0.0000  0.4064\n",
      "     34            \u001b[36m0.9765\u001b[0m        0.4777       0.4028            0.4028        \u001b[94m1.3628\u001b[0m  0.0000  0.3906\n",
      "     35            0.9706        0.4020       0.4028            0.4028        \u001b[94m1.3604\u001b[0m  0.0000  0.4844\n",
      "     36            0.9706        0.4496       0.3958            0.3958        \u001b[94m1.3579\u001b[0m  0.0000  0.4370\n",
      "     37            0.9706        0.3988       0.3993            0.3993        \u001b[94m1.3560\u001b[0m  0.0000  0.4223\n",
      "     38            0.9647        0.3653       0.4062            0.4062        \u001b[94m1.3538\u001b[0m  0.0000  0.4361\n",
      "     39            0.9647        0.4316       0.4062            0.4062        \u001b[94m1.3528\u001b[0m  0.0000  0.4067\n",
      "     40            0.9647        0.5062       0.4062            0.4062        \u001b[94m1.3520\u001b[0m  0.0000  0.4551\n",
      "Training model for subject 9 with 180 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3111\u001b[0m        \u001b[32m1.7123\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m2.8029\u001b[0m  0.0006  0.4557\n",
      "      2            0.2611        \u001b[32m1.4174\u001b[0m       0.2500            0.2500        3.4717  0.0006  0.4636\n",
      "      3            0.2778        \u001b[32m1.2156\u001b[0m       0.2569            0.2569        4.3265  0.0006  0.5038\n",
      "      4            0.2833        \u001b[32m1.1499\u001b[0m       0.2535            0.2535        4.7898  0.0006  0.4996\n",
      "      5            0.2667        \u001b[32m1.0678\u001b[0m       0.2569            0.2569        4.9057  0.0006  0.5155\n",
      "      6            0.2556        1.0733       0.2500            0.2500        5.0140  0.0006  0.5435\n",
      "      7            0.2556        \u001b[32m0.9689\u001b[0m       0.2500            0.2500        4.6344  0.0006  0.5003\n",
      "      8            0.2556        \u001b[32m0.9216\u001b[0m       0.2500            0.2500        4.1695  0.0006  0.3984\n",
      "      9            0.2667        \u001b[32m0.8261\u001b[0m       0.2500            0.2500        3.7240  0.0006  0.4001\n",
      "     10            0.2722        0.8601       0.2535            0.2535        3.3352  0.0005  0.4001\n",
      "     11            \u001b[36m0.3444\u001b[0m        \u001b[32m0.7884\u001b[0m       0.2674            0.2674        2.8600  0.0005  0.4019\n",
      "     12            \u001b[36m0.4222\u001b[0m        \u001b[32m0.7279\u001b[0m       0.2674            0.2674        \u001b[94m2.4640\u001b[0m  0.0005  0.4008\n",
      "     13            \u001b[36m0.4833\u001b[0m        \u001b[32m0.6908\u001b[0m       0.2639            0.2639        \u001b[94m2.2136\u001b[0m  0.0005  0.3833\n",
      "     14            \u001b[36m0.5556\u001b[0m        \u001b[32m0.6602\u001b[0m       0.2743            0.2743        \u001b[94m1.9989\u001b[0m  0.0005  0.3995\n",
      "     15            \u001b[36m0.5889\u001b[0m        0.6918       \u001b[35m0.3090\u001b[0m            \u001b[31m0.3090\u001b[0m        \u001b[94m1.8517\u001b[0m  0.0004  0.3833\n",
      "     16            \u001b[36m0.6389\u001b[0m        \u001b[32m0.5521\u001b[0m       \u001b[35m0.3299\u001b[0m            \u001b[31m0.3299\u001b[0m        \u001b[94m1.7253\u001b[0m  0.0004  0.3999\n",
      "     17            \u001b[36m0.7111\u001b[0m        0.5970       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.6183\u001b[0m  0.0004  0.4005\n",
      "     18            \u001b[36m0.7444\u001b[0m        0.6157       \u001b[35m0.3785\u001b[0m            \u001b[31m0.3785\u001b[0m        \u001b[94m1.5406\u001b[0m  0.0004  0.3831\n",
      "     19            \u001b[36m0.8167\u001b[0m        \u001b[32m0.5374\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.4840\u001b[0m  0.0004  0.4000\n",
      "     20            \u001b[36m0.8500\u001b[0m        \u001b[32m0.5278\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4534\u001b[0m  0.0003  0.3844\n",
      "     21            \u001b[36m0.8889\u001b[0m        \u001b[32m0.5234\u001b[0m       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.4230\u001b[0m  0.0003  0.4012\n",
      "     22            \u001b[36m0.9111\u001b[0m        \u001b[32m0.5065\u001b[0m       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.3900\u001b[0m  0.0003  0.3834\n",
      "     23            \u001b[36m0.9278\u001b[0m        \u001b[32m0.4400\u001b[0m       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.3581\u001b[0m  0.0002  0.3997\n",
      "     24            \u001b[36m0.9389\u001b[0m        \u001b[32m0.4284\u001b[0m       \u001b[35m0.4306\u001b[0m            \u001b[31m0.4306\u001b[0m        \u001b[94m1.3314\u001b[0m  0.0002  0.4009\n",
      "     25            \u001b[36m0.9500\u001b[0m        0.4816       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.3089\u001b[0m  0.0002  0.3995\n",
      "     26            \u001b[36m0.9556\u001b[0m        0.4792       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.2943\u001b[0m  0.0002  0.3997\n",
      "     27            0.9556        0.5177       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2847\u001b[0m  0.0002  0.4000\n",
      "     28            0.9556        \u001b[32m0.4189\u001b[0m       0.4549            0.4549        \u001b[94m1.2758\u001b[0m  0.0001  0.3834\n",
      "     29            0.9556        0.4963       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.2672\u001b[0m  0.0001  0.3999\n",
      "     30            0.9556        0.4817       0.4618            0.4618        \u001b[94m1.2607\u001b[0m  0.0001  0.3832\n",
      "     31            0.9556        0.4460       0.4618            0.4618        \u001b[94m1.2571\u001b[0m  0.0001  0.4166\n",
      "     32            0.9556        \u001b[32m0.3608\u001b[0m       0.4653            0.4653        \u001b[94m1.2541\u001b[0m  0.0001  0.4494\n",
      "     33            \u001b[36m0.9611\u001b[0m        0.4749       0.4618            0.4618        \u001b[94m1.2524\u001b[0m  0.0000  0.4662\n",
      "     34            \u001b[36m0.9667\u001b[0m        0.4314       0.4653            0.4653        \u001b[94m1.2515\u001b[0m  0.0000  0.4500\n",
      "     35            0.9667        0.4296       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2504\u001b[0m  0.0000  0.4333\n",
      "     36            \u001b[36m0.9722\u001b[0m        0.3749       0.4688            0.4688        \u001b[94m1.2498\u001b[0m  0.0000  0.4673\n",
      "     37            0.9722        0.4465       0.4688            0.4688        \u001b[94m1.2490\u001b[0m  0.0000  0.5002\n",
      "     38            0.9722        0.4253       0.4688            0.4688        \u001b[94m1.2484\u001b[0m  0.0000  0.4167\n",
      "     39            0.9722        0.4311       0.4688            0.4688        \u001b[94m1.2484\u001b[0m  0.0000  0.3835\n",
      "     40            0.9722        0.4090       0.4688            0.4688        \u001b[94m1.2478\u001b[0m  0.0000  0.4001\n",
      "Training model for subject 9 with 190 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2684\u001b[0m        \u001b[32m1.6721\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m3.3155\u001b[0m  0.0006  0.4172\n",
      "      2            \u001b[36m0.3263\u001b[0m        \u001b[32m1.3329\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m2.3086\u001b[0m  0.0006  0.4514\n",
      "      3            \u001b[36m0.5105\u001b[0m        \u001b[32m1.0844\u001b[0m       \u001b[35m0.2951\u001b[0m            \u001b[31m0.2951\u001b[0m        \u001b[94m2.0570\u001b[0m  0.0006  0.3998\n",
      "      4            0.2789        \u001b[32m1.0043\u001b[0m       0.2535            0.2535        2.4160  0.0006  0.3750\n",
      "      5            0.2474        \u001b[32m0.9587\u001b[0m       0.2500            0.2500        3.0058  0.0006  0.4402\n",
      "      6            0.2474        1.0015       0.2535            0.2535        2.9513  0.0006  0.4070\n",
      "      7            0.2579        \u001b[32m0.9260\u001b[0m       0.2569            0.2569        2.7161  0.0006  0.4073\n",
      "      8            0.3105        \u001b[32m0.8927\u001b[0m       0.2743            0.2743        2.3886  0.0006  0.4280\n",
      "      9            0.4053        \u001b[32m0.8607\u001b[0m       0.2743            0.2743        2.1194  0.0006  0.4613\n",
      "     10            \u001b[36m0.5158\u001b[0m        0.9632       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m1.8696\u001b[0m  0.0005  0.4064\n",
      "     11            \u001b[36m0.6105\u001b[0m        \u001b[32m0.8369\u001b[0m       \u001b[35m0.3438\u001b[0m            \u001b[31m0.3438\u001b[0m        \u001b[94m1.6741\u001b[0m  0.0005  0.4219\n",
      "     12            \u001b[36m0.6632\u001b[0m        \u001b[32m0.7446\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.5704\u001b[0m  0.0005  0.3924\n",
      "     13            \u001b[36m0.7158\u001b[0m        0.7591       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4802\u001b[0m  0.0005  0.5000\n",
      "     14            \u001b[36m0.7737\u001b[0m        0.8180       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.3985\u001b[0m  0.0005  0.4844\n",
      "     15            \u001b[36m0.8158\u001b[0m        \u001b[32m0.7070\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.3552\u001b[0m  0.0004  0.4221\n",
      "     16            \u001b[36m0.8474\u001b[0m        \u001b[32m0.6705\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.3157\u001b[0m  0.0004  0.4498\n",
      "     17            \u001b[36m0.8842\u001b[0m        \u001b[32m0.6377\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.2853\u001b[0m  0.0004  0.4041\n",
      "     18            \u001b[36m0.8895\u001b[0m        \u001b[32m0.6204\u001b[0m       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.2654\u001b[0m  0.0004  0.4398\n",
      "     19            \u001b[36m0.8947\u001b[0m        0.6497       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2576\u001b[0m  0.0004  0.5085\n",
      "     20            0.8947        0.6494       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2500\u001b[0m  0.0003  0.4777\n",
      "     21            \u001b[36m0.9211\u001b[0m        \u001b[32m0.5331\u001b[0m       0.4688            0.4688        \u001b[94m1.2432\u001b[0m  0.0003  0.5000\n",
      "     22            \u001b[36m0.9316\u001b[0m        \u001b[32m0.5101\u001b[0m       0.4688            0.4688        \u001b[94m1.2384\u001b[0m  0.0003  0.5434\n",
      "     23            0.9316        0.5305       0.4549            0.4549        \u001b[94m1.2368\u001b[0m  0.0002  0.5424\n",
      "     24            0.9316        \u001b[32m0.4979\u001b[0m       0.4618            0.4618        \u001b[94m1.2359\u001b[0m  0.0002  0.6281\n",
      "     25            \u001b[36m0.9421\u001b[0m        0.5120       0.4653            0.4653        \u001b[94m1.2318\u001b[0m  0.0002  0.5810\n",
      "     26            0.9421        \u001b[32m0.4838\u001b[0m       0.4583            0.4583        \u001b[94m1.2279\u001b[0m  0.0002  0.5808\n",
      "     27            0.9421        0.5653       \u001b[35m0.4722\u001b[0m            \u001b[31m0.4722\u001b[0m        \u001b[94m1.2232\u001b[0m  0.0002  0.4692\n",
      "     28            \u001b[36m0.9526\u001b[0m        \u001b[32m0.4477\u001b[0m       0.4653            0.4653        \u001b[94m1.2172\u001b[0m  0.0001  0.4229\n",
      "     29            \u001b[36m0.9632\u001b[0m        0.5132       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        \u001b[94m1.2106\u001b[0m  0.0001  0.4219\n",
      "     30            0.9632        0.4952       0.4757            0.4757        \u001b[94m1.2048\u001b[0m  0.0001  0.4702\n",
      "     31            0.9632        0.5146       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        \u001b[94m1.1994\u001b[0m  0.0001  0.4462\n",
      "     32            \u001b[36m0.9684\u001b[0m        0.4489       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        \u001b[94m1.1945\u001b[0m  0.0001  0.4219\n",
      "     33            0.9684        \u001b[32m0.3918\u001b[0m       0.4861            0.4861        \u001b[94m1.1912\u001b[0m  0.0000  0.4859\n",
      "     34            0.9684        0.5050       0.4861            0.4861        \u001b[94m1.1889\u001b[0m  0.0000  0.3989\n",
      "     35            0.9684        0.5018       0.4896            0.4896        \u001b[94m1.1871\u001b[0m  0.0000  0.4383\n",
      "     36            0.9632        0.4660       \u001b[35m0.4931\u001b[0m            \u001b[31m0.4931\u001b[0m        \u001b[94m1.1851\u001b[0m  0.0000  0.3907\n",
      "     37            0.9632        0.4181       \u001b[35m0.5000\u001b[0m            \u001b[31m0.5000\u001b[0m        \u001b[94m1.1843\u001b[0m  0.0000  0.4082\n",
      "     38            0.9632        0.4704       0.4931            0.4931        \u001b[94m1.1836\u001b[0m  0.0000  0.4350\n",
      "     39            0.9632        \u001b[32m0.3696\u001b[0m       0.4931            0.4931        \u001b[94m1.1835\u001b[0m  0.0000  0.4295\n",
      "     40            0.9632        0.4655       0.4896            0.4896        \u001b[94m1.1829\u001b[0m  0.0000  0.4330\n",
      "Training model for subject 9 with 200 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.6166\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.0336\u001b[0m  0.0006  0.5635\n",
      "      2            0.2500        \u001b[32m1.3210\u001b[0m       0.2500            0.2500        4.7212  0.0006  0.6359\n",
      "      3            0.2500        \u001b[32m1.1870\u001b[0m       0.2500            0.2500        \u001b[94m3.6129\u001b[0m  0.0006  0.7826\n",
      "      4            0.2500        \u001b[32m1.0903\u001b[0m       0.2500            0.2500        \u001b[94m2.9521\u001b[0m  0.0006  0.8608\n",
      "      5            0.2500        \u001b[32m0.9577\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m2.5367\u001b[0m  0.0006  0.7544\n",
      "      6            \u001b[36m0.2850\u001b[0m        \u001b[32m0.8926\u001b[0m       0.2569            0.2569        \u001b[94m2.4357\u001b[0m  0.0006  0.6719\n",
      "      7            \u001b[36m0.3000\u001b[0m        \u001b[32m0.8084\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m2.3362\u001b[0m  0.0006  0.5938\n",
      "      8            \u001b[36m0.3850\u001b[0m        \u001b[32m0.7620\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m2.0325\u001b[0m  0.0006  0.6719\n",
      "      9            \u001b[36m0.4800\u001b[0m        \u001b[32m0.7080\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m1.7788\u001b[0m  0.0006  0.5470\n",
      "     10            \u001b[36m0.6700\u001b[0m        \u001b[32m0.6576\u001b[0m       \u001b[35m0.3611\u001b[0m            \u001b[31m0.3611\u001b[0m        \u001b[94m1.5678\u001b[0m  0.0005  0.5475\n",
      "     11            \u001b[36m0.7800\u001b[0m        \u001b[32m0.5818\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.4733\u001b[0m  0.0005  0.5469\n",
      "     12            \u001b[36m0.8600\u001b[0m        0.6228       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4043\u001b[0m  0.0005  0.5313\n",
      "     13            \u001b[36m0.8900\u001b[0m        \u001b[32m0.5147\u001b[0m       0.3924            0.3924        \u001b[94m1.3764\u001b[0m  0.0005  0.5368\n",
      "     14            \u001b[36m0.9250\u001b[0m        0.5318       0.3958            0.3958        \u001b[94m1.3598\u001b[0m  0.0005  0.8476\n",
      "     15            \u001b[36m0.9350\u001b[0m        \u001b[32m0.5146\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.3470\u001b[0m  0.0004  0.6605\n",
      "     16            0.9350        \u001b[32m0.4933\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.3263\u001b[0m  0.0004  0.6451\n",
      "     17            \u001b[36m0.9450\u001b[0m        \u001b[32m0.3986\u001b[0m       0.4132            0.4132        \u001b[94m1.3142\u001b[0m  0.0004  0.5984\n",
      "     18            \u001b[36m0.9550\u001b[0m        0.4608       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.3065\u001b[0m  0.0004  0.5157\n",
      "     19            \u001b[36m0.9700\u001b[0m        0.4246       0.4236            0.4236        \u001b[94m1.2977\u001b[0m  0.0004  0.5100\n",
      "     20            \u001b[36m0.9750\u001b[0m        0.4027       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.2845\u001b[0m  0.0003  0.5085\n",
      "     21            0.9750        \u001b[32m0.3935\u001b[0m       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.2753\u001b[0m  0.0003  0.6048\n",
      "     22            \u001b[36m0.9800\u001b[0m        \u001b[32m0.3246\u001b[0m       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.2709\u001b[0m  0.0003  0.5166\n",
      "     23            \u001b[36m0.9900\u001b[0m        0.3410       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2642\u001b[0m  0.0002  0.5802\n",
      "     24            0.9900        0.3407       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2611\u001b[0m  0.0002  0.5277\n",
      "     25            0.9900        0.3358       0.4618            0.4618        1.2617  0.0002  0.5680\n",
      "     26            0.9900        0.3362       0.4549            0.4549        1.2641  0.0002  0.7772\n",
      "     27            \u001b[36m0.9950\u001b[0m        \u001b[32m0.2752\u001b[0m       0.4479            0.4479        1.2651  0.0002  0.7130\n",
      "     28            \u001b[36m1.0000\u001b[0m        0.3055       0.4479            0.4479        1.2655  0.0001  0.7041\n",
      "     29            0.9950        0.3289       0.4549            0.4549        1.2659  0.0001  0.6250\n",
      "     30            0.9950        0.3118       0.4514            0.4514        1.2662  0.0001  0.5004\n",
      "     31            0.9950        0.3027       0.4549            0.4549        1.2670  0.0001  0.5163\n",
      "     32            0.9950        0.3037       0.4549            0.4549        1.2666  0.0001  0.5004\n",
      "     33            0.9950        0.2796       0.4549            0.4549        1.2664  0.0000  0.5000\n",
      "     34            0.9950        0.2775       0.4514            0.4514        1.2665  0.0000  0.5152\n",
      "     35            0.9950        \u001b[32m0.2697\u001b[0m       0.4549            0.4549        1.2668  0.0000  0.5003\n",
      "     36            0.9950        \u001b[32m0.2685\u001b[0m       0.4549            0.4549        1.2669  0.0000  0.5004\n",
      "     37            0.9950        0.3116       0.4549            0.4549        1.2668  0.0000  0.5159\n",
      "     38            0.9950        0.2889       0.4549            0.4549        1.2668  0.0000  0.4987\n",
      "     39            0.9950        0.2745       0.4549            0.4549        1.2670  0.0000  0.5000\n",
      "     40            0.9950        \u001b[32m0.2556\u001b[0m       0.4514            0.4514        1.2668  0.0000  0.5161\n",
      "Training model for subject 9 with 210 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3048\u001b[0m        \u001b[32m1.7975\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m3.3516\u001b[0m  0.0006  0.5469\n",
      "      2            0.2714        \u001b[32m1.4383\u001b[0m       0.2500            0.2500        3.4979  0.0006  0.5003\n",
      "      3            0.2524        \u001b[32m1.1286\u001b[0m       0.2500            0.2500        3.4854  0.0006  0.5161\n",
      "      4            \u001b[36m0.3286\u001b[0m        \u001b[32m1.0769\u001b[0m       0.2604            0.2604        \u001b[94m2.6466\u001b[0m  0.0006  0.5004\n",
      "      5            \u001b[36m0.3524\u001b[0m        \u001b[32m0.9926\u001b[0m       0.2639            0.2639        \u001b[94m2.2952\u001b[0m  0.0006  0.5314\n",
      "      6            \u001b[36m0.4190\u001b[0m        \u001b[32m0.9073\u001b[0m       0.2847            0.2847        \u001b[94m2.0056\u001b[0m  0.0006  0.5002\n",
      "      7            \u001b[36m0.4857\u001b[0m        0.9513       \u001b[35m0.2917\u001b[0m            \u001b[31m0.2917\u001b[0m        \u001b[94m1.8080\u001b[0m  0.0006  0.5157\n",
      "      8            \u001b[36m0.5429\u001b[0m        \u001b[32m0.7983\u001b[0m       \u001b[35m0.3056\u001b[0m            \u001b[31m0.3056\u001b[0m        \u001b[94m1.6869\u001b[0m  0.0006  0.5313\n",
      "      9            \u001b[36m0.5952\u001b[0m        \u001b[32m0.6864\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m1.5852\u001b[0m  0.0006  0.6056\n",
      "     10            \u001b[36m0.6571\u001b[0m        0.7436       \u001b[35m0.3576\u001b[0m            \u001b[31m0.3576\u001b[0m        \u001b[94m1.5132\u001b[0m  0.0005  0.5938\n",
      "     11            \u001b[36m0.6952\u001b[0m        \u001b[32m0.6287\u001b[0m       \u001b[35m0.3750\u001b[0m            \u001b[31m0.3750\u001b[0m        \u001b[94m1.4715\u001b[0m  0.0005  0.6094\n",
      "     12            \u001b[36m0.7762\u001b[0m        \u001b[32m0.6282\u001b[0m       \u001b[35m0.4097\u001b[0m            \u001b[31m0.4097\u001b[0m        \u001b[94m1.4052\u001b[0m  0.0005  0.7427\n",
      "     13            \u001b[36m0.8429\u001b[0m        \u001b[32m0.5728\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.3315\u001b[0m  0.0005  0.5313\n",
      "     14            \u001b[36m0.8857\u001b[0m        \u001b[32m0.5497\u001b[0m       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2582\u001b[0m  0.0005  0.7288\n",
      "     15            \u001b[36m0.9476\u001b[0m        \u001b[32m0.5073\u001b[0m       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2153\u001b[0m  0.0004  0.5766\n",
      "     16            \u001b[36m0.9571\u001b[0m        \u001b[32m0.4692\u001b[0m       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        \u001b[94m1.1955\u001b[0m  0.0004  0.5157\n",
      "     17            \u001b[36m0.9619\u001b[0m        \u001b[32m0.4163\u001b[0m       \u001b[35m0.4931\u001b[0m            \u001b[31m0.4931\u001b[0m        \u001b[94m1.1840\u001b[0m  0.0004  0.5000\n",
      "     18            0.9619        0.4299       0.4861            0.4861        \u001b[94m1.1730\u001b[0m  0.0004  0.5333\n",
      "     19            \u001b[36m0.9667\u001b[0m        0.4579       0.4826            0.4826        \u001b[94m1.1667\u001b[0m  0.0004  0.5000\n",
      "     20            \u001b[36m0.9714\u001b[0m        0.4266       0.4896            0.4896        \u001b[94m1.1626\u001b[0m  0.0003  0.5000\n",
      "     21            0.9714        \u001b[32m0.3807\u001b[0m       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        \u001b[94m1.1537\u001b[0m  0.0003  0.5009\n",
      "     22            \u001b[36m0.9810\u001b[0m        0.3856       0.4931            0.4931        \u001b[94m1.1474\u001b[0m  0.0003  0.5638\n",
      "     23            0.9810        \u001b[32m0.3360\u001b[0m       \u001b[35m0.5000\u001b[0m            \u001b[31m0.5000\u001b[0m        1.1484  0.0002  0.5691\n",
      "     24            \u001b[36m0.9905\u001b[0m        0.3708       \u001b[35m0.5035\u001b[0m            \u001b[31m0.5035\u001b[0m        1.1496  0.0002  0.5157\n",
      "     25            0.9905        \u001b[32m0.3239\u001b[0m       \u001b[35m0.5069\u001b[0m            \u001b[31m0.5069\u001b[0m        1.1507  0.0002  0.6028\n",
      "     26            \u001b[36m0.9952\u001b[0m        \u001b[32m0.2948\u001b[0m       \u001b[35m0.5104\u001b[0m            \u001b[31m0.5104\u001b[0m        \u001b[94m1.1463\u001b[0m  0.0002  0.6142\n",
      "     27            0.9952        0.3118       \u001b[35m0.5174\u001b[0m            \u001b[31m0.5174\u001b[0m        \u001b[94m1.1401\u001b[0m  0.0002  0.6036\n",
      "     28            0.9952        0.3342       0.5139            0.5139        \u001b[94m1.1373\u001b[0m  0.0001  0.5221\n",
      "     29            0.9952        0.3442       0.5139            0.5139        \u001b[94m1.1344\u001b[0m  0.0001  0.5349\n",
      "     30            \u001b[36m1.0000\u001b[0m        0.3017       0.5139            0.5139        \u001b[94m1.1309\u001b[0m  0.0001  0.5782\n",
      "     31            1.0000        \u001b[32m0.2857\u001b[0m       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        \u001b[94m1.1299\u001b[0m  0.0001  0.6047\n",
      "     32            1.0000        0.3130       0.5174            0.5174        \u001b[94m1.1297\u001b[0m  0.0001  0.6089\n",
      "     33            1.0000        \u001b[32m0.2547\u001b[0m       0.5174            0.5174        \u001b[94m1.1291\u001b[0m  0.0000  0.6115\n",
      "     34            1.0000        0.3003       0.5174            0.5174        \u001b[94m1.1289\u001b[0m  0.0000  0.6160\n",
      "     35            1.0000        0.3027       0.5104            0.5104        \u001b[94m1.1284\u001b[0m  0.0000  0.5627\n",
      "     36            1.0000        0.2726       0.5104            0.5104        \u001b[94m1.1283\u001b[0m  0.0000  0.5000\n",
      "     37            1.0000        0.2754       0.5139            0.5139        \u001b[94m1.1282\u001b[0m  0.0000  0.5157\n",
      "     38            1.0000        0.2808       0.5104            0.5104        1.1283  0.0000  0.5000\n",
      "     39            1.0000        0.2885       0.5174            0.5174        1.1284  0.0000  0.5157\n",
      "     40            1.0000        0.3035       0.5174            0.5174        1.1284  0.0000  0.5003\n",
      "Training model for subject 9 with 220 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2591\u001b[0m        \u001b[32m1.7449\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m8.1281\u001b[0m  0.0006  0.5158\n",
      "      2            0.2591        \u001b[32m1.3587\u001b[0m       0.2500            0.2500        \u001b[94m5.8045\u001b[0m  0.0006  0.5009\n",
      "      3            \u001b[36m0.2727\u001b[0m        \u001b[32m1.2352\u001b[0m       \u001b[35m0.2708\u001b[0m            \u001b[31m0.2708\u001b[0m        \u001b[94m3.6438\u001b[0m  0.0006  0.5000\n",
      "      4            \u001b[36m0.3500\u001b[0m        \u001b[32m1.0639\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m3.0112\u001b[0m  0.0006  0.5000\n",
      "      5            \u001b[36m0.3864\u001b[0m        \u001b[32m0.8810\u001b[0m       0.2743            0.2743        \u001b[94m2.5851\u001b[0m  0.0006  0.5157\n",
      "      6            \u001b[36m0.4273\u001b[0m        0.9205       \u001b[35m0.3160\u001b[0m            \u001b[31m0.3160\u001b[0m        \u001b[94m1.9613\u001b[0m  0.0006  0.5001\n",
      "      7            \u001b[36m0.5227\u001b[0m        0.8864       \u001b[35m0.3333\u001b[0m            \u001b[31m0.3333\u001b[0m        \u001b[94m1.7158\u001b[0m  0.0006  0.5000\n",
      "      8            \u001b[36m0.5455\u001b[0m        \u001b[32m0.7870\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.6769\u001b[0m  0.0006  0.5005\n",
      "      9            \u001b[36m0.5773\u001b[0m        \u001b[32m0.7600\u001b[0m       \u001b[35m0.3681\u001b[0m            \u001b[31m0.3681\u001b[0m        \u001b[94m1.6356\u001b[0m  0.0006  0.5005\n",
      "     10            \u001b[36m0.6909\u001b[0m        \u001b[32m0.6994\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.4766\u001b[0m  0.0005  0.5158\n",
      "     11            \u001b[36m0.8136\u001b[0m        \u001b[32m0.6788\u001b[0m       \u001b[35m0.4062\u001b[0m            \u001b[31m0.4062\u001b[0m        \u001b[94m1.3697\u001b[0m  0.0005  0.5005\n",
      "     12            \u001b[36m0.8545\u001b[0m        \u001b[32m0.5868\u001b[0m       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.3169\u001b[0m  0.0005  0.5161\n",
      "     13            \u001b[36m0.8818\u001b[0m        0.6023       \u001b[35m0.4444\u001b[0m            \u001b[31m0.4444\u001b[0m        \u001b[94m1.2831\u001b[0m  0.0005  0.5469\n",
      "     14            \u001b[36m0.9045\u001b[0m        \u001b[32m0.5523\u001b[0m       0.4410            0.4410        \u001b[94m1.2522\u001b[0m  0.0005  0.6251\n",
      "     15            \u001b[36m0.9227\u001b[0m        \u001b[32m0.4847\u001b[0m       \u001b[35m0.4722\u001b[0m            \u001b[31m0.4722\u001b[0m        \u001b[94m1.2296\u001b[0m  0.0004  0.5938\n",
      "     16            \u001b[36m0.9455\u001b[0m        0.5320       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        \u001b[94m1.2135\u001b[0m  0.0004  0.6253\n",
      "     17            \u001b[36m0.9545\u001b[0m        \u001b[32m0.4690\u001b[0m       0.4826            0.4826        \u001b[94m1.2091\u001b[0m  0.0004  0.6407\n",
      "     18            0.9500        0.4766       0.4757            0.4757        \u001b[94m1.2034\u001b[0m  0.0004  0.5157\n",
      "     19            \u001b[36m0.9682\u001b[0m        0.4740       0.4792            0.4792        \u001b[94m1.1946\u001b[0m  0.0004  0.5005\n",
      "     20            0.9591        0.4926       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        \u001b[94m1.1895\u001b[0m  0.0003  0.5165\n",
      "     21            0.9591        \u001b[32m0.4181\u001b[0m       0.4965            0.4965        \u001b[94m1.1872\u001b[0m  0.0003  0.5164\n",
      "     22            0.9636        \u001b[32m0.3535\u001b[0m       0.4965            0.4965        \u001b[94m1.1867\u001b[0m  0.0003  0.5003\n",
      "     23            0.9636        0.3629       \u001b[35m0.5069\u001b[0m            \u001b[31m0.5069\u001b[0m        1.1871  0.0002  0.5161\n",
      "     24            0.9682        \u001b[32m0.3334\u001b[0m       0.4965            0.4965        \u001b[94m1.1852\u001b[0m  0.0002  0.5313\n",
      "     25            0.9636        0.4003       0.5000            0.5000        \u001b[94m1.1838\u001b[0m  0.0002  0.5005\n",
      "     26            0.9636        0.3579       0.4931            0.4931        1.1855  0.0002  0.5001\n",
      "     27            \u001b[36m0.9727\u001b[0m        0.3738       0.5069            0.5069        1.1869  0.0002  0.5010\n",
      "     28            0.9727        0.3395       \u001b[35m0.5174\u001b[0m            \u001b[31m0.5174\u001b[0m        1.1858  0.0001  0.5165\n",
      "     29            \u001b[36m0.9864\u001b[0m        \u001b[32m0.3281\u001b[0m       \u001b[35m0.5243\u001b[0m            \u001b[31m0.5243\u001b[0m        \u001b[94m1.1815\u001b[0m  0.0001  0.5313\n",
      "     30            0.9864        \u001b[32m0.3065\u001b[0m       0.5243            0.5243        \u001b[94m1.1788\u001b[0m  0.0001  0.5310\n",
      "     31            0.9864        0.3130       0.5174            0.5174        \u001b[94m1.1769\u001b[0m  0.0001  0.5162\n",
      "     32            0.9864        \u001b[32m0.3057\u001b[0m       0.5104            0.5104        \u001b[94m1.1747\u001b[0m  0.0001  0.5625\n",
      "     33            0.9864        \u001b[32m0.2896\u001b[0m       0.5139            0.5139        \u001b[94m1.1727\u001b[0m  0.0000  0.5313\n",
      "     34            0.9864        0.3149       0.5104            0.5104        \u001b[94m1.1718\u001b[0m  0.0000  0.5313\n",
      "     35            0.9864        0.3210       0.5104            0.5104        \u001b[94m1.1709\u001b[0m  0.0000  0.5007\n",
      "     36            0.9864        0.3080       0.5069            0.5069        \u001b[94m1.1702\u001b[0m  0.0000  0.5160\n",
      "     37            0.9864        0.3124       0.5104            0.5104        \u001b[94m1.1693\u001b[0m  0.0000  0.6094\n",
      "     38            0.9864        0.2901       0.5104            0.5104        \u001b[94m1.1692\u001b[0m  0.0000  0.6251\n",
      "     39            0.9864        0.2983       0.5104            0.5104        \u001b[94m1.1688\u001b[0m  0.0000  0.5625\n",
      "     40            0.9864        0.3167       0.5104            0.5104        \u001b[94m1.1684\u001b[0m  0.0000  0.5782\n",
      "Training model for subject 9 with 230 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2565\u001b[0m        \u001b[32m1.5921\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m3.5723\u001b[0m  0.0006  0.6563\n",
      "      2            0.2522        \u001b[32m1.3060\u001b[0m       0.2500            0.2500        4.4478  0.0006  0.5005\n",
      "      3            0.2522        \u001b[32m1.1550\u001b[0m       0.2500            0.2500        5.1630  0.0006  0.5005\n",
      "      4            0.2522        \u001b[32m1.0642\u001b[0m       0.2500            0.2500        4.7768  0.0006  0.5000\n",
      "      5            0.2522        \u001b[32m0.9881\u001b[0m       0.2500            0.2500        3.8368  0.0006  0.5004\n",
      "      6            \u001b[36m0.2696\u001b[0m        \u001b[32m0.9734\u001b[0m       0.2500            0.2500        \u001b[94m2.7764\u001b[0m  0.0006  0.5157\n",
      "      7            \u001b[36m0.3043\u001b[0m        \u001b[32m0.8457\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m2.2975\u001b[0m  0.0006  0.5004\n",
      "      8            \u001b[36m0.3435\u001b[0m        \u001b[32m0.8333\u001b[0m       \u001b[35m0.2674\u001b[0m            \u001b[31m0.2674\u001b[0m        \u001b[94m2.0522\u001b[0m  0.0006  0.5002\n",
      "      9            \u001b[36m0.3913\u001b[0m        \u001b[32m0.7676\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m1.9043\u001b[0m  0.0006  0.5002\n",
      "     10            \u001b[36m0.5000\u001b[0m        \u001b[32m0.7410\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m1.6875\u001b[0m  0.0005  0.5000\n",
      "     11            \u001b[36m0.6304\u001b[0m        \u001b[32m0.6941\u001b[0m       \u001b[35m0.3507\u001b[0m            \u001b[31m0.3507\u001b[0m        \u001b[94m1.5179\u001b[0m  0.0005  0.5006\n",
      "     12            \u001b[36m0.7304\u001b[0m        \u001b[32m0.6258\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.3683\u001b[0m  0.0005  0.5161\n",
      "     13            \u001b[36m0.8478\u001b[0m        \u001b[32m0.5776\u001b[0m       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2557\u001b[0m  0.0005  0.5003\n",
      "     14            \u001b[36m0.9261\u001b[0m        \u001b[32m0.5533\u001b[0m       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        \u001b[94m1.1938\u001b[0m  0.0005  0.5010\n",
      "     15            \u001b[36m0.9391\u001b[0m        \u001b[32m0.4954\u001b[0m       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        \u001b[94m1.1838\u001b[0m  0.0004  0.5000\n",
      "     16            \u001b[36m0.9522\u001b[0m        0.5321       0.4931            0.4931        \u001b[94m1.1669\u001b[0m  0.0004  0.5157\n",
      "     17            \u001b[36m0.9565\u001b[0m        0.5013       \u001b[35m0.5104\u001b[0m            \u001b[31m0.5104\u001b[0m        \u001b[94m1.1498\u001b[0m  0.0004  0.5157\n",
      "     18            0.9522        \u001b[32m0.4636\u001b[0m       \u001b[35m0.5243\u001b[0m            \u001b[31m0.5243\u001b[0m        \u001b[94m1.1390\u001b[0m  0.0004  0.5000\n",
      "     19            0.9565        \u001b[32m0.4327\u001b[0m       \u001b[35m0.5278\u001b[0m            \u001b[31m0.5278\u001b[0m        \u001b[94m1.1341\u001b[0m  0.0004  0.5004\n",
      "     20            \u001b[36m0.9609\u001b[0m        0.4542       \u001b[35m0.5382\u001b[0m            \u001b[31m0.5382\u001b[0m        \u001b[94m1.1285\u001b[0m  0.0003  0.5939\n",
      "     21            \u001b[36m0.9696\u001b[0m        \u001b[32m0.4065\u001b[0m       0.5312            0.5312        \u001b[94m1.1250\u001b[0m  0.0003  0.5942\n",
      "     22            \u001b[36m0.9739\u001b[0m        \u001b[32m0.3449\u001b[0m       0.5347            0.5347        \u001b[94m1.1190\u001b[0m  0.0003  0.5938\n",
      "     23            0.9739        0.4307       0.5243            0.5243        \u001b[94m1.1152\u001b[0m  0.0002  0.6098\n",
      "     24            \u001b[36m0.9783\u001b[0m        0.3691       0.5347            0.5347        \u001b[94m1.1116\u001b[0m  0.0002  0.6279\n",
      "     25            \u001b[36m0.9870\u001b[0m        \u001b[32m0.3235\u001b[0m       0.5312            0.5312        \u001b[94m1.1097\u001b[0m  0.0002  0.5010\n",
      "     26            0.9870        0.3649       0.5278            0.5278        \u001b[94m1.1051\u001b[0m  0.0002  0.5000\n",
      "     27            0.9870        0.3543       0.5312            0.5312        \u001b[94m1.1012\u001b[0m  0.0002  0.5160\n",
      "     28            \u001b[36m0.9913\u001b[0m        0.3489       0.5347            0.5347        \u001b[94m1.0976\u001b[0m  0.0001  0.5004\n",
      "     29            \u001b[36m0.9957\u001b[0m        0.3241       0.5382            0.5382        \u001b[94m1.0941\u001b[0m  0.0001  0.5157\n",
      "     30            0.9957        \u001b[32m0.3073\u001b[0m       0.5382            0.5382        \u001b[94m1.0908\u001b[0m  0.0001  0.5889\n",
      "     31            0.9957        0.3291       \u001b[35m0.5451\u001b[0m            \u001b[31m0.5451\u001b[0m        \u001b[94m1.0864\u001b[0m  0.0001  0.5003\n",
      "     32            0.9913        0.3291       0.5417            0.5417        \u001b[94m1.0841\u001b[0m  0.0001  0.5158\n",
      "     33            0.9913        0.3639       0.5347            0.5347        \u001b[94m1.0838\u001b[0m  0.0000  0.5000\n",
      "     34            0.9913        0.3094       0.5347            0.5347        \u001b[94m1.0835\u001b[0m  0.0000  0.5009\n",
      "     35            0.9957        0.3130       0.5312            0.5312        \u001b[94m1.0833\u001b[0m  0.0000  0.5159\n",
      "     36            0.9957        0.3214       0.5312            0.5312        \u001b[94m1.0829\u001b[0m  0.0000  0.5164\n",
      "     37            0.9957        \u001b[32m0.3009\u001b[0m       0.5312            0.5312        1.0829  0.0000  0.5000\n",
      "     38            0.9957        0.3257       0.5312            0.5312        1.0830  0.0000  0.4844\n",
      "     39            0.9957        0.3291       0.5312            0.5312        1.0832  0.0000  0.4853\n",
      "     40            0.9957        0.3635       0.5312            0.5312        1.0832  0.0000  0.4851\n",
      "Training model for subject 9 with 240 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.7708\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m4.4131\u001b[0m  0.0006  0.5000\n",
      "      2            \u001b[36m0.3083\u001b[0m        \u001b[32m1.4456\u001b[0m       0.2500            0.2500        \u001b[94m2.8678\u001b[0m  0.0006  0.4688\n",
      "      3            0.2500        \u001b[32m1.2239\u001b[0m       0.2500            0.2500        4.1521  0.0006  0.5313\n",
      "      4            0.2500        \u001b[32m1.1499\u001b[0m       0.2500            0.2500        4.7139  0.0006  0.5938\n",
      "      5            0.2500        \u001b[32m1.0234\u001b[0m       0.2500            0.2500        3.9844  0.0006  0.6094\n",
      "      6            0.2542        \u001b[32m0.9749\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        2.9850  0.0006  0.5782\n",
      "      7            \u001b[36m0.3750\u001b[0m        \u001b[32m0.9097\u001b[0m       \u001b[35m0.2847\u001b[0m            \u001b[31m0.2847\u001b[0m        \u001b[94m2.0995\u001b[0m  0.0006  0.6875\n",
      "      8            \u001b[36m0.5417\u001b[0m        \u001b[32m0.9072\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m1.6441\u001b[0m  0.0006  0.5473\n",
      "      9            \u001b[36m0.6083\u001b[0m        \u001b[32m0.8166\u001b[0m       \u001b[35m0.3854\u001b[0m            \u001b[31m0.3854\u001b[0m        \u001b[94m1.5082\u001b[0m  0.0006  0.5313\n",
      "     10            \u001b[36m0.6875\u001b[0m        \u001b[32m0.7497\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.4607\u001b[0m  0.0005  0.5313\n",
      "     11            \u001b[36m0.7375\u001b[0m        0.7626       \u001b[35m0.3993\u001b[0m            \u001b[31m0.3993\u001b[0m        \u001b[94m1.4131\u001b[0m  0.0005  0.5470\n",
      "     12            \u001b[36m0.8333\u001b[0m        \u001b[32m0.7105\u001b[0m       \u001b[35m0.4132\u001b[0m            \u001b[31m0.4132\u001b[0m        \u001b[94m1.3357\u001b[0m  0.0005  0.5000\n",
      "     13            \u001b[36m0.8708\u001b[0m        \u001b[32m0.6518\u001b[0m       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.2762\u001b[0m  0.0005  0.5004\n",
      "     14            \u001b[36m0.8917\u001b[0m        \u001b[32m0.6517\u001b[0m       0.4410            0.4410        \u001b[94m1.2486\u001b[0m  0.0005  0.5157\n",
      "     15            \u001b[36m0.9083\u001b[0m        \u001b[32m0.6359\u001b[0m       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.2354\u001b[0m  0.0004  0.5160\n",
      "     16            \u001b[36m0.9167\u001b[0m        \u001b[32m0.5813\u001b[0m       0.4514            0.4514        \u001b[94m1.2284\u001b[0m  0.0004  0.5314\n",
      "     17            0.9125        \u001b[32m0.5378\u001b[0m       \u001b[35m0.4618\u001b[0m            \u001b[31m0.4618\u001b[0m        \u001b[94m1.2247\u001b[0m  0.0004  0.5157\n",
      "     18            \u001b[36m0.9333\u001b[0m        0.5392       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        1.2291  0.0004  0.5168\n",
      "     19            0.9208        \u001b[32m0.4727\u001b[0m       0.4618            0.4618        1.2334  0.0004  0.5004\n",
      "     20            0.9292        0.4922       0.4583            0.4583        1.2341  0.0003  0.5006\n",
      "     21            \u001b[36m0.9375\u001b[0m        0.5227       0.4549            0.4549        1.2312  0.0003  0.5161\n",
      "     22            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4640\u001b[0m       \u001b[35m0.4757\u001b[0m            \u001b[31m0.4757\u001b[0m        \u001b[94m1.2213\u001b[0m  0.0003  0.5159\n",
      "     23            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3924\u001b[0m       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        \u001b[94m1.2121\u001b[0m  0.0002  0.5165\n",
      "     24            0.9625        0.4295       \u001b[35m0.4931\u001b[0m            \u001b[31m0.4931\u001b[0m        \u001b[94m1.2058\u001b[0m  0.0002  0.5157\n",
      "     25            0.9667        \u001b[32m0.3899\u001b[0m       0.4896            0.4896        \u001b[94m1.1964\u001b[0m  0.0002  0.5157\n",
      "     26            0.9625        \u001b[32m0.3876\u001b[0m       \u001b[35m0.4965\u001b[0m            \u001b[31m0.4965\u001b[0m        \u001b[94m1.1916\u001b[0m  0.0002  0.6250\n",
      "     27            \u001b[36m0.9708\u001b[0m        0.4426       \u001b[35m0.5035\u001b[0m            \u001b[31m0.5035\u001b[0m        \u001b[94m1.1904\u001b[0m  0.0002  0.6407\n",
      "     28            0.9667        \u001b[32m0.3871\u001b[0m       0.4965            0.4965        1.1925  0.0001  0.6094\n",
      "     29            0.9625        \u001b[32m0.3479\u001b[0m       0.4931            0.4931        1.1955  0.0001  0.6719\n",
      "     30            0.9667        0.3643       0.5035            0.5035        1.1957  0.0001  0.5938\n",
      "     31            0.9708        0.3966       \u001b[35m0.5104\u001b[0m            \u001b[31m0.5104\u001b[0m        1.1935  0.0001  0.5000\n",
      "     32            \u001b[36m0.9750\u001b[0m        0.3993       0.5069            0.5069        1.1931  0.0001  0.5162\n",
      "     33            0.9750        0.3581       0.5035            0.5035        1.1915  0.0000  0.5157\n",
      "     34            0.9750        \u001b[32m0.3449\u001b[0m       0.5069            0.5069        \u001b[94m1.1900\u001b[0m  0.0000  0.7035\n",
      "     35            0.9750        0.3737       0.5069            0.5069        \u001b[94m1.1888\u001b[0m  0.0000  0.5157\n",
      "     36            \u001b[36m0.9792\u001b[0m        0.3645       0.5069            0.5069        \u001b[94m1.1883\u001b[0m  0.0000  0.5005\n",
      "     37            0.9792        0.3659       0.5069            0.5069        \u001b[94m1.1879\u001b[0m  0.0000  0.5157\n",
      "     38            0.9792        0.3543       0.5069            0.5069        \u001b[94m1.1868\u001b[0m  0.0000  0.5165\n",
      "     39            0.9792        0.3846       0.5069            0.5069        \u001b[94m1.1865\u001b[0m  0.0000  0.5004\n",
      "     40            0.9792        0.3548       0.5069            0.5069        1.1869  0.0000  0.5004\n",
      "Training model for subject 9 with 250 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3120\u001b[0m        \u001b[32m1.8111\u001b[0m       \u001b[35m0.2812\u001b[0m            \u001b[31m0.2812\u001b[0m        \u001b[94m1.8378\u001b[0m  0.0006  0.5001\n",
      "      2            0.2920        \u001b[32m1.2853\u001b[0m       0.2743            0.2743        3.3373  0.0006  0.5002\n",
      "      3            \u001b[36m0.3560\u001b[0m        \u001b[32m1.2012\u001b[0m       0.2674            0.2674        3.7930  0.0006  0.5008\n",
      "      4            \u001b[36m0.3960\u001b[0m        \u001b[32m1.1417\u001b[0m       0.2569            0.2569        3.4758  0.0006  0.5158\n",
      "      5            0.3720        \u001b[32m1.0654\u001b[0m       \u001b[35m0.2882\u001b[0m            \u001b[31m0.2882\u001b[0m        3.0178  0.0006  0.5161\n",
      "      6            0.3120        \u001b[32m0.9708\u001b[0m       0.2743            0.2743        2.7753  0.0006  0.5166\n",
      "      7            0.3080        \u001b[32m0.9381\u001b[0m       0.2778            0.2778        2.4880  0.0006  0.5667\n",
      "      8            0.3160        \u001b[32m0.8954\u001b[0m       0.2812            0.2812        2.2051  0.0006  0.5833\n",
      "      9            \u001b[36m0.4480\u001b[0m        \u001b[32m0.8072\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        1.9010  0.0006  0.7982\n",
      "     10            \u001b[36m0.5600\u001b[0m        \u001b[32m0.7906\u001b[0m       \u001b[35m0.3542\u001b[0m            \u001b[31m0.3542\u001b[0m        \u001b[94m1.6265\u001b[0m  0.0005  0.6278\n",
      "     11            \u001b[36m0.7120\u001b[0m        \u001b[32m0.7091\u001b[0m       \u001b[35m0.3958\u001b[0m            \u001b[31m0.3958\u001b[0m        \u001b[94m1.4600\u001b[0m  0.0005  0.5801\n",
      "     12            \u001b[36m0.7880\u001b[0m        \u001b[32m0.6750\u001b[0m       0.3750            0.3750        \u001b[94m1.3845\u001b[0m  0.0005  0.6231\n",
      "     13            \u001b[36m0.8360\u001b[0m        0.6991       0.3924            0.3924        \u001b[94m1.3480\u001b[0m  0.0005  0.5085\n",
      "     14            \u001b[36m0.8600\u001b[0m        \u001b[32m0.6144\u001b[0m       \u001b[35m0.4236\u001b[0m            \u001b[31m0.4236\u001b[0m        \u001b[94m1.3055\u001b[0m  0.0005  0.5740\n",
      "     15            \u001b[36m0.8760\u001b[0m        \u001b[32m0.6000\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.2821\u001b[0m  0.0004  0.5006\n",
      "     16            0.8720        0.6262       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.2655\u001b[0m  0.0004  0.6243\n",
      "     17            \u001b[36m0.8880\u001b[0m        \u001b[32m0.5050\u001b[0m       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.2514\u001b[0m  0.0004  0.5313\n",
      "     18            \u001b[36m0.9000\u001b[0m        0.5322       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.2400\u001b[0m  0.0004  0.5004\n",
      "     19            \u001b[36m0.9080\u001b[0m        \u001b[32m0.4961\u001b[0m       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        \u001b[94m1.2265\u001b[0m  0.0004  0.5161\n",
      "     20            \u001b[36m0.9440\u001b[0m        0.5214       0.4653            0.4653        \u001b[94m1.2192\u001b[0m  0.0003  0.7746\n",
      "     21            0.9440        \u001b[32m0.4703\u001b[0m       0.4688            0.4688        \u001b[94m1.2124\u001b[0m  0.0003  0.6762\n",
      "     22            0.9360        \u001b[32m0.4390\u001b[0m       0.4757            0.4757        \u001b[94m1.2001\u001b[0m  0.0003  0.5810\n",
      "     23            0.9320        0.4812       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        \u001b[94m1.1917\u001b[0m  0.0002  0.7609\n",
      "     24            0.9360        \u001b[32m0.3933\u001b[0m       \u001b[35m0.5000\u001b[0m            \u001b[31m0.5000\u001b[0m        \u001b[94m1.1846\u001b[0m  0.0002  0.9437\n",
      "     25            \u001b[36m0.9480\u001b[0m        0.4328       \u001b[35m0.5104\u001b[0m            \u001b[31m0.5104\u001b[0m        \u001b[94m1.1795\u001b[0m  0.0002  0.6676\n",
      "     26            \u001b[36m0.9520\u001b[0m        \u001b[32m0.3817\u001b[0m       0.5104            0.5104        \u001b[94m1.1757\u001b[0m  0.0002  0.5729\n",
      "     27            0.9520        0.4122       0.4896            0.4896        \u001b[94m1.1746\u001b[0m  0.0002  0.5798\n",
      "     28            \u001b[36m0.9560\u001b[0m        0.3843       0.4861            0.4861        \u001b[94m1.1728\u001b[0m  0.0001  0.5941\n",
      "     29            \u001b[36m0.9680\u001b[0m        0.3918       0.4757            0.4757        \u001b[94m1.1686\u001b[0m  0.0001  0.6250\n",
      "     30            \u001b[36m0.9760\u001b[0m        \u001b[32m0.3660\u001b[0m       0.4861            0.4861        \u001b[94m1.1671\u001b[0m  0.0001  0.6250\n",
      "     31            \u001b[36m0.9800\u001b[0m        0.4274       0.4826            0.4826        1.1677  0.0001  0.7257\n",
      "     32            0.9800        0.3700       0.4826            0.4826        \u001b[94m1.1666\u001b[0m  0.0001  0.5625\n",
      "     33            0.9800        0.4284       0.4826            0.4826        \u001b[94m1.1662\u001b[0m  0.0000  0.5316\n",
      "     34            \u001b[36m0.9840\u001b[0m        0.3755       0.4826            0.4826        1.1681  0.0000  0.5475\n",
      "     35            0.9840        0.3758       0.4861            0.4861        1.1684  0.0000  0.5000\n",
      "     36            0.9840        \u001b[32m0.3653\u001b[0m       0.4896            0.4896        1.1675  0.0000  0.5157\n",
      "     37            0.9840        0.3909       0.4896            0.4896        1.1667  0.0000  0.5161\n",
      "     38            0.9840        0.4103       0.4896            0.4896        1.1666  0.0000  0.5157\n",
      "     39            0.9840        0.3977       0.4896            0.4896        1.1671  0.0000  0.5004\n",
      "     40            0.9840        0.4200       0.4896            0.4896        1.1676  0.0000  0.5313\n",
      "Training model for subject 9 with 260 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2923\u001b[0m        \u001b[32m1.4707\u001b[0m       \u001b[35m0.2604\u001b[0m            \u001b[31m0.2604\u001b[0m        \u001b[94m2.9960\u001b[0m  0.0006  0.6094\n",
      "      2            \u001b[36m0.3192\u001b[0m        \u001b[32m1.2725\u001b[0m       \u001b[35m0.2743\u001b[0m            \u001b[31m0.2743\u001b[0m        \u001b[94m2.3086\u001b[0m  0.0006  0.6250\n",
      "      3            \u001b[36m0.4769\u001b[0m        \u001b[32m1.1367\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m2.2321\u001b[0m  0.0006  0.6094\n",
      "      4            \u001b[36m0.5077\u001b[0m        \u001b[32m0.9672\u001b[0m       \u001b[35m0.3194\u001b[0m            \u001b[31m0.3194\u001b[0m        \u001b[94m2.0125\u001b[0m  0.0006  0.6094\n",
      "      5            \u001b[36m0.5308\u001b[0m        \u001b[32m0.9041\u001b[0m       \u001b[35m0.3264\u001b[0m            \u001b[31m0.3264\u001b[0m        \u001b[94m1.7903\u001b[0m  0.0006  0.6568\n",
      "      6            0.5308        \u001b[32m0.8228\u001b[0m       \u001b[35m0.3472\u001b[0m            \u001b[31m0.3472\u001b[0m        \u001b[94m1.6368\u001b[0m  0.0006  0.6251\n",
      "      7            \u001b[36m0.6731\u001b[0m        \u001b[32m0.7930\u001b[0m       \u001b[35m0.4201\u001b[0m            \u001b[31m0.4201\u001b[0m        \u001b[94m1.4569\u001b[0m  0.0006  0.6254\n",
      "      8            \u001b[36m0.8000\u001b[0m        \u001b[32m0.7423\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.3256\u001b[0m  0.0006  0.6563\n",
      "      9            \u001b[36m0.8615\u001b[0m        \u001b[32m0.7409\u001b[0m       0.3924            0.3924        \u001b[94m1.3005\u001b[0m  0.0006  0.7504\n",
      "     10            \u001b[36m0.8885\u001b[0m        \u001b[32m0.6570\u001b[0m       0.4236            0.4236        \u001b[94m1.2775\u001b[0m  0.0005  0.7344\n",
      "     11            0.8731        \u001b[32m0.6422\u001b[0m       \u001b[35m0.4410\u001b[0m            \u001b[31m0.4410\u001b[0m        \u001b[94m1.2608\u001b[0m  0.0005  0.7974\n",
      "     12            \u001b[36m0.9231\u001b[0m        \u001b[32m0.5635\u001b[0m       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.2428\u001b[0m  0.0005  0.7346\n",
      "     13            \u001b[36m0.9423\u001b[0m        \u001b[32m0.5030\u001b[0m       0.4583            0.4583        \u001b[94m1.2204\u001b[0m  0.0005  0.6721\n",
      "     14            \u001b[36m0.9615\u001b[0m        0.5479       \u001b[35m0.4688\u001b[0m            \u001b[31m0.4688\u001b[0m        \u001b[94m1.1978\u001b[0m  0.0005  0.6102\n",
      "     15            \u001b[36m0.9731\u001b[0m        \u001b[32m0.4708\u001b[0m       \u001b[35m0.4861\u001b[0m            \u001b[31m0.4861\u001b[0m        \u001b[94m1.1810\u001b[0m  0.0004  0.6567\n",
      "     16            \u001b[36m0.9808\u001b[0m        \u001b[32m0.4223\u001b[0m       \u001b[35m0.5035\u001b[0m            \u001b[31m0.5035\u001b[0m        \u001b[94m1.1704\u001b[0m  0.0004  0.6561\n",
      "     17            0.9808        \u001b[32m0.4135\u001b[0m       \u001b[35m0.5243\u001b[0m            \u001b[31m0.5243\u001b[0m        \u001b[94m1.1584\u001b[0m  0.0004  0.6727\n",
      "     18            \u001b[36m0.9846\u001b[0m        \u001b[32m0.3921\u001b[0m       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        \u001b[94m1.1554\u001b[0m  0.0004  0.6719\n",
      "     19            \u001b[36m0.9923\u001b[0m        \u001b[32m0.3793\u001b[0m       \u001b[35m0.5382\u001b[0m            \u001b[31m0.5382\u001b[0m        1.1557  0.0004  0.6563\n",
      "     20            0.9923        \u001b[32m0.3403\u001b[0m       0.5278            0.5278        1.1622  0.0003  0.6560\n",
      "     21            0.9923        0.3609       0.5243            0.5243        1.1616  0.0003  0.6492\n",
      "     22            0.9885        0.3687       0.5382            0.5382        1.1554  0.0003  0.6857\n",
      "     23            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3133\u001b[0m       0.5347            0.5347        \u001b[94m1.1461\u001b[0m  0.0002  0.9646\n",
      "     24            0.9885        0.3258       0.5347            0.5347        \u001b[94m1.1449\u001b[0m  0.0002  0.6955\n",
      "     25            0.9846        \u001b[32m0.2752\u001b[0m       0.5278            0.5278        1.1462  0.0002  0.6326\n",
      "     26            0.9846        \u001b[32m0.2699\u001b[0m       0.5312            0.5312        \u001b[94m1.1449\u001b[0m  0.0002  0.7119\n",
      "     27            0.9962        0.2884       0.5312            0.5312        \u001b[94m1.1444\u001b[0m  0.0002  0.7841\n",
      "     28            0.9962        \u001b[32m0.2602\u001b[0m       0.5278            0.5278        \u001b[94m1.1412\u001b[0m  0.0001  0.7667\n",
      "     29            0.9923        0.2632       0.5382            0.5382        \u001b[94m1.1353\u001b[0m  0.0001  0.7981\n",
      "     30            1.0000        \u001b[32m0.2482\u001b[0m       \u001b[35m0.5486\u001b[0m            \u001b[31m0.5486\u001b[0m        \u001b[94m1.1320\u001b[0m  0.0001  0.6610\n",
      "     31            0.9923        0.2802       \u001b[35m0.5556\u001b[0m            \u001b[31m0.5556\u001b[0m        \u001b[94m1.1308\u001b[0m  0.0001  0.5918\n",
      "     32            0.9923        0.2642       0.5556            0.5556        \u001b[94m1.1283\u001b[0m  0.0001  0.7415\n",
      "     33            0.9962        0.2795       0.5556            0.5556        \u001b[94m1.1277\u001b[0m  0.0000  0.7880\n",
      "     34            1.0000        0.2754       0.5556            0.5556        \u001b[94m1.1273\u001b[0m  0.0000  0.7748\n",
      "     35            1.0000        0.2824       0.5521            0.5521        \u001b[94m1.1271\u001b[0m  0.0000  0.7594\n",
      "     36            1.0000        0.2996       0.5521            0.5521        \u001b[94m1.1268\u001b[0m  0.0000  0.7313\n",
      "     37            1.0000        \u001b[32m0.2421\u001b[0m       0.5556            0.5556        1.1269  0.0000  0.6789\n",
      "     38            1.0000        \u001b[32m0.2195\u001b[0m       0.5556            0.5556        \u001b[94m1.1268\u001b[0m  0.0000  0.6228\n",
      "     39            1.0000        0.2579       0.5521            0.5521        \u001b[94m1.1266\u001b[0m  0.0000  0.5938\n",
      "     40            1.0000        0.3030       0.5486            0.5486        \u001b[94m1.1265\u001b[0m  0.0000  0.5934\n",
      "Training model for subject 9 with 270 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2630\u001b[0m        \u001b[32m1.6147\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m2.7525\u001b[0m  0.0006  0.6094\n",
      "      2            \u001b[36m0.2741\u001b[0m        \u001b[32m1.2814\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        2.9123  0.0006  0.5895\n",
      "      3            \u001b[36m0.2815\u001b[0m        \u001b[32m1.1032\u001b[0m       \u001b[35m0.2569\u001b[0m            \u001b[31m0.2569\u001b[0m        \u001b[94m2.3424\u001b[0m  0.0006  0.5940\n",
      "      4            \u001b[36m0.3407\u001b[0m        \u001b[32m1.0483\u001b[0m       \u001b[35m0.2778\u001b[0m            \u001b[31m0.2778\u001b[0m        \u001b[94m2.0207\u001b[0m  0.0006  0.8470\n",
      "      5            \u001b[36m0.5519\u001b[0m        \u001b[32m0.9312\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m1.7035\u001b[0m  0.0006  0.7504\n",
      "      6            \u001b[36m0.6296\u001b[0m        \u001b[32m0.8503\u001b[0m       \u001b[35m0.3368\u001b[0m            \u001b[31m0.3368\u001b[0m        \u001b[94m1.5918\u001b[0m  0.0006  0.6875\n",
      "      7            \u001b[36m0.6630\u001b[0m        \u001b[32m0.8242\u001b[0m       0.3299            0.3299        \u001b[94m1.5309\u001b[0m  0.0006  0.7447\n",
      "      8            \u001b[36m0.7630\u001b[0m        \u001b[32m0.7263\u001b[0m       \u001b[35m0.3889\u001b[0m            \u001b[31m0.3889\u001b[0m        \u001b[94m1.3742\u001b[0m  0.0006  0.6719\n",
      "      9            \u001b[36m0.8370\u001b[0m        \u001b[32m0.6994\u001b[0m       \u001b[35m0.4340\u001b[0m            \u001b[31m0.4340\u001b[0m        \u001b[94m1.2682\u001b[0m  0.0006  0.5902\n",
      "     10            \u001b[36m0.8593\u001b[0m        \u001b[32m0.6449\u001b[0m       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.2493\u001b[0m  0.0005  0.5945\n",
      "     11            \u001b[36m0.9111\u001b[0m        \u001b[32m0.6159\u001b[0m       \u001b[35m0.4549\u001b[0m            \u001b[31m0.4549\u001b[0m        \u001b[94m1.2221\u001b[0m  0.0005  0.5782\n",
      "     12            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5714\u001b[0m       0.4444            0.4444        \u001b[94m1.2168\u001b[0m  0.0005  0.5938\n",
      "     13            0.9333        \u001b[32m0.5088\u001b[0m       0.4479            0.4479        1.2271  0.0005  0.5942\n",
      "     14            \u001b[36m0.9370\u001b[0m        0.5203       0.4236            0.4236        1.2336  0.0005  0.5938\n",
      "     15            0.9370        \u001b[32m0.4690\u001b[0m       0.4410            0.4410        1.2196  0.0004  0.5941\n",
      "     16            \u001b[36m0.9407\u001b[0m        \u001b[32m0.4273\u001b[0m       \u001b[35m0.4722\u001b[0m            \u001b[31m0.4722\u001b[0m        \u001b[94m1.2063\u001b[0m  0.0004  0.5782\n",
      "     17            \u001b[36m0.9556\u001b[0m        0.4314       \u001b[35m0.5035\u001b[0m            \u001b[31m0.5035\u001b[0m        \u001b[94m1.1682\u001b[0m  0.0004  0.6129\n",
      "     18            \u001b[36m0.9630\u001b[0m        \u001b[32m0.3602\u001b[0m       \u001b[35m0.5139\u001b[0m            \u001b[31m0.5139\u001b[0m        \u001b[94m1.1430\u001b[0m  0.0004  0.5798\n",
      "     19            \u001b[36m0.9704\u001b[0m        0.3811       0.5035            0.5035        \u001b[94m1.1316\u001b[0m  0.0004  0.5938\n",
      "     20            \u001b[36m0.9741\u001b[0m        0.3820       0.5035            0.5035        1.1356  0.0003  0.5866\n",
      "     21            0.9741        0.4050       0.5139            0.5139        \u001b[94m1.1299\u001b[0m  0.0003  0.5899\n",
      "     22            \u001b[36m0.9852\u001b[0m        0.3666       0.5139            0.5139        \u001b[94m1.1248\u001b[0m  0.0003  0.6186\n",
      "     23            \u001b[36m0.9889\u001b[0m        \u001b[32m0.3393\u001b[0m       0.5104            0.5104        \u001b[94m1.1225\u001b[0m  0.0002  0.5982\n",
      "     24            0.9889        \u001b[32m0.3029\u001b[0m       \u001b[35m0.5174\u001b[0m            \u001b[31m0.5174\u001b[0m        1.1274  0.0002  0.6407\n",
      "     25            \u001b[36m0.9926\u001b[0m        0.3201       \u001b[35m0.5278\u001b[0m            \u001b[31m0.5278\u001b[0m        1.1343  0.0002  0.7133\n",
      "     26            0.9926        \u001b[32m0.2744\u001b[0m       \u001b[35m0.5382\u001b[0m            \u001b[31m0.5382\u001b[0m        1.1302  0.0002  0.7329\n",
      "     27            0.9926        \u001b[32m0.2542\u001b[0m       0.5347            0.5347        1.1273  0.0002  0.7344\n",
      "     28            \u001b[36m0.9963\u001b[0m        0.2793       0.5278            0.5278        1.1229  0.0001  0.6875\n",
      "     29            0.9963        0.2771       0.5312            0.5312        \u001b[94m1.1195\u001b[0m  0.0001  0.6250\n",
      "     30            0.9963        0.2834       0.5278            0.5278        \u001b[94m1.1170\u001b[0m  0.0001  0.6250\n",
      "     31            0.9926        0.2847       0.5312            0.5312        \u001b[94m1.1157\u001b[0m  0.0001  0.6991\n",
      "     32            0.9926        \u001b[32m0.2421\u001b[0m       0.5382            0.5382        \u001b[94m1.1147\u001b[0m  0.0001  0.5938\n",
      "     33            0.9926        0.2585       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        \u001b[94m1.1137\u001b[0m  0.0000  0.6088\n",
      "     34            0.9963        \u001b[32m0.2411\u001b[0m       0.5347            0.5347        \u001b[94m1.1131\u001b[0m  0.0000  0.6250\n",
      "     35            0.9963        0.3007       0.5382            0.5382        \u001b[94m1.1123\u001b[0m  0.0000  0.6251\n",
      "     36            0.9963        0.2592       0.5382            0.5382        \u001b[94m1.1117\u001b[0m  0.0000  0.6563\n",
      "     37            0.9963        0.2497       0.5312            0.5312        \u001b[94m1.1109\u001b[0m  0.0000  0.6567\n",
      "     38            0.9963        0.2440       0.5312            0.5312        \u001b[94m1.1108\u001b[0m  0.0000  0.6253\n",
      "     39            0.9963        0.2658       0.5312            0.5312        \u001b[94m1.1103\u001b[0m  0.0000  0.6250\n",
      "     40            0.9963        \u001b[32m0.2388\u001b[0m       0.5278            0.5278        1.1106  0.0000  0.6255\n"
     ]
    }
   ],
   "source": [
    "dict_results = {}\n",
    "\n",
    "for subj_id, subj_dataset in windows_dataset.split('subject').items():\n",
    "\n",
    "    dict_subj_results = {}\n",
    "    \n",
    "    cur_splitted = subj_dataset.split('session')\n",
    "    subj_train_set = splitted['0train']\n",
    "    subj_valid_set = splitted['1test']\n",
    "    train_trials_num = len(subj_train_set.get_metadata())\n",
    "\n",
    "    for training_data_amount in np.arange(1, train_trials_num//10)*10:\n",
    "    \n",
    "        cur_model = ShallowFBCSPNet(\n",
    "            n_chans,\n",
    "            n_classes,\n",
    "            input_window_samples=input_window_samples,\n",
    "            final_conv_length='auto',\n",
    "        )\n",
    "        \n",
    "        lr = 0.0625 * 0.01\n",
    "        weight_decay = 0\n",
    "        # batch_size = 64\n",
    "        batch_size = int(min(training_data_amount // 2, 64))\n",
    "        n_epochs = 40\n",
    "    \n",
    "        # Re-initialize EEGClassifier\n",
    "        cur_clf = EEGClassifier(\n",
    "            cur_model,\n",
    "            criterion=torch.nn.NLLLoss,\n",
    "            optimizer=torch.optim.AdamW,\n",
    "            train_split=predefined_split(subj_valid_set),  # using valid_set for validation\n",
    "            optimizer__lr=lr,\n",
    "            optimizer__weight_decay=weight_decay,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[\n",
    "                \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
    "            ],\n",
    "            device=device,\n",
    "            classes=classes,\n",
    "        )\n",
    "    \n",
    "        # Get current training set\n",
    "        cur_train_set = get_subset(subj_train_set, training_data_amount)\n",
    "    \n",
    "        # Fit model\n",
    "        print(f'Training model for subject {subj_id} with {training_data_amount} trials')\n",
    "        _ = cur_clf.fit(cur_train_set, y=None, epochs=n_epochs)\n",
    "    \n",
    "        # results_columns = ['train_loss', 'valid_loss', 'train_accuracy', 'valid_accuracy']\n",
    "        results_columns = ['valid_accuracy',]\n",
    "        df = pd.DataFrame(cur_clf.history[:, results_columns], columns=results_columns,\n",
    "                          index=cur_clf.history[:, 'epoch'])\n",
    "        \n",
    "        # get percent of misclass for better visual comparison to loss\n",
    "        # df = df.assign(train_misclass=100 - 100 * df.train_accuracy,\n",
    "        #                valid_misclass=100 - 100 * df.valid_accuracy)\n",
    "    \n",
    "        cur_final_acc = np.mean(df.tail(5).valid_accuracy)\n",
    "        dict_subj_results.update({training_data_amount: cur_final_acc})\n",
    "\n",
    "    dict_results.update({subj_id: dict_subj_results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'results'))\n",
    "file_name = 'ShallowFBCSPNet_BNCI2014_001_from_scratch_1'\n",
    "with open(f'{results_dir}\\\\{file_name}.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2500\u001b[0m        \u001b[32m1.5433\u001b[0m       \u001b[35m0.2500\u001b[0m            \u001b[31m0.2500\u001b[0m        \u001b[94m4.7090\u001b[0m  0.0006  0.7202\n",
      "      2            0.2500        \u001b[32m1.3147\u001b[0m       0.2500            0.2500        \u001b[94m3.0130\u001b[0m  0.0006  0.7215\n",
      "      3            0.2500        \u001b[32m1.1779\u001b[0m       \u001b[35m0.2535\u001b[0m            \u001b[31m0.2535\u001b[0m        \u001b[94m2.8156\u001b[0m  0.0006  0.7002\n",
      "      4            0.2500        \u001b[32m1.0561\u001b[0m       0.2535            0.2535        \u001b[94m2.6800\u001b[0m  0.0006  0.7298\n",
      "      5            \u001b[36m0.3090\u001b[0m        \u001b[32m1.0431\u001b[0m       \u001b[35m0.2639\u001b[0m            \u001b[31m0.2639\u001b[0m        \u001b[94m1.9925\u001b[0m  0.0006  0.6048\n",
      "      6            \u001b[36m0.5069\u001b[0m        \u001b[32m0.9747\u001b[0m       \u001b[35m0.3125\u001b[0m            \u001b[31m0.3125\u001b[0m        \u001b[94m1.6356\u001b[0m  0.0006  0.6118\n",
      "      7            \u001b[36m0.6354\u001b[0m        \u001b[32m0.8840\u001b[0m       \u001b[35m0.3924\u001b[0m            \u001b[31m0.3924\u001b[0m        \u001b[94m1.4755\u001b[0m  0.0006  0.5872\n",
      "      8            \u001b[36m0.6910\u001b[0m        \u001b[32m0.7712\u001b[0m       0.3924            0.3924        \u001b[94m1.3855\u001b[0m  0.0006  0.6417\n",
      "      9            \u001b[36m0.7569\u001b[0m        \u001b[32m0.7233\u001b[0m       \u001b[35m0.4028\u001b[0m            \u001b[31m0.4028\u001b[0m        \u001b[94m1.3199\u001b[0m  0.0006  0.6406\n",
      "     10            \u001b[36m0.8368\u001b[0m        \u001b[32m0.7139\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.2431\u001b[0m  0.0005  0.6093\n",
      "     11            \u001b[36m0.8715\u001b[0m        \u001b[32m0.7031\u001b[0m       0.4479            0.4479        \u001b[94m1.2128\u001b[0m  0.0005  0.6687\n",
      "     12            \u001b[36m0.8958\u001b[0m        \u001b[32m0.6117\u001b[0m       0.4479            0.4479        \u001b[94m1.2056\u001b[0m  0.0005  0.6343\n",
      "     13            \u001b[36m0.8993\u001b[0m        \u001b[32m0.5827\u001b[0m       \u001b[35m0.4514\u001b[0m            \u001b[31m0.4514\u001b[0m        \u001b[94m1.2039\u001b[0m  0.0005  0.6364\n",
      "     14            \u001b[36m0.9167\u001b[0m        \u001b[32m0.5581\u001b[0m       0.4514            0.4514        \u001b[94m1.2020\u001b[0m  0.0005  0.6240\n",
      "     15            \u001b[36m0.9201\u001b[0m        \u001b[32m0.5547\u001b[0m       \u001b[35m0.4653\u001b[0m            \u001b[31m0.4653\u001b[0m        \u001b[94m1.1870\u001b[0m  0.0004  0.6225\n",
      "     16            \u001b[36m0.9444\u001b[0m        \u001b[32m0.5183\u001b[0m       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        \u001b[94m1.1815\u001b[0m  0.0004  0.6237\n",
      "     17            0.9444        \u001b[32m0.5067\u001b[0m       0.4722            0.4722        \u001b[94m1.1761\u001b[0m  0.0004  0.5976\n",
      "     18            0.9410        \u001b[32m0.4607\u001b[0m       \u001b[35m0.4826\u001b[0m            \u001b[31m0.4826\u001b[0m        \u001b[94m1.1693\u001b[0m  0.0004  0.6206\n",
      "     19            \u001b[36m0.9583\u001b[0m        0.4643       0.4722            0.4722        \u001b[94m1.1617\u001b[0m  0.0004  0.6249\n",
      "     20            0.9549        \u001b[32m0.4475\u001b[0m       \u001b[35m0.5243\u001b[0m            \u001b[31m0.5243\u001b[0m        \u001b[94m1.1489\u001b[0m  0.0003  0.7171\n",
      "     21            \u001b[36m0.9618\u001b[0m        \u001b[32m0.4243\u001b[0m       \u001b[35m0.5278\u001b[0m            \u001b[31m0.5278\u001b[0m        \u001b[94m1.1455\u001b[0m  0.0003  0.7476\n",
      "     22            \u001b[36m0.9792\u001b[0m        0.4441       \u001b[35m0.5556\u001b[0m            \u001b[31m0.5556\u001b[0m        \u001b[94m1.1230\u001b[0m  0.0003  0.6996\n",
      "     23            \u001b[36m0.9826\u001b[0m        0.4251       \u001b[35m0.5590\u001b[0m            \u001b[31m0.5590\u001b[0m        1.1248  0.0002  0.7715\n",
      "     24            0.9826        \u001b[32m0.3731\u001b[0m       0.5451            0.5451        1.1239  0.0002  0.7191\n",
      "     25            \u001b[36m0.9861\u001b[0m        \u001b[32m0.3359\u001b[0m       0.5382            0.5382        \u001b[94m1.1218\u001b[0m  0.0002  0.6589\n",
      "     26            0.9861        0.3818       0.5521            0.5521        1.1232  0.0002  0.6379\n",
      "     27            0.9861        0.3508       0.5486            0.5486        1.1230  0.0002  0.6251\n",
      "     28            0.9826        \u001b[32m0.2941\u001b[0m       0.5486            0.5486        1.1248  0.0001  0.6686\n",
      "     29            0.9861        0.3192       0.5417            0.5417        \u001b[94m1.1207\u001b[0m  0.0001  0.6053\n",
      "     30            \u001b[36m0.9931\u001b[0m        0.3345       0.5521            0.5521        \u001b[94m1.1181\u001b[0m  0.0001  0.5995\n",
      "     31            \u001b[36m0.9965\u001b[0m        0.3493       0.5486            0.5486        \u001b[94m1.1162\u001b[0m  0.0001  0.6725\n",
      "     32            0.9965        0.2967       0.5417            0.5417        \u001b[94m1.1149\u001b[0m  0.0001  0.6360\n",
      "     33            0.9965        0.2944       0.5451            0.5451        \u001b[94m1.1128\u001b[0m  0.0000  0.6352\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2894\u001b[0m       0.5556            0.5556        \u001b[94m1.1112\u001b[0m  0.0000  0.6119\n",
      "     35            1.0000        0.3417       0.5521            0.5521        \u001b[94m1.1102\u001b[0m  0.0000  0.6003\n",
      "     36            1.0000        0.3008       0.5486            0.5486        \u001b[94m1.1099\u001b[0m  0.0000  0.6080\n",
      "     37            1.0000        0.3216       0.5486            0.5486        \u001b[94m1.1098\u001b[0m  0.0000  0.6411\n",
      "     38            1.0000        \u001b[32m0.2783\u001b[0m       0.5486            0.5486        \u001b[94m1.1097\u001b[0m  0.0000  0.6327\n",
      "     39            1.0000        0.2899       0.5451            0.5451        1.1097  0.0000  0.7499\n",
      "     40            1.0000        0.3005       0.5451            0.5451        1.1098  0.0000  0.7285\n"
     ]
    }
   ],
   "source": [
    "model = ShallowFBCSPNet(\n",
    "    n_chans,\n",
    "    n_classes,\n",
    "    input_window_samples=input_window_samples,\n",
    "    final_conv_length='auto',\n",
    ")\n",
    "\n",
    "# Send model to GPU\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "lr = 0.0625 * 0.01\n",
    "weight_decay = 0\n",
    "batch_size = 64\n",
    "n_epochs = 40\n",
    "\n",
    "clf = EEGClassifier(\n",
    "    model,\n",
    "    criterion=torch.nn.NLLLoss,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    train_split=predefined_split(valid_set),  # using valid_set for validation\n",
    "    optimizer__lr=lr,\n",
    "    optimizer__weight_decay=weight_decay,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[\n",
    "        \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
    "    ],\n",
    "    device=device,\n",
    "    classes=classes,\n",
    ")\n",
    "\n",
    "_ = clf.fit(train_set, y=None, epochs=n_epochs)\n",
    "\n",
    "results_columns = ['train_loss', 'valid_loss', 'train_accuracy', 'valid_accuracy']\n",
    "df = pd.DataFrame(clf.history[:, results_columns], columns=results_columns,\n",
    "                  index=clf.history[:, 'epoch'])\n",
    "\n",
    "# get percent of misclass for better visual comparison to loss\n",
    "df = df.assign(train_misclass=100 - 100 * df.train_accuracy,\n",
    "               valid_misclass=100 - 100 * df.valid_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(dict_results)\n",
    "subject_averaged_df = df_results.mean(axis=1)\n",
    "# Calculate the standard error of the mean\n",
    "std_err_df = df_results.sem(axis=1)\n",
    "# Calculate the confidence interval (95% confidence level)\n",
    "conf_interval_df = stats.t.interval(0.95, len(df_results.columns) - 1, loc=subject_averaged_df, scale=std_err_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns\n",
    "\n",
    "for subj_id, subj_res in dict_results.items():\n",
    "    ax1.plot(subj_res.keys(), subj_res.values(), label=f'Subject {subj_id}')\n",
    "\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('Amount of training data (trials, 4 secs each)')\n",
    "ax1.set_ylabel('Validation Accuracy')\n",
    "\n",
    "ax2.plot(subject_averaged_df, label='Subject averaged')\n",
    "ax2.fill_between(subject_averaged_df.index, conf_interval_df[0], conf_interval_df[1], color='b', alpha=0.3, label='95% CI')\n",
    "ax2.legend()\n",
    "ax2.set_xlabel('Amount of training data (trials, 4 secs each)')\n",
    "\n",
    "plt.suptitle('ShallowFBCSPNet on BNCI2014_001 Dataset \\n Train model from scratch for each subject')\n",
    "\n",
    "# plt.savefig(f'{results_dir}//{file_name}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
