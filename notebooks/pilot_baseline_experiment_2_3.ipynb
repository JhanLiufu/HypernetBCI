{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Baseline Experiment 1\n",
    "\n",
    "Train model from scratch for each subject. \n",
    "\n",
    "Model: BSFShallowNet\n",
    "\n",
    "Dataset: BCI Competitin IV 2a, BCNI2014001 via MOABB library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.pick_types is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.pick_channels_regexp is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.channel_type is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\moabb\\pipelines\\__init__.py:26: ModuleNotFoundError: Tensorflow is not installed. You won't be able to use these MOABB pipelines if you attempt to do so.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from braindecode.datasets import MOABBDataset\n",
    "from numpy import multiply\n",
    "from braindecode.preprocessing import (Preprocessor,\n",
    "                                       exponential_moving_standardize,\n",
    "                                       preprocess)\n",
    "from braindecode.preprocessing import create_windows_from_events\n",
    "import torch\n",
    "from braindecode.models import ShallowFBCSPNet\n",
    "from braindecode.util import set_random_seeds\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "from braindecode import EEGClassifier\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import os\n",
    "import pickle\n",
    "from matplotlib.lines import Line2D\n",
    "# from braindecode.visualization import plot_confusion_matrix\n",
    "\n",
    "from braindecode.datasets import BaseConcatDataset\n",
    "from braindecode.datasets.base import EEGWindowsDataset\n",
    "from braindecode.preprocessing.windowers import _create_windows_from_events\n",
    "import numpy as np\n",
    "import mne\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing the data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = MOABBDataset(dataset_name=\"BNCI2014_001\", subject_ids=list(range(1, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\preprocessing\\preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
      "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<braindecode.datasets.moabb.MOABBDataset at 0x147acabdc90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_cut_hz = 4.  # low cut frequency for filtering\n",
    "high_cut_hz = 38.  # high cut frequency for filtering\n",
    "# Parameters for exponential moving standardization\n",
    "factor_new = 1e-3\n",
    "init_block_size = 1000\n",
    "# Factor to convert from V to uV\n",
    "factor = 1e6\n",
    "\n",
    "preprocessors = [\n",
    "    Preprocessor('pick_types', eeg=True, meg=False, stim=False),  # Keep EEG sensors\n",
    "    Preprocessor(lambda data: multiply(data, factor)),  # Convert from V to uV\n",
    "    Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz),  # Bandpass filter\n",
    "    Preprocessor(exponential_moving_standardize,  # Exponential moving standardization\n",
    "                 factor_new=factor_new, init_block_size=init_block_size)\n",
    "]\n",
    "\n",
    "# Transform the data\n",
    "preprocess(dataset, preprocessors, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Compute Windows\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n"
     ]
    }
   ],
   "source": [
    "trial_start_offset_seconds = -0.5\n",
    "# Extract sampling frequency, check that they are same in all datasets\n",
    "sfreq = dataset.datasets[0].raw.info['sfreq']\n",
    "assert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])\n",
    "# Calculate the trial start offset in samples.\n",
    "trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
    "\n",
    "# Create windows using braindecode function for this. It needs parameters to define how\n",
    "# trials should be used.\n",
    "windows_dataset = create_windows_from_events(\n",
    "    dataset,\n",
    "    trial_start_offset_samples=trial_start_offset_samples,\n",
    "    trial_stop_offset_samples=0,\n",
    "    preload=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_non_repeating_integers(x, y):\n",
    "    # Check if y is greater than x\n",
    "    if y < x:\n",
    "        raise ValueError(\"y must be greater than or equal to x\")\n",
    "    \n",
    "    # Generate x non-repeating integers between 0 and y\n",
    "    return random.sample(range(y), x)\n",
    "\n",
    "def sample_integers_sum_to_x(x, k):\n",
    "    '''\n",
    "    k >= 2\n",
    "    '''\n",
    "    # Generate k-1 random integers between 1 and x\n",
    "    parts = sorted(random.randint(1, x) for _ in range(k-1))\n",
    "    \n",
    "    # Calculate the differences between consecutive numbers\n",
    "    differences = [parts[0]] + [parts[i] - parts[i-1] for i in range(1, k-1)] + [x - parts[-1]]\n",
    "    \n",
    "    return differences\n",
    "\n",
    "def get_subset(input_set, target_trial_num, random_sample=False):\n",
    "    # check inputs\n",
    "    assert isinstance(input_set, BaseConcatDataset)\n",
    "    assert isinstance(target_trial_num, int)\n",
    "    \n",
    "    new_ds_lst = []\n",
    "\n",
    "    if random_sample:\n",
    "        \n",
    "        trial_cnt_from_each_base_ds = sample_integers_sum_to_x(target_trial_num, len(input_set.datasets))\n",
    "        for i, cnt in enumerate(trial_cnt_from_each_base_ds):\n",
    "            if not cnt:\n",
    "                # no sampling in current base dataset\n",
    "                continue\n",
    "        \n",
    "            # Access current base dataset\n",
    "            cur_ds = input_set.datasets[i]\n",
    "            assert isinstance(cur_ds, EEGWindowsDataset)\n",
    "            # Randomly sample trial index\n",
    "            try:\n",
    "                trial_idx = generate_non_repeating_integers(cnt, len(cur_ds))\n",
    "                new_ds_lst.append(EEGWindowsDataset(cur_ds.raw, cur_ds.metadata.iloc[trial_idx], \n",
    "                                                    description=cur_ds.description))\n",
    "            except ValueError:\n",
    "                # If trying to sample more trials in current ds than there are\n",
    "                # Get entire cur_ds, and get what's missing fromt the next ds\n",
    "                new_ds_lst.append(cur_ds)\n",
    "                trial_cnt_from_each_base_ds[i+1] += (cnt - len(cur_ds))\n",
    "\n",
    "    else:\n",
    "    \n",
    "        for ds in input_set.datasets:\n",
    "            assert isinstance(ds, EEGWindowsDataset)\n",
    "            cur_run_trial_num = len(ds.metadata)\n",
    "            if target_trial_num > cur_run_trial_num:\n",
    "                new_ds_lst.append(ds)\n",
    "                target_trial_num -= cur_run_trial_num\n",
    "            else:\n",
    "                new_ds_lst.append(EEGWindowsDataset(ds.raw, ds.metadata[:target_trial_num], description=ds.description))\n",
    "                break\n",
    "\n",
    "    return BaseConcatDataset(new_ds_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune for holdout subject set; Pre-train with data from all other subjects\n",
    "fine tune train set size up to 3 mins (= 180 sec = 45 trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold out data from subject 1\n",
      "Pre-training model with data from all subjects but subject 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr      dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  -------\n",
      "      1            \u001b[36m0.3927\u001b[0m        \u001b[32m1.6082\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m1.3990\u001b[0m  0.0007  15.5884\n",
      "      2            \u001b[36m0.4753\u001b[0m        \u001b[32m1.4408\u001b[0m       \u001b[35m0.3789\u001b[0m            \u001b[31m0.3789\u001b[0m        \u001b[94m1.3238\u001b[0m  0.0007  16.3731\n",
      "      3            \u001b[36m0.5302\u001b[0m        \u001b[32m1.3646\u001b[0m       \u001b[35m0.3906\u001b[0m            \u001b[31m0.3906\u001b[0m        \u001b[94m1.2766\u001b[0m  0.0007  15.5282\n",
      "      4            \u001b[36m0.5469\u001b[0m        \u001b[32m1.2703\u001b[0m       \u001b[35m0.4401\u001b[0m            \u001b[31m0.4401\u001b[0m        \u001b[94m1.2645\u001b[0m  0.0007  16.9943\n",
      "      5            \u001b[36m0.5771\u001b[0m        \u001b[32m1.2070\u001b[0m       0.4349            0.4349        \u001b[94m1.2614\u001b[0m  0.0007  16.7029\n",
      "      6            \u001b[36m0.6047\u001b[0m        \u001b[32m1.1570\u001b[0m       \u001b[35m0.4492\u001b[0m            \u001b[31m0.4492\u001b[0m        \u001b[94m1.2434\u001b[0m  0.0007  16.8423\n",
      "      7            0.5901        \u001b[32m1.1199\u001b[0m       \u001b[35m0.4583\u001b[0m            \u001b[31m0.4583\u001b[0m        1.2616  0.0007  16.3374\n",
      "      8            \u001b[36m0.6320\u001b[0m        \u001b[32m1.0753\u001b[0m       0.4583            0.4583        \u001b[94m1.2191\u001b[0m  0.0007  16.7508\n",
      "      9            0.6286        \u001b[32m1.0712\u001b[0m       \u001b[35m0.4661\u001b[0m            \u001b[31m0.4661\u001b[0m        1.2257  0.0007  16.4081\n",
      "     10            \u001b[36m0.6682\u001b[0m        \u001b[32m1.0388\u001b[0m       \u001b[35m0.4974\u001b[0m            \u001b[31m0.4974\u001b[0m        \u001b[94m1.1930\u001b[0m  0.0006  17.4073\n",
      "     11            0.6427        \u001b[32m0.9794\u001b[0m       0.4779            0.4779        1.2315  0.0006  18.6412\n",
      "     12            \u001b[36m0.6727\u001b[0m        0.9826       0.4466            0.4466        1.2197  0.0006  17.7300\n",
      "     13            0.6698        \u001b[32m0.9559\u001b[0m       0.4974            0.4974        1.2127  0.0006  16.9940\n",
      "     14            \u001b[36m0.7042\u001b[0m        \u001b[32m0.9445\u001b[0m       0.4961            0.4961        1.2087  0.0006  18.3728\n",
      "     15            0.6714        \u001b[32m0.9358\u001b[0m       0.4818            0.4818        1.2228  0.0006  16.8546\n",
      "     16            \u001b[36m0.7089\u001b[0m        \u001b[32m0.9114\u001b[0m       0.4922            0.4922        1.2085  0.0006  17.0535\n",
      "     17            \u001b[36m0.7271\u001b[0m        \u001b[32m0.8699\u001b[0m       \u001b[35m0.5000\u001b[0m            \u001b[31m0.5000\u001b[0m        \u001b[94m1.1915\u001b[0m  0.0005  15.9609\n",
      "     18            0.7034        0.8742       0.4844            0.4844        1.2580  0.0005  13.3435\n",
      "     19            \u001b[36m0.7396\u001b[0m        \u001b[32m0.8616\u001b[0m       \u001b[35m0.5013\u001b[0m            \u001b[31m0.5013\u001b[0m        1.2023  0.0005  20.7346\n",
      "     20            0.7279        \u001b[32m0.8458\u001b[0m       \u001b[35m0.5052\u001b[0m            \u001b[31m0.5052\u001b[0m        \u001b[94m1.1845\u001b[0m  0.0005  16.0226\n",
      "     21            \u001b[36m0.7526\u001b[0m        \u001b[32m0.8347\u001b[0m       \u001b[35m0.5169\u001b[0m            \u001b[31m0.5169\u001b[0m        \u001b[94m1.1613\u001b[0m  0.0004  17.2108\n",
      "     22            0.7385        \u001b[32m0.8027\u001b[0m       0.4961            0.4961        1.2043  0.0004  16.4547\n",
      "     23            \u001b[36m0.7648\u001b[0m        \u001b[32m0.7932\u001b[0m       0.5169            0.5169        1.1807  0.0004  16.4944\n",
      "     24            \u001b[36m0.7911\u001b[0m        \u001b[32m0.7879\u001b[0m       0.5130            0.5130        1.1867  0.0004  15.8004\n",
      "     25            0.7667        \u001b[32m0.7548\u001b[0m       0.5039            0.5039        1.2097  0.0004  17.4089\n",
      "     26            0.7852        \u001b[32m0.7395\u001b[0m       \u001b[35m0.5247\u001b[0m            \u001b[31m0.5247\u001b[0m        1.1873  0.0003  16.5237\n",
      "     27            \u001b[36m0.7958\u001b[0m        \u001b[32m0.7370\u001b[0m       \u001b[35m0.5404\u001b[0m            \u001b[31m0.5404\u001b[0m        \u001b[94m1.1596\u001b[0m  0.0003  17.2550\n",
      "     28            \u001b[36m0.8143\u001b[0m        \u001b[32m0.7307\u001b[0m       0.5208            0.5208        \u001b[94m1.1455\u001b[0m  0.0003  14.8640\n",
      "     29            0.8102        \u001b[32m0.7202\u001b[0m       0.5195            0.5195        1.1493  0.0003  17.2574\n",
      "     30            \u001b[36m0.8247\u001b[0m        \u001b[32m0.7003\u001b[0m       0.5339            0.5339        1.1476  0.0003  17.1068\n",
      "     31            0.8036        \u001b[32m0.6988\u001b[0m       0.5195            0.5195        1.1652  0.0002  16.3798\n",
      "     32            \u001b[36m0.8281\u001b[0m        \u001b[32m0.6958\u001b[0m       0.5339            0.5339        \u001b[94m1.1413\u001b[0m  0.0002  17.1199\n",
      "     33            \u001b[36m0.8385\u001b[0m        \u001b[32m0.6791\u001b[0m       0.5195            0.5195        \u001b[94m1.1384\u001b[0m  0.0002  16.7250\n",
      "     34            \u001b[36m0.8427\u001b[0m        \u001b[32m0.6737\u001b[0m       \u001b[35m0.5430\u001b[0m            \u001b[31m0.5430\u001b[0m        1.1485  0.0002  16.1285\n",
      "     35            0.8409        \u001b[32m0.6521\u001b[0m       0.5260            0.5260        1.1470  0.0001  16.4181\n",
      "     36            \u001b[36m0.8466\u001b[0m        \u001b[32m0.6509\u001b[0m       0.5260            0.5260        1.1389  0.0001  16.5290\n",
      "     37            0.8344        0.6543       0.5326            0.5326        1.1390  0.0001  16.3244\n",
      "     38            \u001b[36m0.8526\u001b[0m        \u001b[32m0.6392\u001b[0m       0.5378            0.5378        \u001b[94m1.1332\u001b[0m  0.0001  17.2600\n",
      "     39            \u001b[36m0.8581\u001b[0m        \u001b[32m0.6354\u001b[0m       0.5391            0.5391        1.1337  0.0001  16.7494\n",
      "     40            \u001b[36m0.8630\u001b[0m        \u001b[32m0.6251\u001b[0m       0.5417            0.5417        \u001b[94m1.1309\u001b[0m  0.0001  18.4633\n",
      "     41            0.8628        \u001b[32m0.6168\u001b[0m       0.5299            0.5299        \u001b[94m1.1303\u001b[0m  0.0001  15.6811\n",
      "     42            0.8617        \u001b[32m0.6104\u001b[0m       0.5404            0.5404        \u001b[94m1.1293\u001b[0m  0.0000  16.1090\n",
      "     43            \u001b[36m0.8664\u001b[0m        0.6230       0.5430            0.5430        \u001b[94m1.1279\u001b[0m  0.0000  15.9698\n",
      "     44            0.8630        0.6104       \u001b[35m0.5456\u001b[0m            \u001b[31m0.5456\u001b[0m        \u001b[94m1.1271\u001b[0m  0.0000  17.3214\n",
      "     45            0.8659        \u001b[32m0.5994\u001b[0m       0.5417            0.5417        \u001b[94m1.1259\u001b[0m  0.0000  16.0468\n",
      "     46            \u001b[36m0.8688\u001b[0m        \u001b[32m0.5982\u001b[0m       0.5443            0.5443        1.1267  0.0000  17.0946\n"
     ]
    }
   ],
   "source": [
    "results_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'results'))\n",
    "exp_name = f'baseline_2_3_pretrain'\n",
    "\n",
    "### ---------- Pre-training parameters ----------\n",
    "lr = 0.07 * 0.01\n",
    "weight_decay = 0\n",
    "batch_size = 64\n",
    "n_epochs = 30\n",
    "\n",
    "splitted_by_subj = windows_dataset.split('subject')\n",
    "\n",
    "data_amount_step = 5 # trials\n",
    "finetune_trials_num = 45 # trials\n",
    "repetition = 20\n",
    "results_columns = ['valid_accuracy',]\n",
    "dict_results = {}\n",
    "\n",
    "for holdout_subj_id in range(1, 10):\n",
    "    \n",
    "    print(f'Hold out data from subject {holdout_subj_id}')\n",
    "    \n",
    "    ### ---------- Split dataset into pre-train set and fine-tune (holdout) set ----------\n",
    "    pre_train_set = BaseConcatDataset([splitted_by_subj.get(f'{i}') for i in range(1, 10) if i != holdout_subj_id])\n",
    "    fine_tune_set = BaseConcatDataset([splitted_by_subj.get(f'{holdout_subj_id}'),])\n",
    "\n",
    "    ### ---------- Split pre-train set into pre-train-train set and pre-train-test set ----------\n",
    "    pre_train_train_set_lst = []\n",
    "    pre_train_test_set_lst = []\n",
    "    pre_train_test_set_size = 1 # runs\n",
    "    for key, val in pre_train_set.split('subject').items():\n",
    "        subj_splitted_lst_by_run = list(val.split('run').values())\n",
    "        pre_train_train_set_lst.extend(subj_splitted_lst_by_run[:-pre_train_test_set_size])\n",
    "        pre_train_test_set_lst.extend(subj_splitted_lst_by_run[-pre_train_test_set_size:])\n",
    "    \n",
    "    pre_train_train_set = BaseConcatDataset(pre_train_train_set_lst)\n",
    "    pre_train_test_set = BaseConcatDataset(pre_train_test_set_lst)\n",
    "\n",
    "    ### ---------- Pre-training ----------\n",
    "    cuda = torch.cuda.is_available() \n",
    "    device = 'cuda' if cuda else 'cpu'\n",
    "    if cuda:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    seed = 20200220\n",
    "    set_random_seeds(seed=seed, cuda=cuda)\n",
    "    \n",
    "    n_classes = 4\n",
    "    classes = list(range(n_classes))\n",
    "    # Extract number of chans and time steps from dataset\n",
    "    n_chans = windows_dataset[0][0].shape[0]\n",
    "    input_window_samples = windows_dataset[0][0].shape[1]\n",
    "    \n",
    "    cur_model = ShallowFBCSPNet(\n",
    "        n_chans,\n",
    "        n_classes,\n",
    "        input_window_samples=input_window_samples,\n",
    "        final_conv_length='auto',\n",
    "    )\n",
    "    \n",
    "    cur_clf = EEGClassifier(\n",
    "        cur_model,\n",
    "        criterion=torch.nn.NLLLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        train_split=predefined_split(pre_train_test_set),  # using valid_set for validation\n",
    "        optimizer__lr=lr,\n",
    "        optimizer__weight_decay=weight_decay,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[\n",
    "            \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
    "        ],\n",
    "        device=device,\n",
    "        classes=classes,\n",
    "        warm_start=False\n",
    "    )\n",
    "\n",
    "    print(f'Pre-training model with data from all subjects but subject {holdout_subj_id}')\n",
    "    _ = cur_clf.fit(pre_train_train_set, y=None, epochs=n_epochs)\n",
    "\n",
    "    cur_clf.save_params(f_params=os.path.join(results_dir, f'{exp_name}_without_subj_{holdout_subj_id}_model.pkl'), \n",
    "                        f_optimizer=os.path.join(results_dir, f'{exp_name}_without_subj_{holdout_subj_id}_opt.pkl'), \n",
    "                        f_history=os.path.join(results_dir, f'{exp_name}_without_subj_{holdout_subj_id}_history.json'))\n",
    "\n",
    "    ### ---------- Split fine tune set into fine tune-train set and fine tune-valid set ----------\n",
    "    finetune_splitted_lst_by_run = list(fine_tune_set.split('run').values())\n",
    "    finetune_subj_train_set = BaseConcatDataset(finetune_splitted_lst_by_run[:-1])\n",
    "    finetune_subj_valid_set = BaseConcatDataset(finetune_splitted_lst_by_run[-1:])\n",
    "    \n",
    "    ### Baseline accuracy on the finetune_valid set\n",
    "    finetune_valid_predicted = cur_clf.predict(finetune_subj_valid_set)\n",
    "    finetune_valid_true = np.array(finetune_subj_valid_set.get_metadata().target)\n",
    "    finetune_baseline_correct = np.equal(finetune_valid_predicted, finetune_valid_true)\n",
    "    finetune_baseline_acc = np.sum(finetune_baseline_correct) / len(finetune_baseline_correct)\n",
    "    print(f'Before finetuning for subject {holdout_subj_id}, the baseline accuracy is {finetune_baseline_acc}')\n",
    "\n",
    "    ### ---------- Fine tuning ----------\n",
    "    dict_subj_results = {0: [finetune_baseline_acc,]}\n",
    "\n",
    "    ### Finetune with different amount of new data\n",
    "    for finetune_training_data_amount in np.arange(1, (finetune_trials_num // data_amount_step) + 1) * data_amount_step:\n",
    "\n",
    "        final_accuracy = []\n",
    "        \n",
    "        ### Since we're sampling randomly, repeat for 'repetition' times\n",
    "        for i in range(repetition):\n",
    "\n",
    "            ## Get current finetune samples\n",
    "            cur_finetune_subj_train_subset = get_subset(finetune_subj_train_set, int(finetune_training_data_amount), random_sample=True)\n",
    "    \n",
    "            finetune_model = ShallowFBCSPNet(\n",
    "                n_chans,\n",
    "                n_classes,\n",
    "                input_window_samples=input_window_samples,\n",
    "                final_conv_length='auto',\n",
    "            )\n",
    "    \n",
    "            ### ---------- Fine tune parameters ----------\n",
    "            finetune_lr = 0.07 * 0.01\n",
    "            finetune_weight_decay = 0\n",
    "            finetune_batch_size = int(min(finetune_training_data_amount, 64))\n",
    "            finetune_n_epochs = 20\n",
    "            \n",
    "            new_clf = EEGClassifier(\n",
    "                finetune_model,\n",
    "                criterion=torch.nn.NLLLoss,\n",
    "                optimizer=torch.optim.AdamW,\n",
    "                train_split=predefined_split(finetune_subj_valid_set), \n",
    "                optimizer__lr=finetune_lr,\n",
    "                optimizer__weight_decay=finetune_weight_decay,\n",
    "                batch_size=finetune_batch_size,\n",
    "                callbacks=[\n",
    "                    \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=finetune_n_epochs - 1)),\n",
    "                ],\n",
    "                device=device,\n",
    "                classes=classes,\n",
    "            )\n",
    "            new_clf.initialize()\n",
    "            \n",
    "            ## Load pretrained model\n",
    "            new_clf.load_params(f_params=os.path.join(results_dir, f'{exp_name}_without_subj_{holdout_subj_id}_model.pkl'), \n",
    "                                f_optimizer=os.path.join(results_dir, f'{exp_name}_without_subj_{holdout_subj_id}_opt.pkl'), \n",
    "                                f_history=os.path.join(results_dir, f'{exp_name}_without_subj_{holdout_subj_id}_history.json'))\n",
    "    \n",
    "            ## Continue training / finetuning\n",
    "            print(f'Fine tuning model for subject {holdout_subj_id} with {finetune_training_data_amount} trials')\n",
    "            _ = new_clf.partial_fit(cur_finetune_subj_train_subset, y=None, epochs=finetune_n_epochs)\n",
    "    \n",
    "            ## Get results after fine tuning\n",
    "            df = pd.DataFrame(new_clf.history[:, results_columns], columns=results_columns,\n",
    "                              # index=new_clf.history[:, 'epoch'],\n",
    "                             )\n",
    "    \n",
    "            cur_final_acc = np.mean(df.tail(5).valid_accuracy)\n",
    "            final_accuracy.append(cur_final_acc)\n",
    "        \n",
    "        dict_subj_results.update({finetune_training_data_amount: final_accuracy})\n",
    "\n",
    "    dict_results.update({holdout_subj_id: dict_subj_results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'ShallowFBCSPNet_BNCI2014_001_finetuning_3'\n",
    "file_path = os.path.join(results_dir, f'{file_name}.pkl')\n",
    "\n",
    "with open(f'{results_dir}\\\\{file_name}.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
    "#     with open(file_path, 'rb') as f:\n",
    "#         baseline_2_1 = pickle.load(f)\n",
    "#     print(\"Dictionary loaded successfully.\")\n",
    "# else:\n",
    "#     print(f\"Error: File '{file_path}' does not exist or is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_results = pd.DataFrame(dict_results)\n",
    "# display(df_results)\n",
    "\n",
    "# subject_averaged_df = df_results.mean(axis=1)\n",
    "# # Calculate the standard error of the mean\n",
    "# std_err_df = df_results.sem(axis=1)\n",
    "# # Calculate the confidence interval (95% confidence level)\n",
    "# conf_interval_df = stats.t.interval(0.95, len(df_results.columns) - 1, loc=subject_averaged_df, scale=std_err_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns\n",
    "\n",
    "# for subj_id, subj_res in dict_results.items():\n",
    "#     ax1.plot(subj_res.keys(), subj_res.values(), label=f'Subject {subj_id}')\n",
    "\n",
    "# ax1.legend()\n",
    "# ax1.set_xlabel('Amount of fine tuning data (trials, 4 secs each)')\n",
    "# ax1.set_ylabel('Validation Accuracy')\n",
    "\n",
    "# ax2.plot(subject_averaged_df, label='Subject averaged')\n",
    "# ax2.fill_between(subject_averaged_df.index, conf_interval_df[0], conf_interval_df[1], color='b', alpha=0.3, label='95% CI')\n",
    "# ax2.legend()\n",
    "# ax2.set_xlabel('Amount of fine tuning data (trials, 4 secs each)')\n",
    "\n",
    "# plt.suptitle('ShallowFBCSPNet on BNCI2014_001 Dataset \\n Fine-tuning (using each subject as holdout)')\n",
    "\n",
    "# plt.savefig(os.path.join(results_dir, f'{file_name}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
