{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Baseline Experiment 1\n",
    "\n",
    "Train model from scratch for each subject. \n",
    "\n",
    "Model: BSFShallowNet\n",
    "\n",
    "Dataset: BCI Competitin IV 2a, BCNI2014001 via MOABB library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.pick_types is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.pick_channels_regexp is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.channel_type is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\moabb\\pipelines\\__init__.py:26: ModuleNotFoundError: Tensorflow is not installed. You won't be able to use these MOABB pipelines if you attempt to do so.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from braindecode.datasets import MOABBDataset\n",
    "from numpy import multiply\n",
    "from braindecode.preprocessing import (Preprocessor,\n",
    "                                       exponential_moving_standardize,\n",
    "                                       preprocess)\n",
    "from braindecode.preprocessing import create_windows_from_events\n",
    "import torch\n",
    "from braindecode.models import ShallowFBCSPNet\n",
    "from braindecode.util import set_random_seeds\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "from braindecode import EEGClassifier\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import os\n",
    "import pickle\n",
    "from matplotlib.lines import Line2D\n",
    "# from braindecode.visualization import plot_confusion_matrix\n",
    "\n",
    "from braindecode.datasets import BaseConcatDataset\n",
    "from braindecode.datasets.base import EEGWindowsDataset\n",
    "from braindecode.preprocessing.windowers import _create_windows_from_events\n",
    "import numpy as np\n",
    "import mne\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing the data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = MOABBDataset(dataset_name=\"BNCI2014_001\", subject_ids=list(range(1, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\preprocessing\\preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
      "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<braindecode.datasets.moabb.MOABBDataset at 0x198d6a90450>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_cut_hz = 4.  # low cut frequency for filtering\n",
    "high_cut_hz = 38.  # high cut frequency for filtering\n",
    "# Parameters for exponential moving standardization\n",
    "factor_new = 1e-3\n",
    "init_block_size = 1000\n",
    "# Factor to convert from V to uV\n",
    "factor = 1e6\n",
    "\n",
    "preprocessors = [\n",
    "    Preprocessor('pick_types', eeg=True, meg=False, stim=False),  # Keep EEG sensors\n",
    "    Preprocessor(lambda data: multiply(data, factor)),  # Convert from V to uV\n",
    "    Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz),  # Bandpass filter\n",
    "    Preprocessor(exponential_moving_standardize,  # Exponential moving standardization\n",
    "                 factor_new=factor_new, init_block_size=init_block_size)\n",
    "]\n",
    "\n",
    "# Transform the data\n",
    "preprocess(dataset, preprocessors, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Compute Windows\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n"
     ]
    }
   ],
   "source": [
    "trial_start_offset_seconds = -0.5\n",
    "# Extract sampling frequency, check that they are same in all datasets\n",
    "sfreq = dataset.datasets[0].raw.info['sfreq']\n",
    "assert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])\n",
    "# Calculate the trial start offset in samples.\n",
    "trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
    "\n",
    "# Create windows using braindecode function for this. It needs parameters to define how\n",
    "# trials should be used.\n",
    "windows_dataset = create_windows_from_events(\n",
    "    dataset,\n",
    "    trial_start_offset_samples=trial_start_offset_samples,\n",
    "    trial_stop_offset_samples=0,\n",
    "    preload=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_non_repeating_integers(x, y):\n",
    "    # Check if y is greater than x\n",
    "    if y < x:\n",
    "        raise ValueError(\"y must be greater than or equal to x\")\n",
    "    \n",
    "    # Generate x non-repeating integers between 0 and y\n",
    "    return random.sample(range(y), x)\n",
    "\n",
    "def sample_integers_sum_to_x(x, k):\n",
    "    '''\n",
    "    k >= 2\n",
    "    '''\n",
    "    # Generate k-1 random integers between 1 and x\n",
    "    parts = sorted(random.randint(1, x) for _ in range(k-1))\n",
    "    \n",
    "    # Calculate the differences between consecutive numbers\n",
    "    differences = [parts[0]] + [parts[i] - parts[i-1] for i in range(1, k-1)] + [x - parts[-1]]\n",
    "    \n",
    "    return differences\n",
    "\n",
    "def get_subset(input_set, target_trial_num, random_sample=False):\n",
    "    # check inputs\n",
    "    assert isinstance(input_set, BaseConcatDataset)\n",
    "    assert isinstance(target_trial_num, int)\n",
    "    \n",
    "    new_ds_lst = []\n",
    "\n",
    "    if random_sample:\n",
    "        \n",
    "        trial_cnt_from_each_base_ds = sample_integers_sum_to_x(target_trial_num, len(input_set.datasets))\n",
    "        for i, cnt in enumerate(trial_cnt_from_each_base_ds):\n",
    "            if not cnt:\n",
    "                # no sampling in current base dataset\n",
    "                continue\n",
    "        \n",
    "            # Access current base dataset\n",
    "            cur_ds = input_set.datasets[i]\n",
    "            assert isinstance(cur_ds, EEGWindowsDataset)\n",
    "            # Randomly sample trial index\n",
    "            try:\n",
    "                trial_idx = generate_non_repeating_integers(cnt, len(cur_ds))\n",
    "                new_ds_lst.append(EEGWindowsDataset(cur_ds.raw, cur_ds.metadata.iloc[trial_idx], \n",
    "                                                    description=cur_ds.description))\n",
    "            except ValueError:\n",
    "                # If trying to sample more trials in current ds than there are\n",
    "                # Get entire cur_ds, and get what's missing fromt the next ds\n",
    "                new_ds_lst.append(cur_ds)\n",
    "                trial_cnt_from_each_base_ds[i+1] += (cnt - len(cur_ds))\n",
    "\n",
    "    else:\n",
    "    \n",
    "        for ds in input_set.datasets:\n",
    "            assert isinstance(ds, EEGWindowsDataset)\n",
    "            cur_run_trial_num = len(ds.metadata)\n",
    "            if target_trial_num > cur_run_trial_num:\n",
    "                new_ds_lst.append(ds)\n",
    "                target_trial_num -= cur_run_trial_num\n",
    "            else:\n",
    "                new_ds_lst.append(EEGWindowsDataset(ds.raw, ds.metadata[:target_trial_num], description=ds.description))\n",
    "                break\n",
    "\n",
    "    return BaseConcatDataset(new_ds_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune for holdout subject set; Pre-train with data from all other subjects\n",
    "fine tune train set size up to 3 mins (= 180 sec = 45 trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold out data from subject 1\n",
      "Pre-training model with data from all subjects but subject 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3927\u001b[0m        \u001b[32m1.6082\u001b[0m       \u001b[35m0.3229\u001b[0m            \u001b[31m0.3229\u001b[0m        \u001b[94m1.3990\u001b[0m  0.0007  6.5710\n",
      "      2            \u001b[36m0.4750\u001b[0m        \u001b[32m1.4408\u001b[0m       \u001b[35m0.3789\u001b[0m            \u001b[31m0.3789\u001b[0m        \u001b[94m1.3237\u001b[0m  0.0007  6.8124\n",
      "      3            \u001b[36m0.5305\u001b[0m        \u001b[32m1.3643\u001b[0m       \u001b[35m0.3906\u001b[0m            \u001b[31m0.3906\u001b[0m        \u001b[94m1.2767\u001b[0m  0.0007  7.7969\n",
      "      4            \u001b[36m0.5490\u001b[0m        \u001b[32m1.2694\u001b[0m       \u001b[35m0.4427\u001b[0m            \u001b[31m0.4427\u001b[0m        \u001b[94m1.2639\u001b[0m  0.0007  6.8819\n",
      "      5            \u001b[36m0.5792\u001b[0m        \u001b[32m1.2056\u001b[0m       0.4362            0.4362        \u001b[94m1.2602\u001b[0m  0.0007  7.3188\n",
      "      6            \u001b[36m0.6070\u001b[0m        \u001b[32m1.1552\u001b[0m       \u001b[35m0.4531\u001b[0m            \u001b[31m0.4531\u001b[0m        \u001b[94m1.2417\u001b[0m  0.0006  6.6638\n",
      "      7            0.5961        \u001b[32m1.1168\u001b[0m       \u001b[35m0.4544\u001b[0m            \u001b[31m0.4544\u001b[0m        1.2547  0.0006  6.6729\n",
      "      8            \u001b[36m0.6336\u001b[0m        \u001b[32m1.0715\u001b[0m       \u001b[35m0.4557\u001b[0m            \u001b[31m0.4557\u001b[0m        \u001b[94m1.2152\u001b[0m  0.0006  7.1189\n",
      "      9            0.6318        \u001b[32m1.0649\u001b[0m       \u001b[35m0.4674\u001b[0m            \u001b[31m0.4674\u001b[0m        1.2200  0.0006  6.5313\n",
      "     10            \u001b[36m0.6711\u001b[0m        \u001b[32m1.0305\u001b[0m       \u001b[35m0.4974\u001b[0m            \u001b[31m0.4974\u001b[0m        \u001b[94m1.1880\u001b[0m  0.0005  7.2103\n",
      "     11            0.6424        \u001b[32m0.9717\u001b[0m       0.4740            0.4740        1.2267  0.0005  6.5865\n",
      "     12            \u001b[36m0.6755\u001b[0m        \u001b[32m0.9700\u001b[0m       0.4714            0.4714        1.2015  0.0005  6.8961\n",
      "     13            \u001b[36m0.6826\u001b[0m        \u001b[32m0.9428\u001b[0m       0.4961            0.4961        \u001b[94m1.1833\u001b[0m  0.0004  6.8619\n",
      "     14            \u001b[36m0.7060\u001b[0m        \u001b[32m0.9268\u001b[0m       \u001b[35m0.4987\u001b[0m            \u001b[31m0.4987\u001b[0m        1.2032  0.0004  6.5465\n",
      "     15            0.6906        \u001b[32m0.9146\u001b[0m       0.4896            0.4896        1.1893  0.0004  7.3059\n",
      "     16            \u001b[36m0.7109\u001b[0m        \u001b[32m0.8880\u001b[0m       \u001b[35m0.5026\u001b[0m            \u001b[31m0.5026\u001b[0m        1.1944  0.0003  6.9055\n",
      "     17            \u001b[36m0.7391\u001b[0m        \u001b[32m0.8539\u001b[0m       \u001b[35m0.5104\u001b[0m            \u001b[31m0.5104\u001b[0m        \u001b[94m1.1630\u001b[0m  0.0003  7.2293\n",
      "     18            0.7357        0.8587       0.5026            0.5026        1.1890  0.0003  6.7856\n",
      "     19            \u001b[36m0.7482\u001b[0m        \u001b[32m0.8339\u001b[0m       0.5065            0.5065        1.1698  0.0002  9.8912\n",
      "     20            \u001b[36m0.7576\u001b[0m        \u001b[32m0.8212\u001b[0m       \u001b[35m0.5130\u001b[0m            \u001b[31m0.5130\u001b[0m        \u001b[94m1.1564\u001b[0m  0.0002  7.2437\n",
      "     21            \u001b[36m0.7648\u001b[0m        \u001b[32m0.8095\u001b[0m       \u001b[35m0.5299\u001b[0m            \u001b[31m0.5299\u001b[0m        \u001b[94m1.1441\u001b[0m  0.0002  6.6253\n",
      "     22            \u001b[36m0.7688\u001b[0m        \u001b[32m0.7879\u001b[0m       0.5286            0.5286        1.1453  0.0001  6.5464\n",
      "     23            \u001b[36m0.7708\u001b[0m        \u001b[32m0.7866\u001b[0m       0.5130            0.5130        1.1443  0.0001  7.0833\n",
      "     24            \u001b[36m0.7737\u001b[0m        \u001b[32m0.7772\u001b[0m       0.5052            0.5052        1.1526  0.0001  6.5661\n",
      "     25            \u001b[36m0.7805\u001b[0m        \u001b[32m0.7574\u001b[0m       0.5156            0.5156        1.1447  0.0001  7.2616\n",
      "     26            0.7779        \u001b[32m0.7464\u001b[0m       0.5208            0.5208        1.1473  0.0000  6.5110\n",
      "     27            0.7797        0.7499       0.5169            0.5169        \u001b[94m1.1440\u001b[0m  0.0000  7.2027\n",
      "     28            \u001b[36m0.7826\u001b[0m        0.7539       0.5273            0.5273        1.1455  0.0000  6.9722\n",
      "     29            0.7826        0.7490       0.5208            0.5208        1.1447  0.0000  6.5225\n",
      "     30            \u001b[36m0.7828\u001b[0m        \u001b[32m0.7433\u001b[0m       0.5169            0.5169        \u001b[94m1.1440\u001b[0m  0.0000  7.2394\n",
      "Before finetuning for subject 1, the baseline accuracy is 0.6041666666666666\n",
      "Fine tuning model for subject 1 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        1.1639       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9247\u001b[0m  0.0004  0.1191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        \u001b[32m0.6132\u001b[0m       0.5729            0.5729        0.9285  0.0005  0.1325\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3357\u001b[0m       0.5417            0.5417        0.9510  0.0005  0.1312\n",
      "     34            1.0000        \u001b[32m0.2297\u001b[0m       0.5312            0.5312        0.9696  0.0006  0.1652\n",
      "     35            1.0000        \u001b[32m0.0724\u001b[0m       0.5625            0.5625        0.9798  0.0006  0.1202\n",
      "     36            1.0000        \u001b[32m0.0352\u001b[0m       0.5729            0.5729        0.9866  0.0007  0.1183\n",
      "     37            1.0000        \u001b[32m0.0227\u001b[0m       0.5729            0.5729        1.0016  0.0007  0.1216\n",
      "     38            1.0000        0.0320       0.5521            0.5521        1.0300  0.0007  0.1020\n",
      "     39            1.0000        \u001b[32m0.0117\u001b[0m       0.4896            0.4896        1.0720  0.0007  0.1109\n",
      "     40            1.0000        \u001b[32m0.0074\u001b[0m       0.4896            0.4896        1.1229  0.0007  0.1180\n",
      "     41            1.0000        0.0124       0.4688            0.4688        1.1760  0.0007  0.1242\n",
      "     42            1.0000        \u001b[32m0.0068\u001b[0m       0.4167            0.4167        1.2264  0.0007  0.1147\n",
      "     43            1.0000        0.0088       0.4375            0.4375        1.2694  0.0006  0.1061\n",
      "     44            1.0000        0.0102       0.4375            0.4375        1.3024  0.0006  0.1013\n",
      "     45            1.0000        0.0118       0.4375            0.4375        1.3238  0.0005  0.1199\n",
      "     46            1.0000        0.0127       0.4271            0.4271        1.3335  0.0005  0.1500\n",
      "     47            1.0000        0.0075       0.4479            0.4479        1.3339  0.0004  0.1142\n",
      "     48            1.0000        \u001b[32m0.0038\u001b[0m       0.4375            0.4375        1.3294  0.0004  0.1124\n",
      "     49            1.0000        0.0040       0.4375            0.4375        1.3211  0.0003  0.1181\n",
      "     50            1.0000        0.0103       0.4479            0.4479        1.3106  0.0003  0.1249\n",
      "Fine tuning model for subject 1 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m1.0000\u001b[0m        \u001b[32m0.7110\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9437\u001b[0m  0.0004  0.1209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            1.0000        \u001b[32m0.4259\u001b[0m       0.5729            0.5729        0.9534  0.0005  0.1166\n",
      "     33            1.0000        \u001b[32m0.1958\u001b[0m       0.6042            0.6042        0.9779  0.0005  0.0978\n",
      "     34            1.0000        \u001b[32m0.1388\u001b[0m       0.5938            0.5938        1.0163  0.0006  0.1199\n",
      "     35            1.0000        \u001b[32m0.0615\u001b[0m       0.5312            0.5312        1.0655  0.0006  0.1142\n",
      "     36            1.0000        0.0650       0.5104            0.5104        1.1223  0.0007  0.1074\n",
      "     37            1.0000        \u001b[32m0.0268\u001b[0m       0.4792            0.4792        1.1817  0.0007  0.1014\n",
      "     38            1.0000        \u001b[32m0.0253\u001b[0m       0.4688            0.4688        1.2363  0.0007  0.1189\n",
      "     39            1.0000        \u001b[32m0.0208\u001b[0m       0.4271            0.4271        1.2835  0.0007  0.1191\n",
      "     40            1.0000        \u001b[32m0.0088\u001b[0m       0.4271            0.4271        1.3213  0.0007  0.1122\n",
      "     41            1.0000        \u001b[32m0.0044\u001b[0m       0.4062            0.4062        1.3495  0.0007  0.1160\n",
      "     42            1.0000        0.0092       0.4062            0.4062        1.3680  0.0007  0.1052\n",
      "     43            1.0000        0.0123       0.4062            0.4062        1.3775  0.0006  0.1260\n",
      "     44            1.0000        0.0059       0.4167            0.4167        1.3798  0.0006  0.1142\n",
      "     45            1.0000        0.0069       0.3958            0.3958        1.3770  0.0005  0.1150\n",
      "     46            1.0000        0.0080       0.4167            0.4167        1.3699  0.0005  0.1208\n",
      "     47            1.0000        0.0058       0.4167            0.4167        1.3600  0.0004  0.1198\n",
      "     48            1.0000        \u001b[32m0.0033\u001b[0m       0.4167            0.4167        1.3489  0.0004  0.1033\n",
      "     49            1.0000        0.0035       0.4375            0.4375        1.3374  0.0003  0.1316\n",
      "     50            1.0000        \u001b[32m0.0023\u001b[0m       0.4375            0.4375        1.3261  0.0003  0.1198\n",
      "Fine tuning model for subject 1 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m1.0000\u001b[0m        1.0347       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.9337\u001b[0m  0.0004  0.1294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            1.0000        1.0795       0.6146            0.6146        \u001b[94m0.9186\u001b[0m  0.0005  0.1151\n",
      "     33            1.0000        \u001b[32m0.1885\u001b[0m       0.6146            0.6146        0.9222  0.0005  0.1155\n",
      "     34            1.0000        \u001b[32m0.1201\u001b[0m       0.6146            0.6146        0.9420  0.0006  0.1032\n",
      "     35            1.0000        \u001b[32m0.0725\u001b[0m       0.5938            0.5938        0.9635  0.0006  0.1169\n",
      "     36            1.0000        \u001b[32m0.0194\u001b[0m       0.6042            0.6042        0.9803  0.0007  0.1032\n",
      "     37            1.0000        0.0199       0.6042            0.6042        0.9903  0.0007  0.1141\n",
      "     38            1.0000        \u001b[32m0.0100\u001b[0m       0.5938            0.5938        0.9962  0.0007  0.1180\n",
      "     39            1.0000        \u001b[32m0.0058\u001b[0m       0.5729            0.5729        1.0017  0.0007  0.1355\n",
      "     40            1.0000        0.0107       0.5625            0.5625        1.0084  0.0007  0.1345\n",
      "     41            1.0000        0.0082       0.5729            0.5729        1.0169  0.0007  0.1049\n",
      "     42            1.0000        \u001b[32m0.0049\u001b[0m       0.5729            0.5729        1.0260  0.0007  0.1159\n",
      "     43            1.0000        0.0051       0.5417            0.5417        1.0343  0.0006  0.1146\n",
      "     44            1.0000        0.0104       0.5417            0.5417        1.0405  0.0006  0.1509\n",
      "     45            1.0000        \u001b[32m0.0037\u001b[0m       0.5417            0.5417        1.0446  0.0005  0.1350\n",
      "     46            1.0000        0.0079       0.5417            0.5417        1.0467  0.0005  0.1248\n",
      "     47            1.0000        0.0051       0.5417            0.5417        1.0471  0.0004  0.1132\n",
      "     48            1.0000        0.0042       0.5417            0.5417        1.0467  0.0004  0.1166\n",
      "     49            1.0000        \u001b[32m0.0036\u001b[0m       0.5417            0.5417        1.0462  0.0003  0.1202\n",
      "     50            1.0000        \u001b[32m0.0019\u001b[0m       0.5208            0.5208        1.0461  0.0003  0.1211\n",
      "Fine tuning model for subject 1 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.1705       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9422\u001b[0m  0.0004  0.1329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        \u001b[32m0.6633\u001b[0m       0.5938            0.5938        0.9674  0.0005  0.1312\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5922\u001b[0m       0.5417            0.5417        1.0328  0.0005  0.1303\n",
      "     34            1.0000        \u001b[32m0.1257\u001b[0m       0.5000            0.5000        1.1243  0.0006  0.1510\n",
      "     35            1.0000        \u001b[32m0.0592\u001b[0m       0.5104            0.5104        1.2259  0.0006  0.1521\n",
      "     36            1.0000        \u001b[32m0.0369\u001b[0m       0.5312            0.5312        1.3234  0.0007  0.1624\n",
      "     37            1.0000        \u001b[32m0.0146\u001b[0m       0.5104            0.5104        1.4087  0.0007  0.1322\n",
      "     38            1.0000        \u001b[32m0.0040\u001b[0m       0.4896            0.4896        1.4784  0.0007  0.1501\n",
      "     39            1.0000        0.0041       0.4896            0.4896        1.5334  0.0007  0.1521\n",
      "     40            1.0000        \u001b[32m0.0022\u001b[0m       0.4896            0.4896        1.5764  0.0007  0.1507\n",
      "     41            1.0000        0.0040       0.4896            0.4896        1.6100  0.0007  0.1665\n",
      "     42            1.0000        0.0079       0.4896            0.4896        1.6365  0.0007  0.1349\n",
      "     43            1.0000        0.0026       0.5000            0.5000        1.6570  0.0006  0.1648\n",
      "     44            1.0000        0.0025       0.5104            0.5104        1.6723  0.0006  0.1509\n",
      "     45            1.0000        0.0051       0.5208            0.5208        1.6829  0.0005  0.1511\n",
      "     46            1.0000        0.0032       0.5208            0.5208        1.6895  0.0005  0.1355\n",
      "     47            1.0000        \u001b[32m0.0013\u001b[0m       0.5208            0.5208        1.6924  0.0004  0.1509\n",
      "     48            1.0000        0.0015       0.5104            0.5104        1.6924  0.0004  0.1798\n",
      "     49            1.0000        \u001b[32m0.0006\u001b[0m       0.5000            0.5000        1.6901  0.0003  0.1322\n",
      "     50            1.0000        0.0018       0.5000            0.5000        1.6859  0.0003  0.1270\n",
      "Fine tuning model for subject 1 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.1909       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9287\u001b[0m  0.0004  0.1180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        0.8749       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.9154\u001b[0m  0.0005  0.1313\n",
      "     33            1.0000        \u001b[32m0.6778\u001b[0m       0.6250            0.6250        0.9169  0.0005  0.1302\n",
      "     34            1.0000        \u001b[32m0.2730\u001b[0m       0.5938            0.5938        0.9364  0.0006  0.1193\n",
      "     35            1.0000        \u001b[32m0.0766\u001b[0m       0.5833            0.5833        0.9672  0.0006  0.1304\n",
      "     36            1.0000        \u001b[32m0.0564\u001b[0m       0.5729            0.5729        1.0009  0.0007  0.1152\n",
      "     37            1.0000        \u001b[32m0.0357\u001b[0m       0.5521            0.5521        1.0353  0.0007  0.1017\n",
      "     38            1.0000        \u001b[32m0.0086\u001b[0m       0.5000            0.5000        1.0700  0.0007  0.1170\n",
      "     39            1.0000        \u001b[32m0.0053\u001b[0m       0.4896            0.4896        1.1053  0.0007  0.1198\n",
      "     40            1.0000        \u001b[32m0.0051\u001b[0m       0.5104            0.5104        1.1419  0.0007  0.1238\n",
      "     41            1.0000        0.0053       0.5208            0.5208        1.1796  0.0007  0.1163\n",
      "     42            1.0000        \u001b[32m0.0050\u001b[0m       0.5104            0.5104        1.2174  0.0007  0.1139\n",
      "     43            1.0000        \u001b[32m0.0034\u001b[0m       0.4896            0.4896        1.2542  0.0006  0.1220\n",
      "     44            1.0000        0.0035       0.4792            0.4792        1.2889  0.0006  0.1260\n",
      "     45            1.0000        0.0038       0.4792            0.4792        1.3211  0.0005  0.1249\n",
      "     46            1.0000        \u001b[32m0.0013\u001b[0m       0.4792            0.4792        1.3504  0.0005  0.1120\n",
      "     47            1.0000        \u001b[32m0.0007\u001b[0m       0.4792            0.4792        1.3766  0.0004  0.1169\n",
      "     48            1.0000        0.0023       0.4479            0.4479        1.3999  0.0004  0.1209\n",
      "     49            1.0000        0.0021       0.4375            0.4375        1.4207  0.0003  0.1178\n",
      "     50            1.0000        0.0016       0.4271            0.4271        1.4392  0.0003  0.1218\n",
      "Fine tuning model for subject 1 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.8356       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9257\u001b[0m  0.0004  0.1245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        1.2870       0.5833            0.5833        \u001b[94m0.9159\u001b[0m  0.0005  0.1136\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4976\u001b[0m       0.5833            0.5833        0.9201  0.0005  0.1165\n",
      "     34            1.0000        \u001b[32m0.2495\u001b[0m       0.6250            0.6250        0.9263  0.0006  0.1198\n",
      "     35            1.0000        \u001b[32m0.0546\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        0.9352  0.0006  0.1173\n",
      "     36            1.0000        \u001b[32m0.0228\u001b[0m       0.6146            0.6146        0.9596  0.0007  0.1048\n",
      "     37            1.0000        \u001b[32m0.0147\u001b[0m       0.5729            0.5729        1.0152  0.0007  0.1004\n",
      "     38            1.0000        \u001b[32m0.0078\u001b[0m       0.5104            0.5104        1.1078  0.0007  0.1186\n",
      "     39            1.0000        \u001b[32m0.0062\u001b[0m       0.4583            0.4583        1.2279  0.0007  0.1138\n",
      "     40            1.0000        0.0121       0.4375            0.4375        1.3584  0.0007  0.1080\n",
      "     41            1.0000        \u001b[32m0.0056\u001b[0m       0.3958            0.3958        1.4818  0.0007  0.1170\n",
      "     42            1.0000        \u001b[32m0.0046\u001b[0m       0.3958            0.3958        1.5858  0.0007  0.1197\n",
      "     43            1.0000        0.0121       0.3854            0.3854        1.6626  0.0006  0.1303\n",
      "     44            1.0000        0.0120       0.3854            0.3854        1.7093  0.0006  0.1074\n",
      "     45            1.0000        0.0179       0.4167            0.4167        1.7302  0.0005  0.1139\n",
      "     46            1.0000        \u001b[32m0.0036\u001b[0m       0.4062            0.4062        1.7316  0.0005  0.1179\n",
      "     47            1.0000        0.0069       0.4062            0.4062        1.7178  0.0004  0.1155\n",
      "     48            1.0000        \u001b[32m0.0023\u001b[0m       0.4062            0.4062        1.6931  0.0004  0.1042\n",
      "     49            1.0000        0.0159       0.3958            0.3958        1.6600  0.0003  0.1085\n",
      "     50            1.0000        0.0025       0.3854            0.3854        1.6241  0.0003  0.1169\n",
      "Fine tuning model for subject 1 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2000        0.8035       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9192\u001b[0m  0.0004  0.1092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        \u001b[32m0.6226\u001b[0m       0.5938            0.5938        \u001b[94m0.9142\u001b[0m  0.0005  0.1327\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5427\u001b[0m       0.5833            0.5833        0.9387  0.0005  0.1156\n",
      "     34            1.0000        \u001b[32m0.1125\u001b[0m       0.5625            0.5625        0.9878  0.0006  0.1198\n",
      "     35            1.0000        \u001b[32m0.0579\u001b[0m       0.5521            0.5521        1.0412  0.0006  0.1310\n",
      "     36            1.0000        \u001b[32m0.0393\u001b[0m       0.5521            0.5521        1.0896  0.0007  0.1208\n",
      "     37            1.0000        \u001b[32m0.0028\u001b[0m       0.5312            0.5312        1.1310  0.0007  0.1157\n",
      "     38            1.0000        0.0039       0.5208            0.5208        1.1652  0.0007  0.1171\n",
      "     39            1.0000        0.0034       0.5208            0.5208        1.1944  0.0007  0.1208\n",
      "     40            1.0000        \u001b[32m0.0025\u001b[0m       0.5208            0.5208        1.2203  0.0007  0.1432\n",
      "     41            1.0000        0.0096       0.5312            0.5312        1.2451  0.0007  0.1255\n",
      "     42            1.0000        0.0039       0.5312            0.5312        1.2695  0.0007  0.1149\n",
      "     43            1.0000        \u001b[32m0.0011\u001b[0m       0.5312            0.5312        1.2947  0.0006  0.1298\n",
      "     44            1.0000        0.0013       0.5312            0.5312        1.3211  0.0006  0.1321\n",
      "     45            1.0000        \u001b[32m0.0008\u001b[0m       0.5208            0.5208        1.3489  0.0005  0.1394\n",
      "     46            1.0000        0.0014       0.5312            0.5312        1.3780  0.0005  0.1386\n",
      "     47            1.0000        \u001b[32m0.0008\u001b[0m       0.5312            0.5312        1.4082  0.0004  0.1180\n",
      "     48            1.0000        \u001b[32m0.0006\u001b[0m       0.5104            0.5104        1.4388  0.0004  0.1033\n",
      "     49            1.0000        \u001b[32m0.0005\u001b[0m       0.5104            0.5104        1.4694  0.0003  0.1168\n",
      "     50            1.0000        0.0007       0.5104            0.5104        1.4995  0.0003  0.1188\n",
      "Fine tuning model for subject 1 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6771\u001b[0m       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9334\u001b[0m  0.0004  0.1163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            1.0000        \u001b[32m0.3034\u001b[0m       0.6250            0.6250        \u001b[94m0.9249\u001b[0m  0.0005  0.1334\n",
      "     33            1.0000        \u001b[32m0.2357\u001b[0m       0.6146            0.6146        0.9266  0.0005  0.1145\n",
      "     34            1.0000        \u001b[32m0.1541\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        0.9386  0.0006  0.1197\n",
      "     35            1.0000        \u001b[32m0.0730\u001b[0m       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        0.9653  0.0006  0.1344\n",
      "     36            1.0000        \u001b[32m0.0315\u001b[0m       0.6146            0.6146        1.0056  0.0007  0.1242\n",
      "     37            1.0000        \u001b[32m0.0192\u001b[0m       0.6042            0.6042        1.0549  0.0007  0.1045\n",
      "     38            1.0000        0.0260       0.5729            0.5729        1.1102  0.0007  0.1260\n",
      "     39            1.0000        \u001b[32m0.0053\u001b[0m       0.5521            0.5521        1.1638  0.0007  0.1208\n",
      "     40            1.0000        \u001b[32m0.0046\u001b[0m       0.5417            0.5417        1.2117  0.0007  0.1189\n",
      "     41            1.0000        \u001b[32m0.0023\u001b[0m       0.5417            0.5417        1.2504  0.0007  0.1040\n",
      "     42            1.0000        0.0042       0.5417            0.5417        1.2786  0.0007  0.1141\n",
      "     43            1.0000        0.0057       0.5417            0.5417        1.2960  0.0006  0.1181\n",
      "     44            1.0000        0.0063       0.5417            0.5417        1.3034  0.0006  0.1268\n",
      "     45            1.0000        \u001b[32m0.0015\u001b[0m       0.5312            0.5312        1.3017  0.0005  0.1181\n",
      "     46            1.0000        0.0072       0.5521            0.5521        1.2932  0.0005  0.1030\n",
      "     47            1.0000        0.0028       0.5521            0.5521        1.2788  0.0004  0.1170\n",
      "     48            1.0000        0.0019       0.5521            0.5521        1.2606  0.0004  0.1199\n",
      "     49            1.0000        0.0018       0.5625            0.5625        1.2400  0.0003  0.1207\n",
      "     50            1.0000        0.0023       0.5729            0.5729        1.2187  0.0003  0.1024\n",
      "Fine tuning model for subject 1 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        0.8749       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9394\u001b[0m  0.0004  0.1193\n",
      "     32            \u001b[36m1.0000\u001b[0m        0.7437       0.5833            0.5833        0.9536  0.0005  0.1142\n",
      "     33            1.0000        \u001b[32m0.3918\u001b[0m       0.5625            0.5625        0.9983  0.0005  0.1197\n",
      "     34            1.0000        \u001b[32m0.1100\u001b[0m       0.5625            0.5625        1.0578  0.0006  0.1171\n",
      "     35            1.0000        \u001b[32m0.0354\u001b[0m       0.5625            0.5625        1.1138  0.0006  0.1041\n",
      "     36            1.0000        \u001b[32m0.0181\u001b[0m       0.5208            0.5208        1.1603  0.0007  0.1170\n",
      "     37            1.0000        \u001b[32m0.0147\u001b[0m       0.5208            0.5208        1.1997  0.0007  0.1198\n",
      "     38            1.0000        0.0522       0.4792            0.4792        1.2385  0.0007  0.1199\n",
      "     39            1.0000        \u001b[32m0.0054\u001b[0m       0.4896            0.4896        1.2748  0.0007  0.1072\n",
      "     40            1.0000        \u001b[32m0.0050\u001b[0m       0.4792            0.4792        1.3101  0.0007  0.1142\n",
      "     41            1.0000        \u001b[32m0.0016\u001b[0m       0.4688            0.4688        1.3441  0.0007  0.1179\n",
      "     42            1.0000        0.0018       0.4479            0.4479        1.3755  0.0007  0.1189\n",
      "     43            1.0000        0.0029       0.4375            0.4375        1.4031  0.0006  0.1050\n",
      "     44            1.0000        0.0024       0.4375            0.4375        1.4260  0.0006  0.1124\n",
      "     45            1.0000        \u001b[32m0.0014\u001b[0m       0.4479            0.4479        1.4443  0.0005  0.1179\n",
      "     46            1.0000        0.0033       0.4375            0.4375        1.4586  0.0005  0.1230\n",
      "     47            1.0000        0.0042       0.4375            0.4375        1.4697  0.0004  0.1034\n",
      "     48            1.0000        0.0015       0.4479            0.4479        1.4788  0.0004  0.1097\n",
      "     49            1.0000        0.0018       0.4479            0.4479        1.4869  0.0003  0.1170\n",
      "     50            1.0000        \u001b[32m0.0012\u001b[0m       0.4375            0.4375        1.4945  0.0003  0.1231\n",
      "Fine tuning model for subject 1 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4963\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9338\u001b[0m  0.0004  0.1262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            1.0000        \u001b[32m0.3712\u001b[0m       0.6146            0.6146        \u001b[94m0.9266\u001b[0m  0.0005  0.1389\n",
      "     33            1.0000        \u001b[32m0.1860\u001b[0m       0.6146            0.6146        0.9268  0.0005  0.1469\n",
      "     34            1.0000        \u001b[32m0.1710\u001b[0m       0.6042            0.6042        0.9314  0.0006  0.1390\n",
      "     35            1.0000        \u001b[32m0.0468\u001b[0m       0.5938            0.5938        0.9403  0.0006  0.1307\n",
      "     36            1.0000        \u001b[32m0.0176\u001b[0m       0.5729            0.5729        0.9521  0.0007  0.1510\n",
      "     37            1.0000        \u001b[32m0.0174\u001b[0m       0.5625            0.5625        0.9649  0.0007  0.1355\n",
      "     38            1.0000        \u001b[32m0.0087\u001b[0m       0.5625            0.5625        0.9781  0.0007  0.1363\n",
      "     39            1.0000        \u001b[32m0.0048\u001b[0m       0.5521            0.5521        0.9914  0.0007  0.1511\n",
      "     40            1.0000        \u001b[32m0.0046\u001b[0m       0.5312            0.5312        1.0042  0.0007  0.1510\n",
      "     41            1.0000        0.0080       0.5417            0.5417        1.0164  0.0007  0.1556\n",
      "     42            1.0000        0.0068       0.5104            0.5104        1.0279  0.0007  0.1642\n",
      "     43            1.0000        \u001b[32m0.0026\u001b[0m       0.5208            0.5208        1.0391  0.0006  0.1365\n",
      "     44            1.0000        0.0048       0.5000            0.5000        1.0496  0.0006  0.1855\n",
      "     45            1.0000        0.0038       0.4792            0.4792        1.0596  0.0005  0.1479\n",
      "     46            1.0000        0.0048       0.4688            0.4688        1.0689  0.0005  0.1510\n",
      "     47            1.0000        0.0040       0.4688            0.4688        1.0775  0.0004  0.1465\n",
      "     48            1.0000        \u001b[32m0.0012\u001b[0m       0.4792            0.4792        1.0853  0.0004  0.1202\n",
      "     49            1.0000        0.0034       0.4792            0.4792        1.0923  0.0003  0.1258\n",
      "     50            1.0000        0.0031       0.4896            0.4896        1.0986  0.0003  0.1110\n",
      "Fine tuning model for subject 1 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        2.0593       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9317\u001b[0m  0.0004  0.0913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        2.0273       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9296\u001b[0m  0.0005  0.1203\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.9135       0.5833            0.5833        0.9517  0.0005  0.1170\n",
      "     34            1.0000        \u001b[32m0.5982\u001b[0m       0.5208            0.5208        1.0121  0.0006  0.0966\n",
      "     35            1.0000        \u001b[32m0.4557\u001b[0m       0.5000            0.5000        1.1054  0.0006  0.1110\n",
      "     36            1.0000        \u001b[32m0.2428\u001b[0m       0.4896            0.4896        1.2198  0.0007  0.0978\n",
      "     37            1.0000        \u001b[32m0.0931\u001b[0m       0.4583            0.4583        1.3393  0.0007  0.0979\n",
      "     38            1.0000        0.1360       0.4479            0.4479        1.4516  0.0007  0.1137\n",
      "     39            1.0000        \u001b[32m0.0666\u001b[0m       0.4479            0.4479        1.5497  0.0007  0.0914\n",
      "     40            1.0000        \u001b[32m0.0262\u001b[0m       0.4479            0.4479        1.6279  0.0007  0.0994\n",
      "     41            1.0000        \u001b[32m0.0200\u001b[0m       0.4167            0.4167        1.6871  0.0007  0.1079\n",
      "     42            1.0000        0.0207       0.3854            0.3854        1.7293  0.0007  0.1239\n",
      "     43            1.0000        \u001b[32m0.0177\u001b[0m       0.3958            0.3958        1.7573  0.0006  0.1076\n",
      "     44            1.0000        \u001b[32m0.0097\u001b[0m       0.3854            0.3854        1.7727  0.0006  0.1004\n",
      "     45            1.0000        \u001b[32m0.0074\u001b[0m       0.3750            0.3750        1.7778  0.0005  0.1157\n",
      "     46            1.0000        0.0119       0.3750            0.3750        1.7743  0.0005  0.0904\n",
      "     47            1.0000        \u001b[32m0.0061\u001b[0m       0.3646            0.3646        1.7650  0.0004  0.0933\n",
      "     48            1.0000        0.0189       0.3646            0.3646        1.7515  0.0004  0.1025\n",
      "     49            1.0000        0.0073       0.3438            0.3438        1.7350  0.0003  0.1241\n",
      "     50            1.0000        0.0075       0.3438            0.3438        1.7168  0.0003  0.0959\n",
      "Fine tuning model for subject 1 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.4986       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9295\u001b[0m  0.0004  0.0997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        1.0628       0.6042            0.6042        \u001b[94m0.9233\u001b[0m  0.0005  0.1016\n",
      "     33            \u001b[36m1.0000\u001b[0m        1.0256       0.5938            0.5938        0.9332  0.0005  0.0973\n",
      "     34            1.0000        \u001b[32m0.5483\u001b[0m       0.5938            0.5938        0.9752  0.0006  0.1139\n",
      "     35            1.0000        \u001b[32m0.1963\u001b[0m       0.5104            0.5104        1.0351  0.0006  0.0986\n",
      "     36            1.0000        \u001b[32m0.1304\u001b[0m       0.4896            0.4896        1.0969  0.0007  0.0995\n",
      "     37            1.0000        \u001b[32m0.1049\u001b[0m       0.4896            0.4896        1.1505  0.0007  0.1180\n",
      "     38            1.0000        \u001b[32m0.0752\u001b[0m       0.4688            0.4688        1.1919  0.0007  0.0978\n",
      "     39            1.0000        \u001b[32m0.0534\u001b[0m       0.4479            0.4479        1.2246  0.0007  0.1014\n",
      "     40            1.0000        \u001b[32m0.0370\u001b[0m       0.4688            0.4688        1.2519  0.0007  0.0987\n",
      "     41            1.0000        \u001b[32m0.0363\u001b[0m       0.4688            0.4688        1.2759  0.0007  0.1022\n",
      "     42            1.0000        0.0468       0.4583            0.4583        1.2958  0.0007  0.1086\n",
      "     43            1.0000        0.0441       0.4479            0.4479        1.3132  0.0006  0.1009\n",
      "     44            1.0000        \u001b[32m0.0263\u001b[0m       0.4167            0.4167        1.3266  0.0006  0.1110\n",
      "     45            1.0000        \u001b[32m0.0226\u001b[0m       0.3854            0.3854        1.3371  0.0005  0.0885\n",
      "     46            1.0000        0.0240       0.3438            0.3438        1.3447  0.0005  0.1128\n",
      "     47            1.0000        \u001b[32m0.0196\u001b[0m       0.3646            0.3646        1.3508  0.0004  0.1030\n",
      "     48            1.0000        \u001b[32m0.0162\u001b[0m       0.3646            0.3646        1.3563  0.0004  0.0974\n",
      "     49            1.0000        0.0200       0.3438            0.3438        1.3614  0.0003  0.1036\n",
      "     50            1.0000        0.0183       0.3438            0.3438        1.3669  0.0003  0.1006\n",
      "Fine tuning model for subject 1 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.0653       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9343\u001b[0m  0.0004  0.1021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.2714       0.6042            0.6042        \u001b[94m0.9198\u001b[0m  0.0005  0.1136\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.7461       0.5938            0.5938        \u001b[94m0.9144\u001b[0m  0.0005  0.1101\n",
      "     34            1.0000        \u001b[32m0.6225\u001b[0m       0.6250            0.6250        0.9228  0.0006  0.1081\n",
      "     35            1.0000        \u001b[32m0.3745\u001b[0m       0.6146            0.6146        0.9411  0.0006  0.0986\n",
      "     36            1.0000        \u001b[32m0.2173\u001b[0m       0.5938            0.5938        0.9684  0.0007  0.1014\n",
      "     37            1.0000        \u001b[32m0.1573\u001b[0m       0.5833            0.5833        0.9989  0.0007  0.1178\n",
      "     38            1.0000        \u001b[32m0.0459\u001b[0m       0.5833            0.5833        1.0333  0.0007  0.1203\n",
      "     39            1.0000        0.0504       0.5208            0.5208        1.0692  0.0007  0.1006\n",
      "     40            1.0000        \u001b[32m0.0333\u001b[0m       0.5312            0.5312        1.1041  0.0007  0.1113\n",
      "     41            1.0000        0.0399       0.5104            0.5104        1.1324  0.0007  0.1147\n",
      "     42            1.0000        \u001b[32m0.0289\u001b[0m       0.5208            0.5208        1.1550  0.0007  0.0905\n",
      "     43            1.0000        \u001b[32m0.0271\u001b[0m       0.5208            0.5208        1.1705  0.0006  0.1034\n",
      "     44            1.0000        0.0275       0.5104            0.5104        1.1810  0.0006  0.1383\n",
      "     45            1.0000        \u001b[32m0.0255\u001b[0m       0.5104            0.5104        1.1863  0.0005  0.1171\n",
      "     46            1.0000        \u001b[32m0.0168\u001b[0m       0.5104            0.5104        1.1877  0.0005  0.0986\n",
      "     47            1.0000        0.0301       0.5104            0.5104        1.1856  0.0004  0.0851\n",
      "     48            1.0000        \u001b[32m0.0148\u001b[0m       0.5000            0.5000        1.1816  0.0004  0.1166\n",
      "     49            1.0000        \u001b[32m0.0137\u001b[0m       0.5000            0.5000        1.1763  0.0003  0.1031\n",
      "     50            1.0000        0.0138       0.5000            0.5000        1.1704  0.0003  0.0864\n",
      "Fine tuning model for subject 1 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        0.8014       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m0.9412\u001b[0m  0.0004  0.0991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.9000\u001b[0m        0.8735       0.5938            0.5938        0.9601  0.0005  0.1079\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5094\u001b[0m       0.5625            0.5625        1.0027  0.0005  0.0963\n",
      "     34            1.0000        \u001b[32m0.4785\u001b[0m       0.5104            0.5104        1.0747  0.0006  0.1138\n",
      "     35            1.0000        \u001b[32m0.3367\u001b[0m       0.5104            0.5104        1.1493  0.0006  0.0979\n",
      "     36            1.0000        \u001b[32m0.1726\u001b[0m       0.4896            0.4896        1.2076  0.0007  0.0932\n",
      "     37            1.0000        \u001b[32m0.0773\u001b[0m       0.4792            0.4792        1.2490  0.0007  0.1024\n",
      "     38            1.0000        0.1024       0.4688            0.4688        1.2679  0.0007  0.0986\n",
      "     39            1.0000        \u001b[32m0.0599\u001b[0m       0.4896            0.4896        1.2807  0.0007  0.0900\n",
      "     40            1.0000        0.0985       0.5312            0.5312        1.2926  0.0007  0.1022\n",
      "     41            1.0000        \u001b[32m0.0551\u001b[0m       0.5521            0.5521        1.3130  0.0007  0.1074\n",
      "     42            1.0000        \u001b[32m0.0382\u001b[0m       0.5938            0.5938        1.3415  0.0007  0.0963\n",
      "     43            1.0000        \u001b[32m0.0280\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.3700  0.0006  0.1025\n",
      "     44            1.0000        \u001b[32m0.0237\u001b[0m       0.5625            0.5625        1.3953  0.0006  0.1141\n",
      "     45            1.0000        0.0296       0.5312            0.5312        1.4139  0.0005  0.0873\n",
      "     46            1.0000        \u001b[32m0.0146\u001b[0m       0.5312            0.5312        1.4231  0.0005  0.1004\n",
      "     47            1.0000        0.0220       0.5521            0.5521        1.4227  0.0004  0.1063\n",
      "     48            1.0000        0.0168       0.5521            0.5521        1.4162  0.0004  0.0845\n",
      "     49            1.0000        \u001b[32m0.0120\u001b[0m       0.5521            0.5521        1.4046  0.0003  0.0994\n",
      "     50            1.0000        \u001b[32m0.0092\u001b[0m       0.5625            0.5625        1.3898  0.0003  0.1122\n",
      "Fine tuning model for subject 1 with 10 = 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        \u001b[32m0.7095\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9331\u001b[0m  0.0004  0.1132\n",
      "     32            \u001b[36m0.9000\u001b[0m        0.9473       0.6146            0.6146        \u001b[94m0.9252\u001b[0m  0.0005  0.1072\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4495\u001b[0m       0.5833            0.5833        0.9268  0.0005  0.1093\n",
      "     34            1.0000        0.5045       0.5729            0.5729        0.9378  0.0006  0.1083\n",
      "     35            1.0000        \u001b[32m0.2888\u001b[0m       0.5729            0.5729        0.9593  0.0006  0.1063\n",
      "     36            1.0000        \u001b[32m0.1316\u001b[0m       0.5521            0.5521        0.9901  0.0007  0.1112\n",
      "     37            1.0000        \u001b[32m0.0575\u001b[0m       0.5521            0.5521        1.0248  0.0007  0.1164\n",
      "     38            1.0000        \u001b[32m0.0573\u001b[0m       0.5521            0.5521        1.0615  0.0007  0.1203\n",
      "     39            1.0000        \u001b[32m0.0249\u001b[0m       0.5521            0.5521        1.0990  0.0007  0.1018\n",
      "     40            1.0000        \u001b[32m0.0167\u001b[0m       0.5625            0.5625        1.1350  0.0007  0.1071\n",
      "     41            1.0000        0.0278       0.5625            0.5625        1.1681  0.0007  0.1168\n",
      "     42            1.0000        0.0189       0.5625            0.5625        1.1959  0.0007  0.1164\n",
      "     43            1.0000        0.0214       0.5625            0.5625        1.2177  0.0006  0.0974\n",
      "     44            1.0000        \u001b[32m0.0163\u001b[0m       0.5729            0.5729        1.2338  0.0006  0.1333\n",
      "     45            1.0000        0.0163       0.5625            0.5625        1.2447  0.0005  0.0978\n",
      "     46            1.0000        \u001b[32m0.0110\u001b[0m       0.5417            0.5417        1.2509  0.0005  0.1209\n",
      "     47            1.0000        \u001b[32m0.0095\u001b[0m       0.5417            0.5417        1.2538  0.0004  0.1217\n",
      "     48            1.0000        0.0105       0.5417            0.5417        1.2538  0.0004  0.1038\n",
      "     49            1.0000        \u001b[32m0.0086\u001b[0m       0.5417            0.5417        1.2521  0.0003  0.1217\n",
      "     50            1.0000        0.0117       0.5417            0.5417        1.2492  0.0003  0.1020\n",
      "Fine tuning model for subject 1 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        1.4447       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9282\u001b[0m  0.0004  0.0971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        0.7797       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9155\u001b[0m  0.0005  0.1338\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.9194       0.5833            0.5833        \u001b[94m0.9137\u001b[0m  0.0005  0.0952\n",
      "     34            1.0000        \u001b[32m0.4608\u001b[0m       0.5833            0.5833        0.9321  0.0006  0.0930\n",
      "     35            1.0000        \u001b[32m0.3571\u001b[0m       0.5625            0.5625        0.9746  0.0006  0.1062\n",
      "     36            1.0000        \u001b[32m0.0851\u001b[0m       0.5833            0.5833        1.0324  0.0007  0.0965\n",
      "     37            1.0000        \u001b[32m0.0707\u001b[0m       0.5521            0.5521        1.0987  0.0007  0.1013\n",
      "     38            1.0000        0.0838       0.5104            0.5104        1.1719  0.0007  0.1270\n",
      "     39            1.0000        \u001b[32m0.0258\u001b[0m       0.5000            0.5000        1.2527  0.0007  0.1300\n",
      "     40            1.0000        \u001b[32m0.0139\u001b[0m       0.4479            0.4479        1.3377  0.0007  0.1425\n",
      "     41            1.0000        0.0172       0.3854            0.3854        1.4164  0.0007  0.1355\n",
      "     42            1.0000        0.0227       0.3958            0.3958        1.4811  0.0007  0.1387\n",
      "     43            1.0000        \u001b[32m0.0106\u001b[0m       0.3958            0.3958        1.5277  0.0006  0.1374\n",
      "     44            1.0000        0.0155       0.4062            0.4062        1.5539  0.0006  0.1633\n",
      "     45            1.0000        \u001b[32m0.0049\u001b[0m       0.4062            0.4062        1.5617  0.0005  0.1218\n",
      "     46            1.0000        0.0097       0.4167            0.4167        1.5535  0.0005  0.1457\n",
      "     47            1.0000        0.0096       0.4167            0.4167        1.5332  0.0004  0.1398\n",
      "     48            1.0000        0.0133       0.4271            0.4271        1.5046  0.0004  0.1099\n",
      "     49            1.0000        0.0101       0.4271            0.4271        1.4714  0.0003  0.1178\n",
      "     50            1.0000        0.0068       0.4583            0.4583        1.4371  0.0003  0.1234\n",
      "Fine tuning model for subject 1 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        0.7617       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9294\u001b[0m  0.0004  0.1393\n",
      "     32            \u001b[36m0.9000\u001b[0m        0.8613       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9241\u001b[0m  0.0005  0.1497\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5313\u001b[0m       0.6250            0.6250        0.9295  0.0005  0.1292\n",
      "     34            1.0000        \u001b[32m0.5177\u001b[0m       0.5938            0.5938        0.9517  0.0006  0.1363\n",
      "     35            1.0000        \u001b[32m0.4878\u001b[0m       0.6042            0.6042        0.9796  0.0006  0.1490\n",
      "     36            1.0000        \u001b[32m0.1488\u001b[0m       0.6146            0.6146        1.0079  0.0007  0.1203\n",
      "     37            1.0000        \u001b[32m0.0927\u001b[0m       0.5625            0.5625        1.0302  0.0007  0.1160\n",
      "     38            1.0000        \u001b[32m0.0769\u001b[0m       0.5312            0.5312        1.0487  0.0007  0.0865\n",
      "     39            1.0000        \u001b[32m0.0537\u001b[0m       0.5208            0.5208        1.0642  0.0007  0.1023\n",
      "     40            1.0000        0.1370       0.5208            0.5208        1.0665  0.0007  0.1146\n",
      "     41            1.0000        \u001b[32m0.0449\u001b[0m       0.5417            0.5417        1.0686  0.0007  0.0945\n",
      "     42            1.0000        \u001b[32m0.0195\u001b[0m       0.5521            0.5521        1.0722  0.0007  0.1331\n",
      "     43            1.0000        0.0323       0.5521            0.5521        1.0766  0.0006  0.0969\n",
      "     44            1.0000        0.0243       0.5521            0.5521        1.0815  0.0006  0.1134\n",
      "     45            1.0000        0.0200       0.5729            0.5729        1.0873  0.0005  0.0886\n",
      "     46            1.0000        0.0211       0.5521            0.5521        1.0936  0.0005  0.1031\n",
      "     47            1.0000        \u001b[32m0.0095\u001b[0m       0.5625            0.5625        1.1006  0.0004  0.1073\n",
      "     48            1.0000        0.0126       0.5625            0.5625        1.1077  0.0004  0.0964\n",
      "     49            1.0000        0.0123       0.5729            0.5729        1.1150  0.0003  0.1169\n",
      "     50            1.0000        0.0113       0.5833            0.5833        1.1222  0.0003  0.1138\n",
      "Fine tuning model for subject 1 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        1.0036       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9426\u001b[0m  0.0004  0.1078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        0.8799       0.6146            0.6146        0.9551  0.0005  0.1000\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5710\u001b[0m       0.5729            0.5729        0.9919  0.0005  0.0980\n",
      "     34            1.0000        \u001b[32m0.3018\u001b[0m       0.5312            0.5312        1.0463  0.0006  0.1063\n",
      "     35            1.0000        \u001b[32m0.1862\u001b[0m       0.5417            0.5417        1.1082  0.0006  0.0854\n",
      "     36            1.0000        \u001b[32m0.0804\u001b[0m       0.5417            0.5417        1.1687  0.0007  0.0984\n",
      "     37            1.0000        0.1050       0.5208            0.5208        1.2206  0.0007  0.1111\n",
      "     38            1.0000        0.0811       0.5312            0.5312        1.2647  0.0007  0.1032\n",
      "     39            1.0000        \u001b[32m0.0271\u001b[0m       0.5000            0.5000        1.2990  0.0007  0.0991\n",
      "     40            1.0000        \u001b[32m0.0201\u001b[0m       0.5000            0.5000        1.3244  0.0007  0.1122\n",
      "     41            1.0000        0.0202       0.5000            0.5000        1.3448  0.0007  0.0992\n",
      "     42            1.0000        \u001b[32m0.0141\u001b[0m       0.4896            0.4896        1.3614  0.0007  0.0965\n",
      "     43            1.0000        \u001b[32m0.0094\u001b[0m       0.5000            0.5000        1.3758  0.0006  0.1154\n",
      "     44            1.0000        0.0137       0.5104            0.5104        1.3885  0.0006  0.0986\n",
      "     45            1.0000        0.0100       0.5208            0.5208        1.4001  0.0005  0.0916\n",
      "     46            1.0000        0.0111       0.5104            0.5104        1.4109  0.0005  0.1023\n",
      "     47            1.0000        \u001b[32m0.0077\u001b[0m       0.5000            0.5000        1.4211  0.0004  0.1073\n",
      "     48            1.0000        0.0081       0.5000            0.5000        1.4304  0.0004  0.1108\n",
      "     49            1.0000        \u001b[32m0.0076\u001b[0m       0.4896            0.4896        1.4390  0.0003  0.1026\n",
      "     50            1.0000        0.0139       0.4896            0.4896        1.4464  0.0003  0.1104\n",
      "Fine tuning model for subject 1 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        0.9574       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9315\u001b[0m  0.0004  0.1008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        \u001b[32m0.6702\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.9192\u001b[0m  0.0005  0.1167\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6290\u001b[0m       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.9098\u001b[0m  0.0005  0.0978\n",
      "     34            1.0000        \u001b[32m0.2226\u001b[0m       0.6458            0.6458        0.9102  0.0006  0.1176\n",
      "     35            1.0000        \u001b[32m0.1753\u001b[0m       0.6146            0.6146        0.9164  0.0006  0.1143\n",
      "     36            1.0000        \u001b[32m0.1195\u001b[0m       0.6146            0.6146        0.9279  0.0007  0.0888\n",
      "     37            1.0000        \u001b[32m0.0424\u001b[0m       0.6042            0.6042        0.9438  0.0007  0.1011\n",
      "     38            1.0000        \u001b[32m0.0350\u001b[0m       0.5833            0.5833        0.9613  0.0007  0.1061\n",
      "     39            1.0000        \u001b[32m0.0329\u001b[0m       0.5625            0.5625        0.9792  0.0007  0.0974\n",
      "     40            1.0000        \u001b[32m0.0173\u001b[0m       0.5521            0.5521        0.9978  0.0007  0.1025\n",
      "     41            1.0000        0.0372       0.5521            0.5521        1.0170  0.0007  0.1151\n",
      "     42            1.0000        \u001b[32m0.0087\u001b[0m       0.5417            0.5417        1.0370  0.0007  0.0897\n",
      "     43            1.0000        0.0137       0.5417            0.5417        1.0574  0.0006  0.1001\n",
      "     44            1.0000        0.0098       0.5208            0.5208        1.0779  0.0006  0.1084\n",
      "     45            1.0000        0.0149       0.5000            0.5000        1.0981  0.0005  0.0830\n",
      "     46            1.0000        \u001b[32m0.0057\u001b[0m       0.5104            0.5104        1.1179  0.0005  0.0992\n",
      "     47            1.0000        0.0084       0.5208            0.5208        1.1366  0.0004  0.1183\n",
      "     48            1.0000        \u001b[32m0.0042\u001b[0m       0.5104            0.5104        1.1543  0.0004  0.0987\n",
      "     49            1.0000        0.0052       0.5000            0.5000        1.1709  0.0003  0.0925\n",
      "     50            1.0000        0.0048       0.5104            0.5104        1.1864  0.0003  0.1003\n",
      "Fine tuning model for subject 1 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.2886       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9406\u001b[0m  0.0004  0.0989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        1.3633       0.6042            0.6042        0.9522  0.0005  0.1053\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.9468       0.5729            0.5729        0.9848  0.0005  0.1004\n",
      "     34            1.0000        \u001b[32m0.4043\u001b[0m       0.5417            0.5417        1.0530  0.0006  0.1111\n",
      "     35            1.0000        \u001b[32m0.2574\u001b[0m       0.5208            0.5208        1.1561  0.0006  0.1167\n",
      "     36            1.0000        \u001b[32m0.1333\u001b[0m       0.4792            0.4792        1.2823  0.0007  0.1170\n",
      "     37            1.0000        \u001b[32m0.0937\u001b[0m       0.4583            0.4583        1.4204  0.0007  0.0942\n",
      "     38            1.0000        \u001b[32m0.0394\u001b[0m       0.4479            0.4479        1.5592  0.0007  0.1125\n",
      "     39            1.0000        \u001b[32m0.0249\u001b[0m       0.4271            0.4271        1.6856  0.0007  0.0987\n",
      "     40            1.0000        \u001b[32m0.0133\u001b[0m       0.3646            0.3646        1.7926  0.0007  0.0973\n",
      "     41            1.0000        0.0218       0.3333            0.3333        1.8748  0.0007  0.1008\n",
      "     42            1.0000        \u001b[32m0.0115\u001b[0m       0.3125            0.3125        1.9304  0.0007  0.0986\n",
      "     43            1.0000        0.0153       0.3125            0.3125        1.9593  0.0006  0.0892\n",
      "     44            1.0000        0.0147       0.3125            0.3125        1.9651  0.0006  0.1024\n",
      "     45            1.0000        0.0152       0.3125            0.3125        1.9521  0.0005  0.1015\n",
      "     46            1.0000        \u001b[32m0.0104\u001b[0m       0.3021            0.3021        1.9252  0.0005  0.0871\n",
      "     47            1.0000        0.0113       0.3021            0.3021        1.8893  0.0004  0.1022\n",
      "     48            1.0000        0.0203       0.3125            0.3125        1.8477  0.0004  0.1082\n",
      "     49            1.0000        0.0159       0.3333            0.3333        1.8047  0.0003  0.0965\n",
      "     50            1.0000        0.0299       0.3333            0.3333        1.7616  0.0003  0.1200\n",
      "Fine tuning model for subject 1 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        1.0691       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9283\u001b[0m  0.0004  0.1043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        0.9500       0.5833            0.5833        \u001b[94m0.9207\u001b[0m  0.0005  0.0971\n",
      "     33            \u001b[36m0.8667\u001b[0m        \u001b[32m0.7175\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        0.9264  0.0005  0.0999\n",
      "     34            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5372\u001b[0m       0.6146            0.6146        0.9333  0.0006  0.0997\n",
      "     35            0.9333        \u001b[32m0.2368\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        0.9355  0.0006  0.0988\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1927\u001b[0m       0.6250            0.6250        0.9362  0.0007  0.0879\n",
      "     37            1.0000        \u001b[32m0.1334\u001b[0m       0.6250            0.6250        0.9463  0.0007  0.1024\n",
      "     38            1.0000        \u001b[32m0.1299\u001b[0m       0.6042            0.6042        0.9728  0.0007  0.1010\n",
      "     39            1.0000        \u001b[32m0.0673\u001b[0m       0.6042            0.6042        1.0105  0.0007  0.0954\n",
      "     40            1.0000        \u001b[32m0.0540\u001b[0m       0.6146            0.6146        1.0534  0.0007  0.1001\n",
      "     41            1.0000        \u001b[32m0.0378\u001b[0m       0.5833            0.5833        1.0945  0.0007  0.1185\n",
      "     42            1.0000        \u001b[32m0.0304\u001b[0m       0.5312            0.5312        1.1283  0.0007  0.0986\n",
      "     43            1.0000        \u001b[32m0.0269\u001b[0m       0.5208            0.5208        1.1535  0.0006  0.0864\n",
      "     44            1.0000        0.0422       0.5104            0.5104        1.1703  0.0006  0.1019\n",
      "     45            1.0000        0.0476       0.5104            0.5104        1.1787  0.0005  0.1344\n",
      "     46            1.0000        \u001b[32m0.0267\u001b[0m       0.5208            0.5208        1.1812  0.0005  0.1066\n",
      "     47            1.0000        0.0312       0.5208            0.5208        1.1806  0.0004  0.0909\n",
      "     48            1.0000        \u001b[32m0.0190\u001b[0m       0.5417            0.5417        1.1777  0.0004  0.1023\n",
      "     49            1.0000        0.0212       0.5521            0.5521        1.1744  0.0003  0.1198\n",
      "     50            1.0000        0.0264       0.5625            0.5625        1.1722  0.0003  0.0975\n",
      "Fine tuning model for subject 1 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7333        0.8983       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9302\u001b[0m  0.0004  0.1047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8667\u001b[0m        0.7518       0.5625            0.5625        \u001b[94m0.9221\u001b[0m  0.0005  0.1140\n",
      "     33            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5541\u001b[0m       0.5521            0.5521        0.9283  0.0005  0.1131\n",
      "     34            0.9333        \u001b[32m0.5148\u001b[0m       0.5417            0.5417        0.9434  0.0006  0.0992\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2987\u001b[0m       0.5833            0.5833        0.9524  0.0006  0.1178\n",
      "     36            1.0000        \u001b[32m0.1478\u001b[0m       0.5833            0.5833        0.9610  0.0007  0.1020\n",
      "     37            1.0000        \u001b[32m0.1313\u001b[0m       0.6042            0.6042        0.9672  0.0007  0.1116\n",
      "     38            1.0000        \u001b[32m0.0747\u001b[0m       0.6146            0.6146        0.9740  0.0007  0.1021\n",
      "     39            1.0000        0.0995       0.5938            0.5938        0.9819  0.0007  0.1128\n",
      "     40            1.0000        \u001b[32m0.0735\u001b[0m       0.5729            0.5729        0.9913  0.0007  0.1025\n",
      "     41            1.0000        \u001b[32m0.0517\u001b[0m       0.5521            0.5521        1.0004  0.0007  0.1225\n",
      "     42            1.0000        \u001b[32m0.0309\u001b[0m       0.5417            0.5417        1.0117  0.0007  0.0996\n",
      "     43            1.0000        0.0392       0.5417            0.5417        1.0228  0.0006  0.1019\n",
      "     44            1.0000        0.0448       0.5208            0.5208        1.0314  0.0006  0.0979\n",
      "     45            1.0000        \u001b[32m0.0273\u001b[0m       0.5312            0.5312        1.0386  0.0005  0.1019\n",
      "     46            1.0000        \u001b[32m0.0239\u001b[0m       0.5521            0.5521        1.0451  0.0005  0.0902\n",
      "     47            1.0000        0.0291       0.5625            0.5625        1.0508  0.0004  0.1197\n",
      "     48            1.0000        \u001b[32m0.0193\u001b[0m       0.5625            0.5625        1.0556  0.0004  0.1454\n",
      "     49            1.0000        0.0386       0.5521            0.5521        1.0595  0.0003  0.1155\n",
      "     50            1.0000        \u001b[32m0.0167\u001b[0m       0.5521            0.5521        1.0625  0.0003  0.1415\n",
      "Fine tuning model for subject 1 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        1.1522       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.9278\u001b[0m  0.0004  0.1302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        1.1719       0.5729            0.5729        \u001b[94m0.9244\u001b[0m  0.0005  0.1479\n",
      "     33            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6294\u001b[0m       0.5729            0.5729        0.9401  0.0005  0.1428\n",
      "     34            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4998\u001b[0m       0.5417            0.5417        0.9675  0.0006  0.1136\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2161\u001b[0m       0.5521            0.5521        1.0008  0.0006  0.1198\n",
      "     36            1.0000        \u001b[32m0.2089\u001b[0m       0.5521            0.5521        1.0347  0.0007  0.1354\n",
      "     37            1.0000        \u001b[32m0.1036\u001b[0m       0.5417            0.5417        1.0630  0.0007  0.1345\n",
      "     38            1.0000        \u001b[32m0.0726\u001b[0m       0.5625            0.5625        1.0840  0.0007  0.1270\n",
      "     39            1.0000        0.0742       0.5625            0.5625        1.0984  0.0007  0.1049\n",
      "     40            1.0000        \u001b[32m0.0485\u001b[0m       0.5417            0.5417        1.1060  0.0007  0.1198\n",
      "     41            1.0000        0.0762       0.5417            0.5417        1.1091  0.0007  0.1174\n",
      "     42            1.0000        0.0710       0.5521            0.5521        1.1092  0.0007  0.1358\n",
      "     43            1.0000        0.0540       0.5625            0.5625        1.1062  0.0006  0.1513\n",
      "     44            1.0000        0.0626       0.5625            0.5625        1.1043  0.0006  0.1352\n",
      "     45            1.0000        \u001b[32m0.0344\u001b[0m       0.5625            0.5625        1.1001  0.0005  0.1512\n",
      "     46            1.0000        \u001b[32m0.0228\u001b[0m       0.5417            0.5417        1.0950  0.0005  0.1093\n",
      "     47            1.0000        0.0254       0.5312            0.5312        1.0889  0.0004  0.0986\n",
      "     48            1.0000        \u001b[32m0.0207\u001b[0m       0.5417            0.5417        1.0824  0.0004  0.0973\n",
      "     49            1.0000        0.0360       0.5312            0.5312        1.0761  0.0003  0.1010\n",
      "     50            1.0000        0.0233       0.5104            0.5104        1.0698  0.0003  0.1234\n",
      "Fine tuning model for subject 1 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.1205       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9262\u001b[0m  0.0004  0.1125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.9333\u001b[0m        \u001b[32m0.7316\u001b[0m       0.5938            0.5938        \u001b[94m0.9188\u001b[0m  0.0005  0.1079\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6427\u001b[0m       0.5729            0.5729        0.9289  0.0005  0.1172\n",
      "     34            1.0000        \u001b[32m0.3749\u001b[0m       0.5521            0.5521        0.9558  0.0006  0.0957\n",
      "     35            1.0000        \u001b[32m0.2979\u001b[0m       0.5312            0.5312        0.9883  0.0006  0.1042\n",
      "     36            1.0000        \u001b[32m0.2290\u001b[0m       0.5208            0.5208        1.0149  0.0007  0.0818\n",
      "     37            1.0000        \u001b[32m0.1198\u001b[0m       0.5104            0.5104        1.0320  0.0007  0.1020\n",
      "     38            1.0000        0.1211       0.5208            0.5208        1.0373  0.0007  0.0980\n",
      "     39            1.0000        \u001b[32m0.0961\u001b[0m       0.5312            0.5312        1.0384  0.0007  0.0865\n",
      "     40            1.0000        \u001b[32m0.0452\u001b[0m       0.5417            0.5417        1.0391  0.0007  0.1008\n",
      "     41            1.0000        0.0604       0.5312            0.5312        1.0396  0.0007  0.1021\n",
      "     42            1.0000        \u001b[32m0.0438\u001b[0m       0.5000            0.5000        1.0422  0.0007  0.1002\n",
      "     43            1.0000        0.0485       0.5208            0.5208        1.0461  0.0006  0.0975\n",
      "     44            1.0000        \u001b[32m0.0334\u001b[0m       0.5208            0.5208        1.0500  0.0006  0.1145\n",
      "     45            1.0000        0.0433       0.5208            0.5208        1.0542  0.0005  0.1051\n",
      "     46            1.0000        0.0357       0.5104            0.5104        1.0585  0.0005  0.1054\n",
      "     47            1.0000        \u001b[32m0.0313\u001b[0m       0.5000            0.5000        1.0635  0.0004  0.0969\n",
      "     48            1.0000        \u001b[32m0.0246\u001b[0m       0.5104            0.5104        1.0691  0.0004  0.1177\n",
      "     49            1.0000        0.0277       0.5104            0.5104        1.0752  0.0003  0.1124\n",
      "     50            1.0000        0.0276       0.5104            0.5104        1.0816  0.0003  0.1006\n",
      "Fine tuning model for subject 1 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7333        0.8668       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9342\u001b[0m  0.0004  0.1095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        \u001b[32m0.7211\u001b[0m       0.5833            0.5833        \u001b[94m0.9320\u001b[0m  0.0005  0.1175\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.8031       0.5417            0.5417        0.9441  0.0005  0.1112\n",
      "     34            1.0000        \u001b[32m0.3364\u001b[0m       0.5417            0.5417        0.9745  0.0006  0.1033\n",
      "     35            1.0000        \u001b[32m0.2196\u001b[0m       0.5417            0.5417        1.0117  0.0006  0.0979\n",
      "     36            1.0000        \u001b[32m0.1913\u001b[0m       0.5104            0.5104        1.0533  0.0007  0.0985\n",
      "     37            1.0000        \u001b[32m0.1498\u001b[0m       0.5312            0.5312        1.0999  0.0007  0.0917\n",
      "     38            1.0000        \u001b[32m0.0590\u001b[0m       0.5312            0.5312        1.1470  0.0007  0.0865\n",
      "     39            1.0000        \u001b[32m0.0380\u001b[0m       0.5208            0.5208        1.1928  0.0007  0.1061\n",
      "     40            1.0000        0.0490       0.5000            0.5000        1.2337  0.0007  0.1158\n",
      "     41            1.0000        0.0478       0.5000            0.5000        1.2678  0.0007  0.0849\n",
      "     42            1.0000        0.0389       0.5104            0.5104        1.2942  0.0007  0.1009\n",
      "     43            1.0000        0.0388       0.5104            0.5104        1.3103  0.0006  0.0987\n",
      "     44            1.0000        \u001b[32m0.0198\u001b[0m       0.5208            0.5208        1.3199  0.0006  0.0968\n",
      "     45            1.0000        \u001b[32m0.0189\u001b[0m       0.5417            0.5417        1.3237  0.0005  0.1017\n",
      "     46            1.0000        \u001b[32m0.0147\u001b[0m       0.5417            0.5417        1.3227  0.0005  0.0978\n",
      "     47            1.0000        0.0163       0.5417            0.5417        1.3182  0.0004  0.0875\n",
      "     48            1.0000        \u001b[32m0.0125\u001b[0m       0.5417            0.5417        1.3108  0.0004  0.0858\n",
      "     49            1.0000        0.0137       0.5312            0.5312        1.3018  0.0003  0.0997\n",
      "     50            1.0000        \u001b[32m0.0112\u001b[0m       0.5208            0.5208        1.2922  0.0003  0.1013\n",
      "Fine tuning model for subject 1 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7333        0.8030       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9261\u001b[0m  0.0004  0.0939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        \u001b[32m0.6363\u001b[0m       0.6250            0.6250        \u001b[94m0.9090\u001b[0m  0.0005  0.0975\n",
      "     33            \u001b[36m0.8667\u001b[0m        \u001b[32m0.4788\u001b[0m       0.6042            0.6042        \u001b[94m0.9004\u001b[0m  0.0005  0.0938\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4046\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        0.9063  0.0006  0.0994\n",
      "     35            1.0000        \u001b[32m0.1958\u001b[0m       0.6354            0.6354        0.9384  0.0006  0.1166\n",
      "     36            1.0000        \u001b[32m0.1309\u001b[0m       0.5833            0.5833        0.9981  0.0007  0.0945\n",
      "     37            1.0000        \u001b[32m0.0767\u001b[0m       0.5521            0.5521        1.0813  0.0007  0.0937\n",
      "     38            1.0000        0.0931       0.5417            0.5417        1.1789  0.0007  0.1052\n",
      "     39            1.0000        \u001b[32m0.0554\u001b[0m       0.5312            0.5312        1.2723  0.0007  0.0979\n",
      "     40            1.0000        \u001b[32m0.0522\u001b[0m       0.5417            0.5417        1.3554  0.0007  0.0990\n",
      "     41            1.0000        \u001b[32m0.0462\u001b[0m       0.5417            0.5417        1.4198  0.0007  0.0853\n",
      "     42            1.0000        \u001b[32m0.0288\u001b[0m       0.5521            0.5521        1.4664  0.0007  0.0998\n",
      "     43            1.0000        0.0431       0.5521            0.5521        1.5068  0.0006  0.1116\n",
      "     44            1.0000        \u001b[32m0.0280\u001b[0m       0.5521            0.5521        1.5295  0.0006  0.0980\n",
      "     45            1.0000        \u001b[32m0.0222\u001b[0m       0.5417            0.5417        1.5370  0.0005  0.0855\n",
      "     46            1.0000        \u001b[32m0.0213\u001b[0m       0.5417            0.5417        1.5321  0.0005  0.0984\n",
      "     47            1.0000        0.0225       0.5417            0.5417        1.5187  0.0004  0.0852\n",
      "     48            1.0000        \u001b[32m0.0143\u001b[0m       0.5312            0.5312        1.5004  0.0004  0.1004\n",
      "     49            1.0000        0.0245       0.5208            0.5208        1.4794  0.0003  0.1015\n",
      "     50            1.0000        0.0212       0.5208            0.5208        1.4566  0.0003  0.0863\n",
      "Fine tuning model for subject 1 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4667        1.0291       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9292\u001b[0m  0.0004  0.0838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6667        1.2343       0.5833            0.5833        \u001b[94m0.9225\u001b[0m  0.0005  0.1020\n",
      "     33            \u001b[36m0.8000\u001b[0m        0.8952       0.5729            0.5729        0.9334  0.0005  0.0938\n",
      "     34            \u001b[36m0.9333\u001b[0m        0.8597       0.5521            0.5521        0.9645  0.0006  0.0908\n",
      "     35            0.9333        \u001b[32m0.4339\u001b[0m       0.5104            0.5104        1.0184  0.0006  0.0986\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2545\u001b[0m       0.5208            0.5208        1.0872  0.0007  0.0860\n",
      "     37            1.0000        \u001b[32m0.2092\u001b[0m       0.5312            0.5312        1.1769  0.0007  0.1217\n",
      "     38            1.0000        \u001b[32m0.1639\u001b[0m       0.5208            0.5208        1.2659  0.0007  0.1038\n",
      "     39            1.0000        \u001b[32m0.0909\u001b[0m       0.5208            0.5208        1.3477  0.0007  0.0963\n",
      "     40            1.0000        \u001b[32m0.0707\u001b[0m       0.5208            0.5208        1.4205  0.0007  0.1096\n",
      "     41            1.0000        0.0898       0.5312            0.5312        1.4807  0.0007  0.1164\n",
      "     42            1.0000        \u001b[32m0.0495\u001b[0m       0.5417            0.5417        1.5293  0.0007  0.0973\n",
      "     43            1.0000        \u001b[32m0.0413\u001b[0m       0.5312            0.5312        1.5610  0.0006  0.0926\n",
      "     44            1.0000        0.0535       0.5312            0.5312        1.5773  0.0006  0.0876\n",
      "     45            1.0000        0.0476       0.5312            0.5312        1.5778  0.0005  0.0995\n",
      "     46            1.0000        \u001b[32m0.0248\u001b[0m       0.5312            0.5312        1.5675  0.0005  0.0996\n",
      "     47            1.0000        0.0382       0.5312            0.5312        1.5485  0.0004  0.0838\n",
      "     48            1.0000        0.0350       0.5104            0.5104        1.5241  0.0004  0.1064\n",
      "     49            1.0000        0.0451       0.5208            0.5208        1.4965  0.0003  0.1034\n",
      "     50            1.0000        0.0322       0.5208            0.5208        1.4690  0.0003  0.1172\n",
      "Fine tuning model for subject 1 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5333        1.3781       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m0.9361\u001b[0m  0.0004  0.0886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.1264       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.9337\u001b[0m  0.0005  0.0979\n",
      "     33            \u001b[36m0.8000\u001b[0m        1.1373       0.6250            0.6250        0.9393  0.0005  0.0937\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6238\u001b[0m       0.5938            0.5938        0.9615  0.0006  0.0835\n",
      "     35            1.0000        \u001b[32m0.4220\u001b[0m       0.5104            0.5104        1.0087  0.0006  0.1197\n",
      "     36            1.0000        \u001b[32m0.3168\u001b[0m       0.5104            0.5104        1.0759  0.0007  0.1038\n",
      "     37            1.0000        \u001b[32m0.1551\u001b[0m       0.4688            0.4688        1.1511  0.0007  0.1270\n",
      "     38            1.0000        \u001b[32m0.1413\u001b[0m       0.4792            0.4792        1.2279  0.0007  0.0847\n",
      "     39            1.0000        \u001b[32m0.0605\u001b[0m       0.4583            0.4583        1.3029  0.0007  0.1052\n",
      "     40            1.0000        0.1101       0.4375            0.4375        1.3617  0.0007  0.0813\n",
      "     41            1.0000        \u001b[32m0.0411\u001b[0m       0.4167            0.4167        1.4113  0.0007  0.0989\n",
      "     42            1.0000        0.0646       0.4271            0.4271        1.4483  0.0007  0.0860\n",
      "     43            1.0000        0.0587       0.4479            0.4479        1.4713  0.0006  0.0995\n",
      "     44            1.0000        0.0632       0.4375            0.4375        1.4826  0.0006  0.1252\n",
      "     45            1.0000        \u001b[32m0.0374\u001b[0m       0.4375            0.4375        1.4844  0.0005  0.1067\n",
      "     46            1.0000        \u001b[32m0.0261\u001b[0m       0.4375            0.4375        1.4787  0.0005  0.0941\n",
      "     47            1.0000        \u001b[32m0.0259\u001b[0m       0.4479            0.4479        1.4678  0.0004  0.1166\n",
      "     48            1.0000        0.0427       0.4479            0.4479        1.4532  0.0004  0.0944\n",
      "     49            1.0000        0.0373       0.4583            0.4583        1.4364  0.0003  0.1020\n",
      "     50            1.0000        0.0296       0.4583            0.4583        1.4188  0.0003  0.0835\n",
      "Fine tuning model for subject 1 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7333        1.1490       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9394\u001b[0m  0.0004  0.0990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7333        1.0712       0.6042            0.6042        \u001b[94m0.9389\u001b[0m  0.0005  0.0834\n",
      "     33            \u001b[36m0.9333\u001b[0m        \u001b[32m0.7325\u001b[0m       0.5833            0.5833        0.9470  0.0005  0.1246\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6570\u001b[0m       0.5729            0.5729        0.9599  0.0006  0.1161\n",
      "     35            1.0000        \u001b[32m0.3014\u001b[0m       0.5625            0.5625        0.9719  0.0006  0.0961\n",
      "     36            1.0000        \u001b[32m0.1162\u001b[0m       0.5729            0.5729        0.9852  0.0007  0.0828\n",
      "     37            1.0000        0.1162       0.5521            0.5521        1.0016  0.0007  0.1010\n",
      "     38            1.0000        0.1322       0.5104            0.5104        1.0204  0.0007  0.1110\n",
      "     39            1.0000        \u001b[32m0.0442\u001b[0m       0.5104            0.5104        1.0485  0.0007  0.1479\n",
      "     40            1.0000        0.0721       0.5208            0.5208        1.0854  0.0007  0.1200\n",
      "     41            1.0000        \u001b[32m0.0398\u001b[0m       0.5312            0.5312        1.1252  0.0007  0.1198\n",
      "     42            1.0000        \u001b[32m0.0221\u001b[0m       0.5104            0.5104        1.1622  0.0007  0.1187\n",
      "     43            1.0000        \u001b[32m0.0174\u001b[0m       0.4896            0.4896        1.1923  0.0006  0.1299\n",
      "     44            1.0000        \u001b[32m0.0084\u001b[0m       0.4896            0.4896        1.2131  0.0006  0.1198\n",
      "     45            1.0000        0.0264       0.4896            0.4896        1.2246  0.0005  0.1169\n",
      "     46            1.0000        0.0163       0.4792            0.4792        1.2271  0.0005  0.1280\n",
      "     47            1.0000        0.0141       0.4688            0.4688        1.2231  0.0004  0.1303\n",
      "     48            1.0000        0.0110       0.4688            0.4688        1.2150  0.0004  0.1549\n",
      "     49            1.0000        \u001b[32m0.0077\u001b[0m       0.4792            0.4792        1.2047  0.0003  0.1303\n",
      "     50            1.0000        \u001b[32m0.0069\u001b[0m       0.4583            0.4583        1.1946  0.0003  0.1327\n",
      "Fine tuning model for subject 1 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7333        1.1719       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.9339\u001b[0m  0.0004  0.1281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        1.0335       0.6354            0.6354        \u001b[94m0.9298\u001b[0m  0.0005  0.1337\n",
      "     33            \u001b[36m0.9333\u001b[0m        0.8820       0.6146            0.6146        0.9459  0.0005  0.1597\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5939\u001b[0m       0.5938            0.5938        0.9744  0.0006  0.1060\n",
      "     35            1.0000        \u001b[32m0.2203\u001b[0m       0.5729            0.5729        1.0138  0.0006  0.1287\n",
      "     36            1.0000        \u001b[32m0.1905\u001b[0m       0.5312            0.5312        1.0643  0.0007  0.1293\n",
      "     37            1.0000        \u001b[32m0.1250\u001b[0m       0.5312            0.5312        1.1181  0.0007  0.1024\n",
      "     38            1.0000        0.1385       0.5104            0.5104        1.1724  0.0007  0.1166\n",
      "     39            1.0000        \u001b[32m0.0934\u001b[0m       0.5104            0.5104        1.2255  0.0007  0.1199\n",
      "     40            1.0000        \u001b[32m0.0847\u001b[0m       0.4896            0.4896        1.2693  0.0007  0.1328\n",
      "     41            1.0000        \u001b[32m0.0489\u001b[0m       0.4688            0.4688        1.3119  0.0007  0.0839\n",
      "     42            1.0000        \u001b[32m0.0429\u001b[0m       0.4688            0.4688        1.3503  0.0007  0.0862\n",
      "     43            1.0000        \u001b[32m0.0363\u001b[0m       0.4583            0.4583        1.3841  0.0006  0.1060\n",
      "     44            1.0000        0.0762       0.4688            0.4688        1.4092  0.0006  0.0807\n",
      "     45            1.0000        \u001b[32m0.0284\u001b[0m       0.4688            0.4688        1.4275  0.0005  0.0995\n",
      "     46            1.0000        0.0350       0.4583            0.4583        1.4396  0.0005  0.1014\n",
      "     47            1.0000        0.0297       0.4479            0.4479        1.4458  0.0004  0.0995\n",
      "     48            1.0000        0.0773       0.4375            0.4375        1.4496  0.0004  0.1015\n",
      "     49            1.0000        0.0400       0.4375            0.4375        1.4490  0.0003  0.1011\n",
      "     50            1.0000        \u001b[32m0.0190\u001b[0m       0.4375            0.4375        1.4444  0.0003  0.1116\n",
      "Fine tuning model for subject 1 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        0.8216       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9374\u001b[0m  0.0004  0.1011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        0.9677       0.6146            0.6146        \u001b[94m0.9339\u001b[0m  0.0005  0.0966\n",
      "     33            \u001b[36m0.8000\u001b[0m        \u001b[32m0.6656\u001b[0m       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        0.9366  0.0005  0.1094\n",
      "     34            \u001b[36m0.8500\u001b[0m        0.7060       0.6042            0.6042        0.9512  0.0006  0.1170\n",
      "     35            0.8500        \u001b[32m0.5187\u001b[0m       0.6042            0.6042        0.9785  0.0006  0.1166\n",
      "     36            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2932\u001b[0m       0.5938            0.5938        1.0107  0.0007  0.1125\n",
      "     37            0.9500        0.3878       0.5938            0.5938        1.0295  0.0007  0.0933\n",
      "     38            0.9500        \u001b[32m0.2194\u001b[0m       0.5833            0.5833        1.0274  0.0007  0.1064\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2108\u001b[0m       0.5625            0.5625        1.0039  0.0007  0.1000\n",
      "     40            1.0000        0.2217       0.5729            0.5729        0.9813  0.0007  0.0950\n",
      "     41            1.0000        \u001b[32m0.1568\u001b[0m       0.5729            0.5729        0.9685  0.0007  0.0965\n",
      "     42            1.0000        \u001b[32m0.0701\u001b[0m       0.5729            0.5729        0.9651  0.0007  0.1012\n",
      "     43            1.0000        0.1186       0.5729            0.5729        0.9648  0.0006  0.1019\n",
      "     44            1.0000        \u001b[32m0.0570\u001b[0m       0.5729            0.5729        0.9664  0.0006  0.1007\n",
      "     45            1.0000        0.0988       0.5729            0.5729        0.9696  0.0005  0.1007\n",
      "     46            1.0000        \u001b[32m0.0440\u001b[0m       0.5417            0.5417        0.9722  0.0005  0.1008\n",
      "     47            1.0000        0.0461       0.5521            0.5521        0.9743  0.0004  0.1035\n",
      "     48            1.0000        0.0488       0.5521            0.5521        0.9757  0.0004  0.1230\n",
      "     49            1.0000        0.0667       0.5521            0.5521        0.9768  0.0003  0.1218\n",
      "     50            1.0000        \u001b[32m0.0408\u001b[0m       0.5417            0.5417        0.9781  0.0003  0.0870\n",
      "Fine tuning model for subject 1 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6500        1.1453       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9304\u001b[0m  0.0004  0.0940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        0.8817       0.5833            0.5833        0.9332  0.0005  0.0994\n",
      "     33            \u001b[36m0.8500\u001b[0m        \u001b[32m0.6634\u001b[0m       0.5729            0.5729        0.9673  0.0005  0.1094\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4877\u001b[0m       0.5521            0.5521        1.0162  0.0006  0.0880\n",
      "     35            1.0000        \u001b[32m0.3683\u001b[0m       0.5417            0.5417        1.0729  0.0006  0.1224\n",
      "     36            1.0000        \u001b[32m0.2699\u001b[0m       0.5625            0.5625        1.1256  0.0007  0.1162\n",
      "     37            1.0000        \u001b[32m0.1790\u001b[0m       0.5417            0.5417        1.1693  0.0007  0.1125\n",
      "     38            1.0000        \u001b[32m0.1719\u001b[0m       0.5208            0.5208        1.2036  0.0007  0.1009\n",
      "     39            1.0000        \u001b[32m0.1070\u001b[0m       0.5417            0.5417        1.2302  0.0007  0.0974\n",
      "     40            1.0000        0.1082       0.5417            0.5417        1.2511  0.0007  0.0970\n",
      "     41            1.0000        \u001b[32m0.0941\u001b[0m       0.5521            0.5521        1.2671  0.0007  0.1173\n",
      "     42            1.0000        0.1089       0.5521            0.5521        1.2863  0.0007  0.0990\n",
      "     43            1.0000        \u001b[32m0.0882\u001b[0m       0.5521            0.5521        1.3100  0.0006  0.1170\n",
      "     44            1.0000        \u001b[32m0.0697\u001b[0m       0.5417            0.5417        1.3355  0.0006  0.1061\n",
      "     45            1.0000        0.0799       0.5521            0.5521        1.3639  0.0005  0.1067\n",
      "     46            1.0000        \u001b[32m0.0528\u001b[0m       0.5417            0.5417        1.3879  0.0005  0.0976\n",
      "     47            1.0000        \u001b[32m0.0345\u001b[0m       0.5312            0.5312        1.4082  0.0004  0.1200\n",
      "     48            1.0000        0.0522       0.5208            0.5208        1.4243  0.0004  0.1313\n",
      "     49            1.0000        \u001b[32m0.0293\u001b[0m       0.5208            0.5208        1.4377  0.0003  0.1108\n",
      "     50            1.0000        \u001b[32m0.0256\u001b[0m       0.5208            0.5208        1.4485  0.0003  0.1037\n",
      "Fine tuning model for subject 1 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        0.8745       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9310\u001b[0m  0.0004  0.1155\n",
      "     32            \u001b[36m0.8000\u001b[0m        0.7878       0.5729            0.5729        \u001b[94m0.9291\u001b[0m  0.0005  0.1150\n",
      "     33            \u001b[36m0.8500\u001b[0m        \u001b[32m0.7342\u001b[0m       0.5729            0.5729        0.9476  0.0005  0.1036\n",
      "     34            \u001b[36m0.9500\u001b[0m        \u001b[32m0.6199\u001b[0m       0.5521            0.5521        0.9895  0.0006  0.1168\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4041\u001b[0m       0.5625            0.5625        1.0457  0.0006  0.0943\n",
      "     36            1.0000        \u001b[32m0.3087\u001b[0m       0.5729            0.5729        1.1003  0.0007  0.1068\n",
      "     37            1.0000        \u001b[32m0.2210\u001b[0m       0.5417            0.5417        1.1385  0.0007  0.1136\n",
      "     38            1.0000        \u001b[32m0.1347\u001b[0m       0.5417            0.5417        1.1571  0.0007  0.0995\n",
      "     39            1.0000        \u001b[32m0.1096\u001b[0m       0.5417            0.5417        1.1624  0.0007  0.1093\n",
      "     40            1.0000        0.1365       0.5417            0.5417        1.1597  0.0007  0.0989\n",
      "     41            1.0000        \u001b[32m0.0662\u001b[0m       0.5208            0.5208        1.1524  0.0007  0.1010\n",
      "     42            1.0000        \u001b[32m0.0416\u001b[0m       0.5208            0.5208        1.1439  0.0007  0.1400\n",
      "     43            1.0000        0.0776       0.5208            0.5208        1.1360  0.0006  0.1246\n",
      "     44            1.0000        0.0527       0.5208            0.5208        1.1284  0.0006  0.1052\n",
      "     45            1.0000        \u001b[32m0.0347\u001b[0m       0.5208            0.5208        1.1211  0.0005  0.1081\n",
      "     46            1.0000        \u001b[32m0.0313\u001b[0m       0.5312            0.5312        1.1144  0.0005  0.1021\n",
      "     47            1.0000        0.0527       0.5312            0.5312        1.1080  0.0004  0.0989\n",
      "     48            1.0000        0.0353       0.5521            0.5521        1.1015  0.0004  0.1229\n",
      "     49            1.0000        0.0367       0.5521            0.5521        1.0954  0.0003  0.0985\n",
      "     50            1.0000        0.0407       0.5625            0.5625        1.0897  0.0003  0.0891\n",
      "Fine tuning model for subject 1 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6500        1.0883       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9266\u001b[0m  0.0004  0.1129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7500        0.8958       0.5833            0.5833        \u001b[94m0.9160\u001b[0m  0.0005  0.1304\n",
      "     33            \u001b[36m0.9000\u001b[0m        0.7662       0.5625            0.5625        0.9220  0.0005  0.1072\n",
      "     34            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5246\u001b[0m       0.5625            0.5625        0.9461  0.0006  0.1178\n",
      "     35            0.9500        \u001b[32m0.4504\u001b[0m       0.5417            0.5417        0.9787  0.0006  0.1185\n",
      "     36            \u001b[36m1.0000\u001b[0m        0.4907       0.5208            0.5208        1.0159  0.0007  0.1106\n",
      "     37            1.0000        \u001b[32m0.1918\u001b[0m       0.5104            0.5104        1.0483  0.0007  0.1171\n",
      "     38            1.0000        \u001b[32m0.1021\u001b[0m       0.5000            0.5000        1.0717  0.0007  0.1028\n",
      "     39            1.0000        0.1213       0.4792            0.4792        1.0849  0.0007  0.1230\n",
      "     40            1.0000        \u001b[32m0.0551\u001b[0m       0.4792            0.4792        1.0881  0.0007  0.1171\n",
      "     41            1.0000        0.0610       0.5000            0.5000        1.0857  0.0007  0.1008\n",
      "     42            1.0000        0.0651       0.5312            0.5312        1.0811  0.0007  0.1323\n",
      "     43            1.0000        \u001b[32m0.0402\u001b[0m       0.5208            0.5208        1.0748  0.0006  0.0909\n",
      "     44            1.0000        0.0538       0.5312            0.5312        1.0691  0.0006  0.1063\n",
      "     45            1.0000        \u001b[32m0.0381\u001b[0m       0.5312            0.5312        1.0640  0.0005  0.1153\n",
      "     46            1.0000        0.0401       0.5312            0.5312        1.0606  0.0005  0.1138\n",
      "     47            1.0000        \u001b[32m0.0313\u001b[0m       0.5312            0.5312        1.0595  0.0004  0.1147\n",
      "     48            1.0000        \u001b[32m0.0207\u001b[0m       0.5312            0.5312        1.0604  0.0004  0.1247\n",
      "     49            1.0000        0.0399       0.5312            0.5312        1.0633  0.0003  0.1112\n",
      "     50            1.0000        0.0277       0.5417            0.5417        1.0673  0.0003  0.1016\n",
      "Fine tuning model for subject 1 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        \u001b[32m0.7267\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9368\u001b[0m  0.0004  0.1169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8500\u001b[0m        0.8658       0.5938            0.5938        \u001b[94m0.9325\u001b[0m  0.0005  0.1197\n",
      "     33            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5223\u001b[0m       0.6042            0.6042        0.9406  0.0005  0.1175\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4739\u001b[0m       0.5625            0.5625        0.9667  0.0006  0.1130\n",
      "     35            1.0000        0.5066       0.5417            0.5417        1.0210  0.0006  0.1056\n",
      "     36            1.0000        \u001b[32m0.2377\u001b[0m       0.5104            0.5104        1.0915  0.0007  0.0924\n",
      "     37            1.0000        \u001b[32m0.1679\u001b[0m       0.5104            0.5104        1.1563  0.0007  0.1031\n",
      "     38            1.0000        \u001b[32m0.1150\u001b[0m       0.4792            0.4792        1.2091  0.0007  0.1167\n",
      "     39            1.0000        \u001b[32m0.0859\u001b[0m       0.4896            0.4896        1.2467  0.0007  0.1042\n",
      "     40            1.0000        \u001b[32m0.0735\u001b[0m       0.4792            0.4792        1.2709  0.0007  0.1252\n",
      "     41            1.0000        0.0760       0.4583            0.4583        1.2801  0.0007  0.1117\n",
      "     42            1.0000        0.0962       0.4375            0.4375        1.2860  0.0007  0.1280\n",
      "     43            1.0000        \u001b[32m0.0676\u001b[0m       0.4479            0.4479        1.2853  0.0006  0.1281\n",
      "     44            1.0000        \u001b[32m0.0445\u001b[0m       0.4583            0.4583        1.2802  0.0006  0.1324\n",
      "     45            1.0000        \u001b[32m0.0389\u001b[0m       0.4583            0.4583        1.2742  0.0005  0.1403\n",
      "     46            1.0000        0.0475       0.4583            0.4583        1.2676  0.0005  0.1483\n",
      "     47            1.0000        0.0661       0.4375            0.4375        1.2602  0.0004  0.1354\n",
      "     48            1.0000        \u001b[32m0.0332\u001b[0m       0.4479            0.4479        1.2520  0.0004  0.1511\n",
      "     49            1.0000        \u001b[32m0.0243\u001b[0m       0.4583            0.4583        1.2437  0.0003  0.1676\n",
      "     50            1.0000        0.0376       0.4688            0.4688        1.2354  0.0003  0.1355\n",
      "Fine tuning model for subject 1 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5500        1.0178       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9272\u001b[0m  0.0004  0.1282\n",
      "     32            0.6500        0.9210       0.5729            0.5729        \u001b[94m0.9137\u001b[0m  0.0005  0.1413\n",
      "     33            \u001b[36m0.8000\u001b[0m        0.8126       0.5625            0.5625        \u001b[94m0.9058\u001b[0m  0.0005  0.1349\n",
      "     34            \u001b[36m0.9500\u001b[0m        0.7973       0.5833            0.5833        \u001b[94m0.9017\u001b[0m  0.0006  0.1500\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4104\u001b[0m       0.5521            0.5521        \u001b[94m0.9001\u001b[0m  0.0006  0.1364\n",
      "     36            1.0000        \u001b[32m0.3890\u001b[0m       0.5312            0.5312        0.9013  0.0007  0.1645\n",
      "     37            1.0000        \u001b[32m0.2785\u001b[0m       0.5521            0.5521        0.9105  0.0007  0.1039\n",
      "     38            1.0000        \u001b[32m0.1959\u001b[0m       0.5312            0.5312        0.9227  0.0007  0.1047\n",
      "     39            1.0000        \u001b[32m0.1524\u001b[0m       0.5312            0.5312        0.9347  0.0007  0.1091\n",
      "     40            1.0000        \u001b[32m0.0903\u001b[0m       0.5312            0.5312        0.9468  0.0007  0.1061\n",
      "     41            1.0000        0.0926       0.5208            0.5208        0.9582  0.0007  0.1159\n",
      "     42            1.0000        \u001b[32m0.0903\u001b[0m       0.5208            0.5208        0.9683  0.0007  0.1042\n",
      "     43            1.0000        \u001b[32m0.0740\u001b[0m       0.5104            0.5104        0.9766  0.0006  0.1189\n",
      "     44            1.0000        \u001b[32m0.0686\u001b[0m       0.5208            0.5208        0.9811  0.0006  0.0986\n",
      "     45            1.0000        \u001b[32m0.0593\u001b[0m       0.5208            0.5208        0.9842  0.0005  0.0946\n",
      "     46            1.0000        \u001b[32m0.0452\u001b[0m       0.5312            0.5312        0.9861  0.0005  0.1179\n",
      "     47            1.0000        0.0564       0.5312            0.5312        0.9872  0.0004  0.1086\n",
      "     48            1.0000        0.0542       0.5521            0.5521        0.9881  0.0004  0.0911\n",
      "     49            1.0000        \u001b[32m0.0273\u001b[0m       0.5521            0.5521        0.9889  0.0003  0.1165\n",
      "     50            1.0000        0.0441       0.5521            0.5521        0.9901  0.0003  0.1109\n",
      "Fine tuning model for subject 1 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6500        1.2038       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9344\u001b[0m  0.0004  0.1044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        1.0790       0.6250            0.6250        0.9346  0.0005  0.1156\n",
      "     33            \u001b[36m0.8000\u001b[0m        0.9382       0.6042            0.6042        0.9503  0.0005  0.0990\n",
      "     34            \u001b[36m0.9500\u001b[0m        \u001b[32m0.6586\u001b[0m       0.5938            0.5938        0.9854  0.0006  0.1026\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3871\u001b[0m       0.5417            0.5417        1.0420  0.0006  0.1236\n",
      "     36            1.0000        \u001b[32m0.3499\u001b[0m       0.5104            0.5104        1.1155  0.0007  0.0906\n",
      "     37            1.0000        \u001b[32m0.2510\u001b[0m       0.5000            0.5000        1.1913  0.0007  0.1021\n",
      "     38            1.0000        \u001b[32m0.1718\u001b[0m       0.5104            0.5104        1.2644  0.0007  0.1023\n",
      "     39            1.0000        \u001b[32m0.1467\u001b[0m       0.4688            0.4688        1.3278  0.0007  0.1095\n",
      "     40            1.0000        \u001b[32m0.1434\u001b[0m       0.4688            0.4688        1.3735  0.0007  0.0896\n",
      "     41            1.0000        \u001b[32m0.0919\u001b[0m       0.4583            0.4583        1.4076  0.0007  0.1179\n",
      "     42            1.0000        \u001b[32m0.0689\u001b[0m       0.4583            0.4583        1.4290  0.0007  0.1113\n",
      "     43            1.0000        \u001b[32m0.0328\u001b[0m       0.4479            0.4479        1.4407  0.0006  0.1143\n",
      "     44            1.0000        0.0476       0.4583            0.4583        1.4433  0.0006  0.0983\n",
      "     45            1.0000        0.0385       0.4688            0.4688        1.4387  0.0005  0.1015\n",
      "     46            1.0000        0.0498       0.4688            0.4688        1.4288  0.0005  0.1199\n",
      "     47            1.0000        0.0378       0.4688            0.4688        1.4150  0.0004  0.1044\n",
      "     48            1.0000        0.0396       0.4792            0.4792        1.3986  0.0004  0.0935\n",
      "     49            1.0000        0.0355       0.4792            0.4792        1.3797  0.0003  0.1013\n",
      "     50            1.0000        0.0378       0.4792            0.4792        1.3595  0.0003  0.1210\n",
      "Fine tuning model for subject 1 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6500        0.9718       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9264\u001b[0m  0.0004  0.1147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        0.8926       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9028\u001b[0m  0.0005  0.1081\n",
      "     33            0.8000        \u001b[32m0.6210\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.8814\u001b[0m  0.0005  0.1137\n",
      "     34            \u001b[36m1.0000\u001b[0m        0.6552       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8734\u001b[0m  0.0006  0.1092\n",
      "     35            1.0000        \u001b[32m0.3782\u001b[0m       0.6354            0.6354        0.8820  0.0006  0.0990\n",
      "     36            1.0000        \u001b[32m0.3540\u001b[0m       0.5938            0.5938        0.9039  0.0007  0.0861\n",
      "     37            1.0000        \u001b[32m0.1938\u001b[0m       0.5938            0.5938        0.9298  0.0007  0.1054\n",
      "     38            1.0000        \u001b[32m0.1669\u001b[0m       0.6146            0.6146        0.9557  0.0007  0.1062\n",
      "     39            1.0000        \u001b[32m0.1645\u001b[0m       0.6146            0.6146        0.9830  0.0007  0.0999\n",
      "     40            1.0000        0.2110       0.6250            0.6250        1.0024  0.0007  0.1187\n",
      "     41            1.0000        \u001b[32m0.0835\u001b[0m       0.6042            0.6042        1.0265  0.0007  0.1030\n",
      "     42            1.0000        \u001b[32m0.0556\u001b[0m       0.5833            0.5833        1.0432  0.0007  0.1191\n",
      "     43            1.0000        0.0689       0.5729            0.5729        1.0558  0.0006  0.1102\n",
      "     44            1.0000        0.0581       0.5729            0.5729        1.0616  0.0006  0.0891\n",
      "     45            1.0000        \u001b[32m0.0479\u001b[0m       0.5833            0.5833        1.0576  0.0005  0.1166\n",
      "     46            1.0000        \u001b[32m0.0453\u001b[0m       0.5833            0.5833        1.0458  0.0005  0.1028\n",
      "     47            1.0000        \u001b[32m0.0319\u001b[0m       0.5938            0.5938        1.0297  0.0004  0.1168\n",
      "     48            1.0000        \u001b[32m0.0281\u001b[0m       0.5938            0.5938        1.0108  0.0004  0.0933\n",
      "     49            1.0000        0.0551       0.6042            0.6042        0.9928  0.0003  0.1158\n",
      "     50            1.0000        0.0326       0.6042            0.6042        0.9736  0.0003  0.1041\n",
      "Fine tuning model for subject 1 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5500        1.3380       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9305\u001b[0m  0.0004  0.0996\n",
      "     32            0.6000        1.2166       0.6250            0.6250        \u001b[94m0.9117\u001b[0m  0.0005  0.0969\n",
      "     33            0.6500        1.1410       0.5833            0.5833        \u001b[94m0.8922\u001b[0m  0.0005  0.1172\n",
      "     34            \u001b[36m0.8500\u001b[0m        0.7595       0.6042            0.6042        \u001b[94m0.8763\u001b[0m  0.0006  0.1189\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5445\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.8654\u001b[0m  0.0006  0.0945\n",
      "     36            1.0000        \u001b[32m0.4116\u001b[0m       0.6250            0.6250        \u001b[94m0.8622\u001b[0m  0.0007  0.1013\n",
      "     37            1.0000        \u001b[32m0.2327\u001b[0m       0.6250            0.6250        0.8667  0.0007  0.1200\n",
      "     38            1.0000        \u001b[32m0.1839\u001b[0m       0.6354            0.6354        0.8765  0.0007  0.0978\n",
      "     39            1.0000        \u001b[32m0.1341\u001b[0m       0.6250            0.6250        0.8917  0.0007  0.0900\n",
      "     40            1.0000        \u001b[32m0.1195\u001b[0m       0.6146            0.6146        0.9122  0.0007  0.1172\n",
      "     41            1.0000        \u001b[32m0.1133\u001b[0m       0.6250            0.6250        0.9370  0.0007  0.1109\n",
      "     42            1.0000        \u001b[32m0.0934\u001b[0m       0.5938            0.5938        0.9628  0.0007  0.0904\n",
      "     43            1.0000        \u001b[32m0.0673\u001b[0m       0.5938            0.5938        0.9873  0.0006  0.1155\n",
      "     44            1.0000        \u001b[32m0.0611\u001b[0m       0.5833            0.5833        1.0092  0.0006  0.1047\n",
      "     45            1.0000        \u001b[32m0.0442\u001b[0m       0.5729            0.5729        1.0270  0.0005  0.1109\n",
      "     46            1.0000        \u001b[32m0.0360\u001b[0m       0.5417            0.5417        1.0404  0.0005  0.1028\n",
      "     47            1.0000        0.0404       0.5417            0.5417        1.0499  0.0004  0.1033\n",
      "     48            1.0000        0.0466       0.5312            0.5312        1.0558  0.0004  0.1191\n",
      "     49            1.0000        0.0380       0.5312            0.5312        1.0585  0.0003  0.1104\n",
      "     50            1.0000        \u001b[32m0.0317\u001b[0m       0.5208            0.5208        1.0592  0.0003  0.1143\n",
      "Fine tuning model for subject 1 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.4596       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9293\u001b[0m  0.0004  0.1043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4500        1.1581       0.5833            0.5833        0.9322  0.0005  0.1117\n",
      "     33            0.6000        1.0378       0.5729            0.5729        0.9713  0.0005  0.1135\n",
      "     34            \u001b[36m0.8000\u001b[0m        0.8902       0.5417            0.5417        1.0586  0.0006  0.1017\n",
      "     35            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6094\u001b[0m       0.5417            0.5417        1.1819  0.0006  0.1082\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4229\u001b[0m       0.5208            0.5208        1.3045  0.0007  0.0904\n",
      "     37            1.0000        \u001b[32m0.2710\u001b[0m       0.5208            0.5208        1.4123  0.0007  0.1161\n",
      "     38            0.9500        \u001b[32m0.1993\u001b[0m       0.5521            0.5521        1.4893  0.0007  0.1041\n",
      "     39            0.9500        \u001b[32m0.1824\u001b[0m       0.5625            0.5625        1.5276  0.0007  0.1148\n",
      "     40            0.9500        \u001b[32m0.1022\u001b[0m       0.5521            0.5521        1.5409  0.0007  0.1001\n",
      "     41            1.0000        \u001b[32m0.0994\u001b[0m       0.5521            0.5521        1.5312  0.0007  0.1014\n",
      "     42            1.0000        \u001b[32m0.0927\u001b[0m       0.5417            0.5417        1.5098  0.0007  0.1018\n",
      "     43            1.0000        \u001b[32m0.0720\u001b[0m       0.5417            0.5417        1.4801  0.0006  0.1132\n",
      "     44            1.0000        \u001b[32m0.0633\u001b[0m       0.5417            0.5417        1.4486  0.0006  0.1185\n",
      "     45            1.0000        0.0736       0.5312            0.5312        1.4169  0.0005  0.0947\n",
      "     46            1.0000        \u001b[32m0.0398\u001b[0m       0.5417            0.5417        1.3889  0.0005  0.0890\n",
      "     47            1.0000        0.0426       0.5417            0.5417        1.3665  0.0004  0.1194\n",
      "     48            1.0000        0.0499       0.5417            0.5417        1.3465  0.0004  0.0988\n",
      "     49            1.0000        0.0465       0.5312            0.5312        1.3291  0.0003  0.1148\n",
      "     50            1.0000        \u001b[32m0.0367\u001b[0m       0.5417            0.5417        1.3138  0.0003  0.1051\n",
      "Fine tuning model for subject 1 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7200        0.8925       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m0.9349\u001b[0m  0.0004  0.1141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7600        1.0466       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.9234\u001b[0m  0.0005  0.1188\n",
      "     33            \u001b[36m0.8800\u001b[0m        0.8329       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.9142\u001b[0m  0.0005  0.1155\n",
      "     34            0.8800        \u001b[32m0.5980\u001b[0m       0.6458            0.6458        \u001b[94m0.9066\u001b[0m  0.0006  0.1198\n",
      "     35            \u001b[36m0.9200\u001b[0m        0.6530       0.6354            0.6354        \u001b[94m0.9013\u001b[0m  0.0006  0.1201\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4678\u001b[0m       0.5938            0.5938        \u001b[94m0.8965\u001b[0m  0.0007  0.1146\n",
      "     37            1.0000        \u001b[32m0.2643\u001b[0m       0.6042            0.6042        \u001b[94m0.8956\u001b[0m  0.0007  0.1124\n",
      "     38            1.0000        \u001b[32m0.2082\u001b[0m       0.6250            0.6250        0.8957  0.0007  0.1146\n",
      "     39            1.0000        \u001b[32m0.1681\u001b[0m       0.6146            0.6146        0.8995  0.0007  0.1053\n",
      "     40            1.0000        \u001b[32m0.1020\u001b[0m       0.5938            0.5938        0.9036  0.0007  0.1143\n",
      "     41            1.0000        0.1223       0.5833            0.5833        0.9065  0.0007  0.1366\n",
      "     42            1.0000        \u001b[32m0.0892\u001b[0m       0.5938            0.5938        0.9082  0.0007  0.1368\n",
      "     43            1.0000        \u001b[32m0.0865\u001b[0m       0.5521            0.5521        0.9092  0.0006  0.1490\n",
      "     44            1.0000        0.0910       0.5521            0.5521        0.9085  0.0006  0.1376\n",
      "     45            1.0000        \u001b[32m0.0511\u001b[0m       0.5521            0.5521        0.9061  0.0005  0.1278\n",
      "     46            1.0000        0.0668       0.5625            0.5625        0.9026  0.0005  0.1374\n",
      "     47            1.0000        0.0836       0.5625            0.5625        0.8986  0.0004  0.1457\n",
      "     48            1.0000        0.0644       0.5625            0.5625        \u001b[94m0.8952\u001b[0m  0.0004  0.1408\n",
      "     49            1.0000        0.0793       0.5625            0.5625        \u001b[94m0.8919\u001b[0m  0.0003  0.1178\n",
      "     50            1.0000        0.0619       0.5729            0.5729        \u001b[94m0.8893\u001b[0m  0.0003  0.1241\n",
      "Fine tuning model for subject 1 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6800        0.8776       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9344\u001b[0m  0.0004  0.1404\n",
      "     32            0.7200        1.0051       0.6146            0.6146        \u001b[94m0.9288\u001b[0m  0.0005  0.1265\n",
      "     33            \u001b[36m0.8400\u001b[0m        \u001b[32m0.7392\u001b[0m       0.5729            0.5729        0.9297  0.0005  0.1298\n",
      "     34            \u001b[36m0.8800\u001b[0m        \u001b[32m0.6936\u001b[0m       0.5729            0.5729        0.9353  0.0006  0.1493\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5523\u001b[0m       0.5417            0.5417        0.9477  0.0006  0.1323\n",
      "     36            1.0000        \u001b[32m0.3391\u001b[0m       0.5417            0.5417        0.9622  0.0007  0.1286\n",
      "     37            1.0000        \u001b[32m0.2961\u001b[0m       0.5729            0.5729        0.9768  0.0007  0.1813\n",
      "     38            1.0000        \u001b[32m0.2204\u001b[0m       0.5833            0.5833        0.9932  0.0007  0.1144\n",
      "     39            1.0000        \u001b[32m0.1726\u001b[0m       0.5625            0.5625        1.0148  0.0007  0.1004\n",
      "     40            1.0000        \u001b[32m0.1413\u001b[0m       0.5312            0.5312        1.0407  0.0007  0.1019\n",
      "     41            1.0000        \u001b[32m0.0950\u001b[0m       0.5312            0.5312        1.0697  0.0007  0.1207\n",
      "     42            1.0000        0.0967       0.5208            0.5208        1.0981  0.0007  0.1272\n",
      "     43            1.0000        \u001b[32m0.0742\u001b[0m       0.5000            0.5000        1.1236  0.0006  0.1276\n",
      "     44            1.0000        0.0881       0.4792            0.4792        1.1447  0.0006  0.1001\n",
      "     45            1.0000        \u001b[32m0.0662\u001b[0m       0.4792            0.4792        1.1603  0.0005  0.0989\n",
      "     46            1.0000        \u001b[32m0.0448\u001b[0m       0.4896            0.4896        1.1709  0.0005  0.1054\n",
      "     47            1.0000        0.0690       0.4896            0.4896        1.1765  0.0004  0.1167\n",
      "     48            1.0000        0.0601       0.4792            0.4792        1.1771  0.0004  0.1176\n",
      "     49            1.0000        \u001b[32m0.0440\u001b[0m       0.4896            0.4896        1.1748  0.0003  0.1074\n",
      "     50            1.0000        \u001b[32m0.0399\u001b[0m       0.4896            0.4896        1.1705  0.0003  0.1076\n",
      "Fine tuning model for subject 1 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5600        0.9845       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9326\u001b[0m  0.0004  0.1059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6800        0.8709       0.6146            0.6146        \u001b[94m0.9210\u001b[0m  0.0005  0.1187\n",
      "     33            0.7600        0.7892       0.6146            0.6146        \u001b[94m0.9106\u001b[0m  0.0005  0.1025\n",
      "     34            \u001b[36m0.8000\u001b[0m        \u001b[32m0.6734\u001b[0m       0.6146            0.6146        \u001b[94m0.9074\u001b[0m  0.0006  0.1005\n",
      "     35            \u001b[36m0.8800\u001b[0m        \u001b[32m0.5197\u001b[0m       0.6042            0.6042        0.9074  0.0006  0.1195\n",
      "     36            \u001b[36m0.9600\u001b[0m        \u001b[32m0.4319\u001b[0m       0.5833            0.5833        0.9127  0.0007  0.1240\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2446\u001b[0m       0.6042            0.6042        0.9251  0.0007  0.1354\n",
      "     38            1.0000        0.2504       0.6042            0.6042        0.9356  0.0007  0.0996\n",
      "     39            1.0000        \u001b[32m0.1545\u001b[0m       0.6146            0.6146        0.9417  0.0007  0.1165\n",
      "     40            1.0000        \u001b[32m0.1278\u001b[0m       0.6042            0.6042        0.9440  0.0007  0.1270\n",
      "     41            1.0000        \u001b[32m0.1180\u001b[0m       0.6042            0.6042        0.9434  0.0007  0.1146\n",
      "     42            1.0000        \u001b[32m0.0962\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        0.9381  0.0007  0.1198\n",
      "     43            1.0000        \u001b[32m0.0613\u001b[0m       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        0.9309  0.0006  0.1255\n",
      "     44            1.0000        0.0624       0.6458            0.6458        0.9247  0.0006  0.1148\n",
      "     45            1.0000        \u001b[32m0.0564\u001b[0m       0.6562            0.6562        0.9206  0.0005  0.1106\n",
      "     46            1.0000        \u001b[32m0.0448\u001b[0m       0.6354            0.6354        0.9184  0.0005  0.1176\n",
      "     47            1.0000        \u001b[32m0.0394\u001b[0m       0.6250            0.6250        0.9182  0.0004  0.1051\n",
      "     48            1.0000        0.0454       0.6146            0.6146        0.9193  0.0004  0.1154\n",
      "     49            1.0000        \u001b[32m0.0392\u001b[0m       0.6250            0.6250        0.9219  0.0003  0.0986\n",
      "     50            1.0000        0.0405       0.6146            0.6146        0.9256  0.0003  0.0962\n",
      "Fine tuning model for subject 1 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5200        0.9216       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9355\u001b[0m  0.0004  0.1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.0598       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9283\u001b[0m  0.0005  0.1097\n",
      "     33            \u001b[36m0.8000\u001b[0m        0.8420       0.6250            0.6250        \u001b[94m0.9260\u001b[0m  0.0005  0.1182\n",
      "     34            \u001b[36m0.8400\u001b[0m        \u001b[32m0.6954\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        0.9279  0.0006  0.1197\n",
      "     35            \u001b[36m0.9200\u001b[0m        \u001b[32m0.5842\u001b[0m       0.5833            0.5833        0.9374  0.0006  0.1188\n",
      "     36            \u001b[36m0.9600\u001b[0m        \u001b[32m0.3887\u001b[0m       0.6042            0.6042        0.9677  0.0007  0.1205\n",
      "     37            0.9600        \u001b[32m0.2697\u001b[0m       0.5521            0.5521        1.0163  0.0007  0.1138\n",
      "     38            0.9600        \u001b[32m0.1773\u001b[0m       0.5208            0.5208        1.0739  0.0007  0.1073\n",
      "     39            0.9600        0.1902       0.5104            0.5104        1.1291  0.0007  0.1147\n",
      "     40            0.9600        \u001b[32m0.1147\u001b[0m       0.4688            0.4688        1.1872  0.0007  0.1211\n",
      "     41            0.9600        \u001b[32m0.0793\u001b[0m       0.4583            0.4583        1.2372  0.0007  0.1301\n",
      "     42            0.9600        0.0918       0.4583            0.4583        1.2802  0.0007  0.0987\n",
      "     43            0.9600        0.0949       0.4479            0.4479        1.3057  0.0006  0.0974\n",
      "     44            0.9600        0.0825       0.4375            0.4375        1.3192  0.0006  0.1186\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0525\u001b[0m       0.4167            0.4167        1.3177  0.0005  0.1025\n",
      "     46            1.0000        \u001b[32m0.0380\u001b[0m       0.4479            0.4479        1.3097  0.0005  0.0991\n",
      "     47            1.0000        0.0550       0.4688            0.4688        1.2924  0.0004  0.1196\n",
      "     48            1.0000        0.0891       0.4688            0.4688        1.2685  0.0004  0.1312\n",
      "     49            1.0000        0.0410       0.4688            0.4688        1.2462  0.0003  0.0996\n",
      "     50            1.0000        0.0501       0.4583            0.4583        1.2257  0.0003  0.1118\n",
      "Fine tuning model for subject 1 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7200        \u001b[32m0.7222\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9296\u001b[0m  0.0004  0.1072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        0.7561       0.5833            0.5833        \u001b[94m0.9246\u001b[0m  0.0005  0.1300\n",
      "     33            \u001b[36m0.8800\u001b[0m        \u001b[32m0.5843\u001b[0m       0.5833            0.5833        0.9372  0.0005  0.1126\n",
      "     34            \u001b[36m0.9600\u001b[0m        \u001b[32m0.4410\u001b[0m       0.5938            0.5938        0.9643  0.0006  0.1037\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4358\u001b[0m       0.5729            0.5729        1.0053  0.0006  0.1197\n",
      "     36            1.0000        \u001b[32m0.3082\u001b[0m       0.5521            0.5521        1.0483  0.0007  0.1251\n",
      "     37            1.0000        \u001b[32m0.2747\u001b[0m       0.5625            0.5625        1.0839  0.0007  0.1009\n",
      "     38            1.0000        \u001b[32m0.1499\u001b[0m       0.5417            0.5417        1.1080  0.0007  0.1010\n",
      "     39            1.0000        0.1526       0.5521            0.5521        1.1215  0.0007  0.1014\n",
      "     40            1.0000        \u001b[32m0.0809\u001b[0m       0.5729            0.5729        1.1286  0.0007  0.1052\n",
      "     41            1.0000        \u001b[32m0.0793\u001b[0m       0.5833            0.5833        1.1321  0.0007  0.1140\n",
      "     42            1.0000        0.0886       0.5729            0.5729        1.1330  0.0007  0.1001\n",
      "     43            1.0000        \u001b[32m0.0631\u001b[0m       0.5729            0.5729        1.1317  0.0006  0.0976\n",
      "     44            1.0000        \u001b[32m0.0616\u001b[0m       0.5833            0.5833        1.1299  0.0006  0.1155\n",
      "     45            1.0000        \u001b[32m0.0514\u001b[0m       0.5833            0.5833        1.1287  0.0005  0.1167\n",
      "     46            1.0000        \u001b[32m0.0437\u001b[0m       0.5833            0.5833        1.1283  0.0005  0.1143\n",
      "     47            1.0000        0.0463       0.5938            0.5938        1.1288  0.0004  0.1115\n",
      "     48            1.0000        \u001b[32m0.0426\u001b[0m       0.6042            0.6042        1.1291  0.0004  0.0987\n",
      "     49            1.0000        \u001b[32m0.0358\u001b[0m       0.6042            0.6042        1.1292  0.0003  0.1307\n",
      "     50            1.0000        \u001b[32m0.0333\u001b[0m       0.6042            0.6042        1.1292  0.0003  0.1197\n",
      "Fine tuning model for subject 1 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6400        0.9980       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9295\u001b[0m  0.0004  0.0973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6800        1.0409       0.6042            0.6042        \u001b[94m0.9175\u001b[0m  0.0005  0.1164\n",
      "     33            \u001b[36m0.8000\u001b[0m        1.0145       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9098\u001b[0m  0.0005  0.1168\n",
      "     34            \u001b[36m0.8400\u001b[0m        0.8250       0.6250            0.6250        \u001b[94m0.9054\u001b[0m  0.0006  0.1059\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4763\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.9047\u001b[0m  0.0006  0.1025\n",
      "     36            1.0000        \u001b[32m0.3545\u001b[0m       0.6354            0.6354        0.9078  0.0007  0.1275\n",
      "     37            1.0000        \u001b[32m0.2958\u001b[0m       0.6354            0.6354        0.9149  0.0007  0.1006\n",
      "     38            1.0000        \u001b[32m0.1926\u001b[0m       0.6354            0.6354        0.9233  0.0007  0.0990\n",
      "     39            1.0000        \u001b[32m0.1589\u001b[0m       0.6354            0.6354        0.9314  0.0007  0.1025\n",
      "     40            1.0000        \u001b[32m0.1217\u001b[0m       0.6250            0.6250        0.9392  0.0007  0.1042\n",
      "     41            1.0000        \u001b[32m0.0929\u001b[0m       0.6354            0.6354        0.9474  0.0007  0.1131\n",
      "     42            1.0000        \u001b[32m0.0751\u001b[0m       0.6042            0.6042        0.9546  0.0007  0.1164\n",
      "     43            1.0000        \u001b[32m0.0588\u001b[0m       0.5729            0.5729        0.9615  0.0006  0.0966\n",
      "     44            1.0000        0.0786       0.5833            0.5833        0.9686  0.0006  0.1158\n",
      "     45            1.0000        \u001b[32m0.0566\u001b[0m       0.5833            0.5833        0.9751  0.0005  0.1207\n",
      "     46            1.0000        0.0693       0.5938            0.5938        0.9814  0.0005  0.1202\n",
      "     47            1.0000        0.0615       0.5938            0.5938        0.9863  0.0004  0.1186\n",
      "     48            1.0000        0.0738       0.5938            0.5938        0.9908  0.0004  0.1058\n",
      "     49            1.0000        \u001b[32m0.0478\u001b[0m       0.5938            0.5938        0.9940  0.0003  0.1223\n",
      "     50            1.0000        \u001b[32m0.0313\u001b[0m       0.5833            0.5833        0.9968  0.0003  0.1183\n",
      "Fine tuning model for subject 1 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5600        1.2062       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9289\u001b[0m  0.0004  0.0946\n",
      "     32            0.5600        1.1633       0.6042            0.6042        \u001b[94m0.9233\u001b[0m  0.0005  0.1079\n",
      "     33            0.7600        0.8853       0.5833            0.5833        0.9326  0.0005  0.1367\n",
      "     34            \u001b[36m0.8800\u001b[0m        0.9875       0.5938            0.5938        0.9572  0.0006  0.1347\n",
      "     35            \u001b[36m0.9200\u001b[0m        \u001b[32m0.6935\u001b[0m       0.6042            0.6042        0.9850  0.0006  0.1370\n",
      "     36            \u001b[36m0.9600\u001b[0m        \u001b[32m0.4836\u001b[0m       0.6042            0.6042        1.0109  0.0007  0.1167\n",
      "     37            \u001b[36m1.0000\u001b[0m        0.5020       0.6042            0.6042        1.0260  0.0007  0.1331\n",
      "     38            1.0000        \u001b[32m0.2197\u001b[0m       0.6042            0.6042        1.0351  0.0007  0.1342\n",
      "     39            1.0000        \u001b[32m0.2162\u001b[0m       0.5833            0.5833        1.0416  0.0007  0.1467\n",
      "     40            1.0000        \u001b[32m0.1695\u001b[0m       0.5833            0.5833        1.0506  0.0007  0.1345\n",
      "     41            1.0000        0.1905       0.5521            0.5521        1.0591  0.0007  0.1666\n",
      "     42            1.0000        0.2022       0.5729            0.5729        1.0658  0.0007  0.1499\n",
      "     43            1.0000        \u001b[32m0.1005\u001b[0m       0.5625            0.5625        1.0716  0.0006  0.1344\n",
      "     44            1.0000        0.1803       0.5417            0.5417        1.0732  0.0006  0.1319\n",
      "     45            1.0000        0.1451       0.5729            0.5729        1.0733  0.0005  0.1319\n",
      "     46            1.0000        \u001b[32m0.0894\u001b[0m       0.5833            0.5833        1.0690  0.0005  0.1443\n",
      "     47            1.0000        0.1087       0.5833            0.5833        1.0605  0.0004  0.1479\n",
      "     48            1.0000        \u001b[32m0.0886\u001b[0m       0.5833            0.5833        1.0501  0.0004  0.1582\n",
      "     49            1.0000        \u001b[32m0.0539\u001b[0m       0.5833            0.5833        1.0381  0.0003  0.1335\n",
      "     50            1.0000        \u001b[32m0.0507\u001b[0m       0.5833            0.5833        1.0259  0.0003  0.1823\n",
      "Fine tuning model for subject 1 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4800        1.5614       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9298\u001b[0m  0.0004  0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5200        1.3408       0.6146            0.6146        \u001b[94m0.9127\u001b[0m  0.0005  0.1193\n",
      "     33            0.7600        1.0750       0.6250            0.6250        \u001b[94m0.9019\u001b[0m  0.0005  0.1154\n",
      "     34            \u001b[36m0.8400\u001b[0m        0.8713       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8982\u001b[0m  0.0006  0.1044\n",
      "     35            \u001b[36m0.9200\u001b[0m        \u001b[32m0.6550\u001b[0m       0.6458            0.6458        0.9033  0.0006  0.1126\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5657\u001b[0m       0.6458            0.6458        0.9263  0.0007  0.1262\n",
      "     37            1.0000        \u001b[32m0.2875\u001b[0m       0.6250            0.6250        0.9747  0.0007  0.1000\n",
      "     38            1.0000        \u001b[32m0.2183\u001b[0m       0.5938            0.5938        1.0471  0.0007  0.1116\n",
      "     39            1.0000        0.2551       0.5625            0.5625        1.1379  0.0007  0.1168\n",
      "     40            1.0000        \u001b[32m0.1637\u001b[0m       0.5521            0.5521        1.2273  0.0007  0.1199\n",
      "     41            1.0000        \u001b[32m0.1394\u001b[0m       0.5417            0.5417        1.3078  0.0007  0.1252\n",
      "     42            1.0000        \u001b[32m0.1347\u001b[0m       0.5208            0.5208        1.3708  0.0007  0.1023\n",
      "     43            1.0000        \u001b[32m0.1151\u001b[0m       0.5000            0.5000        1.4118  0.0006  0.1029\n",
      "     44            1.0000        \u001b[32m0.0912\u001b[0m       0.5208            0.5208        1.4315  0.0006  0.1168\n",
      "     45            1.0000        \u001b[32m0.0856\u001b[0m       0.5417            0.5417        1.4346  0.0005  0.1205\n",
      "     46            1.0000        \u001b[32m0.0685\u001b[0m       0.5417            0.5417        1.4229  0.0005  0.1356\n",
      "     47            1.0000        \u001b[32m0.0662\u001b[0m       0.5208            0.5208        1.4004  0.0004  0.1377\n",
      "     48            1.0000        0.0752       0.5104            0.5104        1.3725  0.0004  0.1238\n",
      "     49            1.0000        0.0747       0.5104            0.5104        1.3413  0.0003  0.1023\n",
      "     50            1.0000        0.0669       0.5104            0.5104        1.3100  0.0003  0.1167\n",
      "Fine tuning model for subject 1 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        0.9766       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9323\u001b[0m  0.0004  0.0936\n",
      "     32            0.6000        0.9764       0.6146            0.6146        \u001b[94m0.9216\u001b[0m  0.0005  0.1126\n",
      "     33            0.7200        0.7908       0.5833            0.5833        \u001b[94m0.9114\u001b[0m  0.0005  0.1352\n",
      "     34            \u001b[36m0.8800\u001b[0m        \u001b[32m0.6477\u001b[0m       0.5833            0.5833        \u001b[94m0.9036\u001b[0m  0.0006  0.1188\n",
      "     35            \u001b[36m0.9600\u001b[0m        \u001b[32m0.5970\u001b[0m       0.6042            0.6042        \u001b[94m0.8961\u001b[0m  0.0006  0.1187\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3219\u001b[0m       0.5625            0.5625        0.8979  0.0007  0.1259\n",
      "     37            1.0000        0.3372       0.5729            0.5729        0.9137  0.0007  0.1475\n",
      "     38            1.0000        \u001b[32m0.2436\u001b[0m       0.5833            0.5833        0.9412  0.0007  0.1189\n",
      "     39            1.0000        \u001b[32m0.1900\u001b[0m       0.5833            0.5833        0.9726  0.0007  0.1078\n",
      "     40            1.0000        \u001b[32m0.1382\u001b[0m       0.5833            0.5833        1.0008  0.0007  0.1168\n",
      "     41            1.0000        \u001b[32m0.0979\u001b[0m       0.5729            0.5729        1.0295  0.0007  0.1197\n",
      "     42            1.0000        0.1215       0.5729            0.5729        1.0445  0.0007  0.1269\n",
      "     43            1.0000        \u001b[32m0.0717\u001b[0m       0.5625            0.5625        1.0523  0.0006  0.1259\n",
      "     44            1.0000        \u001b[32m0.0527\u001b[0m       0.5625            0.5625        1.0580  0.0006  0.1020\n",
      "     45            1.0000        0.0750       0.5521            0.5521        1.0580  0.0005  0.1005\n",
      "     46            1.0000        0.0566       0.5625            0.5625        1.0538  0.0005  0.1040\n",
      "     47            1.0000        0.0528       0.5833            0.5833        1.0470  0.0004  0.1153\n",
      "     48            1.0000        0.0643       0.6146            0.6146        1.0382  0.0004  0.1178\n",
      "     49            1.0000        0.0601       0.6042            0.6042        1.0297  0.0003  0.1178\n",
      "     50            1.0000        \u001b[32m0.0431\u001b[0m       0.6042            0.6042        1.0227  0.0003  0.1133\n",
      "Fine tuning model for subject 1 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5600        1.1344       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9266\u001b[0m  0.0004  0.1269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        0.9836       0.5625            0.5625        \u001b[94m0.9131\u001b[0m  0.0005  0.1514\n",
      "     33            0.7200        0.9504       0.5729            0.5729        \u001b[94m0.9061\u001b[0m  0.0005  0.1490\n",
      "     34            \u001b[36m0.8000\u001b[0m        \u001b[32m0.7114\u001b[0m       0.5521            0.5521        \u001b[94m0.9040\u001b[0m  0.0006  0.1145\n",
      "     35            \u001b[36m0.9200\u001b[0m        \u001b[32m0.6029\u001b[0m       0.5521            0.5521        \u001b[94m0.9003\u001b[0m  0.0006  0.1356\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3714\u001b[0m       0.5625            0.5625        \u001b[94m0.8992\u001b[0m  0.0007  0.1117\n",
      "     37            1.0000        \u001b[32m0.3139\u001b[0m       0.5312            0.5312        0.9001  0.0007  0.1040\n",
      "     38            1.0000        \u001b[32m0.2410\u001b[0m       0.5521            0.5521        0.9056  0.0007  0.1136\n",
      "     39            1.0000        \u001b[32m0.1758\u001b[0m       0.5833            0.5833        0.9149  0.0007  0.1300\n",
      "     40            1.0000        \u001b[32m0.1240\u001b[0m       0.5729            0.5729        0.9213  0.0007  0.0988\n",
      "     41            1.0000        \u001b[32m0.1020\u001b[0m       0.5833            0.5833        0.9241  0.0007  0.0986\n",
      "     42            1.0000        0.1043       0.5729            0.5729        0.9219  0.0007  0.1181\n",
      "     43            1.0000        0.1199       0.5833            0.5833        0.9165  0.0006  0.1198\n",
      "     44            1.0000        \u001b[32m0.0756\u001b[0m       0.5833            0.5833        0.9139  0.0006  0.1156\n",
      "     45            1.0000        \u001b[32m0.0705\u001b[0m       0.5833            0.5833        0.9123  0.0005  0.1211\n",
      "     46            1.0000        \u001b[32m0.0589\u001b[0m       0.5938            0.5938        0.9113  0.0005  0.1598\n",
      "     47            1.0000        \u001b[32m0.0480\u001b[0m       0.6042            0.6042        0.9109  0.0004  0.1155\n",
      "     48            1.0000        \u001b[32m0.0442\u001b[0m       0.6042            0.6042        0.9110  0.0004  0.1278\n",
      "     49            1.0000        \u001b[32m0.0368\u001b[0m       0.5833            0.5833        0.9116  0.0003  0.1174\n",
      "     50            1.0000        0.0509       0.5833            0.5833        0.9124  0.0003  0.1057\n",
      "Fine tuning model for subject 1 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7333        0.8730       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9338\u001b[0m  0.0004  0.1291\n",
      "     32            0.7333        0.9383       0.5938            0.5938        \u001b[94m0.9311\u001b[0m  0.0005  0.1133\n",
      "     33            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6940\u001b[0m       0.5833            0.5833        0.9380  0.0005  0.1178\n",
      "     34            \u001b[36m0.9667\u001b[0m        \u001b[32m0.6356\u001b[0m       0.5625            0.5625        0.9493  0.0006  0.1338\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4967\u001b[0m       0.5312            0.5312        0.9695  0.0006  0.1217\n",
      "     36            1.0000        \u001b[32m0.3854\u001b[0m       0.5312            0.5312        0.9923  0.0007  0.1191\n",
      "     37            1.0000        \u001b[32m0.2827\u001b[0m       0.5104            0.5104        1.0136  0.0007  0.1304\n",
      "     38            1.0000        \u001b[32m0.2431\u001b[0m       0.5104            0.5104        1.0318  0.0007  0.1139\n",
      "     39            1.0000        \u001b[32m0.1870\u001b[0m       0.5104            0.5104        1.0480  0.0007  0.1151\n",
      "     40            1.0000        \u001b[32m0.1507\u001b[0m       0.5104            0.5104        1.0617  0.0007  0.1186\n",
      "     41            1.0000        \u001b[32m0.1397\u001b[0m       0.5000            0.5000        1.0751  0.0007  0.1183\n",
      "     42            1.0000        \u001b[32m0.1291\u001b[0m       0.4896            0.4896        1.0853  0.0007  0.1206\n",
      "     43            1.0000        \u001b[32m0.1138\u001b[0m       0.4896            0.4896        1.0937  0.0006  0.1199\n",
      "     44            1.0000        0.1513       0.5000            0.5000        1.1002  0.0006  0.1291\n",
      "     45            1.0000        \u001b[32m0.0811\u001b[0m       0.5104            0.5104        1.1054  0.0005  0.1231\n",
      "     46            1.0000        \u001b[32m0.0624\u001b[0m       0.5208            0.5208        1.1096  0.0005  0.1167\n",
      "     47            1.0000        0.0748       0.5208            0.5208        1.1117  0.0004  0.1218\n",
      "     48            1.0000        0.0662       0.5208            0.5208        1.1130  0.0004  0.1332\n",
      "     49            1.0000        0.0699       0.5000            0.5000        1.1143  0.0003  0.1358\n",
      "     50            1.0000        \u001b[32m0.0452\u001b[0m       0.5000            0.5000        1.1153  0.0003  0.1227\n",
      "Fine tuning model for subject 1 with 30 = 30 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        0.9641       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9359\u001b[0m  0.0004  0.1147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7667        1.1277       0.6042            0.6042        0.9373  0.0005  0.1197\n",
      "     33            \u001b[36m0.8333\u001b[0m        \u001b[32m0.7173\u001b[0m       0.5833            0.5833        0.9498  0.0005  0.1171\n",
      "     34            \u001b[36m0.8667\u001b[0m        \u001b[32m0.7011\u001b[0m       0.5729            0.5729        0.9730  0.0006  0.1198\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4776\u001b[0m       0.5729            0.5729        1.0020  0.0006  0.1298\n",
      "     36            0.9667        \u001b[32m0.4248\u001b[0m       0.5521            0.5521        1.0397  0.0007  0.1205\n",
      "     37            1.0000        \u001b[32m0.3972\u001b[0m       0.5208            0.5208        1.0884  0.0007  0.1678\n",
      "     38            0.9667        \u001b[32m0.3335\u001b[0m       0.4688            0.4688        1.1381  0.0007  0.1327\n",
      "     39            0.9667        \u001b[32m0.2970\u001b[0m       0.4375            0.4375        1.1896  0.0007  0.1241\n",
      "     40            0.9333        \u001b[32m0.1628\u001b[0m       0.4167            0.4167        1.2281  0.0007  0.1502\n",
      "     41            0.9667        0.1879       0.4167            0.4167        1.2430  0.0007  0.1571\n",
      "     42            0.9667        \u001b[32m0.1183\u001b[0m       0.4271            0.4271        1.2495  0.0007  0.1563\n",
      "     43            0.9667        \u001b[32m0.1106\u001b[0m       0.4375            0.4375        1.2468  0.0006  0.1608\n",
      "     44            1.0000        \u001b[32m0.0946\u001b[0m       0.4479            0.4479        1.2367  0.0006  0.1346\n",
      "     45            1.0000        0.1123       0.4479            0.4479        1.2219  0.0005  0.1523\n",
      "     46            1.0000        \u001b[32m0.0884\u001b[0m       0.4479            0.4479        1.2060  0.0005  0.1510\n",
      "     47            1.0000        \u001b[32m0.0822\u001b[0m       0.4583            0.4583        1.1905  0.0004  0.1350\n",
      "     48            1.0000        \u001b[32m0.0724\u001b[0m       0.5000            0.5000        1.1783  0.0004  0.1514\n",
      "     49            1.0000        \u001b[32m0.0502\u001b[0m       0.5104            0.5104        1.1687  0.0003  0.1353\n",
      "     50            1.0000        0.0609       0.5208            0.5208        1.1612  0.0003  0.1353\n",
      "Fine tuning model for subject 1 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6333        1.0720       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9341\u001b[0m  0.0004  0.1354\n",
      "     32            0.6667        0.8961       0.5938            0.5938        \u001b[94m0.9296\u001b[0m  0.0005  0.1368\n",
      "     33            \u001b[36m0.8000\u001b[0m        0.8314       0.6042            0.6042        0.9327  0.0005  0.1434\n",
      "     34            \u001b[36m0.8333\u001b[0m        0.7694       0.5729            0.5729        0.9421  0.0006  0.1461\n",
      "     35            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6342\u001b[0m       0.5729            0.5729        0.9544  0.0006  0.1697\n",
      "     36            0.9000        \u001b[32m0.4525\u001b[0m       0.6042            0.6042        0.9656  0.0007  0.1309\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3924\u001b[0m       0.6146            0.6146        0.9724  0.0007  0.1273\n",
      "     38            1.0000        \u001b[32m0.2961\u001b[0m       0.6146            0.6146        0.9798  0.0007  0.1154\n",
      "     39            1.0000        \u001b[32m0.2633\u001b[0m       0.6146            0.6146        0.9832  0.0007  0.1217\n",
      "     40            1.0000        \u001b[32m0.1839\u001b[0m       0.6042            0.6042        0.9848  0.0007  0.1145\n",
      "     41            1.0000        \u001b[32m0.1298\u001b[0m       0.5938            0.5938        0.9864  0.0007  0.1325\n",
      "     42            1.0000        0.1354       0.5833            0.5833        0.9879  0.0007  0.1209\n",
      "     43            1.0000        0.1321       0.6042            0.6042        0.9889  0.0006  0.1353\n",
      "     44            1.0000        \u001b[32m0.1005\u001b[0m       0.5833            0.5833        0.9906  0.0006  0.1286\n",
      "     45            1.0000        \u001b[32m0.0837\u001b[0m       0.5833            0.5833        0.9925  0.0005  0.1336\n",
      "     46            1.0000        0.1410       0.5833            0.5833        0.9942  0.0005  0.1276\n",
      "     47            1.0000        \u001b[32m0.0719\u001b[0m       0.5833            0.5833        0.9959  0.0004  0.1199\n",
      "     48            1.0000        0.0778       0.5833            0.5833        0.9976  0.0004  0.1033\n",
      "     49            1.0000        \u001b[32m0.0625\u001b[0m       0.5938            0.5938        0.9992  0.0003  0.1111\n",
      "     50            1.0000        0.0628       0.6042            0.6042        1.0003  0.0003  0.1172\n",
      "Fine tuning model for subject 1 with 30 = 30 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5667        0.9104       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9300\u001b[0m  0.0004  0.1174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        0.9186       0.5833            0.5833        \u001b[94m0.9297\u001b[0m  0.0005  0.1300\n",
      "     33            \u001b[36m0.8333\u001b[0m        \u001b[32m0.6794\u001b[0m       0.5729            0.5729        0.9529  0.0005  0.1136\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5740\u001b[0m       0.5521            0.5521        0.9880  0.0006  0.1180\n",
      "     35            1.0000        \u001b[32m0.4926\u001b[0m       0.5521            0.5521        1.0214  0.0006  0.1190\n",
      "     36            1.0000        \u001b[32m0.3824\u001b[0m       0.5312            0.5312        1.0499  0.0007  0.1211\n",
      "     37            1.0000        \u001b[32m0.3658\u001b[0m       0.5312            0.5312        1.0687  0.0007  0.1316\n",
      "     38            1.0000        \u001b[32m0.2233\u001b[0m       0.5521            0.5521        1.0836  0.0007  0.1239\n",
      "     39            1.0000        \u001b[32m0.1935\u001b[0m       0.5417            0.5417        1.0869  0.0007  0.1186\n",
      "     40            1.0000        0.2119       0.5104            0.5104        1.0780  0.0007  0.1199\n",
      "     41            1.0000        \u001b[32m0.1369\u001b[0m       0.5104            0.5104        1.0657  0.0007  0.1176\n",
      "     42            1.0000        \u001b[32m0.1079\u001b[0m       0.5000            0.5000        1.0524  0.0007  0.1191\n",
      "     43            1.0000        0.1260       0.4896            0.4896        1.0405  0.0006  0.1190\n",
      "     44            1.0000        \u001b[32m0.0888\u001b[0m       0.5000            0.5000        1.0314  0.0006  0.1211\n",
      "     45            1.0000        \u001b[32m0.0704\u001b[0m       0.5000            0.5000        1.0266  0.0005  0.1356\n",
      "     46            1.0000        0.0783       0.5000            0.5000        1.0227  0.0005  0.1205\n",
      "     47            1.0000        \u001b[32m0.0563\u001b[0m       0.5000            0.5000        1.0210  0.0004  0.1107\n",
      "     48            1.0000        0.0633       0.5000            0.5000        1.0227  0.0004  0.1018\n",
      "     49            1.0000        0.0742       0.4896            0.4896        1.0261  0.0003  0.1166\n",
      "     50            1.0000        \u001b[32m0.0550\u001b[0m       0.4792            0.4792        1.0320  0.0003  0.1197\n",
      "Fine tuning model for subject 1 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.3776       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9317\u001b[0m  0.0004  0.0989\n",
      "     32            0.5333        1.1657       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9224\u001b[0m  0.0005  0.1097\n",
      "     33            0.5667        1.0329       0.6146            0.6146        \u001b[94m0.9196\u001b[0m  0.0005  0.1158\n",
      "     34            0.7333        0.9654       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        0.9227  0.0006  0.1177\n",
      "     35            \u001b[36m0.8000\u001b[0m        \u001b[32m0.6851\u001b[0m       0.6146            0.6146        0.9378  0.0006  0.1318\n",
      "     36            \u001b[36m0.9667\u001b[0m        \u001b[32m0.5858\u001b[0m       0.5729            0.5729        0.9684  0.0007  0.1229\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3717\u001b[0m       0.5625            0.5625        1.0094  0.0007  0.1333\n",
      "     38            1.0000        \u001b[32m0.2605\u001b[0m       0.5208            0.5208        1.0547  0.0007  0.1076\n",
      "     39            1.0000        \u001b[32m0.2217\u001b[0m       0.5104            0.5104        1.0966  0.0007  0.1153\n",
      "     40            1.0000        0.2232       0.5000            0.5000        1.1287  0.0007  0.1212\n",
      "     41            1.0000        \u001b[32m0.1540\u001b[0m       0.5000            0.5000        1.1505  0.0007  0.1187\n",
      "     42            1.0000        0.1761       0.5104            0.5104        1.1571  0.0007  0.1354\n",
      "     43            1.0000        0.1801       0.5104            0.5104        1.1593  0.0006  0.1208\n",
      "     44            1.0000        \u001b[32m0.1019\u001b[0m       0.5000            0.5000        1.1583  0.0006  0.1192\n",
      "     45            1.0000        \u001b[32m0.0984\u001b[0m       0.5000            0.5000        1.1564  0.0005  0.1242\n",
      "     46            1.0000        0.1087       0.5000            0.5000        1.1522  0.0005  0.1166\n",
      "     47            1.0000        0.1137       0.5000            0.5000        1.1450  0.0004  0.1094\n",
      "     48            1.0000        \u001b[32m0.0713\u001b[0m       0.5000            0.5000        1.1380  0.0004  0.1120\n",
      "     49            1.0000        \u001b[32m0.0595\u001b[0m       0.5000            0.5000        1.1313  0.0003  0.1322\n",
      "     50            1.0000        0.0834       0.5000            0.5000        1.1239  0.0003  0.1202\n",
      "Fine tuning model for subject 1 with 30 = 30 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4333        1.0821       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9286\u001b[0m  0.0004  0.1176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        0.9855       0.5938            0.5938        \u001b[94m0.9069\u001b[0m  0.0005  0.1371\n",
      "     33            \u001b[36m0.8000\u001b[0m        1.0823       0.6146            0.6146        \u001b[94m0.8904\u001b[0m  0.0005  0.1191\n",
      "     34            \u001b[36m0.8333\u001b[0m        0.8025       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8819\u001b[0m  0.0006  0.1152\n",
      "     35            \u001b[36m0.9333\u001b[0m        \u001b[32m0.6376\u001b[0m       0.6458            0.6458        \u001b[94m0.8798\u001b[0m  0.0006  0.1052\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5402\u001b[0m       0.6250            0.6250        0.8854  0.0007  0.1498\n",
      "     37            1.0000        \u001b[32m0.2985\u001b[0m       0.6146            0.6146        0.8985  0.0007  0.1200\n",
      "     38            1.0000        \u001b[32m0.2519\u001b[0m       0.5938            0.5938        0.9095  0.0007  0.1187\n",
      "     39            1.0000        \u001b[32m0.2077\u001b[0m       0.5938            0.5938        0.9200  0.0007  0.1324\n",
      "     40            1.0000        \u001b[32m0.1656\u001b[0m       0.6042            0.6042        0.9307  0.0007  0.1127\n",
      "     41            1.0000        0.1735       0.5938            0.5938        0.9336  0.0007  0.1116\n",
      "     42            1.0000        \u001b[32m0.1414\u001b[0m       0.6042            0.6042        0.9379  0.0007  0.1323\n",
      "     43            1.0000        \u001b[32m0.1105\u001b[0m       0.5938            0.5938        0.9403  0.0006  0.1187\n",
      "     44            1.0000        \u001b[32m0.0971\u001b[0m       0.6146            0.6146        0.9426  0.0006  0.1213\n",
      "     45            1.0000        0.0984       0.6146            0.6146        0.9457  0.0005  0.1354\n",
      "     46            1.0000        0.1196       0.6146            0.6146        0.9485  0.0005  0.1281\n",
      "     47            1.0000        \u001b[32m0.0965\u001b[0m       0.6146            0.6146        0.9490  0.0004  0.1270\n",
      "     48            1.0000        \u001b[32m0.0912\u001b[0m       0.6146            0.6146        0.9486  0.0004  0.1117\n",
      "     49            1.0000        \u001b[32m0.0711\u001b[0m       0.6250            0.6250        0.9467  0.0003  0.1194\n",
      "     50            1.0000        \u001b[32m0.0553\u001b[0m       0.6458            0.6458        0.9440  0.0003  0.1156\n",
      "Fine tuning model for subject 1 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        1.1251       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9383\u001b[0m  0.0004  0.1245\n",
      "     32            0.7667        0.9616       0.6042            0.6042        0.9385  0.0005  0.1072\n",
      "     33            0.8000        \u001b[32m0.7264\u001b[0m       0.5729            0.5729        0.9446  0.0005  0.1197\n",
      "     34            \u001b[36m0.8667\u001b[0m        0.9620       0.5729            0.5729        0.9735  0.0006  0.1345\n",
      "     35            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6592\u001b[0m       0.5625            0.5625        1.0319  0.0006  0.1199\n",
      "     36            0.8667        \u001b[32m0.4318\u001b[0m       0.5417            0.5417        1.0960  0.0007  0.1186\n",
      "     37            0.8667        \u001b[32m0.3054\u001b[0m       0.5417            0.5417        1.1490  0.0007  0.1292\n",
      "     38            0.9000        0.3110       0.5000            0.5000        1.1849  0.0007  0.1142\n",
      "     39            \u001b[36m0.9667\u001b[0m        \u001b[32m0.2085\u001b[0m       0.5000            0.5000        1.2030  0.0007  0.1158\n",
      "     40            \u001b[36m1.0000\u001b[0m        0.2300       0.5208            0.5208        1.1987  0.0007  0.1193\n",
      "     41            1.0000        \u001b[32m0.1452\u001b[0m       0.5104            0.5104        1.1874  0.0007  0.1175\n",
      "     42            1.0000        \u001b[32m0.1206\u001b[0m       0.5312            0.5312        1.1721  0.0007  0.1208\n",
      "     43            1.0000        0.1308       0.5521            0.5521        1.1541  0.0006  0.1266\n",
      "     44            1.0000        \u001b[32m0.1164\u001b[0m       0.5521            0.5521        1.1373  0.0006  0.1350\n",
      "     45            1.0000        \u001b[32m0.1051\u001b[0m       0.5417            0.5417        1.1224  0.0005  0.1665\n",
      "     46            1.0000        \u001b[32m0.0918\u001b[0m       0.5417            0.5417        1.1102  0.0005  0.1376\n",
      "     47            1.0000        0.1015       0.5312            0.5312        1.0985  0.0004  0.1500\n",
      "     48            1.0000        \u001b[32m0.0574\u001b[0m       0.5312            0.5312        1.0897  0.0004  0.1510\n",
      "     49            1.0000        0.0728       0.5312            0.5312        1.0839  0.0003  0.1511\n",
      "     50            1.0000        \u001b[32m0.0431\u001b[0m       0.5417            0.5417        1.0798  0.0003  0.1512\n",
      "Fine tuning model for subject 1 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5667        1.1781       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9374\u001b[0m  0.0004  0.1473\n",
      "     32            0.5667        1.1879       0.6042            0.6042        0.9412  0.0005  0.1455\n",
      "     33            0.6000        0.8649       0.5833            0.5833        0.9572  0.0005  0.1318\n",
      "     34            0.7333        0.9113       0.5938            0.5938        0.9813  0.0006  0.1292\n",
      "     35            \u001b[36m0.8667\u001b[0m        \u001b[32m0.7348\u001b[0m       0.5729            0.5729        1.0110  0.0006  0.1512\n",
      "     36            0.8667        \u001b[32m0.6839\u001b[0m       0.5729            0.5729        1.0426  0.0007  0.1522\n",
      "     37            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4349\u001b[0m       0.5521            0.5521        1.0636  0.0007  0.1359\n",
      "     38            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3159\u001b[0m       0.5417            0.5417        1.0786  0.0007  0.1409\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2933\u001b[0m       0.5417            0.5417        1.0921  0.0007  0.1967\n",
      "     40            1.0000        \u001b[32m0.2178\u001b[0m       0.5312            0.5312        1.1052  0.0007  0.1198\n",
      "     41            1.0000        \u001b[32m0.1733\u001b[0m       0.5104            0.5104        1.1164  0.0007  0.1356\n",
      "     42            1.0000        \u001b[32m0.1676\u001b[0m       0.5104            0.5104        1.1258  0.0007  0.1436\n",
      "     43            1.0000        \u001b[32m0.1072\u001b[0m       0.4896            0.4896        1.1335  0.0006  0.1244\n",
      "     44            1.0000        0.1133       0.4896            0.4896        1.1399  0.0006  0.1152\n",
      "     45            1.0000        \u001b[32m0.0890\u001b[0m       0.4792            0.4792        1.1452  0.0005  0.1373\n",
      "     46            1.0000        0.1174       0.4896            0.4896        1.1497  0.0005  0.1027\n",
      "     47            1.0000        0.0890       0.5000            0.5000        1.1528  0.0004  0.1169\n",
      "     48            1.0000        0.0922       0.5104            0.5104        1.1546  0.0004  0.1195\n",
      "     49            1.0000        \u001b[32m0.0858\u001b[0m       0.5208            0.5208        1.1562  0.0003  0.1193\n",
      "     50            1.0000        \u001b[32m0.0765\u001b[0m       0.5104            0.5104        1.1573  0.0003  0.1364\n",
      "Fine tuning model for subject 1 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7333        0.9958       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9303\u001b[0m  0.0004  0.0990\n",
      "     32            0.7667        0.8768       0.6250            0.6250        \u001b[94m0.9143\u001b[0m  0.0005  0.1222\n",
      "     33            \u001b[36m0.8333\u001b[0m        0.8286       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.9028\u001b[0m  0.0005  0.1186\n",
      "     34            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5990\u001b[0m       0.6354            0.6354        \u001b[94m0.8964\u001b[0m  0.0006  0.1197\n",
      "     35            0.9000        \u001b[32m0.5618\u001b[0m       0.6458            0.6458        \u001b[94m0.8902\u001b[0m  0.0006  0.1354\n",
      "     36            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4873\u001b[0m       0.6562            0.6562        \u001b[94m0.8815\u001b[0m  0.0007  0.1198\n",
      "     37            0.9667        \u001b[32m0.3272\u001b[0m       0.6458            0.6458        \u001b[94m0.8691\u001b[0m  0.0007  0.1354\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2789\u001b[0m       0.6250            0.6250        \u001b[94m0.8565\u001b[0m  0.0007  0.1257\n",
      "     39            1.0000        0.2793       0.6042            0.6042        \u001b[94m0.8416\u001b[0m  0.0007  0.1191\n",
      "     40            1.0000        \u001b[32m0.2053\u001b[0m       0.6250            0.6250        \u001b[94m0.8302\u001b[0m  0.0007  0.1207\n",
      "     41            1.0000        \u001b[32m0.1648\u001b[0m       0.6146            0.6146        \u001b[94m0.8217\u001b[0m  0.0007  0.1080\n",
      "     42            1.0000        \u001b[32m0.1329\u001b[0m       0.6042            0.6042        \u001b[94m0.8173\u001b[0m  0.0007  0.1163\n",
      "     43            1.0000        \u001b[32m0.0882\u001b[0m       0.6146            0.6146        \u001b[94m0.8154\u001b[0m  0.0006  0.1191\n",
      "     44            1.0000        \u001b[32m0.0806\u001b[0m       0.5938            0.5938        \u001b[94m0.8140\u001b[0m  0.0006  0.1383\n",
      "     45            1.0000        0.1025       0.5938            0.5938        0.8140  0.0005  0.1347\n",
      "     46            1.0000        \u001b[32m0.0757\u001b[0m       0.5833            0.5833        0.8142  0.0005  0.1190\n",
      "     47            1.0000        \u001b[32m0.0609\u001b[0m       0.6042            0.6042        \u001b[94m0.8126\u001b[0m  0.0004  0.1297\n",
      "     48            1.0000        0.0635       0.6146            0.6146        \u001b[94m0.8105\u001b[0m  0.0004  0.1144\n",
      "     49            1.0000        0.0933       0.6042            0.6042        \u001b[94m0.8073\u001b[0m  0.0003  0.1126\n",
      "     50            1.0000        0.0710       0.6250            0.6250        \u001b[94m0.8046\u001b[0m  0.0003  0.1345\n",
      "Fine tuning model for subject 1 with 30 = 30 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        0.9549       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9311\u001b[0m  0.0004  0.1060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        1.0716       0.5938            0.5938        0.9344  0.0005  0.1146\n",
      "     33            \u001b[36m0.8000\u001b[0m        0.8864       0.5625            0.5625        0.9713  0.0005  0.1166\n",
      "     34            \u001b[36m0.8333\u001b[0m        \u001b[32m0.6384\u001b[0m       0.5417            0.5417        1.0514  0.0006  0.1348\n",
      "     35            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4641\u001b[0m       0.5312            0.5312        1.1585  0.0006  0.1212\n",
      "     36            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3817\u001b[0m       0.4896            0.4896        1.2757  0.0007  0.1201\n",
      "     37            0.9333        \u001b[32m0.3105\u001b[0m       0.4896            0.4896        1.3804  0.0007  0.1309\n",
      "     38            \u001b[36m0.9667\u001b[0m        \u001b[32m0.2637\u001b[0m       0.4792            0.4792        1.4597  0.0007  0.1252\n",
      "     39            0.9667        \u001b[32m0.1729\u001b[0m       0.4792            0.4792        1.5068  0.0007  0.1329\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1427\u001b[0m       0.4583            0.4583        1.5310  0.0007  0.1080\n",
      "     41            1.0000        0.1485       0.4688            0.4688        1.5385  0.0007  0.1151\n",
      "     42            1.0000        \u001b[32m0.1398\u001b[0m       0.4583            0.4583        1.5295  0.0007  0.1186\n",
      "     43            1.0000        \u001b[32m0.0804\u001b[0m       0.4688            0.4688        1.5086  0.0006  0.1207\n",
      "     44            1.0000        \u001b[32m0.0785\u001b[0m       0.4688            0.4688        1.4785  0.0006  0.1356\n",
      "     45            1.0000        0.1160       0.4688            0.4688        1.4437  0.0005  0.1332\n",
      "     46            1.0000        \u001b[32m0.0599\u001b[0m       0.4688            0.4688        1.4094  0.0005  0.1491\n",
      "     47            1.0000        \u001b[32m0.0563\u001b[0m       0.5104            0.5104        1.3763  0.0004  0.1267\n",
      "     48            1.0000        0.0936       0.5104            0.5104        1.3445  0.0004  0.1158\n",
      "     49            1.0000        \u001b[32m0.0463\u001b[0m       0.5208            0.5208        1.3180  0.0003  0.1196\n",
      "     50            1.0000        0.0771       0.5312            0.5312        1.2958  0.0003  0.1184\n",
      "Fine tuning model for subject 1 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.0145       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9345\u001b[0m  0.0004  0.1409\n",
      "     32            0.6571        1.0442       0.6146            0.6146        \u001b[94m0.9294\u001b[0m  0.0005  0.1123\n",
      "     33            0.7143        \u001b[32m0.7062\u001b[0m       0.6042            0.6042        \u001b[94m0.9292\u001b[0m  0.0005  0.1197\n",
      "     34            \u001b[36m0.8000\u001b[0m        0.7566       0.6042            0.6042        0.9406  0.0006  0.1186\n",
      "     35            \u001b[36m0.8857\u001b[0m        0.7470       0.6042            0.6042        0.9737  0.0006  0.1173\n",
      "     36            \u001b[36m0.9143\u001b[0m        \u001b[32m0.5122\u001b[0m       0.5729            0.5729        1.0304  0.0007  0.1198\n",
      "     37            \u001b[36m0.9714\u001b[0m        \u001b[32m0.5049\u001b[0m       0.5521            0.5521        1.0863  0.0007  0.1279\n",
      "     38            0.9714        \u001b[32m0.3467\u001b[0m       0.5312            0.5312        1.1364  0.0007  0.1273\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2743\u001b[0m       0.5000            0.5000        1.1715  0.0007  0.1276\n",
      "     40            1.0000        \u001b[32m0.2153\u001b[0m       0.4896            0.4896        1.1917  0.0007  0.1145\n",
      "     41            1.0000        \u001b[32m0.2107\u001b[0m       0.4896            0.4896        1.2048  0.0007  0.1099\n",
      "     42            1.0000        \u001b[32m0.1494\u001b[0m       0.4688            0.4688        1.2115  0.0007  0.1102\n",
      "     43            1.0000        \u001b[32m0.1079\u001b[0m       0.4792            0.4792        1.2085  0.0006  0.1292\n",
      "     44            1.0000        0.1344       0.4896            0.4896        1.2031  0.0006  0.1385\n",
      "     45            1.0000        \u001b[32m0.0865\u001b[0m       0.4688            0.4688        1.1936  0.0005  0.1338\n",
      "     46            1.0000        0.0971       0.4583            0.4583        1.1808  0.0005  0.1354\n",
      "     47            1.0000        0.1010       0.4792            0.4792        1.1643  0.0004  0.1240\n",
      "     48            1.0000        \u001b[32m0.0859\u001b[0m       0.4792            0.4792        1.1479  0.0004  0.1395\n",
      "     49            1.0000        \u001b[32m0.0650\u001b[0m       0.5000            0.5000        1.1322  0.0003  0.1264\n",
      "     50            1.0000        0.0938       0.5000            0.5000        1.1169  0.0003  0.1325\n",
      "Fine tuning model for subject 1 with 35 = 35 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7143        0.8260       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9307\u001b[0m  0.0004  0.1304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7429        \u001b[32m0.6938\u001b[0m       0.6146            0.6146        \u001b[94m0.9179\u001b[0m  0.0005  0.1147\n",
      "     33            0.7714        \u001b[32m0.6476\u001b[0m       0.5938            0.5938        \u001b[94m0.9079\u001b[0m  0.0005  0.1341\n",
      "     34            \u001b[36m0.8571\u001b[0m        \u001b[32m0.6079\u001b[0m       0.6042            0.6042        \u001b[94m0.8996\u001b[0m  0.0006  0.1348\n",
      "     35            \u001b[36m0.9143\u001b[0m        \u001b[32m0.5282\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.8966\u001b[0m  0.0006  0.1370\n",
      "     36            \u001b[36m0.9429\u001b[0m        \u001b[32m0.3715\u001b[0m       0.6250            0.6250        0.8986  0.0007  0.1332\n",
      "     37            \u001b[36m0.9714\u001b[0m        \u001b[32m0.3255\u001b[0m       0.6250            0.6250        0.9074  0.0007  0.1364\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3039\u001b[0m       0.6250            0.6250        0.9187  0.0007  0.1268\n",
      "     39            1.0000        \u001b[32m0.2695\u001b[0m       0.6146            0.6146        0.9295  0.0007  0.1299\n",
      "     40            1.0000        \u001b[32m0.2081\u001b[0m       0.6250            0.6250        0.9346  0.0007  0.1336\n",
      "     41            1.0000        \u001b[32m0.1374\u001b[0m       0.6042            0.6042        0.9355  0.0007  0.1354\n",
      "     42            1.0000        \u001b[32m0.1289\u001b[0m       0.5938            0.5938        0.9336  0.0007  0.1178\n",
      "     43            1.0000        \u001b[32m0.1099\u001b[0m       0.6042            0.6042        0.9297  0.0006  0.1138\n",
      "     44            1.0000        \u001b[32m0.0977\u001b[0m       0.6042            0.6042        0.9249  0.0006  0.1143\n",
      "     45            1.0000        \u001b[32m0.0845\u001b[0m       0.6042            0.6042        0.9200  0.0005  0.1320\n",
      "     46            1.0000        0.0882       0.6146            0.6146        0.9148  0.0005  0.1345\n",
      "     47            1.0000        0.0976       0.6146            0.6146        0.9106  0.0004  0.1668\n",
      "     48            1.0000        0.0886       0.6146            0.6146        0.9069  0.0004  0.1393\n",
      "     49            1.0000        \u001b[32m0.0690\u001b[0m       0.6250            0.6250        0.9040  0.0003  0.1484\n",
      "     50            1.0000        0.0914       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        0.9017  0.0003  0.1383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning model for subject 1 with 35 = 35 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4857        1.1378       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9289\u001b[0m  0.0004  0.1553\n",
      "     32            0.5714        1.0599       0.6042            0.6042        \u001b[94m0.9199\u001b[0m  0.0005  0.1636\n",
      "     33            0.6571        0.9574       0.5938            0.5938        \u001b[94m0.9188\u001b[0m  0.0005  0.1485\n",
      "     34            0.7429        0.7477       0.5938            0.5938        0.9211  0.0006  0.1585\n",
      "     35            \u001b[36m0.8857\u001b[0m        \u001b[32m0.6601\u001b[0m       0.6042            0.6042        0.9253  0.0006  0.1671\n",
      "     36            \u001b[36m0.9429\u001b[0m        \u001b[32m0.5130\u001b[0m       0.6250            0.6250        0.9231  0.0007  0.1365\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4185\u001b[0m       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        0.9198  0.0007  0.1671\n",
      "     38            1.0000        \u001b[32m0.3617\u001b[0m       0.6458            0.6458        0.9201  0.0007  0.1345\n",
      "     39            1.0000        \u001b[32m0.3046\u001b[0m       \u001b[35m0.6771\u001b[0m            \u001b[31m0.6771\u001b[0m        0.9290  0.0007  0.1547\n",
      "     40            1.0000        \u001b[32m0.2809\u001b[0m       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        0.9498  0.0007  0.1630\n",
      "     41            0.9714        \u001b[32m0.1421\u001b[0m       0.6667            0.6667        0.9737  0.0007  0.1396\n",
      "     42            0.9429        0.2021       0.6250            0.6250        0.9970  0.0007  0.1278\n",
      "     43            0.9429        \u001b[32m0.1324\u001b[0m       0.6146            0.6146        1.0154  0.0006  0.1222\n",
      "     44            0.9429        \u001b[32m0.1030\u001b[0m       0.6146            0.6146        1.0246  0.0006  0.1326\n",
      "     45            0.9714        0.1189       0.6250            0.6250        1.0256  0.0005  0.1206\n",
      "     46            1.0000        \u001b[32m0.0703\u001b[0m       0.6250            0.6250        1.0215  0.0005  0.1207\n",
      "     47            1.0000        0.1017       0.6354            0.6354        1.0144  0.0004  0.1310\n",
      "     48            1.0000        \u001b[32m0.0532\u001b[0m       0.6562            0.6562        1.0074  0.0004  0.1346\n",
      "     49            1.0000        0.0810       0.6875            0.6875        1.0015  0.0003  0.1396\n",
      "     50            1.0000        0.0720       0.6875            0.6875        0.9978  0.0003  0.1263\n",
      "Fine tuning model for subject 1 with 35 = 35 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4571        1.1547       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9301\u001b[0m  0.0004  0.1237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4857        1.1430       0.5938            0.5938        \u001b[94m0.9256\u001b[0m  0.0005  0.1261\n",
      "     33            0.6000        1.0541       0.5625            0.5625        0.9450  0.0005  0.1342\n",
      "     34            0.6571        0.9331       0.5417            0.5417        0.9803  0.0006  0.1503\n",
      "     35            0.6571        \u001b[32m0.7147\u001b[0m       0.5521            0.5521        1.0215  0.0006  0.1424\n",
      "     36            0.7714        \u001b[32m0.6757\u001b[0m       0.5417            0.5417        1.0554  0.0007  0.1297\n",
      "     37            \u001b[36m0.8571\u001b[0m        \u001b[32m0.5808\u001b[0m       0.5521            0.5521        1.0736  0.0007  0.1345\n",
      "     38            \u001b[36m0.8857\u001b[0m        \u001b[32m0.3782\u001b[0m       0.5417            0.5417        1.0649  0.0007  0.1371\n",
      "     39            \u001b[36m0.9429\u001b[0m        \u001b[32m0.3358\u001b[0m       0.5417            0.5417        1.0468  0.0007  0.1342\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2291\u001b[0m       0.5417            0.5417        1.0261  0.0007  0.1339\n",
      "     41            1.0000        0.2520       0.5417            0.5417        1.0087  0.0007  0.1359\n",
      "     42            1.0000        \u001b[32m0.2049\u001b[0m       0.5625            0.5625        0.9941  0.0007  0.1356\n",
      "     43            1.0000        \u001b[32m0.1727\u001b[0m       0.5729            0.5729        0.9809  0.0006  0.1369\n",
      "     44            1.0000        \u001b[32m0.1052\u001b[0m       0.5833            0.5833        0.9742  0.0006  0.1474\n",
      "     45            1.0000        0.1272       0.5625            0.5625        0.9709  0.0005  0.1383\n",
      "     46            1.0000        0.1333       0.5729            0.5729        0.9702  0.0005  0.1404\n",
      "     47            1.0000        0.1266       0.5729            0.5729        0.9713  0.0004  0.1284\n",
      "     48            1.0000        \u001b[32m0.1019\u001b[0m       0.5729            0.5729        0.9739  0.0004  0.1304\n",
      "     49            1.0000        \u001b[32m0.0943\u001b[0m       0.5938            0.5938        0.9761  0.0003  0.1219\n",
      "     50            1.0000        0.1248       0.6042            0.6042        0.9781  0.0003  0.1176\n",
      "Fine tuning model for subject 1 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        0.9486       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9288\u001b[0m  0.0004  0.1183\n",
      "     32            0.6857        0.8798       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9107\u001b[0m  0.0005  0.1187\n",
      "     33            0.7429        0.9746       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8966\u001b[0m  0.0005  0.1345\n",
      "     34            \u001b[36m0.8286\u001b[0m        0.8442       0.5938            0.5938        \u001b[94m0.8940\u001b[0m  0.0006  0.1375\n",
      "     35            0.8286        \u001b[32m0.5683\u001b[0m       0.5833            0.5833        0.9093  0.0006  0.1297\n",
      "     36            \u001b[36m0.9714\u001b[0m        \u001b[32m0.4649\u001b[0m       0.5625            0.5625        0.9360  0.0007  0.1204\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3492\u001b[0m       0.5729            0.5729        0.9722  0.0007  0.1208\n",
      "     38            1.0000        \u001b[32m0.2747\u001b[0m       0.5417            0.5417        1.0099  0.0007  0.1188\n",
      "     39            1.0000        \u001b[32m0.2146\u001b[0m       0.5625            0.5625        1.0409  0.0007  0.1355\n",
      "     40            1.0000        \u001b[32m0.1782\u001b[0m       0.5729            0.5729        1.0636  0.0007  0.1394\n",
      "     41            1.0000        0.1873       0.5625            0.5625        1.0781  0.0007  0.1405\n",
      "     42            1.0000        \u001b[32m0.1220\u001b[0m       0.5312            0.5312        1.0881  0.0007  0.1288\n",
      "     43            1.0000        \u001b[32m0.1075\u001b[0m       0.5521            0.5521        1.0903  0.0006  0.1337\n",
      "     44            1.0000        0.1278       0.5625            0.5625        1.0849  0.0006  0.1200\n",
      "     45            1.0000        \u001b[32m0.0777\u001b[0m       0.5729            0.5729        1.0773  0.0005  0.1155\n",
      "     46            1.0000        \u001b[32m0.0692\u001b[0m       0.5729            0.5729        1.0688  0.0005  0.1140\n",
      "     47            1.0000        0.1003       0.6042            0.6042        1.0566  0.0004  0.1547\n",
      "     48            1.0000        0.0907       0.6042            0.6042        1.0476  0.0004  0.1320\n",
      "     49            1.0000        \u001b[32m0.0684\u001b[0m       0.6146            0.6146        1.0356  0.0003  0.1206\n",
      "     50            1.0000        0.0812       0.6146            0.6146        1.0243  0.0003  0.1366\n",
      "Fine tuning model for subject 1 with 35 = 35 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8286\u001b[0m        0.8467       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9317\u001b[0m  0.0004  0.1209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8571\u001b[0m        \u001b[32m0.7121\u001b[0m       0.6042            0.6042        \u001b[94m0.9163\u001b[0m  0.0005  0.1372\n",
      "     33            \u001b[36m0.8857\u001b[0m        0.7324       0.5833            0.5833        \u001b[94m0.9048\u001b[0m  0.0005  0.1201\n",
      "     34            \u001b[36m0.9143\u001b[0m        \u001b[32m0.5229\u001b[0m       0.5833            0.5833        \u001b[94m0.8951\u001b[0m  0.0006  0.1307\n",
      "     35            \u001b[36m0.9714\u001b[0m        0.5419       0.5938            0.5938        \u001b[94m0.8864\u001b[0m  0.0006  0.1349\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2791\u001b[0m       0.5833            0.5833        \u001b[94m0.8802\u001b[0m  0.0007  0.1213\n",
      "     37            1.0000        \u001b[32m0.2644\u001b[0m       0.5833            0.5833        \u001b[94m0.8785\u001b[0m  0.0007  0.1356\n",
      "     38            1.0000        \u001b[32m0.1792\u001b[0m       0.5833            0.5833        0.8820  0.0007  0.1465\n",
      "     39            1.0000        \u001b[32m0.1645\u001b[0m       0.5833            0.5833        0.8932  0.0007  0.1183\n",
      "     40            1.0000        \u001b[32m0.1158\u001b[0m       0.5729            0.5729        0.9106  0.0007  0.1356\n",
      "     41            1.0000        0.1181       0.5938            0.5938        0.9324  0.0007  0.1382\n",
      "     42            1.0000        \u001b[32m0.1008\u001b[0m       0.5521            0.5521        0.9597  0.0007  0.1386\n",
      "     43            1.0000        \u001b[32m0.0612\u001b[0m       0.5625            0.5625        0.9886  0.0006  0.1156\n",
      "     44            1.0000        0.1124       0.5833            0.5833        1.0140  0.0006  0.1223\n",
      "     45            1.0000        0.0944       0.5729            0.5729        1.0313  0.0005  0.1230\n",
      "     46            1.0000        0.0842       0.5729            0.5729        1.0409  0.0005  0.1412\n",
      "     47            1.0000        0.0634       0.5521            0.5521        1.0457  0.0004  0.1297\n",
      "     48            1.0000        \u001b[32m0.0592\u001b[0m       0.5521            0.5521        1.0458  0.0004  0.1213\n",
      "     49            1.0000        \u001b[32m0.0564\u001b[0m       0.5521            0.5521        1.0412  0.0003  0.1353\n",
      "     50            1.0000        0.0651       0.5521            0.5521        1.0328  0.0003  0.1198\n",
      "Fine tuning model for subject 1 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5714        1.1365       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9344\u001b[0m  0.0004  0.1046\n",
      "     32            0.6000        1.1602       0.6042            0.6042        \u001b[94m0.9226\u001b[0m  0.0005  0.1281\n",
      "     33            0.6571        1.0351       0.6042            0.6042        \u001b[94m0.9133\u001b[0m  0.0005  0.1297\n",
      "     34            0.7429        0.8831       0.6042            0.6042        \u001b[94m0.9091\u001b[0m  0.0006  0.1205\n",
      "     35            \u001b[36m0.8286\u001b[0m        0.8017       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        0.9121  0.0006  0.1194\n",
      "     36            \u001b[36m0.9143\u001b[0m        \u001b[32m0.6629\u001b[0m       0.6354            0.6354        0.9139  0.0007  0.1095\n",
      "     37            \u001b[36m0.9714\u001b[0m        \u001b[32m0.3949\u001b[0m       0.6042            0.6042        0.9159  0.0007  0.1356\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3477\u001b[0m       0.6146            0.6146        0.9176  0.0007  0.1347\n",
      "     39            1.0000        0.3501       0.6042            0.6042        0.9189  0.0007  0.1402\n",
      "     40            1.0000        \u001b[32m0.2414\u001b[0m       0.5938            0.5938        0.9197  0.0007  0.1245\n",
      "     41            1.0000        0.2470       0.5938            0.5938        0.9208  0.0007  0.1138\n",
      "     42            1.0000        \u001b[32m0.1621\u001b[0m       0.5833            0.5833        0.9223  0.0007  0.1249\n",
      "     43            1.0000        \u001b[32m0.1598\u001b[0m       0.5833            0.5833        0.9248  0.0006  0.1361\n",
      "     44            1.0000        \u001b[32m0.1396\u001b[0m       0.5938            0.5938        0.9284  0.0006  0.1502\n",
      "     45            1.0000        \u001b[32m0.1214\u001b[0m       0.5938            0.5938        0.9303  0.0005  0.1547\n",
      "     46            1.0000        \u001b[32m0.1117\u001b[0m       0.6042            0.6042        0.9327  0.0005  0.1514\n",
      "     47            1.0000        \u001b[32m0.0910\u001b[0m       0.6042            0.6042        0.9341  0.0004  0.1352\n",
      "     48            1.0000        \u001b[32m0.0716\u001b[0m       0.5938            0.5938        0.9348  0.0004  0.1822\n",
      "     49            1.0000        0.0749       0.5938            0.5938        0.9350  0.0003  0.1531\n",
      "     50            1.0000        \u001b[32m0.0711\u001b[0m       0.6146            0.6146        0.9353  0.0003  0.1709\n",
      "Fine tuning model for subject 1 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.1646       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9267\u001b[0m  0.0004  0.1439\n",
      "     32            0.6571        1.1642       0.6042            0.6042        \u001b[94m0.9105\u001b[0m  0.0005  0.1760\n",
      "     33            0.7429        1.0091       0.6042            0.6042        \u001b[94m0.8981\u001b[0m  0.0005  0.1373\n",
      "     34            \u001b[36m0.8571\u001b[0m        0.8863       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8912\u001b[0m  0.0006  0.1592\n",
      "     35            \u001b[36m0.9143\u001b[0m        0.7566       0.5938            0.5938        0.8921  0.0006  0.1250\n",
      "     36            \u001b[36m0.9429\u001b[0m        \u001b[32m0.5009\u001b[0m       0.6042            0.6042        0.9021  0.0007  0.1956\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4450\u001b[0m       0.6042            0.6042        0.9223  0.0007  0.1449\n",
      "     38            1.0000        \u001b[32m0.2833\u001b[0m       0.5729            0.5729        0.9467  0.0007  0.1468\n",
      "     39            1.0000        \u001b[32m0.1981\u001b[0m       0.5417            0.5417        0.9745  0.0007  0.1852\n",
      "     40            1.0000        \u001b[32m0.1823\u001b[0m       0.5417            0.5417        0.9972  0.0007  0.1345\n",
      "     41            1.0000        \u001b[32m0.1620\u001b[0m       0.5625            0.5625        1.0128  0.0007  0.1312\n",
      "     42            1.0000        \u001b[32m0.1358\u001b[0m       0.5625            0.5625        1.0247  0.0007  0.1350\n",
      "     43            1.0000        \u001b[32m0.1038\u001b[0m       0.5729            0.5729        1.0335  0.0006  0.1356\n",
      "     44            1.0000        \u001b[32m0.1034\u001b[0m       0.5625            0.5625        1.0370  0.0006  0.1375\n",
      "     45            1.0000        0.1105       0.5625            0.5625        1.0355  0.0005  0.1259\n",
      "     46            1.0000        \u001b[32m0.0934\u001b[0m       0.5625            0.5625        1.0301  0.0005  0.1321\n",
      "     47            1.0000        0.0956       0.5938            0.5938        1.0222  0.0004  0.1341\n",
      "     48            1.0000        0.1060       0.6042            0.6042        1.0121  0.0004  0.1384\n",
      "     49            1.0000        \u001b[32m0.0674\u001b[0m       0.6042            0.6042        1.0009  0.0003  0.1161\n",
      "     50            1.0000        0.0770       0.5938            0.5938        0.9897  0.0003  0.1115\n",
      "Fine tuning model for subject 1 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6571        0.8724       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.9284\u001b[0m  0.0004  0.1328\n",
      "     32            0.6571        0.8619       0.6146            0.6146        \u001b[94m0.9178\u001b[0m  0.0005  0.1120\n",
      "     33            0.7143        0.8076       0.5938            0.5938        \u001b[94m0.9169\u001b[0m  0.0005  0.1209\n",
      "     34            \u001b[36m0.8000\u001b[0m        \u001b[32m0.6177\u001b[0m       0.5833            0.5833        0.9257  0.0006  0.1191\n",
      "     35            \u001b[36m0.8571\u001b[0m        0.7346       0.5729            0.5729        0.9425  0.0006  0.1351\n",
      "     36            \u001b[36m0.9429\u001b[0m        \u001b[32m0.4522\u001b[0m       0.5729            0.5729        0.9681  0.0007  0.1357\n",
      "     37            0.9429        \u001b[32m0.4202\u001b[0m       0.5521            0.5521        1.0015  0.0007  0.1365\n",
      "     38            0.9429        \u001b[32m0.3235\u001b[0m       0.5625            0.5625        1.0376  0.0007  0.1342\n",
      "     39            0.9429        \u001b[32m0.2254\u001b[0m       0.5521            0.5521        1.0747  0.0007  0.1187\n",
      "     40            \u001b[36m0.9714\u001b[0m        0.2429       0.5417            0.5417        1.0979  0.0007  0.1320\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1698\u001b[0m       0.5521            0.5521        1.1153  0.0007  0.1330\n",
      "     42            1.0000        \u001b[32m0.1509\u001b[0m       0.5625            0.5625        1.1222  0.0007  0.1416\n",
      "     43            1.0000        \u001b[32m0.1461\u001b[0m       0.5625            0.5625        1.1247  0.0006  0.1302\n",
      "     44            1.0000        \u001b[32m0.1257\u001b[0m       0.5521            0.5521        1.1207  0.0006  0.1096\n",
      "     45            1.0000        0.1277       0.5521            0.5521        1.1092  0.0005  0.1217\n",
      "     46            1.0000        \u001b[32m0.1175\u001b[0m       0.5729            0.5729        1.0964  0.0005  0.1181\n",
      "     47            1.0000        \u001b[32m0.0814\u001b[0m       0.5938            0.5938        1.0797  0.0004  0.1177\n",
      "     48            1.0000        \u001b[32m0.0617\u001b[0m       0.5938            0.5938        1.0639  0.0004  0.1195\n",
      "     49            1.0000        0.0634       0.5833            0.5833        1.0475  0.0003  0.1211\n",
      "     50            1.0000        0.0680       0.5938            0.5938        1.0307  0.0003  0.1198\n",
      "Fine tuning model for subject 1 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.2652       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9281\u001b[0m  0.0004  0.1175\n",
      "     32            0.5143        1.2951       0.6250            0.6250        \u001b[94m0.9089\u001b[0m  0.0005  0.1214\n",
      "     33            0.6000        0.9701       0.5833            0.5833        \u001b[94m0.8974\u001b[0m  0.0005  0.1199\n",
      "     34            0.6571        1.0115       0.5521            0.5521        0.8998  0.0006  0.1207\n",
      "     35            \u001b[36m0.8857\u001b[0m        0.8911       0.5312            0.5312        0.9163  0.0006  0.1355\n",
      "     36            \u001b[36m0.9429\u001b[0m        \u001b[32m0.6921\u001b[0m       0.5938            0.5938        0.9382  0.0007  0.1415\n",
      "     37            \u001b[36m0.9714\u001b[0m        \u001b[32m0.4487\u001b[0m       0.5938            0.5938        0.9509  0.0007  0.1284\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4038\u001b[0m       0.5833            0.5833        0.9447  0.0007  0.1327\n",
      "     39            1.0000        \u001b[32m0.3068\u001b[0m       0.6250            0.6250        0.9260  0.0007  0.1252\n",
      "     40            1.0000        0.3192       0.6146            0.6146        0.9021  0.0007  0.1197\n",
      "     41            1.0000        \u001b[32m0.2577\u001b[0m       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8814\u001b[0m  0.0007  0.1332\n",
      "     42            1.0000        \u001b[32m0.2543\u001b[0m       0.6458            0.6458        \u001b[94m0.8720\u001b[0m  0.0007  0.1527\n",
      "     43            1.0000        0.2725       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8702\u001b[0m  0.0006  0.1220\n",
      "     44            1.0000        \u001b[32m0.1411\u001b[0m       0.6458            0.6458        0.8734  0.0006  0.1270\n",
      "     45            1.0000        0.1506       0.6250            0.6250        0.8776  0.0005  0.1119\n",
      "     46            1.0000        \u001b[32m0.1261\u001b[0m       0.6354            0.6354        0.8811  0.0005  0.1328\n",
      "     47            1.0000        \u001b[32m0.1148\u001b[0m       0.6250            0.6250        0.8827  0.0004  0.1195\n",
      "     48            1.0000        \u001b[32m0.0982\u001b[0m       0.6250            0.6250        0.8830  0.0004  0.1349\n",
      "     49            1.0000        0.1203       0.6250            0.6250        0.8825  0.0003  0.1348\n",
      "     50            1.0000        0.1316       0.6250            0.6250        0.8814  0.0003  0.1191\n",
      "Fine tuning model for subject 1 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7250        0.8337       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9330\u001b[0m  0.0004  0.1425\n",
      "     32            0.7750        0.9449       0.6042            0.6042        \u001b[94m0.9237\u001b[0m  0.0005  0.1237\n",
      "     33            \u001b[36m0.8250\u001b[0m        0.9217       0.5729            0.5729        \u001b[94m0.9199\u001b[0m  0.0005  0.1348\n",
      "     34            \u001b[36m0.8750\u001b[0m        \u001b[32m0.7207\u001b[0m       0.5938            0.5938        0.9203  0.0006  0.1318\n",
      "     35            \u001b[36m0.9250\u001b[0m        \u001b[32m0.5627\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        0.9244  0.0006  0.1353\n",
      "     36            0.9250        \u001b[32m0.4260\u001b[0m       0.6042            0.6042        0.9318  0.0007  0.1400\n",
      "     37            \u001b[36m0.9750\u001b[0m        \u001b[32m0.4121\u001b[0m       0.6042            0.6042        0.9429  0.0007  0.1319\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2743\u001b[0m       0.5938            0.5938        0.9558  0.0007  0.1345\n",
      "     39            1.0000        \u001b[32m0.2400\u001b[0m       0.5729            0.5729        0.9718  0.0007  0.1350\n",
      "     40            1.0000        \u001b[32m0.1810\u001b[0m       0.5833            0.5833        0.9872  0.0007  0.1360\n",
      "     41            1.0000        \u001b[32m0.1576\u001b[0m       0.5625            0.5625        1.0028  0.0007  0.1292\n",
      "     42            1.0000        \u001b[32m0.1490\u001b[0m       0.5521            0.5521        1.0171  0.0007  0.1396\n",
      "     43            1.0000        \u001b[32m0.1042\u001b[0m       0.5417            0.5417        1.0292  0.0006  0.1307\n",
      "     44            1.0000        0.1404       0.5312            0.5312        1.0390  0.0006  0.1352\n",
      "     45            1.0000        0.1367       0.5104            0.5104        1.0466  0.0005  0.1446\n",
      "     46            1.0000        0.1116       0.5208            0.5208        1.0523  0.0005  0.1296\n",
      "     47            1.0000        \u001b[32m0.0819\u001b[0m       0.5000            0.5000        1.0558  0.0004  0.1376\n",
      "     48            1.0000        0.0909       0.4896            0.4896        1.0576  0.0004  0.1388\n",
      "     49            1.0000        \u001b[32m0.0749\u001b[0m       0.4896            0.4896        1.0579  0.0003  0.1471\n",
      "     50            1.0000        0.0834       0.4896            0.4896        1.0578  0.0003  0.1332\n",
      "Fine tuning model for subject 1 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5500        1.0730       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9299\u001b[0m  0.0004  0.1360\n",
      "     32            0.6250        1.1686       0.6250            0.6250        \u001b[94m0.9074\u001b[0m  0.0005  0.1305\n",
      "     33            0.6750        0.9397       0.6146            0.6146        \u001b[94m0.8872\u001b[0m  0.0005  0.1286\n",
      "     34            0.7500        0.9324       0.6146            0.6146        \u001b[94m0.8712\u001b[0m  0.0006  0.1366\n",
      "     35            \u001b[36m0.8250\u001b[0m        \u001b[32m0.7228\u001b[0m       0.6250            0.6250        \u001b[94m0.8598\u001b[0m  0.0006  0.1210\n",
      "     36            \u001b[36m0.9250\u001b[0m        \u001b[32m0.5969\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.8534\u001b[0m  0.0007  0.1337\n",
      "     37            \u001b[36m0.9750\u001b[0m        \u001b[32m0.5187\u001b[0m       0.6354            0.6354        0.8552  0.0007  0.1497\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3794\u001b[0m       0.6146            0.6146        0.8624  0.0007  0.1379\n",
      "     39            1.0000        \u001b[32m0.3131\u001b[0m       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        0.8703  0.0007  0.2012\n",
      "     40            1.0000        \u001b[32m0.2775\u001b[0m       0.6250            0.6250        0.8761  0.0007  0.1833\n",
      "     41            1.0000        \u001b[32m0.2044\u001b[0m       0.6250            0.6250        0.8759  0.0007  0.1666\n",
      "     42            1.0000        0.2057       0.6250            0.6250        0.8738  0.0007  0.1827\n",
      "     43            1.0000        \u001b[32m0.1860\u001b[0m       0.6458            0.6458        0.8746  0.0006  0.1850\n",
      "     44            1.0000        \u001b[32m0.1722\u001b[0m       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        0.8774  0.0006  0.1937\n",
      "     45            1.0000        \u001b[32m0.1179\u001b[0m       0.6354            0.6354        0.8804  0.0005  0.1978\n",
      "     46            1.0000        \u001b[32m0.1093\u001b[0m       0.6250            0.6250        0.8834  0.0005  0.1517\n",
      "     47            1.0000        0.1298       0.6042            0.6042        0.8856  0.0004  0.1660\n",
      "     48            1.0000        0.1177       0.5833            0.5833        0.8874  0.0004  0.1666\n",
      "     49            1.0000        0.1114       0.5833            0.5833        0.8896  0.0003  0.1787\n",
      "     50            1.0000        0.1265       0.5938            0.5938        0.8907  0.0003  0.1599\n",
      "Fine tuning model for subject 1 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6750        0.8419       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9291\u001b[0m  0.0004  0.1834\n",
      "     32            0.7000        0.8506       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9120\u001b[0m  0.0005  0.1291\n",
      "     33            0.7250        0.8413       0.6042            0.6042        \u001b[94m0.8960\u001b[0m  0.0005  0.1599\n",
      "     34            \u001b[36m0.8250\u001b[0m        \u001b[32m0.7374\u001b[0m       0.6042            0.6042        \u001b[94m0.8814\u001b[0m  0.0006  0.1288\n",
      "     35            \u001b[36m0.9750\u001b[0m        \u001b[32m0.7244\u001b[0m       0.5833            0.5833        \u001b[94m0.8723\u001b[0m  0.0006  0.1183\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5694\u001b[0m       0.6042            0.6042        \u001b[94m0.8718\u001b[0m  0.0007  0.1230\n",
      "     37            1.0000        \u001b[32m0.4045\u001b[0m       0.6042            0.6042        0.8787  0.0007  0.1268\n",
      "     38            1.0000        \u001b[32m0.3613\u001b[0m       0.5833            0.5833        0.8926  0.0007  0.1297\n",
      "     39            1.0000        \u001b[32m0.2975\u001b[0m       0.5729            0.5729        0.9138  0.0007  0.1161\n",
      "     40            1.0000        \u001b[32m0.2489\u001b[0m       0.5729            0.5729        0.9388  0.0007  0.1358\n",
      "     41            1.0000        \u001b[32m0.2007\u001b[0m       0.5625            0.5625        0.9655  0.0007  0.1342\n",
      "     42            1.0000        0.2142       0.5625            0.5625        0.9898  0.0007  0.1357\n",
      "     43            1.0000        \u001b[32m0.1734\u001b[0m       0.5521            0.5521        1.0087  0.0006  0.1351\n",
      "     44            1.0000        \u001b[32m0.1365\u001b[0m       0.5521            0.5521        1.0218  0.0006  0.1351\n",
      "     45            1.0000        \u001b[32m0.1143\u001b[0m       0.5417            0.5417        1.0302  0.0005  0.1351\n",
      "     46            1.0000        \u001b[32m0.1059\u001b[0m       0.5417            0.5417        1.0349  0.0005  0.1359\n",
      "     47            1.0000        0.1477       0.5521            0.5521        1.0363  0.0004  0.1353\n",
      "     48            1.0000        0.1249       0.5312            0.5312        1.0341  0.0004  0.1354\n",
      "     49            1.0000        \u001b[32m0.0924\u001b[0m       0.5312            0.5312        1.0294  0.0003  0.1507\n",
      "     50            1.0000        \u001b[32m0.0906\u001b[0m       0.5208            0.5208        1.0232  0.0003  0.1356\n",
      "Fine tuning model for subject 1 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6500        1.0621       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9326\u001b[0m  0.0004  0.1129\n",
      "     32            0.7000        1.1317       0.6146            0.6146        \u001b[94m0.9230\u001b[0m  0.0005  0.1375\n",
      "     33            0.7000        0.8188       0.5833            0.5833        \u001b[94m0.9130\u001b[0m  0.0005  0.1355\n",
      "     34            0.7750        0.7999       0.6042            0.6042        \u001b[94m0.9053\u001b[0m  0.0006  0.1341\n",
      "     35            \u001b[36m0.8500\u001b[0m        0.7510       0.6250            0.6250        \u001b[94m0.8998\u001b[0m  0.0006  0.1352\n",
      "     36            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5395\u001b[0m       0.6146            0.6146        0.9025  0.0007  0.1362\n",
      "     37            \u001b[36m0.9750\u001b[0m        \u001b[32m0.4186\u001b[0m       0.6146            0.6146        0.9099  0.0007  0.1279\n",
      "     38            \u001b[36m1.0000\u001b[0m        0.4622       0.6042            0.6042        0.9234  0.0007  0.1287\n",
      "     39            1.0000        \u001b[32m0.3072\u001b[0m       0.5938            0.5938        0.9370  0.0007  0.1203\n",
      "     40            1.0000        \u001b[32m0.2380\u001b[0m       0.5833            0.5833        0.9523  0.0007  0.1357\n",
      "     41            1.0000        \u001b[32m0.2022\u001b[0m       0.5729            0.5729        0.9618  0.0007  0.1343\n",
      "     42            1.0000        \u001b[32m0.1855\u001b[0m       0.5625            0.5625        0.9666  0.0007  0.1354\n",
      "     43            1.0000        \u001b[32m0.1694\u001b[0m       0.5625            0.5625        0.9675  0.0006  0.1353\n",
      "     44            1.0000        \u001b[32m0.1189\u001b[0m       0.5625            0.5625        0.9656  0.0006  0.1349\n",
      "     45            1.0000        \u001b[32m0.1178\u001b[0m       0.5729            0.5729        0.9616  0.0005  0.1354\n",
      "     46            1.0000        \u001b[32m0.1030\u001b[0m       0.5521            0.5521        0.9563  0.0005  0.1664\n",
      "     47            1.0000        \u001b[32m0.0717\u001b[0m       0.5417            0.5417        0.9494  0.0004  0.1287\n",
      "     48            1.0000        0.1128       0.5625            0.5625        0.9413  0.0004  0.1354\n",
      "     49            1.0000        0.0755       0.5938            0.5938        0.9323  0.0003  0.1365\n",
      "     50            1.0000        0.0908       0.6146            0.6146        0.9228  0.0003  0.1291\n",
      "Fine tuning model for subject 1 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5750        1.0372       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m0.9324\u001b[0m  0.0004  0.1346\n",
      "     32            0.6250        1.1080       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9198\u001b[0m  0.0005  0.1402\n",
      "     33            0.6750        0.9475       0.6146            0.6146        \u001b[94m0.9060\u001b[0m  0.0005  0.1190\n",
      "     34            \u001b[36m0.8000\u001b[0m        0.9069       0.6146            0.6146        \u001b[94m0.8969\u001b[0m  0.0006  0.1363\n",
      "     35            \u001b[36m0.8500\u001b[0m        0.7536       0.6042            0.6042        0.8984  0.0006  0.1516\n",
      "     36            \u001b[36m0.9500\u001b[0m        \u001b[32m0.6626\u001b[0m       0.5833            0.5833        0.9132  0.0007  0.1175\n",
      "     37            \u001b[36m0.9750\u001b[0m        \u001b[32m0.4043\u001b[0m       0.5417            0.5417        0.9418  0.0007  0.1353\n",
      "     38            0.9750        \u001b[32m0.3974\u001b[0m       0.5417            0.5417        0.9832  0.0007  0.1344\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3300\u001b[0m       0.5312            0.5312        1.0252  0.0007  0.1358\n",
      "     40            1.0000        \u001b[32m0.2110\u001b[0m       0.5208            0.5208        1.0614  0.0007  0.1396\n",
      "     41            1.0000        \u001b[32m0.1836\u001b[0m       0.5208            0.5208        1.0889  0.0007  0.1431\n",
      "     42            1.0000        0.2068       0.5208            0.5208        1.1070  0.0007  0.1354\n",
      "     43            1.0000        \u001b[32m0.1392\u001b[0m       0.5208            0.5208        1.1164  0.0006  0.1418\n",
      "     44            1.0000        \u001b[32m0.1103\u001b[0m       0.5208            0.5208        1.1154  0.0006  0.1370\n",
      "     45            1.0000        \u001b[32m0.1038\u001b[0m       0.5104            0.5104        1.1095  0.0005  0.1243\n",
      "     46            1.0000        \u001b[32m0.0969\u001b[0m       0.5104            0.5104        1.0973  0.0005  0.1266\n",
      "     47            1.0000        0.1037       0.5000            0.5000        1.0836  0.0004  0.1374\n",
      "     48            1.0000        \u001b[32m0.0643\u001b[0m       0.5417            0.5417        1.0693  0.0004  0.1437\n",
      "     49            1.0000        0.0731       0.5521            0.5521        1.0542  0.0003  0.1537\n",
      "     50            1.0000        0.0847       0.5521            0.5521        1.0398  0.0003  0.1544\n",
      "Fine tuning model for subject 1 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5500        1.0240       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9255\u001b[0m  0.0004  0.1420\n",
      "     32            0.5750        1.0698       0.5833            0.5833        \u001b[94m0.9089\u001b[0m  0.0005  0.1416\n",
      "     33            0.7000        0.8524       0.5833            0.5833        \u001b[94m0.8965\u001b[0m  0.0005  0.1334\n",
      "     34            \u001b[36m0.8000\u001b[0m        0.7724       0.5833            0.5833        \u001b[94m0.8927\u001b[0m  0.0006  0.1328\n",
      "     35            \u001b[36m0.9250\u001b[0m        \u001b[32m0.5221\u001b[0m       0.5938            0.5938        \u001b[94m0.8880\u001b[0m  0.0006  0.1347\n",
      "     36            \u001b[36m0.9750\u001b[0m        0.5382       0.5625            0.5625        \u001b[94m0.8784\u001b[0m  0.0007  0.1599\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4556\u001b[0m       0.6042            0.6042        \u001b[94m0.8647\u001b[0m  0.0007  0.1432\n",
      "     38            1.0000        \u001b[32m0.3406\u001b[0m       0.6042            0.6042        \u001b[94m0.8540\u001b[0m  0.0007  0.1328\n",
      "     39            1.0000        0.3634       0.5833            0.5833        \u001b[94m0.8496\u001b[0m  0.0007  0.1314\n",
      "     40            1.0000        \u001b[32m0.1848\u001b[0m       0.5833            0.5833        0.8549  0.0007  0.1493\n",
      "     41            1.0000        0.1932       0.5833            0.5833        0.8630  0.0007  0.1338\n",
      "     42            1.0000        \u001b[32m0.1720\u001b[0m       0.5833            0.5833        0.8690  0.0007  0.1415\n",
      "     43            1.0000        \u001b[32m0.1403\u001b[0m       0.6042            0.6042        0.8713  0.0006  0.1532\n",
      "     44            1.0000        0.1734       0.5938            0.5938        0.8703  0.0006  0.1333\n",
      "     45            1.0000        \u001b[32m0.1282\u001b[0m       0.5833            0.5833        0.8682  0.0005  0.1402\n",
      "     46            1.0000        \u001b[32m0.0985\u001b[0m       0.5938            0.5938        0.8670  0.0005  0.1425\n",
      "     47            1.0000        0.1059       0.6042            0.6042        0.8663  0.0004  0.1958\n",
      "     48            1.0000        \u001b[32m0.0889\u001b[0m       0.6146            0.6146        0.8658  0.0004  0.1346\n",
      "     49            1.0000        0.1089       0.6042            0.6042        0.8662  0.0003  0.1572\n",
      "     50            1.0000        \u001b[32m0.0717\u001b[0m       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        0.8676  0.0003  0.1614\n",
      "Fine tuning model for subject 1 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7250        0.9129       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9311\u001b[0m  0.0004  0.1712\n",
      "     32            0.7250        0.8202       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9197\u001b[0m  0.0005  0.1609\n",
      "     33            0.7750        0.8390       0.6250            0.6250        \u001b[94m0.9112\u001b[0m  0.0005  0.1566\n",
      "     34            \u001b[36m0.9000\u001b[0m        \u001b[32m0.7170\u001b[0m       0.5938            0.5938        \u001b[94m0.9047\u001b[0m  0.0006  0.1755\n",
      "     35            \u001b[36m0.9500\u001b[0m        \u001b[32m0.6813\u001b[0m       0.5938            0.5938        \u001b[94m0.9036\u001b[0m  0.0006  0.1811\n",
      "     36            \u001b[36m0.9750\u001b[0m        \u001b[32m0.4294\u001b[0m       0.5833            0.5833        0.9108  0.0007  0.1517\n",
      "     37            \u001b[36m1.0000\u001b[0m        0.4367       0.5833            0.5833        0.9258  0.0007  0.1680\n",
      "     38            1.0000        \u001b[32m0.3295\u001b[0m       0.5521            0.5521        0.9462  0.0007  0.1658\n",
      "     39            1.0000        \u001b[32m0.2439\u001b[0m       0.5312            0.5312        0.9689  0.0007  0.1477\n",
      "     40            1.0000        \u001b[32m0.2012\u001b[0m       0.5521            0.5521        0.9892  0.0007  0.1971\n",
      "     41            1.0000        \u001b[32m0.1796\u001b[0m       0.5417            0.5417        1.0046  0.0007  0.1776\n",
      "     42            1.0000        \u001b[32m0.1169\u001b[0m       0.5417            0.5417        1.0135  0.0007  0.2223\n",
      "     43            1.0000        0.1184       0.5521            0.5521        1.0186  0.0006  0.1487\n",
      "     44            1.0000        0.1224       0.5625            0.5625        1.0206  0.0006  0.1521\n",
      "     45            1.0000        \u001b[32m0.1077\u001b[0m       0.5625            0.5625        1.0177  0.0005  0.1512\n",
      "     46            1.0000        \u001b[32m0.1015\u001b[0m       0.5625            0.5625        1.0127  0.0005  0.1384\n",
      "     47            1.0000        \u001b[32m0.0635\u001b[0m       0.5521            0.5521        1.0085  0.0004  0.1519\n",
      "     48            1.0000        0.0819       0.5417            0.5417        1.0050  0.0004  0.1483\n",
      "     49            1.0000        0.0799       0.5417            0.5417        1.0018  0.0003  0.1532\n",
      "     50            1.0000        0.0640       0.5521            0.5521        0.9993  0.0003  0.1468\n",
      "Fine tuning model for subject 1 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6500        0.8184       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m0.9314\u001b[0m  0.0004  0.1418\n",
      "     32            0.6750        0.9172       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9187\u001b[0m  0.0005  0.1506\n",
      "     33            0.7500        0.8224       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9097\u001b[0m  0.0005  0.1244\n",
      "     34            \u001b[36m0.8250\u001b[0m        0.7679       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9074\u001b[0m  0.0006  0.1316\n",
      "     35            \u001b[36m0.8750\u001b[0m        \u001b[32m0.6224\u001b[0m       0.6042            0.6042        0.9125  0.0006  0.1580\n",
      "     36            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4629\u001b[0m       0.6146            0.6146        0.9280  0.0007  0.1332\n",
      "     37            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3960\u001b[0m       0.5938            0.5938        0.9478  0.0007  0.1415\n",
      "     38            0.9750        \u001b[32m0.3512\u001b[0m       0.5729            0.5729        0.9684  0.0007  0.1480\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2511\u001b[0m       0.5729            0.5729        0.9855  0.0007  0.1455\n",
      "     40            1.0000        0.2730       0.5833            0.5833        0.9991  0.0007  0.1385\n",
      "     41            1.0000        \u001b[32m0.1804\u001b[0m       0.5833            0.5833        1.0005  0.0007  0.1388\n",
      "     42            1.0000        \u001b[32m0.1563\u001b[0m       0.5729            0.5729        0.9981  0.0007  0.1465\n",
      "     43            1.0000        0.1687       0.5729            0.5729        0.9910  0.0006  0.1354\n",
      "     44            1.0000        0.1563       0.5729            0.5729        0.9806  0.0006  0.1354\n",
      "     45            1.0000        \u001b[32m0.1330\u001b[0m       0.5833            0.5833        0.9723  0.0005  0.1503\n",
      "     46            1.0000        0.1436       0.5938            0.5938        0.9629  0.0005  0.1556\n",
      "     47            1.0000        \u001b[32m0.0909\u001b[0m       0.5729            0.5729        0.9537  0.0004  0.1419\n",
      "     48            1.0000        0.0943       0.5729            0.5729        0.9468  0.0004  0.1491\n",
      "     49            1.0000        \u001b[32m0.0742\u001b[0m       0.5729            0.5729        0.9411  0.0003  0.1210\n",
      "     50            1.0000        0.0845       0.5729            0.5729        0.9371  0.0003  0.1546\n",
      "Fine tuning model for subject 1 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7250        0.9855       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9309\u001b[0m  0.0004  0.1287\n",
      "     32            0.7250        1.0069       0.5729            0.5729        0.9329  0.0005  0.1492\n",
      "     33            \u001b[36m0.8500\u001b[0m        0.7526       0.5521            0.5521        0.9653  0.0005  0.1205\n",
      "     34            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6412\u001b[0m       0.5521            0.5521        1.0262  0.0006  0.1331\n",
      "     35            \u001b[36m0.9250\u001b[0m        \u001b[32m0.6142\u001b[0m       0.5312            0.5312        1.1081  0.0006  0.1383\n",
      "     36            0.9000        \u001b[32m0.5074\u001b[0m       0.5417            0.5417        1.1834  0.0007  0.1337\n",
      "     37            0.9000        \u001b[32m0.4407\u001b[0m       0.5208            0.5208        1.2280  0.0007  0.1326\n",
      "     38            \u001b[36m0.9500\u001b[0m        \u001b[32m0.3202\u001b[0m       0.5208            0.5208        1.2460  0.0007  0.1334\n",
      "     39            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2761\u001b[0m       0.5208            0.5208        1.2249  0.0007  0.1414\n",
      "     40            \u001b[36m1.0000\u001b[0m        0.3124       0.5417            0.5417        1.1836  0.0007  0.1449\n",
      "     41            1.0000        \u001b[32m0.2234\u001b[0m       0.5208            0.5208        1.1353  0.0007  0.1537\n",
      "     42            1.0000        \u001b[32m0.1887\u001b[0m       0.5208            0.5208        1.0897  0.0007  0.1511\n",
      "     43            1.0000        \u001b[32m0.1732\u001b[0m       0.5208            0.5208        1.0563  0.0006  0.1344\n",
      "     44            1.0000        \u001b[32m0.1268\u001b[0m       0.5833            0.5833        1.0302  0.0006  0.1511\n",
      "     45            1.0000        0.1382       0.5938            0.5938        1.0123  0.0005  0.1501\n",
      "     46            1.0000        0.1426       0.6042            0.6042        0.9991  0.0005  0.1516\n",
      "     47            1.0000        \u001b[32m0.1159\u001b[0m       0.6042            0.6042        0.9949  0.0004  0.1514\n",
      "     48            1.0000        0.1347       0.6146            0.6146        0.9944  0.0004  0.1372\n",
      "     49            1.0000        \u001b[32m0.1052\u001b[0m       0.6146            0.6146        0.9965  0.0003  0.1523\n",
      "     50            1.0000        \u001b[32m0.0876\u001b[0m       0.5833            0.5833        1.0000  0.0003  0.1344\n",
      "Fine tuning model for subject 1 with 40 = 40 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6750        0.9585       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9326\u001b[0m  0.0004  0.1169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        1.0357       0.6042            0.6042        \u001b[94m0.9251\u001b[0m  0.0005  0.1342\n",
      "     33            0.7500        0.9128       0.5729            0.5729        \u001b[94m0.9245\u001b[0m  0.0005  0.1356\n",
      "     34            \u001b[36m0.8250\u001b[0m        0.7861       0.5833            0.5833        0.9283  0.0006  0.1385\n",
      "     35            \u001b[36m0.8500\u001b[0m        \u001b[32m0.7350\u001b[0m       0.5938            0.5938        0.9329  0.0006  0.1315\n",
      "     36            \u001b[36m0.8750\u001b[0m        \u001b[32m0.6776\u001b[0m       0.5938            0.5938        0.9397  0.0007  0.1339\n",
      "     37            \u001b[36m0.9750\u001b[0m        \u001b[32m0.4518\u001b[0m       0.5833            0.5833        0.9489  0.0007  0.1360\n",
      "     38            0.9750        \u001b[32m0.4021\u001b[0m       0.5833            0.5833        0.9614  0.0007  0.1324\n",
      "     39            0.9750        \u001b[32m0.3428\u001b[0m       0.5833            0.5833        0.9787  0.0007  0.1352\n",
      "     40            0.9750        \u001b[32m0.2860\u001b[0m       0.5417            0.5417        0.9991  0.0007  0.1395\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2211\u001b[0m       0.5417            0.5417        1.0233  0.0007  0.1331\n",
      "     42            1.0000        0.2218       0.5521            0.5521        1.0476  0.0007  0.1420\n",
      "     43            1.0000        \u001b[32m0.1611\u001b[0m       0.5312            0.5312        1.0721  0.0006  0.1324\n",
      "     44            1.0000        0.1805       0.5417            0.5417        1.0930  0.0006  0.1616\n",
      "     45            1.0000        0.1734       0.5417            0.5417        1.1110  0.0005  0.1409\n",
      "     46            1.0000        \u001b[32m0.1245\u001b[0m       0.5417            0.5417        1.1241  0.0005  0.1535\n",
      "     47            1.0000        0.1263       0.5312            0.5312        1.1309  0.0004  0.1738\n",
      "     48            1.0000        \u001b[32m0.0936\u001b[0m       0.5417            0.5417        1.1325  0.0004  0.1418\n",
      "     49            1.0000        0.1018       0.5417            0.5417        1.1297  0.0003  0.1384\n",
      "     50            1.0000        \u001b[32m0.0594\u001b[0m       0.5417            0.5417        1.1246  0.0003  0.1423\n",
      "Fine tuning model for subject 1 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6889        0.9706       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9358\u001b[0m  0.0004  0.1560\n",
      "     32            0.7333        0.8630       0.5938            0.5938        0.9362  0.0005  0.1612\n",
      "     33            0.7333        0.8891       0.5729            0.5729        0.9453  0.0005  0.1600\n",
      "     34            \u001b[36m0.8000\u001b[0m        0.8821       0.5729            0.5729        0.9588  0.0006  0.1662\n",
      "     35            \u001b[36m0.8667\u001b[0m        \u001b[32m0.7172\u001b[0m       0.5833            0.5833        0.9755  0.0006  0.1743\n",
      "     36            \u001b[36m0.9556\u001b[0m        \u001b[32m0.5317\u001b[0m       0.5312            0.5312        0.9964  0.0007  0.1757\n",
      "     37            0.9556        \u001b[32m0.5022\u001b[0m       0.5104            0.5104        1.0184  0.0007  0.1532\n",
      "     38            0.9556        \u001b[32m0.3813\u001b[0m       0.4896            0.4896        1.0386  0.0007  0.1600\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3258\u001b[0m       0.5104            0.5104        1.0584  0.0007  0.1661\n",
      "     40            1.0000        \u001b[32m0.2474\u001b[0m       0.5000            0.5000        1.0754  0.0007  0.1912\n",
      "     41            1.0000        0.2696       0.5104            0.5104        1.0912  0.0007  0.1778\n",
      "     42            1.0000        \u001b[32m0.2321\u001b[0m       0.4896            0.4896        1.1067  0.0007  0.1793\n",
      "     43            1.0000        \u001b[32m0.1996\u001b[0m       0.5000            0.5000        1.1209  0.0006  0.1719\n",
      "     44            1.0000        \u001b[32m0.1542\u001b[0m       0.4896            0.4896        1.1345  0.0006  0.1724\n",
      "     45            1.0000        0.1773       0.4896            0.4896        1.1449  0.0005  0.1565\n",
      "     46            1.0000        \u001b[32m0.1316\u001b[0m       0.5000            0.5000        1.1526  0.0005  0.1913\n",
      "     47            1.0000        \u001b[32m0.1113\u001b[0m       0.4896            0.4896        1.1564  0.0004  0.1724\n",
      "     48            1.0000        0.1466       0.4896            0.4896        1.1554  0.0004  0.1744\n",
      "     49            1.0000        0.1331       0.4896            0.4896        1.1510  0.0003  0.2125\n",
      "     50            1.0000        \u001b[32m0.1030\u001b[0m       0.5208            0.5208        1.1449  0.0003  0.1525\n",
      "Fine tuning model for subject 1 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5333        1.2912       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9310\u001b[0m  0.0004  0.1356\n",
      "     32            0.6000        1.1842       0.6146            0.6146        \u001b[94m0.9149\u001b[0m  0.0005  0.1417\n",
      "     33            0.6444        1.1028       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.8994\u001b[0m  0.0005  0.1459\n",
      "     34            0.6889        1.1261       0.6146            0.6146        \u001b[94m0.8832\u001b[0m  0.0006  0.1629\n",
      "     35            \u001b[36m0.8222\u001b[0m        0.8249       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8737\u001b[0m  0.0006  0.1435\n",
      "     36            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6502\u001b[0m       0.6458            0.6458        \u001b[94m0.8703\u001b[0m  0.0007  0.1510\n",
      "     37            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5170\u001b[0m       0.6458            0.6458        0.8765  0.0007  0.1349\n",
      "     38            \u001b[36m0.9556\u001b[0m        \u001b[32m0.4345\u001b[0m       0.6458            0.6458        0.8918  0.0007  0.1518\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3376\u001b[0m       0.6250            0.6250        0.9156  0.0007  0.1587\n",
      "     40            1.0000        \u001b[32m0.3182\u001b[0m       0.5625            0.5625        0.9492  0.0007  0.1490\n",
      "     41            1.0000        \u001b[32m0.2338\u001b[0m       0.5312            0.5312        0.9858  0.0007  0.1534\n",
      "     42            1.0000        \u001b[32m0.1641\u001b[0m       0.5208            0.5208        1.0224  0.0007  0.1429\n",
      "     43            1.0000        0.1775       0.5208            0.5208        1.0513  0.0006  0.1432\n",
      "     44            0.9778        0.1646       0.5104            0.5104        1.0690  0.0006  0.1432\n",
      "     45            0.9556        \u001b[32m0.1532\u001b[0m       0.5208            0.5208        1.0704  0.0005  0.1431\n",
      "     46            0.9778        \u001b[32m0.1201\u001b[0m       0.5312            0.5312        1.0607  0.0005  0.1708\n",
      "     47            0.9778        0.1373       0.5729            0.5729        1.0423  0.0004  0.1457\n",
      "     48            0.9778        0.1260       0.5833            0.5833        1.0185  0.0004  0.1668\n",
      "     49            0.9778        \u001b[32m0.1136\u001b[0m       0.5833            0.5833        0.9942  0.0003  0.1400\n",
      "     50            1.0000        \u001b[32m0.0794\u001b[0m       0.5938            0.5938        0.9716  0.0003  0.1505\n",
      "Fine tuning model for subject 1 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5111        1.2897       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9279\u001b[0m  0.0004  0.1356\n",
      "     32            0.4889        1.2533       0.6146            0.6146        \u001b[94m0.9116\u001b[0m  0.0005  0.1648\n",
      "     33            0.6000        1.1883       0.5938            0.5938        \u001b[94m0.9006\u001b[0m  0.0005  0.1436\n",
      "     34            0.6889        1.1326       0.6146            0.6146        \u001b[94m0.8934\u001b[0m  0.0006  0.1444\n",
      "     35            0.7556        0.9001       0.6250            0.6250        \u001b[94m0.8901\u001b[0m  0.0006  0.1531\n",
      "     36            \u001b[36m0.8000\u001b[0m        \u001b[32m0.7384\u001b[0m       0.6250            0.6250        \u001b[94m0.8871\u001b[0m  0.0007  0.1455\n",
      "     37            \u001b[36m0.8889\u001b[0m        \u001b[32m0.6781\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.8816\u001b[0m  0.0007  0.1516\n",
      "     38            \u001b[36m0.9111\u001b[0m        \u001b[32m0.5275\u001b[0m       0.6146            0.6146        \u001b[94m0.8783\u001b[0m  0.0007  0.1354\n",
      "     39            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3787\u001b[0m       0.6146            0.6146        \u001b[94m0.8774\u001b[0m  0.0007  0.1530\n",
      "     40            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3364\u001b[0m       0.6146            0.6146        \u001b[94m0.8748\u001b[0m  0.0007  0.1529\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2913\u001b[0m       0.6146            0.6146        \u001b[94m0.8686\u001b[0m  0.0007  0.1440\n",
      "     42            1.0000        \u001b[32m0.2403\u001b[0m       0.6042            0.6042        \u001b[94m0.8645\u001b[0m  0.0007  0.1528\n",
      "     43            1.0000        \u001b[32m0.2377\u001b[0m       0.6250            0.6250        \u001b[94m0.8585\u001b[0m  0.0006  0.1466\n",
      "     44            1.0000        \u001b[32m0.1728\u001b[0m       0.6250            0.6250        \u001b[94m0.8527\u001b[0m  0.0006  0.1431\n",
      "     45            1.0000        0.2333       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8478\u001b[0m  0.0005  0.1494\n",
      "     46            1.0000        \u001b[32m0.1561\u001b[0m       0.6458            0.6458        \u001b[94m0.8425\u001b[0m  0.0005  0.1511\n",
      "     47            1.0000        0.1778       0.6458            0.6458        \u001b[94m0.8364\u001b[0m  0.0004  0.1365\n",
      "     48            1.0000        \u001b[32m0.1466\u001b[0m       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8296\u001b[0m  0.0004  0.1355\n",
      "     49            1.0000        \u001b[32m0.1239\u001b[0m       0.6562            0.6562        \u001b[94m0.8240\u001b[0m  0.0003  0.1509\n",
      "     50            1.0000        0.1275       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8185\u001b[0m  0.0003  0.1531\n",
      "Fine tuning model for subject 1 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6222        1.1789       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9331\u001b[0m  0.0004  0.1303\n",
      "     32            0.6889        1.2482       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.9253\u001b[0m  0.0005  0.1562\n",
      "     33            \u001b[36m0.8000\u001b[0m        0.9576       0.6250            0.6250        \u001b[94m0.9234\u001b[0m  0.0005  0.1446\n",
      "     34            \u001b[36m0.8889\u001b[0m        0.9009       0.5938            0.5938        0.9348  0.0006  0.1305\n",
      "     35            0.8667        \u001b[32m0.6850\u001b[0m       0.5625            0.5625        0.9553  0.0006  0.1332\n",
      "     36            \u001b[36m0.9111\u001b[0m        \u001b[32m0.6471\u001b[0m       0.5625            0.5625        0.9865  0.0007  0.1383\n",
      "     37            \u001b[36m0.9556\u001b[0m        \u001b[32m0.4900\u001b[0m       0.5729            0.5729        1.0151  0.0007  0.1512\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4190\u001b[0m       0.5729            0.5729        1.0445  0.0007  0.1483\n",
      "     39            1.0000        \u001b[32m0.2837\u001b[0m       0.5625            0.5625        1.0636  0.0007  0.1465\n",
      "     40            1.0000        0.3156       0.5312            0.5312        1.0825  0.0007  0.1555\n",
      "     41            1.0000        \u001b[32m0.2685\u001b[0m       0.5521            0.5521        1.0962  0.0007  0.1398\n",
      "     42            1.0000        \u001b[32m0.2223\u001b[0m       0.5417            0.5417        1.1077  0.0007  0.1511\n",
      "     43            1.0000        \u001b[32m0.1869\u001b[0m       0.5208            0.5208        1.1177  0.0006  0.1399\n",
      "     44            1.0000        \u001b[32m0.1571\u001b[0m       0.5208            0.5208        1.1284  0.0006  0.1468\n",
      "     45            1.0000        \u001b[32m0.1515\u001b[0m       0.5312            0.5312        1.1365  0.0005  0.1386\n",
      "     46            1.0000        \u001b[32m0.1208\u001b[0m       0.5208            0.5208        1.1456  0.0005  0.1493\n",
      "     47            1.0000        0.1401       0.5208            0.5208        1.1537  0.0004  0.1504\n",
      "     48            1.0000        \u001b[32m0.1162\u001b[0m       0.5208            0.5208        1.1589  0.0004  0.1482\n",
      "     49            1.0000        0.1260       0.5208            0.5208        1.1617  0.0003  0.1596\n",
      "     50            1.0000        \u001b[32m0.1130\u001b[0m       0.5208            0.5208        1.1640  0.0003  0.1451\n",
      "Fine tuning model for subject 1 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6222        1.1142       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9267\u001b[0m  0.0004  0.1426\n",
      "     32            0.6444        1.1475       0.6042            0.6042        \u001b[94m0.9101\u001b[0m  0.0005  0.1705\n",
      "     33            0.6889        1.0341       0.5833            0.5833        \u001b[94m0.8995\u001b[0m  0.0005  0.1450\n",
      "     34            \u001b[36m0.8000\u001b[0m        0.9350       0.5833            0.5833        \u001b[94m0.8910\u001b[0m  0.0006  0.1513\n",
      "     35            \u001b[36m0.8444\u001b[0m        \u001b[32m0.6967\u001b[0m       0.5938            0.5938        \u001b[94m0.8854\u001b[0m  0.0006  0.1340\n",
      "     36            \u001b[36m0.9111\u001b[0m        \u001b[32m0.6320\u001b[0m       0.6042            0.6042        \u001b[94m0.8837\u001b[0m  0.0007  0.1435\n",
      "     37            0.9111        \u001b[32m0.5748\u001b[0m       0.6146            0.6146        0.8866  0.0007  0.1683\n",
      "     38            \u001b[36m0.9778\u001b[0m        \u001b[32m0.4091\u001b[0m       0.6146            0.6146        0.8938  0.0007  0.1829\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2915\u001b[0m       0.6250            0.6250        0.9100  0.0007  0.1527\n",
      "     40            0.9778        \u001b[32m0.2847\u001b[0m       0.6146            0.6146        0.9353  0.0007  0.1662\n",
      "     41            0.9778        \u001b[32m0.2096\u001b[0m       0.5833            0.5833        0.9643  0.0007  0.1566\n",
      "     42            0.9556        0.2228       0.5833            0.5833        0.9907  0.0007  0.2166\n",
      "     43            0.9556        \u001b[32m0.2045\u001b[0m       0.5625            0.5625        1.0141  0.0006  0.1784\n",
      "     44            0.9556        \u001b[32m0.1732\u001b[0m       0.5521            0.5521        1.0310  0.0006  0.1633\n",
      "     45            0.9778        \u001b[32m0.1605\u001b[0m       0.5625            0.5625        1.0425  0.0005  0.1735\n",
      "     46            1.0000        0.1767       0.5625            0.5625        1.0482  0.0005  0.1487\n",
      "     47            1.0000        \u001b[32m0.1458\u001b[0m       0.5625            0.5625        1.0494  0.0004  0.1639\n",
      "     48            1.0000        \u001b[32m0.1345\u001b[0m       0.5625            0.5625        1.0448  0.0004  0.1827\n",
      "     49            1.0000        \u001b[32m0.1037\u001b[0m       0.5729            0.5729        1.0377  0.0003  0.1621\n",
      "     50            1.0000        0.1302       0.5729            0.5729        1.0294  0.0003  0.1823\n",
      "Fine tuning model for subject 1 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7333        \u001b[32m0.7343\u001b[0m       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9300\u001b[0m  0.0004  0.2050\n",
      "     32            \u001b[36m0.8444\u001b[0m        \u001b[32m0.6901\u001b[0m       0.6146            0.6146        \u001b[94m0.9213\u001b[0m  0.0005  0.1616\n",
      "     33            \u001b[36m0.8889\u001b[0m        \u001b[32m0.6564\u001b[0m       0.5833            0.5833        \u001b[94m0.9206\u001b[0m  0.0005  0.1484\n",
      "     34            0.8889        \u001b[32m0.5805\u001b[0m       0.5729            0.5729        0.9264  0.0006  0.1582\n",
      "     35            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5084\u001b[0m       0.5729            0.5729        0.9354  0.0006  0.1573\n",
      "     36            \u001b[36m0.9556\u001b[0m        \u001b[32m0.4263\u001b[0m       0.6042            0.6042        0.9388  0.0007  0.1478\n",
      "     37            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3423\u001b[0m       0.6042            0.6042        0.9399  0.0007  0.1511\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2788\u001b[0m       0.6042            0.6042        0.9339  0.0007  0.1542\n",
      "     39            1.0000        \u001b[32m0.1950\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        0.9255  0.0007  0.1334\n",
      "     40            1.0000        0.2089       0.6250            0.6250        \u001b[94m0.9161\u001b[0m  0.0007  0.1515\n",
      "     41            1.0000        \u001b[32m0.1675\u001b[0m       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.9091\u001b[0m  0.0007  0.1489\n",
      "     42            1.0000        0.1765       0.6354            0.6354        \u001b[94m0.9062\u001b[0m  0.0007  0.1529\n",
      "     43            1.0000        \u001b[32m0.1380\u001b[0m       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.9045\u001b[0m  0.0006  0.1344\n",
      "     44            1.0000        \u001b[32m0.1217\u001b[0m       0.6250            0.6250        \u001b[94m0.9038\u001b[0m  0.0006  0.1681\n",
      "     45            1.0000        \u001b[32m0.1001\u001b[0m       0.6146            0.6146        \u001b[94m0.9021\u001b[0m  0.0005  0.1449\n",
      "     46            1.0000        \u001b[32m0.0964\u001b[0m       0.6042            0.6042        \u001b[94m0.9017\u001b[0m  0.0005  0.1476\n",
      "     47            1.0000        \u001b[32m0.0829\u001b[0m       0.5938            0.5938        0.9033  0.0004  0.1515\n",
      "     48            1.0000        0.0995       0.6146            0.6146        0.9064  0.0004  0.1664\n",
      "     49            1.0000        0.1022       0.6146            0.6146        0.9110  0.0003  0.1381\n",
      "     50            1.0000        0.0926       0.6146            0.6146        0.9186  0.0003  0.1502\n",
      "Fine tuning model for subject 1 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.0094       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.9305\u001b[0m  0.0004  0.1480\n",
      "     32            0.6222        0.9890       0.6250            0.6250        \u001b[94m0.9157\u001b[0m  0.0005  0.1316\n",
      "     33            0.7111        0.8499       0.6146            0.6146        \u001b[94m0.9064\u001b[0m  0.0005  0.1525\n",
      "     34            0.7333        0.7908       0.6146            0.6146        \u001b[94m0.9031\u001b[0m  0.0006  0.1316\n",
      "     35            0.7778        \u001b[32m0.6227\u001b[0m       0.5833            0.5833        0.9058  0.0006  0.1474\n",
      "     36            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6063\u001b[0m       0.5833            0.5833        0.9121  0.0007  0.1301\n",
      "     37            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4983\u001b[0m       0.5729            0.5729        0.9181  0.0007  0.1390\n",
      "     38            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3556\u001b[0m       0.5729            0.5729        0.9252  0.0007  0.1401\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2956\u001b[0m       0.5833            0.5833        0.9351  0.0007  0.1304\n",
      "     40            1.0000        0.2968       0.5833            0.5833        0.9445  0.0007  0.1544\n",
      "     41            1.0000        \u001b[32m0.2241\u001b[0m       0.5625            0.5625        0.9564  0.0007  0.1528\n",
      "     42            1.0000        \u001b[32m0.2075\u001b[0m       0.5625            0.5625        0.9684  0.0007  0.1454\n",
      "     43            1.0000        0.2099       0.5417            0.5417        0.9789  0.0006  0.1496\n",
      "     44            1.0000        \u001b[32m0.1659\u001b[0m       0.5625            0.5625        0.9842  0.0006  0.1369\n",
      "     45            1.0000        \u001b[32m0.1344\u001b[0m       0.5729            0.5729        0.9868  0.0005  0.1378\n",
      "     46            1.0000        0.1532       0.5729            0.5729        0.9892  0.0005  0.1474\n",
      "     47            1.0000        0.1421       0.5729            0.5729        0.9921  0.0004  0.1364\n",
      "     48            1.0000        \u001b[32m0.1208\u001b[0m       0.5833            0.5833        0.9947  0.0004  0.1532\n",
      "     49            1.0000        0.1406       0.5833            0.5833        0.9961  0.0003  0.1343\n",
      "     50            1.0000        \u001b[32m0.1134\u001b[0m       0.5833            0.5833        0.9973  0.0003  0.1503\n",
      "Fine tuning model for subject 1 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4667        1.2758       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9299\u001b[0m  0.0004  0.1307\n",
      "     32            0.5333        1.1392       0.6146            0.6146        \u001b[94m0.9186\u001b[0m  0.0005  0.1438\n",
      "     33            0.6222        1.0094       0.6042            0.6042        \u001b[94m0.9164\u001b[0m  0.0005  0.1361\n",
      "     34            \u001b[36m0.8000\u001b[0m        \u001b[32m0.7041\u001b[0m       0.6042            0.6042        0.9167  0.0006  0.1414\n",
      "     35            \u001b[36m0.8667\u001b[0m        0.7466       0.6042            0.6042        \u001b[94m0.9158\u001b[0m  0.0006  0.1419\n",
      "     36            \u001b[36m0.9111\u001b[0m        \u001b[32m0.5451\u001b[0m       0.6146            0.6146        \u001b[94m0.9100\u001b[0m  0.0007  0.1417\n",
      "     37            \u001b[36m0.9556\u001b[0m        0.5644       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8996\u001b[0m  0.0007  0.1670\n",
      "     38            \u001b[36m0.9778\u001b[0m        \u001b[32m0.4456\u001b[0m       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        \u001b[94m0.8890\u001b[0m  0.0007  0.1489\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3146\u001b[0m       0.6562            0.6562        \u001b[94m0.8864\u001b[0m  0.0007  0.1443\n",
      "     40            1.0000        0.3221       0.6458            0.6458        0.8932  0.0007  0.1518\n",
      "     41            1.0000        \u001b[32m0.2742\u001b[0m       0.6250            0.6250        0.9041  0.0007  0.1360\n",
      "     42            1.0000        \u001b[32m0.2243\u001b[0m       0.6146            0.6146        0.9142  0.0007  0.1515\n",
      "     43            1.0000        \u001b[32m0.1872\u001b[0m       0.6042            0.6042        0.9224  0.0006  0.1522\n",
      "     44            1.0000        0.1914       0.6146            0.6146        0.9232  0.0006  0.1500\n",
      "     45            1.0000        0.2148       0.6042            0.6042        0.9226  0.0005  0.1366\n",
      "     46            1.0000        \u001b[32m0.1649\u001b[0m       0.6667            0.6667        0.9198  0.0005  0.1509\n",
      "     47            1.0000        \u001b[32m0.1319\u001b[0m       0.6667            0.6667        0.9159  0.0004  0.1520\n",
      "     48            1.0000        \u001b[32m0.1215\u001b[0m       0.6667            0.6667        0.9115  0.0004  0.1322\n",
      "     49            1.0000        0.1283       0.6667            0.6667        0.9090  0.0003  0.1345\n",
      "     50            1.0000        0.1239       0.6667            0.6667        0.9076  0.0003  0.1511\n",
      "Fine tuning model for subject 1 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6222        1.0591       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9312\u001b[0m  0.0004  0.1247\n",
      "     32            0.6444        1.0769       0.6042            0.6042        \u001b[94m0.9205\u001b[0m  0.0005  0.1451\n",
      "     33            0.7556        0.9520       0.5833            0.5833        \u001b[94m0.9165\u001b[0m  0.0005  0.1302\n",
      "     34            \u001b[36m0.8222\u001b[0m        0.7686       0.5833            0.5833        0.9174  0.0006  0.1502\n",
      "     35            \u001b[36m0.8444\u001b[0m        \u001b[32m0.6738\u001b[0m       0.5833            0.5833        0.9291  0.0006  0.1324\n",
      "     36            \u001b[36m0.9556\u001b[0m        \u001b[32m0.4746\u001b[0m       0.5625            0.5625        0.9563  0.0007  0.1404\n",
      "     37            \u001b[36m0.9778\u001b[0m        0.4913       0.5521            0.5521        1.0021  0.0007  0.1371\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4225\u001b[0m       0.5521            0.5521        1.0532  0.0007  0.1406\n",
      "     39            0.9778        \u001b[32m0.3719\u001b[0m       0.5104            0.5104        1.0988  0.0007  0.1323\n",
      "     40            0.9778        \u001b[32m0.3514\u001b[0m       0.5000            0.5000        1.1347  0.0007  0.1271\n",
      "     41            0.9778        \u001b[32m0.2452\u001b[0m       0.4792            0.4792        1.1598  0.0007  0.1338\n",
      "     42            0.9778        0.2620       0.4896            0.4896        1.1706  0.0007  0.1759\n",
      "     43            1.0000        \u001b[32m0.1947\u001b[0m       0.4792            0.4792        1.1740  0.0006  0.1823\n",
      "     44            1.0000        0.2409       0.4688            0.4688        1.1649  0.0006  0.1677\n",
      "     45            1.0000        \u001b[32m0.1781\u001b[0m       0.4688            0.4688        1.1523  0.0005  0.1826\n",
      "     46            1.0000        \u001b[32m0.1422\u001b[0m       0.4792            0.4792        1.1396  0.0005  0.1576\n",
      "     47            1.0000        \u001b[32m0.1200\u001b[0m       0.4583            0.4583        1.1282  0.0004  0.1607\n",
      "     48            1.0000        0.1289       0.4583            0.4583        1.1183  0.0004  0.1676\n",
      "     49            1.0000        0.1393       0.4375            0.4375        1.1087  0.0003  0.1595\n",
      "     50            1.0000        0.1331       0.4375            0.4375        1.0993  0.0003  0.1661\n",
      "Fine tuning model for subject 1 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5111        1.1270       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9338\u001b[0m  0.0004  0.1606\n",
      "     32            0.5333        1.0661       0.6042            0.6042        \u001b[94m0.9266\u001b[0m  0.0005  0.1609\n",
      "     33            0.6444        1.0013       0.5938            0.5938        \u001b[94m0.9237\u001b[0m  0.0005  0.1757\n",
      "     34            0.7333        0.8953       0.5938            0.5938        \u001b[94m0.9222\u001b[0m  0.0006  0.1654\n",
      "     35            \u001b[36m0.8667\u001b[0m        \u001b[32m0.7429\u001b[0m       0.6146            0.6146        0.9232  0.0006  0.1833\n",
      "     36            \u001b[36m0.9111\u001b[0m        0.7765       0.6146            0.6146        0.9226  0.0007  0.1495\n",
      "     37            0.9111        \u001b[32m0.5639\u001b[0m       0.5833            0.5833        0.9289  0.0007  0.2011\n",
      "     38            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4659\u001b[0m       0.5729            0.5729        0.9423  0.0007  0.1835\n",
      "     39            0.9333        \u001b[32m0.4534\u001b[0m       0.5625            0.5625        0.9595  0.0007  0.1827\n",
      "     40            \u001b[36m0.9556\u001b[0m        \u001b[32m0.3687\u001b[0m       0.5417            0.5417        0.9775  0.0007  0.1362\n",
      "     41            \u001b[36m0.9778\u001b[0m        \u001b[32m0.2841\u001b[0m       0.5625            0.5625        0.9954  0.0007  0.1508\n",
      "     42            0.9778        \u001b[32m0.2572\u001b[0m       0.5625            0.5625        1.0113  0.0007  0.1355\n",
      "     43            0.9778        \u001b[32m0.2247\u001b[0m       0.5729            0.5729        1.0255  0.0006  0.1573\n",
      "     44            \u001b[36m1.0000\u001b[0m        0.2332       0.5729            0.5729        1.0356  0.0006  0.1323\n",
      "     45            1.0000        0.2612       0.5625            0.5625        1.0442  0.0005  0.1366\n",
      "     46            1.0000        \u001b[32m0.1541\u001b[0m       0.5729            0.5729        1.0513  0.0005  0.1493\n",
      "     47            1.0000        0.1915       0.5729            0.5729        1.0574  0.0004  0.1388\n",
      "     48            1.0000        0.1603       0.5833            0.5833        1.0624  0.0004  0.1495\n",
      "     49            1.0000        \u001b[32m0.1330\u001b[0m       0.5833            0.5833        1.0659  0.0003  0.1520\n",
      "     50            1.0000        0.1375       0.5938            0.5938        1.0684  0.0003  0.1345\n",
      "Hold out data from subject 2\n",
      "Pre-training model with data from all subjects but subject 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.4128\u001b[0m        \u001b[32m1.5858\u001b[0m       \u001b[35m0.3464\u001b[0m            \u001b[31m0.3464\u001b[0m        \u001b[94m1.3754\u001b[0m  0.0007  6.5652\n",
      "      2            \u001b[36m0.4828\u001b[0m        \u001b[32m1.3867\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.2873\u001b[0m  0.0007  7.4430\n",
      "      3            \u001b[36m0.5482\u001b[0m        \u001b[32m1.2931\u001b[0m       \u001b[35m0.4557\u001b[0m            \u001b[31m0.4557\u001b[0m        \u001b[94m1.2257\u001b[0m  0.0007  6.8734\n",
      "      4            \u001b[36m0.6010\u001b[0m        \u001b[32m1.1851\u001b[0m       \u001b[35m0.4948\u001b[0m            \u001b[31m0.4948\u001b[0m        \u001b[94m1.1655\u001b[0m  0.0007  6.8304\n",
      "      5            \u001b[36m0.6143\u001b[0m        \u001b[32m1.1193\u001b[0m       0.4779            0.4779        \u001b[94m1.1505\u001b[0m  0.0007  7.0323\n",
      "      6            \u001b[36m0.6362\u001b[0m        \u001b[32m1.0686\u001b[0m       0.4935            0.4935        \u001b[94m1.1351\u001b[0m  0.0006  6.6178\n",
      "      7            \u001b[36m0.6448\u001b[0m        \u001b[32m1.0305\u001b[0m       \u001b[35m0.5091\u001b[0m            \u001b[31m0.5091\u001b[0m        1.1499  0.0006  7.3527\n",
      "      8            \u001b[36m0.6740\u001b[0m        \u001b[32m0.9857\u001b[0m       0.4987            0.4987        \u001b[94m1.1285\u001b[0m  0.0006  6.5781\n",
      "      9            0.6698        \u001b[32m0.9635\u001b[0m       0.5039            0.5039        1.1574  0.0006  7.0979\n",
      "     10            \u001b[36m0.7034\u001b[0m        \u001b[32m0.9166\u001b[0m       \u001b[35m0.5273\u001b[0m            \u001b[31m0.5273\u001b[0m        \u001b[94m1.1041\u001b[0m  0.0005  6.8325\n",
      "     11            \u001b[36m0.7081\u001b[0m        \u001b[32m0.8882\u001b[0m       0.5052            0.5052        1.1289  0.0005  6.6699\n",
      "     12            \u001b[36m0.7380\u001b[0m        \u001b[32m0.8758\u001b[0m       0.5234            0.5234        \u001b[94m1.0880\u001b[0m  0.0005  7.2897\n",
      "     13            0.7242        \u001b[32m0.8654\u001b[0m       0.5182            0.5182        1.1139  0.0004  6.5471\n",
      "     14            0.7359        \u001b[32m0.8367\u001b[0m       \u001b[35m0.5404\u001b[0m            \u001b[31m0.5404\u001b[0m        1.1138  0.0004  7.2382\n",
      "     15            \u001b[36m0.7451\u001b[0m        \u001b[32m0.8221\u001b[0m       0.5352            0.5352        \u001b[94m1.0652\u001b[0m  0.0004  6.6128\n",
      "     16            \u001b[36m0.7643\u001b[0m        \u001b[32m0.7925\u001b[0m       \u001b[35m0.5456\u001b[0m            \u001b[31m0.5456\u001b[0m        1.0736  0.0003  6.6576\n",
      "     17            \u001b[36m0.7917\u001b[0m        \u001b[32m0.7677\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        \u001b[94m1.0546\u001b[0m  0.0003  7.0705\n",
      "     18            \u001b[36m0.7937\u001b[0m        \u001b[32m0.7341\u001b[0m       \u001b[35m0.5651\u001b[0m            \u001b[31m0.5651\u001b[0m        1.0605  0.0003  6.5916\n",
      "     19            \u001b[36m0.7997\u001b[0m        \u001b[32m0.7333\u001b[0m       0.5417            0.5417        1.0624  0.0002  7.3565\n",
      "     20            0.7974        \u001b[32m0.7097\u001b[0m       0.5573            0.5573        \u001b[94m1.0521\u001b[0m  0.0002  6.5679\n",
      "     21            \u001b[36m0.8086\u001b[0m        \u001b[32m0.7042\u001b[0m       0.5573            0.5573        \u001b[94m1.0457\u001b[0m  0.0002  6.9844\n",
      "     22            \u001b[36m0.8203\u001b[0m        \u001b[32m0.6937\u001b[0m       0.5599            0.5599        \u001b[94m1.0275\u001b[0m  0.0001  6.9690\n",
      "     23            0.8177        \u001b[32m0.6758\u001b[0m       \u001b[35m0.5781\u001b[0m            \u001b[31m0.5781\u001b[0m        \u001b[94m1.0265\u001b[0m  0.0001  6.5847\n",
      "     24            \u001b[36m0.8208\u001b[0m        \u001b[32m0.6617\u001b[0m       0.5560            0.5560        \u001b[94m1.0263\u001b[0m  0.0001  7.4569\n",
      "     25            \u001b[36m0.8253\u001b[0m        \u001b[32m0.6594\u001b[0m       0.5716            0.5716        1.0282  0.0001  6.6306\n",
      "     26            \u001b[36m0.8271\u001b[0m        \u001b[32m0.6508\u001b[0m       0.5742            0.5742        1.0273  0.0000  7.1106\n",
      "     27            \u001b[36m0.8279\u001b[0m        \u001b[32m0.6467\u001b[0m       0.5729            0.5729        \u001b[94m1.0237\u001b[0m  0.0000  6.6981\n",
      "     28            \u001b[36m0.8299\u001b[0m        \u001b[32m0.6400\u001b[0m       0.5716            0.5716        1.0251  0.0000  6.6019\n",
      "     29            \u001b[36m0.8307\u001b[0m        0.6408       0.5690            0.5690        1.0249  0.0000  7.1097\n",
      "     30            0.8302        \u001b[32m0.6315\u001b[0m       0.5677            0.5677        1.0242  0.0000  7.4480\n",
      "Before finetuning for subject 2, the baseline accuracy is 0.3541666666666667\n",
      "Fine tuning model for subject 2 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.8000        1.3996       0.3125            0.3125        1.7293  0.0004  0.1477\n",
      "     32            \u001b[36m1.0000\u001b[0m        0.6503       0.2812            0.2812        1.8771  0.0005  0.1539\n",
      "     33            1.0000        \u001b[32m0.3097\u001b[0m       0.2812            0.2812        2.0972  0.0005  0.1656\n",
      "     34            1.0000        \u001b[32m0.0628\u001b[0m       0.2708            0.2708        2.2932  0.0006  0.1793\n",
      "     35            1.0000        \u001b[32m0.0435\u001b[0m       0.2500            0.2500        2.4063  0.0006  0.1514\n",
      "     36            1.0000        0.0620       0.2500            0.2500        2.4392  0.0007  0.1624\n",
      "     37            1.0000        \u001b[32m0.0148\u001b[0m       0.2500            0.2500        2.4078  0.0007  0.1501\n",
      "     38            1.0000        0.0168       0.2604            0.2604        2.3343  0.0007  0.1510\n",
      "     39            1.0000        \u001b[32m0.0045\u001b[0m       0.2500            0.2500        2.2441  0.0007  0.1510\n",
      "     40            1.0000        0.0071       0.2708            0.2708        2.1562  0.0007  0.1667\n",
      "     41            1.0000        0.0116       0.2708            0.2708        2.0802  0.0007  0.1553\n",
      "     42            1.0000        0.0055       0.2604            0.2604        2.0234  0.0007  0.1459\n",
      "     43            1.0000        \u001b[32m0.0029\u001b[0m       0.2708            0.2708        1.9860  0.0006  0.1354\n",
      "     44            1.0000        \u001b[32m0.0027\u001b[0m       0.2708            0.2708        1.9653  0.0006  0.1330\n",
      "     45            1.0000        \u001b[32m0.0022\u001b[0m       0.2917            0.2917        1.9576  0.0005  0.1238\n",
      "     46            1.0000        0.0027       0.3021            0.3021        1.9592  0.0005  0.1200\n",
      "     47            1.0000        0.0031       0.3125            0.3125        1.9666  0.0004  0.1156\n",
      "     48            1.0000        0.0038       0.3125            0.3125        1.9774  0.0004  0.1170\n",
      "     49            1.0000        \u001b[32m0.0020\u001b[0m       0.2812            0.2812        1.9901  0.0003  0.1344\n",
      "     50            1.0000        \u001b[32m0.0017\u001b[0m       0.2604            0.2604        2.0035  0.0003  0.1269\n",
      "Fine tuning model for subject 2 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.8000        2.0356       0.3333            0.3333        1.7067  0.0004  0.1189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        1.5345       0.3229            0.3229        1.6740  0.0005  0.1324\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5555\u001b[0m       0.2812            0.2812        1.6963  0.0005  0.1145\n",
      "     34            1.0000        \u001b[32m0.1831\u001b[0m       0.2604            0.2604        1.7441  0.0006  0.1199\n",
      "     35            1.0000        \u001b[32m0.1125\u001b[0m       0.2396            0.2396        1.7799  0.0006  0.1321\n",
      "     36            1.0000        \u001b[32m0.0497\u001b[0m       0.1979            0.1979        1.8198  0.0007  0.1085\n",
      "     37            1.0000        \u001b[32m0.0302\u001b[0m       0.1771            0.1771        1.8817  0.0007  0.1142\n",
      "     38            1.0000        \u001b[32m0.0214\u001b[0m       0.2083            0.2083        1.9674  0.0007  0.1327\n",
      "     39            1.0000        0.0281       0.2292            0.2292        2.0655  0.0007  0.1198\n",
      "     40            1.0000        0.0455       0.2292            0.2292        2.1600  0.0007  0.1182\n",
      "     41            1.0000        \u001b[32m0.0201\u001b[0m       0.2396            0.2396        2.2399  0.0007  0.1082\n",
      "     42            1.0000        \u001b[32m0.0120\u001b[0m       0.2500            0.2500        2.2985  0.0007  0.1170\n",
      "     43            1.0000        0.0177       0.2604            0.2604        2.3351  0.0006  0.1197\n",
      "     44            1.0000        \u001b[32m0.0101\u001b[0m       0.2500            0.2500        2.3485  0.0006  0.1188\n",
      "     45            1.0000        0.0135       0.2500            0.2500        2.3418  0.0005  0.1475\n",
      "     46            1.0000        \u001b[32m0.0076\u001b[0m       0.2500            0.2500        2.3222  0.0005  0.1333\n",
      "     47            1.0000        0.0128       0.2500            0.2500        2.2928  0.0004  0.1190\n",
      "     48            1.0000        0.0084       0.2500            0.2500        2.2574  0.0004  0.1170\n",
      "     49            1.0000        \u001b[32m0.0063\u001b[0m       0.2396            0.2396        2.2196  0.0003  0.1196\n",
      "     50            1.0000        0.0067       0.2292            0.2292        2.1818  0.0003  0.1210\n",
      "Fine tuning model for subject 2 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.8000        1.3489       0.3438            0.3438        1.7935  0.0004  0.1324\n",
      "     32            0.6000        1.1737       0.3125            0.3125        1.9130  0.0005  0.1065\n",
      "     33            0.6000        \u001b[32m0.3078\u001b[0m       0.3125            0.3125        1.9184  0.0005  0.1197\n",
      "     34            0.8000        \u001b[32m0.2460\u001b[0m       0.3125            0.3125        1.8902  0.0006  0.1300\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1936\u001b[0m       0.3021            0.3021        1.9045  0.0006  0.1304\n",
      "     36            1.0000        \u001b[32m0.1453\u001b[0m       0.3438            0.3438        1.9035  0.0007  0.1207\n",
      "     37            1.0000        0.2039       0.3021            0.3021        1.8668  0.0007  0.1160\n",
      "     38            1.0000        \u001b[32m0.0678\u001b[0m       0.2812            0.2812        1.8297  0.0007  0.1219\n",
      "     39            1.0000        \u001b[32m0.0351\u001b[0m       0.2812            0.2812        1.8208  0.0007  0.1385\n",
      "     40            1.0000        0.0588       0.2917            0.2917        1.8471  0.0007  0.1501\n",
      "     41            1.0000        0.0922       0.2917            0.2917        1.8934  0.0007  0.1318\n",
      "     42            1.0000        \u001b[32m0.0207\u001b[0m       0.2917            0.2917        1.9462  0.0007  0.1064\n",
      "     43            1.0000        0.0326       0.2812            0.2812        1.9985  0.0006  0.1171\n",
      "     44            1.0000        \u001b[32m0.0114\u001b[0m       0.3125            0.3125        2.0481  0.0006  0.1354\n",
      "     45            1.0000        0.0124       0.2917            0.2917        2.0937  0.0005  0.1196\n",
      "     46            1.0000        \u001b[32m0.0065\u001b[0m       0.2917            0.2917        2.1353  0.0005  0.1251\n",
      "     47            1.0000        \u001b[32m0.0063\u001b[0m       0.2708            0.2708        2.1736  0.0004  0.1121\n",
      "     48            1.0000        0.0160       0.2812            0.2812        2.2086  0.0004  0.1326\n",
      "     49            1.0000        0.0098       0.2708            0.2708        2.2408  0.0003  0.1208\n",
      "     50            1.0000        0.0313       0.2500            0.2500        2.2704  0.0003  0.1328\n",
      "Fine tuning model for subject 2 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m1.0000\u001b[0m        2.0378       0.3229            0.3229        1.7166  0.0004  0.1085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            1.0000        1.3824       0.3229            0.3229        1.8460  0.0005  0.1146\n",
      "     33            0.8000        \u001b[32m0.3088\u001b[0m       0.2604            0.2604        2.1098  0.0005  0.1177\n",
      "     34            0.8000        \u001b[32m0.3045\u001b[0m       0.2604            0.2604        2.3345  0.0006  0.1198\n",
      "     35            0.8000        \u001b[32m0.1051\u001b[0m       0.2292            0.2292        2.4649  0.0006  0.1218\n",
      "     36            0.8000        \u001b[32m0.0332\u001b[0m       0.2500            0.2500        2.5678  0.0007  0.1011\n",
      "     37            0.8000        \u001b[32m0.0211\u001b[0m       0.2188            0.2188        2.6845  0.0007  0.1159\n",
      "     38            1.0000        0.0296       0.2500            0.2500        2.8176  0.0007  0.1208\n",
      "     39            1.0000        0.0416       0.2604            0.2604        2.9495  0.0007  0.1300\n",
      "     40            1.0000        \u001b[32m0.0170\u001b[0m       0.2292            0.2292        3.0604  0.0007  0.1081\n",
      "     41            1.0000        \u001b[32m0.0101\u001b[0m       0.2396            0.2396        3.1381  0.0007  0.0997\n",
      "     42            1.0000        0.0175       0.2396            0.2396        3.1784  0.0007  0.1180\n",
      "     43            1.0000        0.0164       0.2396            0.2396        3.1854  0.0006  0.1189\n",
      "     44            1.0000        \u001b[32m0.0099\u001b[0m       0.2396            0.2396        3.1666  0.0006  0.1019\n",
      "     45            1.0000        \u001b[32m0.0081\u001b[0m       0.2396            0.2396        3.1311  0.0005  0.1072\n",
      "     46            1.0000        \u001b[32m0.0050\u001b[0m       0.2500            0.2500        3.0860  0.0005  0.1169\n",
      "     47            1.0000        0.0080       0.2396            0.2396        3.0383  0.0004  0.1355\n",
      "     48            1.0000        0.0118       0.2292            0.2292        2.9929  0.0004  0.1309\n",
      "     49            1.0000        0.0081       0.2292            0.2292        2.9536  0.0003  0.1119\n",
      "     50            1.0000        0.0082       0.1875            0.1875        2.9219  0.0003  0.1146\n",
      "Fine tuning model for subject 2 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.4987       0.3542            0.3542        1.6680  0.0004  0.1188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.4108       0.3125            0.3125        1.6902  0.0005  0.1169\n",
      "     33            0.8000        \u001b[32m0.4675\u001b[0m       0.3021            0.3021        1.8010  0.0005  0.1135\n",
      "     34            0.8000        \u001b[32m0.1467\u001b[0m       0.2708            0.2708        1.9702  0.0006  0.1207\n",
      "     35            0.8000        \u001b[32m0.1092\u001b[0m       0.2188            0.2188        2.1354  0.0006  0.1395\n",
      "     36            0.8000        0.1541       0.1979            0.1979        2.2549  0.0007  0.1307\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1005\u001b[0m       0.2083            0.2083        2.3156  0.0007  0.1549\n",
      "     38            1.0000        \u001b[32m0.0347\u001b[0m       0.2188            0.2188        2.3535  0.0007  0.1687\n",
      "     39            1.0000        0.0540       0.2083            0.2083        2.3933  0.0007  0.1667\n",
      "     40            1.0000        \u001b[32m0.0154\u001b[0m       0.2083            0.2083        2.4439  0.0007  0.1512\n",
      "     41            1.0000        0.0282       0.2188            0.2188        2.5125  0.0007  0.1265\n",
      "     42            1.0000        \u001b[32m0.0097\u001b[0m       0.2292            0.2292        2.5939  0.0007  0.1146\n",
      "     43            1.0000        0.0222       0.2292            0.2292        2.6877  0.0006  0.1142\n",
      "     44            1.0000        0.0157       0.2292            0.2292        2.7889  0.0006  0.1196\n",
      "     45            1.0000        0.0114       0.2083            0.2083        2.8944  0.0005  0.1189\n",
      "     46            1.0000        0.0107       0.2083            0.2083        3.0021  0.0005  0.1051\n",
      "     47            1.0000        \u001b[32m0.0034\u001b[0m       0.2292            0.2292        3.1100  0.0004  0.1079\n",
      "     48            1.0000        0.0217       0.2292            0.2292        3.2188  0.0004  0.1166\n",
      "     49            1.0000        0.0092       0.2292            0.2292        3.3250  0.0003  0.1203\n",
      "     50            1.0000        0.0055       0.2396            0.2396        3.4275  0.0003  0.1286\n",
      "Fine tuning model for subject 2 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.8141       0.3750            0.3750        1.7226  0.0004  0.1162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        1.4717       0.3542            0.3542        1.7922  0.0005  0.1324\n",
      "     33            1.0000        0.7040       0.3333            0.3333        1.8630  0.0005  0.1322\n",
      "     34            1.0000        \u001b[32m0.1191\u001b[0m       0.2604            0.2604        1.9762  0.0006  0.1187\n",
      "     35            1.0000        \u001b[32m0.0593\u001b[0m       0.2292            0.2292        2.1271  0.0006  0.1198\n",
      "     36            1.0000        \u001b[32m0.0461\u001b[0m       0.2292            0.2292        2.2907  0.0007  0.1306\n",
      "     37            1.0000        \u001b[32m0.0453\u001b[0m       0.2188            0.2188        2.4331  0.0007  0.1103\n",
      "     38            1.0000        \u001b[32m0.0149\u001b[0m       0.2292            0.2292        2.5441  0.0007  0.1093\n",
      "     39            1.0000        0.0460       0.2500            0.2500        2.6156  0.0007  0.1364\n",
      "     40            1.0000        0.0231       0.2500            0.2500        2.6519  0.0007  0.1198\n",
      "     41            1.0000        0.0347       0.2500            0.2500        2.6640  0.0007  0.1335\n",
      "     42            1.0000        0.0285       0.2500            0.2500        2.6645  0.0007  0.1510\n",
      "     43            1.0000        0.0340       0.2604            0.2604        2.6579  0.0006  0.1520\n",
      "     44            1.0000        0.0338       0.2604            0.2604        2.6485  0.0006  0.1458\n",
      "     45            1.0000        \u001b[32m0.0081\u001b[0m       0.2500            0.2500        2.6368  0.0005  0.1336\n",
      "     46            1.0000        \u001b[32m0.0071\u001b[0m       0.2500            0.2500        2.6242  0.0005  0.1453\n",
      "     47            1.0000        0.0100       0.2396            0.2396        2.6126  0.0004  0.1467\n",
      "     48            1.0000        0.0091       0.2396            0.2396        2.6016  0.0004  0.1649\n",
      "     49            1.0000        \u001b[32m0.0048\u001b[0m       0.2396            0.2396        2.5916  0.0003  0.1307\n",
      "     50            1.0000        0.0090       0.2396            0.2396        2.5832  0.0003  0.1563\n",
      "Fine tuning model for subject 2 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.9421       0.3438            0.3438        1.6656  0.0004  0.1376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        1.2393       0.3333            0.3333        1.6697  0.0005  0.1436\n",
      "     33            1.0000        \u001b[32m0.5530\u001b[0m       0.2708            0.2708        1.8653  0.0005  0.1500\n",
      "     34            1.0000        \u001b[32m0.2286\u001b[0m       0.2396            0.2396        2.2028  0.0006  0.1511\n",
      "     35            1.0000        \u001b[32m0.1849\u001b[0m       0.2292            0.2292        2.5017  0.0006  0.1527\n",
      "     36            1.0000        \u001b[32m0.0856\u001b[0m       0.2292            0.2292        2.6525  0.0007  0.1512\n",
      "     37            1.0000        \u001b[32m0.0750\u001b[0m       0.2188            0.2188        2.6675  0.0007  0.1143\n",
      "     38            1.0000        \u001b[32m0.0294\u001b[0m       0.2188            0.2188        2.5948  0.0007  0.1167\n",
      "     39            1.0000        \u001b[32m0.0209\u001b[0m       0.2396            0.2396        2.4869  0.0007  0.1043\n",
      "     40            1.0000        0.0515       0.2083            0.2083        2.3941  0.0007  0.1288\n",
      "     41            1.0000        0.0220       0.2188            0.2188        2.3127  0.0007  0.1073\n",
      "     42            1.0000        0.0267       0.2292            0.2292        2.2462  0.0007  0.1154\n",
      "     43            1.0000        \u001b[32m0.0131\u001b[0m       0.2188            0.2188        2.1954  0.0006  0.1171\n",
      "     44            1.0000        \u001b[32m0.0079\u001b[0m       0.2083            0.2083        2.1562  0.0006  0.1199\n",
      "     45            1.0000        0.0080       0.2188            0.2188        2.1250  0.0005  0.1312\n",
      "     46            1.0000        0.0176       0.2083            0.2083        2.1001  0.0005  0.1062\n",
      "     47            1.0000        0.0114       0.2083            0.2083        2.0797  0.0004  0.1160\n",
      "     48            1.0000        0.0108       0.2188            0.2188        2.0628  0.0004  0.1208\n",
      "     49            1.0000        \u001b[32m0.0066\u001b[0m       0.1979            0.1979        2.0490  0.0003  0.1240\n",
      "     50            1.0000        \u001b[32m0.0060\u001b[0m       0.1979            0.1979        2.0381  0.0003  0.1147\n",
      "Fine tuning model for subject 2 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.8000        1.4022       0.3438            0.3438        1.7008  0.0004  0.1279\n",
      "     32            \u001b[36m1.0000\u001b[0m        0.6377       0.3021            0.3021        1.7430  0.0005  0.1136\n",
      "     33            1.0000        \u001b[32m0.3718\u001b[0m       0.3333            0.3333        1.8388  0.0005  0.1348\n",
      "     34            1.0000        \u001b[32m0.0773\u001b[0m       0.3125            0.3125        1.9844  0.0006  0.1360\n",
      "     35            1.0000        \u001b[32m0.0700\u001b[0m       0.3125            0.3125        2.1217  0.0006  0.1678\n",
      "     36            1.0000        \u001b[32m0.0365\u001b[0m       0.3021            0.3021        2.2211  0.0007  0.1657\n",
      "     37            1.0000        \u001b[32m0.0152\u001b[0m       0.2812            0.2812        2.2803  0.0007  0.1656\n",
      "     38            1.0000        \u001b[32m0.0148\u001b[0m       0.2812            0.2812        2.3055  0.0007  0.1667\n",
      "     39            1.0000        \u001b[32m0.0051\u001b[0m       0.2708            0.2708        2.3063  0.0007  0.1511\n",
      "     40            1.0000        0.0133       0.2604            0.2604        2.2942  0.0007  0.1510\n",
      "     41            1.0000        0.0110       0.2500            0.2500        2.2759  0.0007  0.1510\n",
      "     42            1.0000        \u001b[32m0.0034\u001b[0m       0.2396            0.2396        2.2577  0.0007  0.1511\n",
      "     43            1.0000        0.0052       0.2500            0.2500        2.2431  0.0006  0.1511\n",
      "     44            1.0000        0.0056       0.2604            0.2604        2.2337  0.0006  0.1332\n",
      "     45            1.0000        0.0063       0.2500            0.2500        2.2298  0.0005  0.1667\n",
      "     46            1.0000        0.0043       0.2500            0.2500        2.2303  0.0005  0.1663\n",
      "     47            1.0000        \u001b[32m0.0021\u001b[0m       0.2708            0.2708        2.2340  0.0004  0.1703\n",
      "     48            1.0000        0.0027       0.2708            0.2708        2.2398  0.0004  0.1634\n",
      "     49            1.0000        0.0038       0.2708            0.2708        2.2466  0.0003  0.1667\n",
      "     50            1.0000        0.0030       0.2708            0.2708        2.2539  0.0003  0.1666\n",
      "Fine tuning model for subject 2 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        2.0566       0.3333            0.3333        1.7369  0.0004  0.1566\n",
      "     32            \u001b[36m1.0000\u001b[0m        1.0546       0.3229            0.3229        1.7755  0.0005  0.1563\n",
      "     33            1.0000        \u001b[32m0.5889\u001b[0m       0.2604            0.2604        1.8261  0.0005  0.1511\n",
      "     34            1.0000        \u001b[32m0.1498\u001b[0m       0.2396            0.2396        1.9091  0.0006  0.1396\n",
      "     35            1.0000        \u001b[32m0.0685\u001b[0m       0.2708            0.2708        1.9906  0.0006  0.1214\n",
      "     36            1.0000        \u001b[32m0.0247\u001b[0m       0.2708            0.2708        2.0382  0.0007  0.1141\n",
      "     37            1.0000        \u001b[32m0.0149\u001b[0m       0.2708            0.2708        2.0504  0.0007  0.1203\n",
      "     38            1.0000        0.0209       0.2812            0.2812        2.0399  0.0007  0.1317\n",
      "     39            1.0000        0.0192       0.2917            0.2917        2.0199  0.0007  0.1226\n",
      "     40            1.0000        0.0275       0.3021            0.3021        1.9998  0.0007  0.1203\n",
      "     41            1.0000        0.0188       0.3021            0.3021        1.9851  0.0007  0.1163\n",
      "     42            1.0000        \u001b[32m0.0134\u001b[0m       0.2812            0.2812        1.9769  0.0007  0.1254\n",
      "     43            1.0000        \u001b[32m0.0070\u001b[0m       0.2604            0.2604        1.9745  0.0006  0.1181\n",
      "     44            1.0000        0.0130       0.2708            0.2708        1.9762  0.0006  0.1048\n",
      "     45            1.0000        0.0112       0.2604            0.2604        1.9805  0.0005  0.1170\n",
      "     46            1.0000        \u001b[32m0.0057\u001b[0m       0.2604            0.2604        1.9859  0.0005  0.1197\n",
      "     47            1.0000        0.0059       0.2604            0.2604        1.9917  0.0004  0.1200\n",
      "     48            1.0000        0.0067       0.2604            0.2604        1.9981  0.0004  0.1178\n",
      "     49            1.0000        \u001b[32m0.0042\u001b[0m       0.2396            0.2396        2.0048  0.0003  0.1326\n",
      "     50            1.0000        0.0077       0.2500            0.2500        2.0122  0.0003  0.1196\n",
      "Fine tuning model for subject 2 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        2.0306       0.3125            0.3125        1.6827  0.0004  0.1136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        1.5033       0.3125            0.3125        1.7722  0.0005  0.1189\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.7781       0.2917            0.2917        1.9218  0.0005  0.1182\n",
      "     34            1.0000        \u001b[32m0.4482\u001b[0m       0.3021            0.3021        2.1454  0.0006  0.1181\n",
      "     35            1.0000        \u001b[32m0.1190\u001b[0m       0.2812            0.2812        2.3356  0.0006  0.1232\n",
      "     36            1.0000        \u001b[32m0.0432\u001b[0m       0.2708            0.2708        2.4080  0.0007  0.1161\n",
      "     37            1.0000        0.0534       0.2708            0.2708        2.3859  0.0007  0.1007\n",
      "     38            1.0000        \u001b[32m0.0380\u001b[0m       0.2917            0.2917        2.3319  0.0007  0.1192\n",
      "     39            1.0000        \u001b[32m0.0228\u001b[0m       0.2708            0.2708        2.2876  0.0007  0.1204\n",
      "     40            1.0000        \u001b[32m0.0114\u001b[0m       0.2604            0.2604        2.2647  0.0007  0.1021\n",
      "     41            1.0000        0.0263       0.2292            0.2292        2.2601  0.0007  0.1098\n",
      "     42            1.0000        0.0140       0.2292            0.2292        2.2639  0.0007  0.1181\n",
      "     43            1.0000        \u001b[32m0.0080\u001b[0m       0.2292            0.2292        2.2690  0.0006  0.1272\n",
      "     44            1.0000        0.0127       0.2708            0.2708        2.2712  0.0006  0.1112\n",
      "     45            1.0000        0.0281       0.2812            0.2812        2.2703  0.0005  0.1121\n",
      "     46            1.0000        \u001b[32m0.0046\u001b[0m       0.2812            0.2812        2.2653  0.0005  0.1180\n",
      "     47            1.0000        0.0072       0.2500            0.2500        2.2577  0.0004  0.1187\n",
      "     48            1.0000        0.0067       0.2500            0.2500        2.2489  0.0004  0.1250\n",
      "     49            1.0000        \u001b[32m0.0042\u001b[0m       0.2396            0.2396        2.2405  0.0003  0.1143\n",
      "     50            1.0000        0.0058       0.2396            0.2396        2.2336  0.0003  0.1289\n",
      "Fine tuning model for subject 2 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        2.1581       0.3438            0.3438        1.6587  0.0004  0.0977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        1.3728       0.2812            0.2812        1.7387  0.0005  0.1168\n",
      "     33            0.8000        0.9459       0.2500            0.2500        1.9471  0.0005  0.0975\n",
      "     34            0.8000        \u001b[32m0.4109\u001b[0m       0.2292            0.2292        2.1450  0.0006  0.1024\n",
      "     35            \u001b[36m0.9000\u001b[0m        \u001b[32m0.1296\u001b[0m       0.2083            0.2083        2.2752  0.0006  0.0986\n",
      "     36            \u001b[36m1.0000\u001b[0m        0.2616       0.1875            0.1875        2.3534  0.0007  0.0904\n",
      "     37            1.0000        \u001b[32m0.1207\u001b[0m       0.1979            0.1979        2.3964  0.0007  0.1024\n",
      "     38            1.0000        \u001b[32m0.0561\u001b[0m       0.1979            0.1979        2.4429  0.0007  0.1032\n",
      "     39            1.0000        \u001b[32m0.0337\u001b[0m       0.1875            0.1875        2.5066  0.0007  0.0845\n",
      "     40            1.0000        0.0614       0.2083            0.2083        2.5909  0.0007  0.1013\n",
      "     41            1.0000        0.0929       0.1771            0.1771        2.6483  0.0007  0.1170\n",
      "     42            1.0000        0.0407       0.1875            0.1875        2.6907  0.0007  0.1064\n",
      "     43            1.0000        0.0731       0.2083            0.2083        2.7048  0.0006  0.1126\n",
      "     44            1.0000        \u001b[32m0.0096\u001b[0m       0.1979            0.1979        2.7085  0.0006  0.1198\n",
      "     45            1.0000        0.0820       0.1875            0.1875        2.7004  0.0005  0.1167\n",
      "     46            1.0000        0.0524       0.1875            0.1875        2.6883  0.0005  0.1019\n",
      "     47            1.0000        0.0169       0.1771            0.1771        2.6721  0.0004  0.1170\n",
      "     48            1.0000        0.0118       0.1771            0.1771        2.6534  0.0004  0.1095\n",
      "     49            1.0000        0.0146       0.1875            0.1875        2.6356  0.0003  0.0955\n",
      "     50            1.0000        0.0169       0.1979            0.1979        2.6190  0.0003  0.1097\n",
      "Fine tuning model for subject 2 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        2.9028       0.3333            0.3333        1.6564  0.0004  0.0952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        1.8588       0.3021            0.3021        1.7361  0.0005  0.1182\n",
      "     33            0.7000        1.4887       0.3229            0.3229        1.8493  0.0005  0.0991\n",
      "     34            0.7000        0.9649       0.3125            0.3125        1.8698  0.0006  0.1039\n",
      "     35            0.8000        0.7052       0.3021            0.3021        1.8056  0.0006  0.1406\n",
      "     36            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5591\u001b[0m       0.2917            0.2917        1.8050  0.0007  0.1317\n",
      "     37            0.6000        \u001b[32m0.2112\u001b[0m       0.2604            0.2604        1.9079  0.0007  0.1692\n",
      "     38            0.6000        \u001b[32m0.1501\u001b[0m       0.2812            0.2812        2.0737  0.0007  0.1310\n",
      "     39            0.6000        0.1805       0.2708            0.2708        2.2622  0.0007  0.1160\n",
      "     40            0.6000        \u001b[32m0.1232\u001b[0m       0.2708            0.2708        2.4386  0.0007  0.1400\n",
      "     41            0.6000        \u001b[32m0.0737\u001b[0m       0.2917            0.2917        2.5761  0.0007  0.1471\n",
      "     42            0.6000        0.1201       0.2812            0.2812        2.6794  0.0007  0.1289\n",
      "     43            0.6000        \u001b[32m0.0538\u001b[0m       0.2708            0.2708        2.7317  0.0006  0.1185\n",
      "     44            0.6000        \u001b[32m0.0431\u001b[0m       0.2708            0.2708        2.7437  0.0006  0.1511\n",
      "     45            0.7000        \u001b[32m0.0360\u001b[0m       0.2708            0.2708        2.7216  0.0005  0.1331\n",
      "     46            0.7000        \u001b[32m0.0351\u001b[0m       0.2812            0.2812        2.6781  0.0005  0.1337\n",
      "     47            0.7000        \u001b[32m0.0257\u001b[0m       0.2917            0.2917        2.6244  0.0004  0.1510\n",
      "     48            0.7000        \u001b[32m0.0206\u001b[0m       0.2812            0.2812        2.5696  0.0004  0.1666\n",
      "     49            0.8000        0.0272       0.2500            0.2500        2.5204  0.0003  0.1506\n",
      "     50            0.9000        \u001b[32m0.0194\u001b[0m       0.2188            0.2188        2.4813  0.0003  0.1391\n",
      "Fine tuning model for subject 2 with 10 = 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        1.5178       0.3542            0.3542        1.6647  0.0004  0.1312\n",
      "     32            0.7000        1.3378       0.3125            0.3125        1.6722  0.0005  0.0985\n",
      "     33            \u001b[36m0.9000\u001b[0m        0.8109       0.2292            0.2292        1.7754  0.0005  0.1041\n",
      "     34            0.9000        \u001b[32m0.5176\u001b[0m       0.2083            0.2083        1.9649  0.0006  0.1076\n",
      "     35            0.9000        \u001b[32m0.2553\u001b[0m       0.2083            0.2083        2.1467  0.0006  0.0956\n",
      "     36            0.9000        \u001b[32m0.2084\u001b[0m       0.2083            0.2083        2.2553  0.0007  0.0958\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0683\u001b[0m       0.1875            0.1875        2.3135  0.0007  0.1199\n",
      "     38            1.0000        \u001b[32m0.0506\u001b[0m       0.2083            0.2083        2.3300  0.0007  0.1168\n",
      "     39            1.0000        0.0520       0.2083            0.2083        2.3250  0.0007  0.1011\n",
      "     40            1.0000        \u001b[32m0.0488\u001b[0m       0.2396            0.2396        2.3143  0.0007  0.1154\n",
      "     41            1.0000        \u001b[32m0.0292\u001b[0m       0.2604            0.2604        2.3026  0.0007  0.1099\n",
      "     42            1.0000        0.0401       0.2500            0.2500        2.2975  0.0007  0.0998\n",
      "     43            1.0000        \u001b[32m0.0261\u001b[0m       0.2396            0.2396        2.2930  0.0006  0.0985\n",
      "     44            1.0000        0.0349       0.2604            0.2604        2.2888  0.0006  0.1012\n",
      "     45            1.0000        0.0345       0.2708            0.2708        2.2840  0.0005  0.1077\n",
      "     46            1.0000        \u001b[32m0.0122\u001b[0m       0.2708            0.2708        2.2801  0.0005  0.0968\n",
      "     47            1.0000        0.0216       0.2604            0.2604        2.2764  0.0004  0.0999\n",
      "     48            1.0000        \u001b[32m0.0112\u001b[0m       0.2604            0.2604        2.2722  0.0004  0.1083\n",
      "     49            1.0000        0.0137       0.2500            0.2500        2.2685  0.0003  0.0987\n",
      "     50            1.0000        0.0135       0.2604            0.2604        2.2650  0.0003  0.0990\n",
      "Fine tuning model for subject 2 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.4031       0.3229            0.3229        1.7374  0.0004  0.1033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.9000\u001b[0m        1.0039       0.2812            0.2812        1.8213  0.0005  0.1171\n",
      "     33            0.9000        \u001b[32m0.5955\u001b[0m       0.2917            0.2917        1.9018  0.0005  0.0990\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4950\u001b[0m       0.3021            0.3021        1.9314  0.0006  0.1041\n",
      "     35            1.0000        \u001b[32m0.1887\u001b[0m       0.2917            0.2917        1.9652  0.0006  0.1161\n",
      "     36            1.0000        \u001b[32m0.1455\u001b[0m       0.2812            0.2812        1.9567  0.0007  0.1033\n",
      "     37            1.0000        \u001b[32m0.1083\u001b[0m       0.2812            0.2812        1.9233  0.0007  0.1003\n",
      "     38            1.0000        0.1123       0.2604            0.2604        1.8884  0.0007  0.1677\n",
      "     39            1.0000        \u001b[32m0.0681\u001b[0m       0.2708            0.2708        1.8644  0.0007  0.1183\n",
      "     40            1.0000        0.0912       0.2812            0.2812        1.8506  0.0007  0.1215\n",
      "     41            1.0000        \u001b[32m0.0373\u001b[0m       0.2604            0.2604        1.8492  0.0007  0.1158\n",
      "     42            1.0000        0.0482       0.2396            0.2396        1.8583  0.0007  0.0915\n",
      "     43            1.0000        0.0727       0.2500            0.2500        1.8716  0.0006  0.1177\n",
      "     44            1.0000        0.0436       0.2604            0.2604        1.8833  0.0006  0.1098\n",
      "     45            1.0000        0.0422       0.2500            0.2500        1.8925  0.0005  0.0955\n",
      "     46            1.0000        \u001b[32m0.0370\u001b[0m       0.2708            0.2708        1.8981  0.0005  0.1159\n",
      "     47            1.0000        \u001b[32m0.0244\u001b[0m       0.2604            0.2604        1.9013  0.0004  0.1207\n",
      "     48            1.0000        \u001b[32m0.0205\u001b[0m       0.2604            0.2604        1.9037  0.0004  0.1165\n",
      "     49            1.0000        0.0270       0.2604            0.2604        1.9065  0.0003  0.1017\n",
      "     50            1.0000        \u001b[32m0.0172\u001b[0m       0.2500            0.2500        1.9102  0.0003  0.1170\n",
      "Fine tuning model for subject 2 with 10 = 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        2.1505       0.3438            0.3438        1.6904  0.0004  0.1250\n",
      "     32            0.6000        1.3946       0.2812            0.2812        1.7274  0.0005  0.1025\n",
      "     33            \u001b[36m0.9000\u001b[0m        1.4069       0.2188            0.2188        1.8202  0.0005  0.1197\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4177\u001b[0m       0.1979            0.1979        1.9495  0.0006  0.1146\n",
      "     35            1.0000        \u001b[32m0.3981\u001b[0m       0.1979            0.1979        2.0315  0.0006  0.1034\n",
      "     36            1.0000        \u001b[32m0.1593\u001b[0m       0.1979            0.1979        2.0604  0.0007  0.1158\n",
      "     37            1.0000        \u001b[32m0.1145\u001b[0m       0.2083            0.2083        2.0321  0.0007  0.1208\n",
      "     38            1.0000        \u001b[32m0.1116\u001b[0m       0.2083            0.2083        1.9899  0.0007  0.1199\n",
      "     39            1.0000        \u001b[32m0.0871\u001b[0m       0.2292            0.2292        1.9487  0.0007  0.1415\n",
      "     40            1.0000        \u001b[32m0.0412\u001b[0m       0.2500            0.2500        1.9345  0.0007  0.1100\n",
      "     41            1.0000        \u001b[32m0.0386\u001b[0m       0.2292            0.2292        1.9465  0.0007  0.1171\n",
      "     42            1.0000        0.0491       0.2500            0.2500        1.9802  0.0007  0.1192\n",
      "     43            1.0000        0.0527       0.2500            0.2500        2.0195  0.0006  0.1359\n",
      "     44            1.0000        \u001b[32m0.0365\u001b[0m       0.2500            0.2500        2.0567  0.0006  0.1405\n",
      "     45            1.0000        0.0478       0.2500            0.2500        2.0832  0.0005  0.1130\n",
      "     46            1.0000        \u001b[32m0.0250\u001b[0m       0.2604            0.2604        2.0992  0.0005  0.1051\n",
      "     47            1.0000        0.0406       0.2812            0.2812        2.1044  0.0004  0.1159\n",
      "     48            1.0000        0.0374       0.2917            0.2917        2.1004  0.0004  0.1210\n",
      "     49            1.0000        \u001b[32m0.0247\u001b[0m       0.2812            0.2812        2.0931  0.0003  0.1246\n",
      "     50            1.0000        0.0314       0.2708            0.2708        2.0840  0.0003  0.1100\n",
      "Fine tuning model for subject 2 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        1.7436       0.3542            0.3542        1.6625  0.0004  0.1218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        1.5570       0.3646            0.3646        1.7205  0.0005  0.1116\n",
      "     33            0.8000        0.8185       0.3125            0.3125        1.8112  0.0005  0.1009\n",
      "     34            0.7000        \u001b[32m0.5863\u001b[0m       0.3125            0.3125        1.9086  0.0006  0.1196\n",
      "     35            0.8000        \u001b[32m0.3367\u001b[0m       0.2812            0.2812        2.0484  0.0006  0.1143\n",
      "     36            \u001b[36m0.9000\u001b[0m        \u001b[32m0.2147\u001b[0m       0.2812            0.2812        2.2407  0.0007  0.1048\n",
      "     37            0.9000        \u001b[32m0.1491\u001b[0m       0.2708            0.2708        2.4841  0.0007  0.1162\n",
      "     38            \u001b[36m1.0000\u001b[0m        0.1862       0.2500            0.2500        2.7563  0.0007  0.1184\n",
      "     39            0.9000        \u001b[32m0.0694\u001b[0m       0.2500            0.2500        3.0141  0.0007  0.1680\n",
      "     40            0.9000        \u001b[32m0.0545\u001b[0m       0.2500            0.2500        3.2292  0.0007  0.2834\n",
      "     41            0.9000        \u001b[32m0.0433\u001b[0m       0.2500            0.2500        3.3858  0.0007  0.2093\n",
      "     42            0.9000        0.0608       0.2604            0.2604        3.4937  0.0007  0.1760\n",
      "     43            0.9000        0.0520       0.2604            0.2604        3.5576  0.0006  0.1317\n",
      "     44            0.9000        \u001b[32m0.0327\u001b[0m       0.2604            0.2604        3.5883  0.0006  0.1385\n",
      "     45            0.9000        0.0436       0.2604            0.2604        3.5915  0.0005  0.1306\n",
      "     46            0.9000        0.0480       0.2500            0.2500        3.5739  0.0005  0.1371\n",
      "     47            1.0000        0.0412       0.2500            0.2500        3.5453  0.0004  0.1356\n",
      "     48            1.0000        \u001b[32m0.0300\u001b[0m       0.2396            0.2396        3.5129  0.0004  0.1279\n",
      "     49            1.0000        0.0360       0.2500            0.2500        3.4786  0.0003  0.1081\n",
      "     50            1.0000        \u001b[32m0.0188\u001b[0m       0.2500            0.2500        3.4433  0.0003  0.1035\n",
      "Fine tuning model for subject 2 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.6582       0.3333            0.3333        1.6924  0.0004  0.1064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        1.3401       0.3125            0.3125        1.7147  0.0005  0.1177\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4995\u001b[0m       0.3021            0.3021        1.7782  0.0005  0.1146\n",
      "     34            1.0000        \u001b[32m0.3171\u001b[0m       0.2917            0.2917        1.8663  0.0006  0.1203\n",
      "     35            1.0000        \u001b[32m0.1453\u001b[0m       0.3021            0.3021        1.9408  0.0006  0.1235\n",
      "     36            1.0000        \u001b[32m0.0664\u001b[0m       0.3125            0.3125        1.9894  0.0007  0.0955\n",
      "     37            1.0000        \u001b[32m0.0607\u001b[0m       0.3021            0.3021        2.0236  0.0007  0.1170\n",
      "     38            1.0000        0.0953       0.2812            0.2812        2.0520  0.0007  0.1041\n",
      "     39            1.0000        0.0627       0.2500            0.2500        2.0888  0.0007  0.1169\n",
      "     40            1.0000        \u001b[32m0.0439\u001b[0m       0.2500            0.2500        2.1273  0.0007  0.1029\n",
      "     41            1.0000        0.0605       0.2396            0.2396        2.1651  0.0007  0.1003\n",
      "     42            1.0000        \u001b[32m0.0333\u001b[0m       0.2396            0.2396        2.1940  0.0007  0.1042\n",
      "     43            1.0000        0.0370       0.2396            0.2396        2.2127  0.0006  0.1220\n",
      "     44            1.0000        \u001b[32m0.0231\u001b[0m       0.2292            0.2292        2.2195  0.0006  0.0990\n",
      "     45            1.0000        0.0454       0.2396            0.2396        2.2215  0.0005  0.0942\n",
      "     46            1.0000        \u001b[32m0.0229\u001b[0m       0.2604            0.2604        2.2186  0.0005  0.1168\n",
      "     47            1.0000        0.0234       0.2500            0.2500        2.2124  0.0004  0.1198\n",
      "     48            1.0000        \u001b[32m0.0167\u001b[0m       0.2708            0.2708        2.2036  0.0004  0.0986\n",
      "     49            1.0000        \u001b[32m0.0155\u001b[0m       0.2708            0.2708        2.1933  0.0003  0.0921\n",
      "     50            1.0000        0.0157       0.2604            0.2604        2.1833  0.0003  0.1170\n",
      "Fine tuning model for subject 2 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.9722       0.2917            0.2917        1.8459  0.0004  0.1308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5000        1.3281       0.2812            0.2812        2.0421  0.0005  0.1665\n",
      "     33            0.5000        1.0138       0.2396            0.2396        2.1758  0.0005  0.1324\n",
      "     34            0.6000        \u001b[32m0.3368\u001b[0m       0.2396            0.2396        2.2431  0.0006  0.1667\n",
      "     35            0.6000        \u001b[32m0.2031\u001b[0m       0.2500            0.2500        2.2478  0.0006  0.1203\n",
      "     36            0.6000        0.2074       0.2396            0.2396        2.2691  0.0007  0.1831\n",
      "     37            0.7000        \u001b[32m0.0845\u001b[0m       0.2604            0.2604        2.3075  0.0007  0.1182\n",
      "     38            0.7000        \u001b[32m0.0805\u001b[0m       0.2292            0.2292        2.3586  0.0007  0.1653\n",
      "     39            0.8000        \u001b[32m0.0743\u001b[0m       0.2500            0.2500        2.4045  0.0007  0.1387\n",
      "     40            0.8000        \u001b[32m0.0328\u001b[0m       0.2500            0.2500        2.4368  0.0007  0.1489\n",
      "     41            \u001b[36m0.9000\u001b[0m        0.0606       0.2604            0.2604        2.4627  0.0007  0.1336\n",
      "     42            \u001b[36m1.0000\u001b[0m        0.0500       0.2812            0.2812        2.4911  0.0007  0.1352\n",
      "     43            1.0000        0.0362       0.2396            0.2396        2.5169  0.0006  0.1824\n",
      "     44            1.0000        0.0398       0.2500            0.2500        2.5454  0.0006  0.1896\n",
      "     45            1.0000        \u001b[32m0.0293\u001b[0m       0.2188            0.2188        2.5768  0.0005  0.1613\n",
      "     46            1.0000        \u001b[32m0.0252\u001b[0m       0.2188            0.2188        2.6133  0.0005  0.1744\n",
      "     47            1.0000        \u001b[32m0.0172\u001b[0m       0.2083            0.2083        2.6510  0.0004  0.1748\n",
      "     48            1.0000        0.0186       0.1979            0.1979        2.6897  0.0004  0.1667\n",
      "     49            1.0000        0.0184       0.1875            0.1875        2.7282  0.0003  0.1823\n",
      "     50            1.0000        0.0266       0.2188            0.2188        2.7653  0.0003  0.1365\n",
      "Fine tuning model for subject 2 with 10 = 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.8045       0.3229            0.3229        1.7317  0.0004  0.1931\n",
      "     32            \u001b[36m0.9000\u001b[0m        1.2905       0.2708            0.2708        1.7918  0.0005  0.1790\n",
      "     33            0.8000        0.6844       0.2708            0.2708        1.8852  0.0005  0.1514\n",
      "     34            0.9000        \u001b[32m0.5803\u001b[0m       0.2500            0.2500        1.9780  0.0006  0.1491\n",
      "     35            0.7000        \u001b[32m0.3080\u001b[0m       0.2917            0.2917        2.1265  0.0006  0.1349\n",
      "     36            0.7000        \u001b[32m0.2869\u001b[0m       0.2500            0.2500        2.1508  0.0007  0.1354\n",
      "     37            0.7000        \u001b[32m0.0852\u001b[0m       0.2188            0.2188        2.1292  0.0007  0.1535\n",
      "     38            \u001b[36m1.0000\u001b[0m        0.1933       0.2604            0.2604        2.0528  0.0007  0.1511\n",
      "     39            1.0000        0.1287       0.2396            0.2396        2.0426  0.0007  0.1657\n",
      "     40            1.0000        \u001b[32m0.0710\u001b[0m       0.2188            0.2188        2.0911  0.0007  0.1666\n",
      "     41            1.0000        0.0759       0.2083            0.2083        2.1638  0.0007  0.1150\n",
      "     42            1.0000        0.0869       0.2083            0.2083        2.2390  0.0007  0.1759\n",
      "     43            1.0000        \u001b[32m0.0702\u001b[0m       0.2292            0.2292        2.3008  0.0006  0.1448\n",
      "     44            1.0000        \u001b[32m0.0695\u001b[0m       0.2188            0.2188        2.3476  0.0006  0.1511\n",
      "     45            1.0000        \u001b[32m0.0311\u001b[0m       0.2604            0.2604        2.3799  0.0005  0.2530\n",
      "     46            1.0000        \u001b[32m0.0222\u001b[0m       0.2396            0.2396        2.3995  0.0005  0.1012\n",
      "     47            1.0000        0.0282       0.2604            0.2604        2.4094  0.0004  0.1363\n",
      "     48            1.0000        0.0277       0.2708            0.2708        2.4147  0.0004  0.1517\n",
      "     49            1.0000        0.0319       0.2500            0.2500        2.4158  0.0003  0.1062\n",
      "     50            1.0000        \u001b[32m0.0181\u001b[0m       0.2604            0.2604        2.4147  0.0003  0.0965\n",
      "Fine tuning model for subject 2 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.1846       0.3333            0.3333        1.7194  0.0004  0.0874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        0.8426       0.3021            0.3021        1.8226  0.0005  0.1160\n",
      "     33            1.0000        \u001b[32m0.4862\u001b[0m       0.2917            0.2917        1.9526  0.0005  0.1145\n",
      "     34            1.0000        \u001b[32m0.3381\u001b[0m       0.2812            0.2812        2.0687  0.0006  0.1042\n",
      "     35            1.0000        \u001b[32m0.1964\u001b[0m       0.3229            0.3229        2.1815  0.0006  0.1457\n",
      "     36            0.8000        \u001b[32m0.1278\u001b[0m       0.2917            0.2917        2.2558  0.0007  0.1201\n",
      "     37            1.0000        \u001b[32m0.1249\u001b[0m       0.2917            0.2917        2.1967  0.0007  0.1029\n",
      "     38            1.0000        \u001b[32m0.0532\u001b[0m       0.2604            0.2604        2.1381  0.0007  0.1012\n",
      "     39            1.0000        \u001b[32m0.0460\u001b[0m       0.2708            0.2708        2.1173  0.0007  0.1192\n",
      "     40            1.0000        \u001b[32m0.0392\u001b[0m       0.2917            0.2917        2.1319  0.0007  0.0847\n",
      "     41            1.0000        0.0601       0.2917            0.2917        2.1518  0.0007  0.0886\n",
      "     42            1.0000        0.0454       0.3021            0.3021        2.1659  0.0007  0.1024\n",
      "     43            1.0000        \u001b[32m0.0332\u001b[0m       0.3021            0.3021        2.1648  0.0006  0.1085\n",
      "     44            1.0000        \u001b[32m0.0147\u001b[0m       0.3021            0.3021        2.1520  0.0006  0.1090\n",
      "     45            1.0000        0.0506       0.3021            0.3021        2.1322  0.0005  0.0979\n",
      "     46            1.0000        \u001b[32m0.0145\u001b[0m       0.3229            0.3229        2.1070  0.0005  0.1113\n",
      "     47            1.0000        0.0342       0.3021            0.3021        2.0762  0.0004  0.0986\n",
      "     48            1.0000        0.0163       0.3229            0.3229        2.0476  0.0004  0.1145\n",
      "     49            1.0000        \u001b[32m0.0074\u001b[0m       0.3542            0.3542        2.0213  0.0003  0.1159\n",
      "     50            1.0000        0.0159       0.3438            0.3438        1.9981  0.0003  0.1156\n",
      "Fine tuning model for subject 2 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3333        1.5910       0.3333            0.3333        1.6962  0.0004  0.0961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4667        1.6804       0.3229            0.3229        1.7329  0.0005  0.1167\n",
      "     33            0.7333        1.1509       0.2500            0.2500        1.7781  0.0005  0.0981\n",
      "     34            0.8000        0.7272       0.2500            0.2500        1.8806  0.0006  0.1353\n",
      "     35            \u001b[36m0.8667\u001b[0m        \u001b[32m0.3564\u001b[0m       0.2604            0.2604        2.0553  0.0006  0.1021\n",
      "     36            \u001b[36m0.9333\u001b[0m        \u001b[32m0.2457\u001b[0m       0.2708            0.2708        2.2518  0.0007  0.0846\n",
      "     37            0.9333        \u001b[32m0.2131\u001b[0m       0.2500            0.2500        2.4225  0.0007  0.1010\n",
      "     38            0.9333        \u001b[32m0.1730\u001b[0m       0.2396            0.2396        2.5423  0.0007  0.1115\n",
      "     39            0.9333        \u001b[32m0.1072\u001b[0m       0.2292            0.2292        2.6186  0.0007  0.1162\n",
      "     40            0.9333        0.1154       0.2396            0.2396        2.6710  0.0007  0.1024\n",
      "     41            0.9333        \u001b[32m0.0675\u001b[0m       0.2292            0.2292        2.7007  0.0007  0.1022\n",
      "     42            0.9333        \u001b[32m0.0648\u001b[0m       0.2396            0.2396        2.7146  0.0007  0.1007\n",
      "     43            0.9333        \u001b[32m0.0478\u001b[0m       0.2396            0.2396        2.7164  0.0006  0.0855\n",
      "     44            \u001b[36m1.0000\u001b[0m        0.0657       0.2396            0.2396        2.7018  0.0006  0.1004\n",
      "     45            1.0000        \u001b[32m0.0377\u001b[0m       0.2500            0.2500        2.6817  0.0005  0.1152\n",
      "     46            1.0000        0.0492       0.2604            0.2604        2.6568  0.0005  0.0986\n",
      "     47            1.0000        \u001b[32m0.0351\u001b[0m       0.2604            0.2604        2.6293  0.0004  0.0863\n",
      "     48            1.0000        \u001b[32m0.0336\u001b[0m       0.2708            0.2708        2.6007  0.0004  0.1015\n",
      "     49            1.0000        0.0419       0.2604            0.2604        2.5712  0.0003  0.1149\n",
      "     50            1.0000        \u001b[32m0.0103\u001b[0m       0.2396            0.2396        2.5446  0.0003  0.0890\n",
      "Fine tuning model for subject 2 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.8420       0.3229            0.3229        1.7100  0.0004  0.0988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5333        1.3699       0.2812            0.2812        1.7268  0.0005  0.1146\n",
      "     33            0.6667        0.9970       0.2812            0.2812        1.7534  0.0005  0.0980\n",
      "     34            \u001b[36m0.9333\u001b[0m        0.6899       0.2396            0.2396        1.8409  0.0006  0.1121\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5223\u001b[0m       0.2292            0.2292        2.0162  0.0006  0.0996\n",
      "     36            0.9333        \u001b[32m0.2853\u001b[0m       0.2188            0.2188        2.2218  0.0007  0.0944\n",
      "     37            0.9333        \u001b[32m0.2077\u001b[0m       0.2292            0.2292        2.3893  0.0007  0.1247\n",
      "     38            0.9333        0.2278       0.2188            0.2188        2.4812  0.0007  0.1099\n",
      "     39            0.9333        \u001b[32m0.1513\u001b[0m       0.2083            0.2083        2.5370  0.0007  0.0894\n",
      "     40            1.0000        0.1941       0.2292            0.2292        2.5499  0.0007  0.1000\n",
      "     41            1.0000        \u001b[32m0.0905\u001b[0m       0.2500            0.2500        2.5553  0.0007  0.1127\n",
      "     42            1.0000        \u001b[32m0.0652\u001b[0m       0.2396            0.2396        2.5534  0.0007  0.0987\n",
      "     43            1.0000        \u001b[32m0.0652\u001b[0m       0.2500            0.2500        2.5487  0.0006  0.0924\n",
      "     44            1.0000        \u001b[32m0.0436\u001b[0m       0.2500            0.2500        2.5365  0.0006  0.1014\n",
      "     45            1.0000        \u001b[32m0.0387\u001b[0m       0.2396            0.2396        2.5219  0.0005  0.1132\n",
      "     46            1.0000        0.0539       0.2396            0.2396        2.5062  0.0005  0.0997\n",
      "     47            1.0000        0.0491       0.2396            0.2396        2.4940  0.0004  0.0942\n",
      "     48            1.0000        \u001b[32m0.0268\u001b[0m       0.2396            0.2396        2.4830  0.0004  0.1180\n",
      "     49            1.0000        \u001b[32m0.0237\u001b[0m       0.2396            0.2396        2.4737  0.0003  0.1052\n",
      "     50            1.0000        0.0262       0.2292            0.2292        2.4659  0.0003  0.0964\n",
      "Fine tuning model for subject 2 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2667        2.0581       0.3438            0.3438        1.6638  0.0004  0.0992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5333        1.6073       0.3333            0.3333        1.6898  0.0005  0.1294\n",
      "     33            0.5333        1.3799       0.2812            0.2812        1.8019  0.0005  0.1002\n",
      "     34            0.5333        0.8257       0.2604            0.2604        1.8822  0.0006  0.0950\n",
      "     35            0.7333        0.8831       0.2604            0.2604        1.8782  0.0006  0.1051\n",
      "     36            \u001b[36m0.8667\u001b[0m        \u001b[32m0.4807\u001b[0m       0.2188            0.2188        1.8992  0.0007  0.0972\n",
      "     37            \u001b[36m0.9333\u001b[0m        \u001b[32m0.2977\u001b[0m       0.2188            0.2188        1.9439  0.0007  0.1003\n",
      "     38            0.8667        0.3563       0.2812            0.2812        1.9960  0.0007  0.1044\n",
      "     39            0.8000        0.3131       0.2292            0.2292        2.0386  0.0007  0.0860\n",
      "     40            0.8667        \u001b[32m0.2048\u001b[0m       0.2396            0.2396        2.0697  0.0007  0.0978\n",
      "     41            0.8667        \u001b[32m0.1623\u001b[0m       0.2604            0.2604        2.0946  0.0007  0.1163\n",
      "     42            0.9333        \u001b[32m0.1125\u001b[0m       0.2708            0.2708        2.1168  0.0007  0.1182\n",
      "     43            0.9333        0.1854       0.2708            0.2708        2.1278  0.0006  0.0987\n",
      "     44            0.9333        \u001b[32m0.0932\u001b[0m       0.2812            0.2812        2.1404  0.0006  0.1012\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0713\u001b[0m       0.2812            0.2812        2.1537  0.0005  0.1084\n",
      "     46            1.0000        \u001b[32m0.0693\u001b[0m       0.2812            0.2812        2.1676  0.0005  0.0987\n",
      "     47            1.0000        0.0759       0.2917            0.2917        2.1835  0.0004  0.1129\n",
      "     48            1.0000        0.0909       0.2917            0.2917        2.2014  0.0004  0.1170\n",
      "     49            1.0000        \u001b[32m0.0664\u001b[0m       0.2917            0.2917        2.2188  0.0003  0.1361\n",
      "     50            1.0000        \u001b[32m0.0581\u001b[0m       0.3021            0.3021        2.2359  0.0003  0.1358\n",
      "Fine tuning model for subject 2 with 15 = 15 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        2.1929       0.3229            0.3229        1.7270  0.0004  0.1397\n",
      "     32            0.4000        1.8339       0.2917            0.2917        1.8130  0.0005  0.1714\n",
      "     33            0.6000        1.1472       0.2708            0.2708        1.8666  0.0005  0.1161\n",
      "     34            \u001b[36m0.8667\u001b[0m        0.7379       0.2917            0.2917        1.9012  0.0006  0.1353\n",
      "     35            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4491\u001b[0m       0.2604            0.2604        1.9157  0.0006  0.1194\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4076\u001b[0m       0.2500            0.2500        1.9453  0.0007  0.1358\n",
      "     37            1.0000        \u001b[32m0.2110\u001b[0m       0.2292            0.2292        1.9942  0.0007  0.1240\n",
      "     38            1.0000        0.2233       0.2500            0.2500        2.0632  0.0007  0.1374\n",
      "     39            1.0000        \u001b[32m0.1213\u001b[0m       0.2708            0.2708        2.1510  0.0007  0.1666\n",
      "     40            1.0000        \u001b[32m0.0752\u001b[0m       0.2604            0.2604        2.2476  0.0007  0.1386\n",
      "     41            0.9333        0.0763       0.2708            0.2708        2.3268  0.0007  0.1412\n",
      "     42            0.8667        0.1575       0.2604            0.2604        2.3512  0.0007  0.1273\n",
      "     43            0.8667        0.0808       0.2604            0.2604        2.3529  0.0006  0.1611\n",
      "     44            0.9333        \u001b[32m0.0669\u001b[0m       0.2708            0.2708        2.3397  0.0006  0.1236\n",
      "     45            0.9333        0.0698       0.2708            0.2708        2.3138  0.0005  0.1010\n",
      "     46            0.9333        \u001b[32m0.0402\u001b[0m       0.2708            0.2708        2.2829  0.0005  0.1025\n",
      "     47            0.9333        0.0643       0.2708            0.2708        2.2465  0.0004  0.1020\n",
      "     48            0.9333        \u001b[32m0.0342\u001b[0m       0.2708            0.2708        2.2123  0.0004  0.0990\n",
      "     49            0.9333        \u001b[32m0.0327\u001b[0m       0.2500            0.2500        2.1815  0.0003  0.1008\n",
      "     50            1.0000        0.0485       0.2292            0.2292        2.1555  0.0003  0.1104\n",
      "Fine tuning model for subject 2 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5333        1.2857       0.3229            0.3229        1.7231  0.0004  0.1093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6667        1.0277       0.2812            0.2812        1.7996  0.0005  0.1189\n",
      "     33            0.7333        \u001b[32m0.6154\u001b[0m       0.2396            0.2396        1.8636  0.0005  0.0958\n",
      "     34            \u001b[36m0.8667\u001b[0m        \u001b[32m0.5112\u001b[0m       0.2188            0.2188        1.9259  0.0006  0.1042\n",
      "     35            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3782\u001b[0m       0.2396            0.2396        1.9366  0.0006  0.1003\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2280\u001b[0m       0.2292            0.2292        1.9520  0.0007  0.0848\n",
      "     37            1.0000        \u001b[32m0.1710\u001b[0m       0.2292            0.2292        1.9559  0.0007  0.1016\n",
      "     38            1.0000        0.1728       0.2292            0.2292        1.9535  0.0007  0.1041\n",
      "     39            1.0000        \u001b[32m0.1417\u001b[0m       0.2500            0.2500        1.9474  0.0007  0.0867\n",
      "     40            1.0000        0.1738       0.2604            0.2604        1.9542  0.0007  0.1001\n",
      "     41            1.0000        \u001b[32m0.0588\u001b[0m       0.2812            0.2812        1.9750  0.0007  0.1183\n",
      "     42            1.0000        \u001b[32m0.0538\u001b[0m       0.2917            0.2917        2.0071  0.0007  0.0986\n",
      "     43            1.0000        \u001b[32m0.0380\u001b[0m       0.2500            0.2500        2.0409  0.0006  0.0868\n",
      "     44            1.0000        0.0468       0.2396            0.2396        2.0741  0.0006  0.1013\n",
      "     45            1.0000        0.0485       0.2500            0.2500        2.1066  0.0005  0.1231\n",
      "     46            1.0000        \u001b[32m0.0372\u001b[0m       0.2604            0.2604        2.1360  0.0005  0.0964\n",
      "     47            1.0000        0.0379       0.2812            0.2812        2.1647  0.0004  0.1045\n",
      "     48            1.0000        0.0394       0.2708            0.2708        2.1910  0.0004  0.1040\n",
      "     49            1.0000        0.0454       0.2396            0.2396        2.2151  0.0003  0.0986\n",
      "     50            1.0000        \u001b[32m0.0255\u001b[0m       0.2396            0.2396        2.2367  0.0003  0.0984\n",
      "Fine tuning model for subject 2 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        1.7880       0.2812            0.2812        1.7878  0.0004  0.0919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6667        1.4138       0.2708            0.2708        1.8951  0.0005  0.1025\n",
      "     33            0.8000        0.7298       0.2500            0.2500        1.9131  0.0005  0.1147\n",
      "     34            \u001b[36m1.0000\u001b[0m        0.6522       0.2812            0.2812        1.8806  0.0006  0.1151\n",
      "     35            1.0000        \u001b[32m0.4594\u001b[0m       0.2396            0.2396        1.9148  0.0006  0.0973\n",
      "     36            1.0000        \u001b[32m0.2476\u001b[0m       0.2292            0.2292        1.9805  0.0007  0.0881\n",
      "     37            1.0000        0.3457       0.1979            0.1979        2.0498  0.0007  0.1026\n",
      "     38            1.0000        \u001b[32m0.2053\u001b[0m       0.1771            0.1771        2.1208  0.0007  0.1098\n",
      "     39            1.0000        \u001b[32m0.1782\u001b[0m       0.1667            0.1667        2.1759  0.0007  0.0907\n",
      "     40            1.0000        0.1822       0.1667            0.1667        2.1969  0.0007  0.1022\n",
      "     41            1.0000        \u001b[32m0.1462\u001b[0m       0.1771            0.1771        2.2118  0.0007  0.1063\n",
      "     42            1.0000        \u001b[32m0.0639\u001b[0m       0.2083            0.2083        2.2257  0.0007  0.0986\n",
      "     43            1.0000        0.0713       0.1979            0.1979        2.2325  0.0006  0.0989\n",
      "     44            1.0000        0.0897       0.1979            0.1979        2.2334  0.0006  0.1070\n",
      "     45            1.0000        0.0983       0.1979            0.1979        2.2354  0.0005  0.1211\n",
      "     46            1.0000        0.1083       0.2083            0.2083        2.2348  0.0005  0.0988\n",
      "     47            1.0000        0.0751       0.2083            0.2083        2.2371  0.0004  0.0992\n",
      "     48            1.0000        \u001b[32m0.0467\u001b[0m       0.1875            0.1875        2.2383  0.0004  0.1030\n",
      "     49            1.0000        \u001b[32m0.0448\u001b[0m       0.1771            0.1771        2.2404  0.0003  0.1082\n",
      "     50            1.0000        \u001b[32m0.0437\u001b[0m       0.1875            0.1875        2.2408  0.0003  0.0936\n",
      "Fine tuning model for subject 2 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4667        1.8770       0.3021            0.3021        1.6791  0.0004  0.1030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4667        1.4917       0.3333            0.3333        1.7140  0.0005  0.0966\n",
      "     33            0.6667        0.9615       0.3229            0.3229        1.8003  0.0005  0.0989\n",
      "     34            0.8000        \u001b[32m0.6042\u001b[0m       0.3125            0.3125        1.8916  0.0006  0.1173\n",
      "     35            \u001b[36m0.8667\u001b[0m        \u001b[32m0.4922\u001b[0m       0.3021            0.3021        1.9230  0.0006  0.1249\n",
      "     36            0.8000        \u001b[32m0.4657\u001b[0m       0.2917            0.2917        1.9256  0.0007  0.0900\n",
      "     37            0.8000        \u001b[32m0.2973\u001b[0m       0.2917            0.2917        1.9221  0.0007  0.1018\n",
      "     38            0.8000        \u001b[32m0.2338\u001b[0m       0.2917            0.2917        1.9156  0.0007  0.1172\n",
      "     39            0.8667        \u001b[32m0.1980\u001b[0m       0.3125            0.3125        1.9153  0.0007  0.0986\n",
      "     40            0.8667        \u001b[32m0.1406\u001b[0m       0.3125            0.3125        1.9217  0.0007  0.0996\n",
      "     41            \u001b[36m0.9333\u001b[0m        0.1845       0.3438            0.3438        1.9316  0.0007  0.1032\n",
      "     42            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1040\u001b[0m       0.3333            0.3333        1.9491  0.0007  0.1016\n",
      "     43            1.0000        \u001b[32m0.1004\u001b[0m       0.3229            0.3229        1.9720  0.0006  0.1477\n",
      "     44            1.0000        \u001b[32m0.0930\u001b[0m       0.3125            0.3125        1.9963  0.0006  0.0885\n",
      "     45            1.0000        \u001b[32m0.0876\u001b[0m       0.3229            0.3229        2.0221  0.0005  0.1003\n",
      "     46            1.0000        \u001b[32m0.0509\u001b[0m       0.3438            0.3438        2.0500  0.0005  0.1154\n",
      "     47            1.0000        0.0694       0.3438            0.3438        2.0780  0.0004  0.0985\n",
      "     48            1.0000        0.0970       0.3542            0.3542        2.1067  0.0004  0.1024\n",
      "     49            1.0000        0.0631       0.3542            0.3542        2.1351  0.0003  0.1032\n",
      "     50            1.0000        0.0579       0.3646            0.3646        2.1612  0.0003  0.1037\n",
      "Fine tuning model for subject 2 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2667        1.6526       0.3125            0.3125        1.7189  0.0004  0.1020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.4883       0.2812            0.2812        1.7861  0.0005  0.1064\n",
      "     33            0.8000        1.2063       0.2708            0.2708        1.8052  0.0005  0.0980\n",
      "     34            \u001b[36m1.0000\u001b[0m        0.6964       0.3021            0.3021        1.7755  0.0006  0.1043\n",
      "     35            1.0000        \u001b[32m0.4112\u001b[0m       0.2917            0.2917        1.8139  0.0006  0.1000\n",
      "     36            1.0000        \u001b[32m0.2562\u001b[0m       0.2708            0.2708        1.9379  0.0007  0.1171\n",
      "     37            0.9333        \u001b[32m0.2248\u001b[0m       0.3125            0.3125        2.0988  0.0007  0.1021\n",
      "     38            0.7333        \u001b[32m0.1483\u001b[0m       0.2812            0.2812        2.3134  0.0007  0.1042\n",
      "     39            0.7333        \u001b[32m0.1422\u001b[0m       0.2812            0.2812        2.5716  0.0007  0.0966\n",
      "     40            0.6667        \u001b[32m0.1052\u001b[0m       0.2708            0.2708        2.8342  0.0007  0.1013\n",
      "     41            0.6667        0.1403       0.2708            0.2708        3.0464  0.0007  0.1153\n",
      "     42            0.6000        0.1132       0.2708            0.2708        3.1884  0.0007  0.1294\n",
      "     43            0.6667        \u001b[32m0.0982\u001b[0m       0.2604            0.2604        3.2606  0.0006  0.1011\n",
      "     44            0.6667        \u001b[32m0.0784\u001b[0m       0.2604            0.2604        3.2651  0.0006  0.1076\n",
      "     45            0.7333        \u001b[32m0.0630\u001b[0m       0.2604            0.2604        3.2201  0.0005  0.1070\n",
      "     46            0.8000        \u001b[32m0.0483\u001b[0m       0.2396            0.2396        3.1399  0.0005  0.0987\n",
      "     47            0.8000        0.0537       0.2396            0.2396        3.0348  0.0004  0.0945\n",
      "     48            0.8000        0.0685       0.2708            0.2708        2.9226  0.0004  0.1025\n",
      "     49            0.8000        \u001b[32m0.0316\u001b[0m       0.2708            0.2708        2.8113  0.0003  0.1064\n",
      "     50            0.9333        0.0355       0.2604            0.2604        2.7090  0.0003  0.0943\n",
      "Fine tuning model for subject 2 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3333        2.4947       0.3333            0.3333        1.6968  0.0004  0.1071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4667        2.0360       0.3021            0.3021        1.7800  0.0005  0.1108\n",
      "     33            0.6667        1.5116       0.2812            0.2812        1.9156  0.0005  0.1235\n",
      "     34            0.7333        0.7225       0.2812            0.2812        2.0474  0.0006  0.1025\n",
      "     35            0.8000        0.7198       0.2812            0.2812        2.1361  0.0006  0.1041\n",
      "     36            \u001b[36m0.8667\u001b[0m        \u001b[32m0.4080\u001b[0m       0.2708            0.2708        2.1834  0.0007  0.1219\n",
      "     37            \u001b[36m0.9333\u001b[0m        \u001b[32m0.2872\u001b[0m       0.2812            0.2812        2.2093  0.0007  0.0947\n",
      "     38            0.9333        \u001b[32m0.1436\u001b[0m       0.2708            0.2708        2.2332  0.0007  0.1011\n",
      "     39            0.9333        \u001b[32m0.0918\u001b[0m       0.2500            0.2500        2.2625  0.0007  0.1073\n",
      "     40            0.8667        0.1289       0.2604            0.2604        2.2953  0.0007  0.0985\n",
      "     41            0.8667        0.1177       0.2812            0.2812        2.3369  0.0007  0.0963\n",
      "     42            0.9333        \u001b[32m0.0878\u001b[0m       0.2500            0.2500        2.3724  0.0007  0.1023\n",
      "     43            0.9333        \u001b[32m0.0796\u001b[0m       0.2396            0.2396        2.3983  0.0006  0.1029\n",
      "     44            0.9333        0.0921       0.2396            0.2396        2.4161  0.0006  0.0982\n",
      "     45            0.9333        0.1075       0.2604            0.2604        2.4245  0.0005  0.1024\n",
      "     46            0.9333        0.0934       0.2500            0.2500        2.4255  0.0005  0.1006\n",
      "     47            0.9333        \u001b[32m0.0532\u001b[0m       0.2396            0.2396        2.4192  0.0004  0.0855\n",
      "     48            0.9333        \u001b[32m0.0523\u001b[0m       0.2500            0.2500        2.4075  0.0004  0.0998\n",
      "     49            0.9333        0.0542       0.2188            0.2188        2.3930  0.0003  0.1052\n",
      "     50            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0277\u001b[0m       0.2292            0.2292        2.3762  0.0003  0.0883\n",
      "Fine tuning model for subject 2 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2000        1.8829       0.3333            0.3333        1.6851  0.0004  0.0967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4000        1.8200       0.2917            0.2917        1.7848  0.0005  0.0979\n",
      "     33            0.4000        1.3327       0.2604            0.2604        2.0220  0.0005  0.1187\n",
      "     34            0.6000        0.8978       0.2500            0.2500        2.1938  0.0006  0.1333\n",
      "     35            0.6000        \u001b[32m0.5562\u001b[0m       0.2500            0.2500        2.2465  0.0006  0.1199\n",
      "     36            0.6667        \u001b[32m0.4242\u001b[0m       0.2292            0.2292        2.2585  0.0007  0.1219\n",
      "     37            0.8000        \u001b[32m0.3556\u001b[0m       0.1875            0.1875        2.2224  0.0007  0.1168\n",
      "     38            \u001b[36m0.8667\u001b[0m        \u001b[32m0.2309\u001b[0m       0.1979            0.1979        2.1754  0.0007  0.1158\n",
      "     39            \u001b[36m0.9333\u001b[0m        0.2423       0.1458            0.1458        2.1489  0.0007  0.1172\n",
      "     40            0.9333        0.2976       0.1771            0.1771        2.1598  0.0007  0.1149\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1847\u001b[0m       0.1771            0.1771        2.1929  0.0007  0.1202\n",
      "     42            1.0000        \u001b[32m0.1510\u001b[0m       0.1771            0.1771        2.2383  0.0007  0.1336\n",
      "     43            1.0000        \u001b[32m0.0898\u001b[0m       0.1771            0.1771        2.2925  0.0006  0.1331\n",
      "     44            1.0000        0.1015       0.1667            0.1667        2.3543  0.0006  0.1331\n",
      "     45            1.0000        \u001b[32m0.0652\u001b[0m       0.1667            0.1667        2.4210  0.0005  0.1446\n",
      "     46            1.0000        0.0833       0.1771            0.1771        2.4879  0.0005  0.1395\n",
      "     47            1.0000        0.0852       0.1875            0.1875        2.5546  0.0004  0.1668\n",
      "     48            1.0000        0.0715       0.1875            0.1875        2.6199  0.0004  0.1455\n",
      "     49            1.0000        0.0713       0.1979            0.1979        2.6850  0.0003  0.1610\n",
      "     50            1.0000        \u001b[32m0.0611\u001b[0m       0.2188            0.2188        2.7468  0.0003  0.1459\n",
      "Fine tuning model for subject 2 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4500        2.2012       0.3125            0.3125        1.7046  0.0004  0.1635\n",
      "     32            0.4500        1.8856       0.2812            0.2812        1.7442  0.0005  0.1027\n",
      "     33            0.5500        1.7491       0.2396            0.2396        1.7839  0.0005  0.1083\n",
      "     34            0.7000        1.2418       0.2396            0.2396        1.8655  0.0006  0.0984\n",
      "     35            0.7000        0.9426       0.2500            0.2500        1.9878  0.0006  0.0895\n",
      "     36            0.7000        0.6551       0.2396            0.2396        2.1409  0.0007  0.1011\n",
      "     37            0.6500        \u001b[32m0.5661\u001b[0m       0.2188            0.2188        2.2285  0.0007  0.1201\n",
      "     38            0.6000        \u001b[32m0.3838\u001b[0m       0.2188            0.2188        2.2688  0.0007  0.1052\n",
      "     39            0.7000        \u001b[32m0.3192\u001b[0m       0.2500            0.2500        2.2606  0.0007  0.0938\n",
      "     40            0.7000        \u001b[32m0.2692\u001b[0m       0.2500            0.2500        2.2307  0.0007  0.1143\n",
      "     41            0.8000        \u001b[32m0.2406\u001b[0m       0.2292            0.2292        2.1947  0.0007  0.1037\n",
      "     42            \u001b[36m0.9000\u001b[0m        \u001b[32m0.2123\u001b[0m       0.2396            0.2396        2.1564  0.0007  0.1061\n",
      "     43            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1772\u001b[0m       0.2292            0.2292        2.1222  0.0006  0.0931\n",
      "     44            1.0000        \u001b[32m0.1146\u001b[0m       0.2500            0.2500        2.0909  0.0006  0.1156\n",
      "     45            1.0000        0.1216       0.2604            0.2604        2.0657  0.0005  0.1036\n",
      "     46            1.0000        0.1149       0.2604            0.2604        2.0472  0.0005  0.1123\n",
      "     47            1.0000        \u001b[32m0.0914\u001b[0m       0.3021            0.3021        2.0323  0.0004  0.1028\n",
      "     48            1.0000        0.1114       0.3125            0.3125        2.0216  0.0004  0.1002\n",
      "     49            1.0000        \u001b[32m0.0602\u001b[0m       0.3021            0.3021        2.0127  0.0003  0.1183\n",
      "     50            1.0000        0.0779       0.3021            0.3021        2.0056  0.0003  0.1206\n",
      "Fine tuning model for subject 2 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        1.9677       0.2917            0.2917        1.7339  0.0004  0.1136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4000        1.9640       0.3229            0.3229        1.8281  0.0005  0.1149\n",
      "     33            0.5000        1.4949       0.2917            0.2917        1.8470  0.0005  0.1136\n",
      "     34            0.8000        1.0297       0.2812            0.2812        1.8148  0.0006  0.1041\n",
      "     35            \u001b[36m0.9500\u001b[0m        0.8099       0.2812            0.2812        1.7758  0.0006  0.1143\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5736\u001b[0m       0.2812            0.2812        1.7541  0.0007  0.1002\n",
      "     37            1.0000        \u001b[32m0.4863\u001b[0m       0.2604            0.2604        1.7614  0.0007  0.1003\n",
      "     38            1.0000        \u001b[32m0.3530\u001b[0m       0.2708            0.2708        1.8415  0.0007  0.1031\n",
      "     39            1.0000        \u001b[32m0.3509\u001b[0m       0.2708            0.2708        1.9955  0.0007  0.1044\n",
      "     40            1.0000        \u001b[32m0.1821\u001b[0m       0.2500            0.2500        2.1672  0.0007  0.0985\n",
      "     41            1.0000        \u001b[32m0.1605\u001b[0m       0.2292            0.2292        2.3333  0.0007  0.1133\n",
      "     42            1.0000        0.1638       0.2604            0.2604        2.4622  0.0007  0.1082\n",
      "     43            1.0000        0.1652       0.2604            0.2604        2.5633  0.0006  0.0986\n",
      "     44            1.0000        \u001b[32m0.1293\u001b[0m       0.2604            0.2604        2.6142  0.0006  0.1040\n",
      "     45            1.0000        \u001b[32m0.0946\u001b[0m       0.2500            0.2500        2.6322  0.0005  0.1002\n",
      "     46            1.0000        \u001b[32m0.0833\u001b[0m       0.2604            0.2604        2.6225  0.0005  0.1117\n",
      "     47            1.0000        0.1076       0.2604            0.2604        2.5939  0.0004  0.0988\n",
      "     48            1.0000        \u001b[32m0.0771\u001b[0m       0.2500            0.2500        2.5486  0.0004  0.1006\n",
      "     49            1.0000        0.1047       0.2500            0.2500        2.4945  0.0003  0.1024\n",
      "     50            1.0000        \u001b[32m0.0684\u001b[0m       0.2500            0.2500        2.4411  0.0003  0.1229\n",
      "Fine tuning model for subject 2 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2500        2.4215       0.3333            0.3333        1.6566  0.0004  0.1070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5000        2.1091       0.3438            0.3438        1.6734  0.0005  0.1169\n",
      "     33            0.7000        1.3944       0.3021            0.3021        1.7760  0.0005  0.1149\n",
      "     34            \u001b[36m0.8500\u001b[0m        1.0203       0.2708            0.2708        1.8847  0.0006  0.1042\n",
      "     35            0.8500        0.7538       0.2604            0.2604        1.9391  0.0006  0.1134\n",
      "     36            0.8500        \u001b[32m0.4762\u001b[0m       0.2604            0.2604        1.9267  0.0007  0.1043\n",
      "     37            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4617\u001b[0m       0.2083            0.2083        1.9063  0.0007  0.0946\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3286\u001b[0m       0.2083            0.2083        1.8873  0.0007  0.1042\n",
      "     39            1.0000        \u001b[32m0.1941\u001b[0m       0.2188            0.2188        1.8736  0.0007  0.1186\n",
      "     40            1.0000        \u001b[32m0.1455\u001b[0m       0.2188            0.2188        1.8694  0.0007  0.0970\n",
      "     41            1.0000        0.1929       0.1667            0.1667        1.8718  0.0007  0.1310\n",
      "     42            1.0000        0.1593       0.1875            0.1875        1.8811  0.0007  0.1509\n",
      "     43            1.0000        \u001b[32m0.1081\u001b[0m       0.2083            0.2083        1.8997  0.0006  0.1896\n",
      "     44            1.0000        \u001b[32m0.0843\u001b[0m       0.1979            0.1979        1.9236  0.0006  0.1785\n",
      "     45            1.0000        0.0909       0.2188            0.2188        1.9450  0.0005  0.1105\n",
      "     46            1.0000        \u001b[32m0.0682\u001b[0m       0.1875            0.1875        1.9628  0.0005  0.0896\n",
      "     47            1.0000        0.1323       0.2188            0.2188        1.9751  0.0004  0.1003\n",
      "     48            1.0000        0.0879       0.2188            0.2188        1.9868  0.0004  0.1042\n",
      "     49            1.0000        0.0886       0.2083            0.2083        1.9953  0.0003  0.1515\n",
      "     50            1.0000        \u001b[32m0.0638\u001b[0m       0.2083            0.2083        2.0032  0.0003  0.1121\n",
      "Fine tuning model for subject 2 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        1.9350       0.3229            0.3229        1.7374  0.0004  0.1044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4500        1.6133       0.3229            0.3229        1.7868  0.0005  0.1105\n",
      "     33            0.6500        1.5874       0.3125            0.3125        1.7746  0.0005  0.0979\n",
      "     34            0.7500        1.0640       0.3021            0.3021        1.7977  0.0006  0.1172\n",
      "     35            \u001b[36m0.9000\u001b[0m        0.8418       0.2812            0.2812        1.8277  0.0006  0.1171\n",
      "     36            0.9000        \u001b[32m0.5528\u001b[0m       0.2812            0.2812        1.9022  0.0007  0.0968\n",
      "     37            0.9000        \u001b[32m0.4868\u001b[0m       0.2917            0.2917        1.9946  0.0007  0.1009\n",
      "     38            0.8500        \u001b[32m0.2550\u001b[0m       0.2812            0.2812        2.0533  0.0007  0.1065\n",
      "     39            0.9000        \u001b[32m0.2330\u001b[0m       0.3021            0.3021        2.0679  0.0007  0.1028\n",
      "     40            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2212\u001b[0m       0.2917            0.2917        2.0270  0.0007  0.0970\n",
      "     41            0.9500        \u001b[32m0.1700\u001b[0m       0.2812            0.2812        1.9798  0.0007  0.1035\n",
      "     42            0.9500        \u001b[32m0.1658\u001b[0m       0.2708            0.2708        1.9338  0.0007  0.1171\n",
      "     43            0.9500        \u001b[32m0.1592\u001b[0m       0.2604            0.2604        1.8979  0.0006  0.1076\n",
      "     44            0.9500        \u001b[32m0.1054\u001b[0m       0.2604            0.2604        1.8882  0.0006  0.0909\n",
      "     45            0.9500        0.1078       0.2604            0.2604        1.8881  0.0005  0.1021\n",
      "     46            \u001b[36m1.0000\u001b[0m        0.1123       0.2604            0.2604        1.8978  0.0005  0.1047\n",
      "     47            1.0000        0.1239       0.2500            0.2500        1.9184  0.0004  0.0885\n",
      "     48            1.0000        \u001b[32m0.0878\u001b[0m       0.2500            0.2500        1.9382  0.0004  0.0997\n",
      "     49            1.0000        0.0948       0.2500            0.2500        1.9570  0.0003  0.1013\n",
      "     50            1.0000        \u001b[32m0.0727\u001b[0m       0.2500            0.2500        1.9714  0.0003  0.1176\n",
      "Fine tuning model for subject 2 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.4431       0.3438            0.3438        1.6836  0.0004  0.0985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5000        1.0083       0.3229            0.3229        1.8180  0.0005  0.1189\n",
      "     33            0.5500        0.8863       0.2708            0.2708        1.9579  0.0005  0.1150\n",
      "     34            0.6500        0.6784       0.2188            0.2188        2.0354  0.0006  0.1036\n",
      "     35            0.7000        \u001b[32m0.3196\u001b[0m       0.1979            0.1979        2.0663  0.0006  0.1113\n",
      "     36            \u001b[36m0.8500\u001b[0m        0.3742       0.1979            0.1979        2.1011  0.0007  0.0987\n",
      "     37            \u001b[36m0.9000\u001b[0m        \u001b[32m0.2484\u001b[0m       0.2083            0.2083        2.1457  0.0007  0.1010\n",
      "     38            0.9000        \u001b[32m0.1905\u001b[0m       0.1979            0.1979        2.2373  0.0007  0.1035\n",
      "     39            0.9000        \u001b[32m0.1682\u001b[0m       0.2188            0.2188        2.3853  0.0007  0.1144\n",
      "     40            0.9000        0.1704       0.2292            0.2292        2.5747  0.0007  0.0997\n",
      "     41            0.9000        \u001b[32m0.0895\u001b[0m       0.2292            0.2292        2.7793  0.0007  0.1025\n",
      "     42            0.9000        \u001b[32m0.0855\u001b[0m       0.2188            0.2188        2.9739  0.0007  0.1024\n",
      "     43            0.9000        0.0891       0.2292            0.2292        3.1619  0.0006  0.1175\n",
      "     44            \u001b[36m0.9500\u001b[0m        \u001b[32m0.0729\u001b[0m       0.2396            0.2396        3.3114  0.0006  0.0961\n",
      "     45            0.9500        0.1247       0.2292            0.2292        3.4384  0.0005  0.1008\n",
      "     46            0.9500        \u001b[32m0.0554\u001b[0m       0.2396            0.2396        3.5220  0.0005  0.1181\n",
      "     47            0.9500        0.0694       0.2396            0.2396        3.5684  0.0004  0.1179\n",
      "     48            0.9500        0.0766       0.2396            0.2396        3.5779  0.0004  0.0954\n",
      "     49            0.9500        \u001b[32m0.0384\u001b[0m       0.2500            0.2500        3.5765  0.0003  0.1014\n",
      "     50            0.9500        \u001b[32m0.0369\u001b[0m       0.2396            0.2396        3.5714  0.0003  0.1212\n",
      "Fine tuning model for subject 2 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2000        2.1605       0.3438            0.3438        1.6594  0.0004  0.1114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.3500        1.9070       0.3438            0.3438        1.6568  0.0005  0.1133\n",
      "     33            0.4000        1.6887       0.2812            0.2812        1.7294  0.0005  0.0999\n",
      "     34            0.6000        1.2892       0.2396            0.2396        1.7982  0.0006  0.1037\n",
      "     35            0.8000        0.9780       0.2604            0.2604        1.8923  0.0006  0.1363\n",
      "     36            \u001b[36m0.8500\u001b[0m        0.7488       0.2292            0.2292        2.0092  0.0007  0.1227\n",
      "     37            0.8000        \u001b[32m0.5349\u001b[0m       0.1979            0.1979        2.1149  0.0007  0.1342\n",
      "     38            0.7500        \u001b[32m0.3935\u001b[0m       0.2188            0.2188        2.2169  0.0007  0.1238\n",
      "     39            \u001b[36m0.9000\u001b[0m        0.4726       0.2083            0.2083        2.3138  0.0007  0.1130\n",
      "     40            \u001b[36m0.9500\u001b[0m        0.4152       0.1979            0.1979        2.4064  0.0007  0.1172\n",
      "     41            0.9500        \u001b[32m0.1981\u001b[0m       0.2188            0.2188        2.4730  0.0007  0.1512\n",
      "     42            0.9500        0.2603       0.2396            0.2396        2.5272  0.0007  0.1341\n",
      "     43            0.9500        0.2573       0.2292            0.2292        2.5673  0.0006  0.1354\n",
      "     44            0.9500        \u001b[32m0.1952\u001b[0m       0.2292            0.2292        2.5960  0.0006  0.1363\n",
      "     45            0.9500        \u001b[32m0.1359\u001b[0m       0.2292            0.2292        2.6117  0.0005  0.1354\n",
      "     46            \u001b[36m1.0000\u001b[0m        0.1893       0.2292            0.2292        2.6050  0.0005  0.1198\n",
      "     47            1.0000        0.1660       0.2083            0.2083        2.5830  0.0004  0.1203\n",
      "     48            1.0000        \u001b[32m0.1007\u001b[0m       0.1979            0.1979        2.5534  0.0004  0.1265\n",
      "     49            1.0000        0.1107       0.2188            0.2188        2.5227  0.0003  0.1173\n",
      "     50            1.0000        0.1101       0.2292            0.2292        2.4930  0.0003  0.1036\n",
      "Fine tuning model for subject 2 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4500        2.1320       0.3333            0.3333        1.6961  0.0004  0.1783\n",
      "     32            0.5500        1.8080       0.3333            0.3333        1.7389  0.0005  0.1293\n",
      "     33            0.7000        1.3058       0.3021            0.3021        1.8046  0.0005  0.1499\n",
      "     34            0.7000        1.0708       0.2917            0.2917        1.8654  0.0006  0.1511\n",
      "     35            0.8000        0.8316       0.2812            0.2812        1.9031  0.0006  0.1041\n",
      "     36            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4706\u001b[0m       0.2812            0.2812        1.9375  0.0007  0.1125\n",
      "     37            0.8500        \u001b[32m0.3952\u001b[0m       0.2500            0.2500        1.9871  0.0007  0.1304\n",
      "     38            0.9000        \u001b[32m0.3558\u001b[0m       0.2604            0.2604        2.0550  0.0007  0.0996\n",
      "     39            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2595\u001b[0m       0.2604            0.2604        2.1059  0.0007  0.1010\n",
      "     40            0.9000        \u001b[32m0.1786\u001b[0m       0.2708            0.2708        2.1494  0.0007  0.1165\n",
      "     41            0.8500        0.2407       0.2604            0.2604        2.1856  0.0007  0.1160\n",
      "     42            0.8500        0.2218       0.2604            0.2604        2.2294  0.0007  0.0981\n",
      "     43            0.8500        \u001b[32m0.1258\u001b[0m       0.2708            0.2708        2.2706  0.0006  0.1165\n",
      "     44            0.8500        \u001b[32m0.1101\u001b[0m       0.2604            0.2604        2.2953  0.0006  0.1092\n",
      "     45            0.9000        \u001b[32m0.0987\u001b[0m       0.2604            0.2604        2.3072  0.0005  0.1104\n",
      "     46            0.9000        \u001b[32m0.0860\u001b[0m       0.2708            0.2708        2.3013  0.0005  0.1019\n",
      "     47            0.9000        \u001b[32m0.0696\u001b[0m       0.2708            0.2708        2.2863  0.0004  0.1012\n",
      "     48            0.9000        0.0871       0.2604            0.2604        2.2642  0.0004  0.1134\n",
      "     49            0.9500        \u001b[32m0.0640\u001b[0m       0.2812            0.2812        2.2392  0.0003  0.1197\n",
      "     50            \u001b[36m1.0000\u001b[0m        0.0943       0.2708            0.2708        2.2133  0.0003  0.0956\n",
      "Fine tuning model for subject 2 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        2.0968       0.3229            0.3229        1.6911  0.0004  0.1144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5500        1.4469       0.2812            0.2812        1.7183  0.0005  0.1159\n",
      "     33            0.7000        1.1992       0.2604            0.2604        1.8010  0.0005  0.1157\n",
      "     34            0.7000        1.1364       0.2604            0.2604        1.9387  0.0006  0.1042\n",
      "     35            0.6500        \u001b[32m0.5870\u001b[0m       0.2500            0.2500        2.0317  0.0006  0.1345\n",
      "     36            0.6000        \u001b[32m0.5767\u001b[0m       0.3021            0.3021        2.0448  0.0007  0.1094\n",
      "     37            0.6000        \u001b[32m0.4935\u001b[0m       0.2917            0.2917        2.0197  0.0007  0.0995\n",
      "     38            0.7500        \u001b[32m0.3495\u001b[0m       0.3021            0.3021        1.9753  0.0007  0.1010\n",
      "     39            0.7500        \u001b[32m0.2645\u001b[0m       0.3021            0.3021        1.9560  0.0007  0.1208\n",
      "     40            \u001b[36m0.8500\u001b[0m        0.2797       0.3021            0.3021        1.9499  0.0007  0.1101\n",
      "     41            \u001b[36m0.9000\u001b[0m        \u001b[32m0.2033\u001b[0m       0.2917            0.2917        1.9543  0.0007  0.1127\n",
      "     42            0.9000        0.2444       0.2917            0.2917        1.9575  0.0007  0.1003\n",
      "     43            0.9000        \u001b[32m0.1720\u001b[0m       0.3021            0.3021        1.9530  0.0006  0.1114\n",
      "     44            0.9000        \u001b[32m0.1080\u001b[0m       0.2917            0.2917        1.9456  0.0006  0.0987\n",
      "     45            \u001b[36m0.9500\u001b[0m        0.1396       0.2917            0.2917        1.9397  0.0005  0.0999\n",
      "     46            0.9500        0.1148       0.3333            0.3333        1.9369  0.0005  0.1013\n",
      "     47            0.9500        \u001b[32m0.0931\u001b[0m       0.3333            0.3333        1.9385  0.0004  0.1151\n",
      "     48            0.9500        \u001b[32m0.0921\u001b[0m       0.3125            0.3125        1.9453  0.0004  0.1014\n",
      "     49            \u001b[36m1.0000\u001b[0m        0.1054       0.3021            0.3021        1.9555  0.0003  0.0975\n",
      "     50            1.0000        \u001b[32m0.0773\u001b[0m       0.2812            0.2812        1.9700  0.0003  0.1027\n",
      "Fine tuning model for subject 2 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.8713       0.3125            0.3125        1.7079  0.0004  0.1014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6500        1.5370       0.3021            0.3021        1.7625  0.0005  0.1031\n",
      "     33            0.5000        1.4238       0.2604            0.2604        1.8379  0.0005  0.0990\n",
      "     34            0.5000        0.8155       0.2396            0.2396        1.9470  0.0006  0.1040\n",
      "     35            0.4500        \u001b[32m0.5367\u001b[0m       0.2396            0.2396        2.0428  0.0006  0.1173\n",
      "     36            0.6000        \u001b[32m0.3208\u001b[0m       0.2083            0.2083        2.0934  0.0007  0.1069\n",
      "     37            0.7500        \u001b[32m0.3193\u001b[0m       0.2292            0.2292        2.1085  0.0007  0.0967\n",
      "     38            0.8000        \u001b[32m0.2348\u001b[0m       0.2708            0.2708        2.1440  0.0007  0.1188\n",
      "     39            \u001b[36m0.8500\u001b[0m        \u001b[32m0.1763\u001b[0m       0.2604            0.2604        2.2254  0.0007  0.1040\n",
      "     40            0.8500        \u001b[32m0.1568\u001b[0m       0.2812            0.2812        2.3372  0.0007  0.0943\n",
      "     41            0.8500        \u001b[32m0.1418\u001b[0m       0.2604            0.2604        2.4501  0.0007  0.1008\n",
      "     42            0.8500        0.1956       0.2604            0.2604        2.5578  0.0007  0.1049\n",
      "     43            0.8000        \u001b[32m0.0769\u001b[0m       0.2500            0.2500        2.6441  0.0006  0.0990\n",
      "     44            0.8000        0.1479       0.2500            0.2500        2.7081  0.0006  0.0964\n",
      "     45            0.8500        0.1192       0.2500            0.2500        2.7477  0.0005  0.1169\n",
      "     46            \u001b[36m0.9500\u001b[0m        0.1140       0.2500            0.2500        2.7644  0.0005  0.1159\n",
      "     47            0.9500        0.0893       0.2500            0.2500        2.7675  0.0004  0.1003\n",
      "     48            \u001b[36m1.0000\u001b[0m        0.0791       0.2396            0.2396        2.7573  0.0004  0.1033\n",
      "     49            1.0000        \u001b[32m0.0751\u001b[0m       0.2500            0.2500        2.7413  0.0003  0.0987\n",
      "     50            1.0000        \u001b[32m0.0594\u001b[0m       0.2500            0.2500        2.7217  0.0003  0.1199\n",
      "Fine tuning model for subject 2 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.6583       0.3333            0.3333        1.7001  0.0004  0.0987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5000        1.5302       0.3021            0.3021        1.7430  0.0005  0.1188\n",
      "     33            0.7000        1.0252       0.2917            0.2917        1.7509  0.0005  0.0983\n",
      "     34            0.6500        0.9927       0.3229            0.3229        1.7440  0.0006  0.1037\n",
      "     35            0.8000        \u001b[32m0.5299\u001b[0m       0.3021            0.3021        1.7324  0.0006  0.1121\n",
      "     36            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5037\u001b[0m       0.3229            0.3229        1.7542  0.0007  0.1021\n",
      "     37            \u001b[36m1.0000\u001b[0m        0.5936       0.3646            0.3646        1.7905  0.0007  0.1317\n",
      "     38            0.9000        \u001b[32m0.3688\u001b[0m       0.3333            0.3333        1.8251  0.0007  0.1031\n",
      "     39            0.9500        \u001b[32m0.3666\u001b[0m       0.3229            0.3229        1.8619  0.0007  0.1153\n",
      "     40            0.9500        \u001b[32m0.2350\u001b[0m       0.3229            0.3229        1.8968  0.0007  0.1167\n",
      "     41            1.0000        \u001b[32m0.2179\u001b[0m       0.3542            0.3542        1.9163  0.0007  0.0960\n",
      "     42            1.0000        \u001b[32m0.1113\u001b[0m       0.3438            0.3438        1.9257  0.0007  0.1025\n",
      "     43            1.0000        0.1709       0.3333            0.3333        1.9262  0.0006  0.1174\n",
      "     44            1.0000        \u001b[32m0.1091\u001b[0m       0.3229            0.3229        1.9189  0.0006  0.1000\n",
      "     45            1.0000        0.1502       0.3021            0.3021        1.9032  0.0005  0.0982\n",
      "     46            1.0000        \u001b[32m0.0736\u001b[0m       0.2917            0.2917        1.8874  0.0005  0.1207\n",
      "     47            1.0000        0.1002       0.2812            0.2812        1.8718  0.0004  0.1083\n",
      "     48            1.0000        0.0858       0.3021            0.3021        1.8573  0.0004  0.1045\n",
      "     49            1.0000        \u001b[32m0.0560\u001b[0m       0.3125            0.3125        1.8449  0.0003  0.0947\n",
      "     50            1.0000        0.0689       0.3229            0.3229        1.8349  0.0003  0.1192\n",
      "Fine tuning model for subject 2 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2400        2.5319       0.3333            0.3333        1.6750  0.0004  0.1006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.2400        2.2023       0.3438            0.3438        1.6541  0.0005  0.1177\n",
      "     33            0.4000        1.8650       0.3333            0.3333        1.6436  0.0005  0.1145\n",
      "     34            0.5600        1.3872       0.2812            0.2812        1.6645  0.0006  0.1053\n",
      "     35            0.7200        1.0513       0.2917            0.2917        1.7209  0.0006  0.1201\n",
      "     36            0.7200        0.8428       0.3438            0.3438        1.8044  0.0007  0.1235\n",
      "     37            0.7200        0.8263       0.3438            0.3438        1.9106  0.0007  0.1012\n",
      "     38            0.7600        0.6551       0.3438            0.3438        2.0145  0.0007  0.1007\n",
      "     39            \u001b[36m0.8400\u001b[0m        \u001b[32m0.4141\u001b[0m       0.3542            0.3542        2.0998  0.0007  0.1135\n",
      "     40            \u001b[36m0.8800\u001b[0m        \u001b[32m0.3923\u001b[0m       0.3229            0.3229        2.1573  0.0007  0.1259\n",
      "     41            0.8400        0.5187       0.3125            0.3125        2.1932  0.0007  0.1255\n",
      "     42            0.8400        0.4120       0.3125            0.3125        2.2016  0.0007  0.1167\n",
      "     43            0.8800        \u001b[32m0.2657\u001b[0m       0.3125            0.3125        2.1906  0.0006  0.1066\n",
      "     44            0.8800        \u001b[32m0.2315\u001b[0m       0.3125            0.3125        2.1629  0.0006  0.1192\n",
      "     45            0.8800        0.2719       0.2812            0.2812        2.1280  0.0005  0.1198\n",
      "     46            0.8800        \u001b[32m0.1129\u001b[0m       0.3021            0.3021        2.0927  0.0005  0.1248\n",
      "     47            \u001b[36m0.9600\u001b[0m        0.1767       0.2917            0.2917        2.0613  0.0004  0.1144\n",
      "     48            0.9600        0.1183       0.3021            0.3021        2.0342  0.0004  0.0993\n",
      "     49            0.9600        0.1178       0.3125            0.3125        2.0120  0.0003  0.1016\n",
      "     50            0.9600        0.1388       0.3125            0.3125        1.9945  0.0003  0.1037\n",
      "Fine tuning model for subject 2 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.1600        2.1818       0.3229            0.3229        1.6607  0.0004  0.1155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.3600        1.8441       0.3542            0.3542        1.6797  0.0005  0.1255\n",
      "     33            0.4800        1.8600       0.3021            0.3021        1.7777  0.0005  0.1322\n",
      "     34            0.6000        1.0172       0.3125            0.3125        1.8876  0.0006  0.1174\n",
      "     35            0.7600        0.8075       0.3333            0.3333        1.9491  0.0006  0.1658\n",
      "     36            0.8000        0.7656       0.2396            0.2396        2.0078  0.0007  0.1500\n",
      "     37            \u001b[36m0.9200\u001b[0m        \u001b[32m0.5060\u001b[0m       0.2396            0.2396        2.1157  0.0007  0.1375\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3469\u001b[0m       0.2500            0.2500        2.2737  0.0007  0.1354\n",
      "     39            0.9600        \u001b[32m0.2488\u001b[0m       0.2604            0.2604        2.4317  0.0007  0.1355\n",
      "     40            0.8800        0.2657       0.2500            0.2500        2.5801  0.0007  0.1509\n",
      "     41            0.8800        \u001b[32m0.2009\u001b[0m       0.2604            0.2604        2.7092  0.0007  0.1355\n",
      "     42            0.8400        0.2026       0.2604            0.2604        2.8113  0.0007  0.1354\n",
      "     43            0.8800        0.2171       0.2604            0.2604        2.8658  0.0006  0.1355\n",
      "     44            0.9200        \u001b[32m0.1748\u001b[0m       0.2708            0.2708        2.8762  0.0006  0.1355\n",
      "     45            0.9200        \u001b[32m0.1339\u001b[0m       0.2708            0.2708        2.8620  0.0005  0.1384\n",
      "     46            0.9200        \u001b[32m0.1272\u001b[0m       0.2812            0.2812        2.8192  0.0005  0.1465\n",
      "     47            0.9200        0.1826       0.2812            0.2812        2.7747  0.0004  0.1276\n",
      "     48            0.9600        \u001b[32m0.1238\u001b[0m       0.2708            0.2708        2.7128  0.0004  0.1308\n",
      "     49            0.9600        0.1525       0.2708            0.2708        2.6491  0.0003  0.1391\n",
      "     50            1.0000        \u001b[32m0.0936\u001b[0m       0.2500            0.2500        2.5879  0.0003  0.1125\n",
      "Fine tuning model for subject 2 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3600        1.6356       0.3333            0.3333        1.7032  0.0004  0.1586\n",
      "     32            0.4400        1.3163       0.3125            0.3125        1.7387  0.0005  0.1309\n",
      "     33            0.6000        1.0873       0.2500            0.2500        1.7889  0.0005  0.1517\n",
      "     34            0.6400        0.9838       0.2188            0.2188        1.8242  0.0006  0.1662\n",
      "     35            0.7200        0.6467       0.2292            0.2292        1.8778  0.0006  0.1172\n",
      "     36            0.7600        \u001b[32m0.5741\u001b[0m       0.2396            0.2396        1.9244  0.0007  0.1187\n",
      "     37            \u001b[36m0.8800\u001b[0m        \u001b[32m0.4640\u001b[0m       0.2604            0.2604        1.9616  0.0007  0.1198\n",
      "     38            \u001b[36m0.9200\u001b[0m        \u001b[32m0.3580\u001b[0m       0.2708            0.2708        1.9890  0.0007  0.1261\n",
      "     39            0.9200        \u001b[32m0.2836\u001b[0m       0.2396            0.2396        2.0110  0.0007  0.1035\n",
      "     40            0.9200        \u001b[32m0.2731\u001b[0m       0.2500            0.2500        2.0507  0.0007  0.0982\n",
      "     41            0.8400        \u001b[32m0.2034\u001b[0m       0.2708            0.2708        2.1165  0.0007  0.1013\n",
      "     42            0.7600        0.2139       0.2917            0.2917        2.2006  0.0007  0.1166\n",
      "     43            0.7600        0.2161       0.2917            0.2917        2.2745  0.0006  0.1142\n",
      "     44            0.6800        \u001b[32m0.1377\u001b[0m       0.3021            0.3021        2.3173  0.0006  0.1151\n",
      "     45            0.6800        0.1545       0.3125            0.3125        2.3296  0.0005  0.0997\n",
      "     46            0.7200        0.1690       0.3229            0.3229        2.3202  0.0005  0.1455\n",
      "     47            0.8400        0.1436       0.3125            0.3125        2.2978  0.0004  0.1047\n",
      "     48            0.8800        0.1412       0.3229            0.3229        2.2745  0.0004  0.1164\n",
      "     49            \u001b[36m0.9600\u001b[0m        \u001b[32m0.1270\u001b[0m       0.3125            0.3125        2.2548  0.0003  0.1220\n",
      "     50            0.9600        \u001b[32m0.0889\u001b[0m       0.3125            0.3125        2.2417  0.0003  0.1266\n",
      "Fine tuning model for subject 2 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        2.1252       0.3333            0.3333        1.6994  0.0004  0.1131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4400        2.2419       0.3021            0.3021        1.7189  0.0005  0.1169\n",
      "     33            0.4400        1.6730       0.2917            0.2917        1.7767  0.0005  0.1136\n",
      "     34            0.5200        1.3765       0.3438            0.3438        1.7810  0.0006  0.0997\n",
      "     35            0.5200        1.0177       0.3229            0.3229        1.8309  0.0006  0.1153\n",
      "     36            0.5200        0.6372       0.3125            0.3125        1.9143  0.0007  0.1283\n",
      "     37            0.5600        \u001b[32m0.6022\u001b[0m       0.2917            0.2917        1.9870  0.0007  0.0978\n",
      "     38            0.6000        \u001b[32m0.5152\u001b[0m       0.2812            0.2812        2.0257  0.0007  0.0999\n",
      "     39            0.6000        \u001b[32m0.4458\u001b[0m       0.2917            0.2917        2.0291  0.0007  0.1025\n",
      "     40            0.6000        \u001b[32m0.3460\u001b[0m       0.2917            0.2917        2.0139  0.0007  0.1127\n",
      "     41            0.6400        \u001b[32m0.3191\u001b[0m       0.2917            0.2917        1.9717  0.0007  0.1264\n",
      "     42            0.6400        \u001b[32m0.3064\u001b[0m       0.3333            0.3333        1.9214  0.0007  0.1007\n",
      "     43            \u001b[36m0.8400\u001b[0m        \u001b[32m0.2150\u001b[0m       0.3229            0.3229        1.8985  0.0006  0.1012\n",
      "     44            \u001b[36m0.8800\u001b[0m        \u001b[32m0.1959\u001b[0m       0.3229            0.3229        1.8915  0.0006  0.1195\n",
      "     45            \u001b[36m0.9200\u001b[0m        0.2146       0.3229            0.3229        1.9005  0.0005  0.1031\n",
      "     46            \u001b[36m0.9600\u001b[0m        \u001b[32m0.1456\u001b[0m       0.3229            0.3229        1.9177  0.0005  0.1133\n",
      "     47            \u001b[36m1.0000\u001b[0m        0.2022       0.3125            0.3125        1.9365  0.0004  0.0986\n",
      "     48            1.0000        0.2089       0.2812            0.2812        1.9599  0.0004  0.0986\n",
      "     49            1.0000        \u001b[32m0.1319\u001b[0m       0.2917            0.2917        1.9853  0.0003  0.1170\n",
      "     50            1.0000        \u001b[32m0.1216\u001b[0m       0.3229            0.3229        2.0113  0.0003  0.1051\n",
      "Fine tuning model for subject 2 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.6949       0.3229            0.3229        1.6714  0.0004  0.1050\n",
      "     32            0.5200        1.4778       0.3125            0.3125        1.7061  0.0005  0.1089\n",
      "     33            0.6400        0.9908       0.2812            0.2812        1.8282  0.0005  0.1197\n",
      "     34            0.7200        0.7501       0.2917            0.2917        1.9344  0.0006  0.1266\n",
      "     35            0.8000        0.6847       0.2708            0.2708        2.0039  0.0006  0.1022\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5462\u001b[0m       0.2708            0.2708        2.0329  0.0007  0.1015\n",
      "     37            1.0000        \u001b[32m0.4116\u001b[0m       0.2917            0.2917        2.0478  0.0007  0.1166\n",
      "     38            1.0000        \u001b[32m0.3509\u001b[0m       0.2812            0.2812        2.0575  0.0007  0.1311\n",
      "     39            0.9600        \u001b[32m0.3485\u001b[0m       0.2708            0.2708        2.0661  0.0007  0.1345\n",
      "     40            1.0000        0.3689       0.2708            0.2708        2.0706  0.0007  0.1149\n",
      "     41            1.0000        \u001b[32m0.2178\u001b[0m       0.2812            0.2812        2.0839  0.0007  0.1102\n",
      "     42            1.0000        \u001b[32m0.1762\u001b[0m       0.2708            0.2708        2.1097  0.0007  0.0984\n",
      "     43            1.0000        \u001b[32m0.1601\u001b[0m       0.2604            0.2604        2.1515  0.0006  0.1168\n",
      "     44            1.0000        \u001b[32m0.1355\u001b[0m       0.2500            0.2500        2.1955  0.0006  0.1178\n",
      "     45            1.0000        \u001b[32m0.1247\u001b[0m       0.2396            0.2396        2.2336  0.0005  0.1212\n",
      "     46            1.0000        0.1684       0.2396            0.2396        2.2601  0.0005  0.1212\n",
      "     47            1.0000        0.1399       0.2396            0.2396        2.2733  0.0004  0.1143\n",
      "     48            1.0000        \u001b[32m0.1098\u001b[0m       0.2396            0.2396        2.2778  0.0004  0.1000\n",
      "     49            1.0000        0.1546       0.2604            0.2604        2.2766  0.0003  0.1011\n",
      "     50            1.0000        \u001b[32m0.0844\u001b[0m       0.2708            0.2708        2.2785  0.0003  0.1041\n",
      "Fine tuning model for subject 2 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2400        1.9613       0.3438            0.3438        1.6595  0.0004  0.1196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4000        1.5692       0.3125            0.3125        1.6693  0.0005  0.1294\n",
      "     33            0.5600        1.5049       0.2812            0.2812        1.7212  0.0005  0.1000\n",
      "     34            0.6000        1.0532       0.2604            0.2604        1.7616  0.0006  0.1033\n",
      "     35            0.7600        1.0871       0.2604            0.2604        1.8049  0.0006  0.1130\n",
      "     36            \u001b[36m0.8800\u001b[0m        0.7829       0.2604            0.2604        1.8303  0.0007  0.1001\n",
      "     37            0.8800        \u001b[32m0.5605\u001b[0m       0.2604            0.2604        1.8279  0.0007  0.0992\n",
      "     38            0.8800        0.5736       0.3021            0.3021        1.8092  0.0007  0.1029\n",
      "     39            0.8000        \u001b[32m0.3753\u001b[0m       0.3125            0.3125        1.8078  0.0007  0.1035\n",
      "     40            0.8400        \u001b[32m0.3563\u001b[0m       0.3229            0.3229        1.8137  0.0007  0.1303\n",
      "     41            0.8000        \u001b[32m0.2126\u001b[0m       0.3229            0.3229        1.8307  0.0007  0.0987\n",
      "     42            0.8000        \u001b[32m0.1720\u001b[0m       0.3229            0.3229        1.8587  0.0007  0.0999\n",
      "     43            0.8000        0.1821       0.3021            0.3021        1.8960  0.0006  0.1001\n",
      "     44            0.8000        0.2312       0.3021            0.3021        1.9327  0.0006  0.1040\n",
      "     45            0.8400        0.1833       0.2604            0.2604        1.9567  0.0005  0.1290\n",
      "     46            0.8800        0.2025       0.2812            0.2812        1.9654  0.0005  0.1274\n",
      "     47            0.8800        \u001b[32m0.1035\u001b[0m       0.2917            0.2917        1.9723  0.0004  0.1010\n",
      "     48            0.8800        0.1175       0.2812            0.2812        1.9781  0.0004  0.1144\n",
      "     49            0.8800        0.1204       0.2812            0.2812        1.9808  0.0003  0.1188\n",
      "     50            \u001b[36m0.9200\u001b[0m        0.1201       0.2917            0.2917        1.9842  0.0003  0.1520\n",
      "Fine tuning model for subject 2 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2800        1.9255       0.3333            0.3333        1.6991  0.0004  0.1053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.3600        2.1692       0.2604            0.2604        1.7716  0.0005  0.1209\n",
      "     33            0.3600        1.5300       0.2292            0.2292        1.9255  0.0005  0.1146\n",
      "     34            0.4400        1.1309       0.1771            0.1771        2.0833  0.0006  0.1052\n",
      "     35            0.5200        0.8891       0.2083            0.2083        2.1971  0.0006  0.1248\n",
      "     36            0.5200        0.6536       0.2083            0.2083        2.2786  0.0007  0.1009\n",
      "     37            0.5600        \u001b[32m0.5128\u001b[0m       0.1979            0.1979        2.2916  0.0007  0.1007\n",
      "     38            0.7200        \u001b[32m0.3118\u001b[0m       0.1667            0.1667        2.2381  0.0007  0.1174\n",
      "     39            0.8000        0.3805       0.1667            0.1667        2.1440  0.0007  0.1209\n",
      "     40            \u001b[36m0.9600\u001b[0m        \u001b[32m0.3080\u001b[0m       0.1562            0.1562        2.0581  0.0007  0.1335\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2093\u001b[0m       0.2188            0.2188        2.0107  0.0007  0.1178\n",
      "     42            1.0000        0.2185       0.2083            0.2083        2.0111  0.0007  0.1098\n",
      "     43            1.0000        \u001b[32m0.1981\u001b[0m       0.2396            0.2396        2.0491  0.0006  0.1167\n",
      "     44            1.0000        \u001b[32m0.1285\u001b[0m       0.2396            0.2396        2.1028  0.0006  0.1200\n",
      "     45            1.0000        \u001b[32m0.1239\u001b[0m       0.2188            0.2188        2.1546  0.0005  0.1122\n",
      "     46            1.0000        0.1797       0.2188            0.2188        2.1936  0.0005  0.0999\n",
      "     47            1.0000        \u001b[32m0.1080\u001b[0m       0.2292            0.2292        2.2179  0.0004  0.1236\n",
      "     48            1.0000        0.1176       0.2292            0.2292        2.2342  0.0004  0.1327\n",
      "     49            1.0000        0.1188       0.2292            0.2292        2.2425  0.0003  0.1244\n",
      "     50            1.0000        \u001b[32m0.0656\u001b[0m       0.2396            0.2396        2.2448  0.0003  0.1159\n",
      "Fine tuning model for subject 2 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.9237       0.3229            0.3229        1.7018  0.0004  0.1248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4400        1.6644       0.2917            0.2917        1.7625  0.0005  0.1231\n",
      "     33            0.5200        1.2637       0.2917            0.2917        1.8034  0.0005  0.1647\n",
      "     34            0.6000        1.1635       0.3021            0.3021        1.8261  0.0006  0.1416\n",
      "     35            0.6400        0.6999       0.2917            0.2917        1.8105  0.0006  0.1206\n",
      "     36            \u001b[36m0.8400\u001b[0m        0.7137       0.3542            0.3542        1.8143  0.0007  0.1344\n",
      "     37            0.8400        \u001b[32m0.4513\u001b[0m       0.3646            0.3646        1.8641  0.0007  0.1649\n",
      "     38            0.8000        \u001b[32m0.3571\u001b[0m       0.3542            0.3542        1.9422  0.0007  0.1342\n",
      "     39            0.8000        0.4111       0.3438            0.3438        2.0381  0.0007  0.1211\n",
      "     40            0.8400        \u001b[32m0.2730\u001b[0m       0.2917            0.2917        2.1112  0.0007  0.1322\n",
      "     41            0.8400        0.2961       0.2812            0.2812        2.1867  0.0007  0.1199\n",
      "     42            0.8400        \u001b[32m0.2603\u001b[0m       0.2812            0.2812        2.2425  0.0007  0.1335\n",
      "     43            \u001b[36m0.8800\u001b[0m        \u001b[32m0.2590\u001b[0m       0.2917            0.2917        2.2881  0.0006  0.1511\n",
      "     44            \u001b[36m0.9200\u001b[0m        \u001b[32m0.1926\u001b[0m       0.2917            0.2917        2.3035  0.0006  0.1218\n",
      "     45            0.9200        \u001b[32m0.1609\u001b[0m       0.3021            0.3021        2.3144  0.0005  0.1665\n",
      "     46            \u001b[36m0.9600\u001b[0m        0.1684       0.3125            0.3125        2.3146  0.0005  0.1326\n",
      "     47            0.9600        \u001b[32m0.1567\u001b[0m       0.3125            0.3125        2.2977  0.0004  0.1208\n",
      "     48            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1386\u001b[0m       0.3229            0.3229        2.2759  0.0004  0.1184\n",
      "     49            1.0000        0.1562       0.3229            0.3229        2.2524  0.0003  0.1047\n",
      "     50            1.0000        \u001b[32m0.1322\u001b[0m       0.3333            0.3333        2.2242  0.0003  0.1247\n",
      "Fine tuning model for subject 2 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3200        2.0043       0.3125            0.3125        1.7354  0.0004  0.1156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5200        1.8763       0.3229            0.3229        1.8591  0.0005  0.1323\n",
      "     33            0.6000        1.4028       0.2500            0.2500        1.9631  0.0005  0.1150\n",
      "     34            0.6400        1.0487       0.2396            0.2396        2.0380  0.0006  0.1193\n",
      "     35            \u001b[36m0.8400\u001b[0m        0.8377       0.2500            0.2500        2.1016  0.0006  0.1109\n",
      "     36            \u001b[36m0.9200\u001b[0m        \u001b[32m0.5852\u001b[0m       0.2396            0.2396        2.1348  0.0007  0.1255\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4361\u001b[0m       0.2604            0.2604        2.2216  0.0007  0.1021\n",
      "     38            0.9600        \u001b[32m0.4015\u001b[0m       0.2292            0.2292        2.3456  0.0007  0.1027\n",
      "     39            0.9600        \u001b[32m0.2376\u001b[0m       0.2292            0.2292        2.4906  0.0007  0.1018\n",
      "     40            0.9200        0.2883       0.2083            0.2083        2.6157  0.0007  0.1054\n",
      "     41            0.9200        \u001b[32m0.1472\u001b[0m       0.2292            0.2292        2.7325  0.0007  0.1151\n",
      "     42            0.9200        0.2098       0.2500            0.2500        2.8230  0.0007  0.0986\n",
      "     43            0.9200        0.1564       0.2708            0.2708        2.8849  0.0006  0.1127\n",
      "     44            0.9200        \u001b[32m0.1231\u001b[0m       0.2708            0.2708        2.9206  0.0006  0.1175\n",
      "     45            0.9600        \u001b[32m0.1201\u001b[0m       0.2708            0.2708        2.9259  0.0005  0.1206\n",
      "     46            0.9600        0.1418       0.2708            0.2708        2.9073  0.0005  0.1219\n",
      "     47            0.9600        0.1220       0.2708            0.2708        2.8732  0.0004  0.1229\n",
      "     48            0.9600        0.1305       0.2708            0.2708        2.8309  0.0004  0.1042\n",
      "     49            0.9600        \u001b[32m0.1014\u001b[0m       0.2708            0.2708        2.7809  0.0003  0.0994\n",
      "     50            0.9600        \u001b[32m0.0742\u001b[0m       0.2812            0.2812        2.7307  0.0003  0.1041\n",
      "Fine tuning model for subject 2 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4800        1.4879       0.3229            0.3229        1.6777  0.0004  0.1149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.3686       0.3125            0.3125        1.6594  0.0005  0.1338\n",
      "     33            0.7200        1.2134       0.3125            0.3125        1.6497  0.0005  0.1034\n",
      "     34            0.7600        0.8707       0.3021            0.3021        1.6902  0.0006  0.1039\n",
      "     35            0.8000        0.7544       0.2292            0.2292        1.7956  0.0006  0.1043\n",
      "     36            \u001b[36m0.8400\u001b[0m        \u001b[32m0.5592\u001b[0m       0.2604            0.2604        1.9415  0.0007  0.0993\n",
      "     37            0.8400        \u001b[32m0.4515\u001b[0m       0.2604            0.2604        2.0637  0.0007  0.0975\n",
      "     38            \u001b[36m0.9200\u001b[0m        \u001b[32m0.3472\u001b[0m       0.2396            0.2396        2.1223  0.0007  0.1016\n",
      "     39            0.9200        \u001b[32m0.2180\u001b[0m       0.2500            0.2500        2.1479  0.0007  0.1040\n",
      "     40            \u001b[36m1.0000\u001b[0m        0.2911       0.2500            0.2500        2.1629  0.0007  0.1124\n",
      "     41            1.0000        \u001b[32m0.1571\u001b[0m       0.2708            0.2708        2.1804  0.0007  0.1024\n",
      "     42            1.0000        \u001b[32m0.1342\u001b[0m       0.2500            0.2500        2.2037  0.0007  0.0993\n",
      "     43            1.0000        \u001b[32m0.1165\u001b[0m       0.2396            0.2396        2.2261  0.0006  0.0994\n",
      "     44            1.0000        0.1228       0.2500            0.2500        2.2557  0.0006  0.1029\n",
      "     45            1.0000        0.1346       0.2500            0.2500        2.2855  0.0005  0.1154\n",
      "     46            1.0000        0.1421       0.2396            0.2396        2.3030  0.0005  0.1288\n",
      "     47            1.0000        \u001b[32m0.1126\u001b[0m       0.2396            0.2396        2.3159  0.0004  0.0971\n",
      "     48            1.0000        \u001b[32m0.0845\u001b[0m       0.2396            0.2396        2.3275  0.0004  0.1001\n",
      "     49            1.0000        \u001b[32m0.0701\u001b[0m       0.2396            0.2396        2.3339  0.0003  0.1015\n",
      "     50            1.0000        0.0897       0.2500            0.2500        2.3359  0.0003  0.1164\n",
      "Fine tuning model for subject 2 with 30 = 30 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3667        1.9898       0.3542            0.3542        1.6562  0.0004  0.1150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4667        1.7474       0.2812            0.2812        1.6769  0.0005  0.1190\n",
      "     33            0.5000        1.2266       0.2708            0.2708        1.7724  0.0005  0.1181\n",
      "     34            0.4667        1.1779       0.2604            0.2604        1.9144  0.0006  0.1193\n",
      "     35            0.5333        0.9753       0.2812            0.2812        2.0903  0.0006  0.1262\n",
      "     36            0.6000        0.8663       0.2812            0.2812        2.2863  0.0007  0.1203\n",
      "     37            0.5667        0.7278       0.2708            0.2708        2.4600  0.0007  0.1291\n",
      "     38            0.4667        \u001b[32m0.5506\u001b[0m       0.2604            0.2604        2.5911  0.0007  0.1211\n",
      "     39            0.4333        \u001b[32m0.5086\u001b[0m       0.2604            0.2604        2.6658  0.0007  0.1182\n",
      "     40            0.4333        \u001b[32m0.4349\u001b[0m       0.2604            0.2604        2.7044  0.0007  0.1093\n",
      "     41            0.4333        \u001b[32m0.3618\u001b[0m       0.2604            0.2604        2.6873  0.0007  0.1192\n",
      "     42            0.4667        \u001b[32m0.2489\u001b[0m       0.2604            0.2604        2.6429  0.0007  0.1172\n",
      "     43            0.5000        0.2748       0.2604            0.2604        2.5781  0.0006  0.1207\n",
      "     44            0.5333        0.3432       0.2708            0.2708        2.4961  0.0006  0.1310\n",
      "     45            0.6000        \u001b[32m0.2214\u001b[0m       0.2604            0.2604        2.4056  0.0005  0.1230\n",
      "     46            0.7000        \u001b[32m0.2192\u001b[0m       0.2500            0.2500        2.3260  0.0005  0.1155\n",
      "     47            0.7333        0.2663       0.2500            0.2500        2.2587  0.0004  0.1089\n",
      "     48            \u001b[36m0.8333\u001b[0m        \u001b[32m0.1589\u001b[0m       0.2604            0.2604        2.2044  0.0004  0.1163\n",
      "     49            \u001b[36m0.8667\u001b[0m        0.1977       0.2708            0.2708        2.1713  0.0003  0.1186\n",
      "     50            \u001b[36m0.9333\u001b[0m        0.1920       0.2604            0.2604        2.1526  0.0003  0.1142\n",
      "Fine tuning model for subject 2 with 30 = 30 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2667        1.9945       0.3542            0.3542        1.6758  0.0004  0.1296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.2333        1.9745       0.3438            0.3438        1.6810  0.0005  0.1347\n",
      "     33            0.2667        1.4928       0.3229            0.3229        1.7451  0.0005  0.1183\n",
      "     34            0.4000        1.4634       0.3021            0.3021        1.8346  0.0006  0.1187\n",
      "     35            0.4667        1.1359       0.2812            0.2812        1.9108  0.0006  0.1207\n",
      "     36            0.5333        0.8533       0.2708            0.2708        1.9562  0.0007  0.1290\n",
      "     37            0.6000        0.7227       0.2708            0.2708        1.9903  0.0007  0.1236\n",
      "     38            0.6333        0.7860       0.2708            0.2708        2.0082  0.0007  0.1211\n",
      "     39            0.7333        0.6566       0.2396            0.2396        2.0403  0.0007  0.1233\n",
      "     40            0.7000        \u001b[32m0.4597\u001b[0m       0.2500            0.2500        2.0733  0.0007  0.1097\n",
      "     41            0.7000        \u001b[32m0.4596\u001b[0m       0.2292            0.2292        2.0944  0.0007  0.1343\n",
      "     42            0.7333        \u001b[32m0.4217\u001b[0m       0.2188            0.2188        2.0968  0.0007  0.1323\n",
      "     43            \u001b[36m0.8333\u001b[0m        \u001b[32m0.4012\u001b[0m       0.1979            0.1979        2.0754  0.0006  0.1197\n",
      "     44            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3426\u001b[0m       0.1875            0.1875        2.0651  0.0006  0.1200\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2870\u001b[0m       0.1979            0.1979        2.0708  0.0005  0.1198\n",
      "     46            1.0000        0.3492       0.1875            0.1875        2.0802  0.0005  0.1360\n",
      "     47            1.0000        \u001b[32m0.2468\u001b[0m       0.1771            0.1771        2.0875  0.0004  0.1299\n",
      "     48            1.0000        0.2812       0.1771            0.1771        2.0929  0.0004  0.1136\n",
      "     49            0.9667        0.2508       0.1875            0.1875        2.0946  0.0003  0.1175\n",
      "     50            0.9667        0.2957       0.1875            0.1875        2.0924  0.0003  0.1332\n",
      "Fine tuning model for subject 2 with 30 = 30 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2667        2.3145       0.3125            0.3125        1.7019  0.0004  0.1033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.3000        2.1198       0.3125            0.3125        1.7675  0.0005  0.1212\n",
      "     33            0.4667        1.7966       0.2708            0.2708        1.8561  0.0005  0.1161\n",
      "     34            0.5000        1.7397       0.2604            0.2604        1.9460  0.0006  0.1181\n",
      "     35            0.5333        1.3125       0.2500            0.2500        2.0084  0.0006  0.1521\n",
      "     36            0.6000        1.0069       0.2500            0.2500        2.0172  0.0007  0.1379\n",
      "     37            0.7333        0.7365       0.2500            0.2500        1.9905  0.0007  0.1318\n",
      "     38            0.8000        0.7648       0.2292            0.2292        1.9623  0.0007  0.1198\n",
      "     39            \u001b[36m0.8667\u001b[0m        \u001b[32m0.5814\u001b[0m       0.2188            0.2188        1.9662  0.0007  0.1229\n",
      "     40            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4817\u001b[0m       0.2292            0.2292        1.9638  0.0007  0.1270\n",
      "     41            0.9667        0.5214       0.2188            0.2188        1.9473  0.0007  0.1984\n",
      "     42            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3823\u001b[0m       0.2292            0.2292        1.9263  0.0007  0.1425\n",
      "     43            1.0000        \u001b[32m0.3543\u001b[0m       0.2188            0.2188        1.9043  0.0006  0.2098\n",
      "     44            1.0000        \u001b[32m0.2757\u001b[0m       0.2188            0.2188        1.9035  0.0006  0.1510\n",
      "     45            1.0000        0.3420       0.2292            0.2292        1.9116  0.0005  0.1512\n",
      "     46            1.0000        \u001b[32m0.2217\u001b[0m       0.2292            0.2292        1.9280  0.0005  0.1324\n",
      "     47            1.0000        \u001b[32m0.1626\u001b[0m       0.2500            0.2500        1.9439  0.0004  0.1586\n",
      "     48            1.0000        0.1735       0.2500            0.2500        1.9576  0.0004  0.1494\n",
      "     49            1.0000        \u001b[32m0.1459\u001b[0m       0.2500            0.2500        1.9684  0.0003  0.1657\n",
      "     50            1.0000        0.1669       0.2604            0.2604        1.9771  0.0003  0.1791\n",
      "Fine tuning model for subject 2 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5333        2.1764       0.3229            0.3229        1.7184  0.0004  0.1721\n",
      "     32            0.6333        1.6430       0.3125            0.3125        1.8104  0.0005  0.1302\n",
      "     33            0.6667        1.4627       0.3021            0.3021        1.9310  0.0005  0.1345\n",
      "     34            0.7667        1.0643       0.2917            0.2917        1.9912  0.0006  0.1183\n",
      "     35            0.8000        0.8120       0.2917            0.2917        1.9686  0.0006  0.1199\n",
      "     36            \u001b[36m0.9333\u001b[0m        0.8499       0.2812            0.2812        1.9323  0.0007  0.1199\n",
      "     37            \u001b[36m0.9667\u001b[0m        0.6746       0.2708            0.2708        1.9249  0.0007  0.1338\n",
      "     38            0.9333        \u001b[32m0.5207\u001b[0m       0.2500            0.2500        1.9142  0.0007  0.1300\n",
      "     39            0.9333        \u001b[32m0.3299\u001b[0m       0.2396            0.2396        1.9272  0.0007  0.1278\n",
      "     40            0.9333        0.3541       0.2604            0.2604        1.9430  0.0007  0.1109\n",
      "     41            0.9333        \u001b[32m0.2978\u001b[0m       0.2708            0.2708        1.9487  0.0007  0.1164\n",
      "     42            0.9333        \u001b[32m0.2320\u001b[0m       0.2812            0.2812        1.9532  0.0007  0.1194\n",
      "     43            0.9333        0.2417       0.2812            0.2812        1.9619  0.0006  0.1170\n",
      "     44            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1768\u001b[0m       0.2708            0.2708        1.9653  0.0006  0.1203\n",
      "     45            1.0000        \u001b[32m0.1469\u001b[0m       0.2604            0.2604        1.9663  0.0005  0.1353\n",
      "     46            1.0000        0.1494       0.2396            0.2396        1.9683  0.0005  0.1230\n",
      "     47            1.0000        0.1491       0.2188            0.2188        1.9711  0.0004  0.1228\n",
      "     48            1.0000        \u001b[32m0.0985\u001b[0m       0.2083            0.2083        1.9745  0.0004  0.1228\n",
      "     49            1.0000        0.2154       0.2396            0.2396        1.9721  0.0003  0.1199\n",
      "     50            1.0000        0.1232       0.2396            0.2396        1.9708  0.0003  0.1053\n",
      "Fine tuning model for subject 2 with 30 = 30 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.8237       0.3229            0.3229        1.7053  0.0004  0.1152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5000        1.5118       0.2708            0.2708        1.7435  0.0005  0.1152\n",
      "     33            0.5667        1.5021       0.2604            0.2604        1.7661  0.0005  0.1152\n",
      "     34            0.6667        1.2572       0.2604            0.2604        1.7755  0.0006  0.1345\n",
      "     35            \u001b[36m0.8333\u001b[0m        0.9322       0.2292            0.2292        1.7674  0.0006  0.1196\n",
      "     36            \u001b[36m0.8667\u001b[0m        0.7973       0.2500            0.2500        1.7794  0.0007  0.1355\n",
      "     37            \u001b[36m0.9333\u001b[0m        0.6404       0.2188            0.2188        1.7996  0.0007  0.1282\n",
      "     38            0.9333        \u001b[32m0.3967\u001b[0m       0.2292            0.2292        1.8022  0.0007  0.1191\n",
      "     39            \u001b[36m1.0000\u001b[0m        0.4017       0.2292            0.2292        1.7738  0.0007  0.1042\n",
      "     40            1.0000        \u001b[32m0.3003\u001b[0m       0.2604            0.2604        1.7510  0.0007  0.1100\n",
      "     41            1.0000        \u001b[32m0.2538\u001b[0m       0.2812            0.2812        1.7389  0.0007  0.1177\n",
      "     42            1.0000        0.3118       0.3021            0.3021        1.7364  0.0007  0.1195\n",
      "     43            1.0000        0.3546       0.3021            0.3021        1.7470  0.0006  0.1200\n",
      "     44            1.0000        \u001b[32m0.2502\u001b[0m       0.2917            0.2917        1.7605  0.0006  0.1349\n",
      "     45            1.0000        \u001b[32m0.2036\u001b[0m       0.2812            0.2812        1.7762  0.0005  0.1270\n",
      "     46            1.0000        \u001b[32m0.1969\u001b[0m       0.2708            0.2708        1.7950  0.0005  0.1250\n",
      "     47            1.0000        0.2097       0.2708            0.2708        1.8075  0.0004  0.1202\n",
      "     48            1.0000        \u001b[32m0.1667\u001b[0m       0.2500            0.2500        1.8210  0.0004  0.1190\n",
      "     49            1.0000        0.1687       0.2500            0.2500        1.8336  0.0003  0.1075\n",
      "     50            1.0000        0.1962       0.2500            0.2500        1.8460  0.0003  0.1173\n",
      "Fine tuning model for subject 2 with 30 = 30 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3667        1.9275       0.3542            0.3542        1.6857  0.0004  0.1114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4000        1.7625       0.3333            0.3333        1.7353  0.0005  0.1120\n",
      "     33            0.3667        1.4985       0.3438            0.3438        1.8424  0.0005  0.1341\n",
      "     34            0.3667        1.2088       0.3021            0.3021        1.9449  0.0006  0.1177\n",
      "     35            0.4333        0.8477       0.2917            0.2917        1.9884  0.0006  0.1207\n",
      "     36            0.5667        \u001b[32m0.5923\u001b[0m       0.2708            0.2708        1.9801  0.0007  0.1199\n",
      "     37            0.7000        \u001b[32m0.4614\u001b[0m       0.2812            0.2812        1.9544  0.0007  0.1345\n",
      "     38            0.7667        \u001b[32m0.2957\u001b[0m       0.2812            0.2812        1.9305  0.0007  0.1288\n",
      "     39            \u001b[36m0.8333\u001b[0m        0.3181       0.2917            0.2917        1.9202  0.0007  0.1138\n",
      "     40            0.8333        0.3065       0.2604            0.2604        1.9307  0.0007  0.1169\n",
      "     41            \u001b[36m0.8667\u001b[0m        0.3026       0.2708            0.2708        1.9526  0.0007  0.1328\n",
      "     42            \u001b[36m0.9000\u001b[0m        \u001b[32m0.2747\u001b[0m       0.2812            0.2812        1.9729  0.0007  0.1203\n",
      "     43            0.9000        \u001b[32m0.2081\u001b[0m       0.3021            0.3021        1.9809  0.0006  0.1193\n",
      "     44            \u001b[36m0.9333\u001b[0m        \u001b[32m0.1812\u001b[0m       0.2917            0.2917        1.9836  0.0006  0.1360\n",
      "     45            0.9333        \u001b[32m0.1543\u001b[0m       0.2812            0.2812        1.9842  0.0005  0.1297\n",
      "     46            \u001b[36m0.9667\u001b[0m        \u001b[32m0.1389\u001b[0m       0.2917            0.2917        1.9822  0.0005  0.1219\n",
      "     47            0.9667        \u001b[32m0.1072\u001b[0m       0.2812            0.2812        1.9798  0.0004  0.1547\n",
      "     48            0.9667        0.1355       0.2812            0.2812        1.9771  0.0004  0.1310\n",
      "     49            0.9667        0.1843       0.2708            0.2708        1.9746  0.0003  0.1086\n",
      "     50            0.9667        \u001b[32m0.1027\u001b[0m       0.2708            0.2708        1.9739  0.0003  0.1141\n",
      "Fine tuning model for subject 2 with 30 = 30 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.6683       0.3021            0.3021        1.7171  0.0004  0.1042\n",
      "     32            0.4333        1.8187       0.3021            0.3021        1.7836  0.0005  0.1105\n",
      "     33            0.5667        1.5858       0.2917            0.2917        1.8469  0.0005  0.1211\n",
      "     34            0.7000        1.3018       0.2292            0.2292        1.9209  0.0006  0.1184\n",
      "     35            0.7667        1.0315       0.2188            0.2188        2.0014  0.0006  0.1323\n",
      "     36            0.7667        0.7586       0.2396            0.2396        2.0584  0.0007  0.1239\n",
      "     37            0.7667        \u001b[32m0.5921\u001b[0m       0.2500            0.2500        2.0774  0.0007  0.1159\n",
      "     38            \u001b[36m0.8333\u001b[0m        \u001b[32m0.5417\u001b[0m       0.2188            0.2188        2.0828  0.0007  0.1227\n",
      "     39            \u001b[36m0.8667\u001b[0m        \u001b[32m0.3808\u001b[0m       0.2083            0.2083        2.0982  0.0007  0.1177\n",
      "     40            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3277\u001b[0m       0.2292            0.2292        2.1216  0.0007  0.1194\n",
      "     41            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3266\u001b[0m       0.2292            0.2292        2.1481  0.0007  0.1194\n",
      "     42            0.9000        \u001b[32m0.2508\u001b[0m       0.2188            0.2188        2.1800  0.0007  0.1310\n",
      "     43            0.9000        0.2635       0.1979            0.1979        2.2083  0.0006  0.1346\n",
      "     44            0.9000        \u001b[32m0.2253\u001b[0m       0.1875            0.1875        2.2296  0.0006  0.1283\n",
      "     45            0.9000        \u001b[32m0.1928\u001b[0m       0.2292            0.2292        2.2429  0.0005  0.1143\n",
      "     46            0.9333        \u001b[32m0.1606\u001b[0m       0.2292            0.2292        2.2474  0.0005  0.1158\n",
      "     47            0.9333        \u001b[32m0.1478\u001b[0m       0.2396            0.2396        2.2448  0.0004  0.1395\n",
      "     48            \u001b[36m0.9667\u001b[0m        0.1501       0.2396            0.2396        2.2378  0.0004  0.1287\n",
      "     49            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0963\u001b[0m       0.2292            0.2292        2.2286  0.0003  0.1355\n",
      "     50            1.0000        \u001b[32m0.0913\u001b[0m       0.2188            0.2188        2.2200  0.0003  0.1200\n",
      "Fine tuning model for subject 2 with 30 = 30 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.5604       0.3438            0.3438        1.7145  0.0004  0.1311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5333        1.4672       0.3438            0.3438        1.7303  0.0005  0.1332\n",
      "     33            0.5667        1.2221       0.3333            0.3333        1.7307  0.0005  0.1207\n",
      "     34            0.6000        0.8901       0.3333            0.3333        1.7398  0.0006  0.1166\n",
      "     35            0.7667        0.9404       0.3333            0.3333        1.7492  0.0006  0.1198\n",
      "     36            0.8000        0.6584       0.3333            0.3333        1.7850  0.0007  0.1354\n",
      "     37            0.7667        \u001b[32m0.6019\u001b[0m       0.3438            0.3438        1.8233  0.0007  0.1297\n",
      "     38            \u001b[36m0.8333\u001b[0m        0.6279       0.3333            0.3333        1.8382  0.0007  0.1502\n",
      "     39            \u001b[36m0.8667\u001b[0m        \u001b[32m0.3595\u001b[0m       0.3542            0.3542        1.8374  0.0007  0.1681\n",
      "     40            \u001b[36m0.9000\u001b[0m        \u001b[32m0.2960\u001b[0m       0.3438            0.3438        1.8311  0.0007  0.1459\n",
      "     41            \u001b[36m0.9667\u001b[0m        0.3230       0.3438            0.3438        1.8312  0.0007  0.1510\n",
      "     42            0.9667        0.3157       0.3438            0.3438        1.8430  0.0007  0.1511\n",
      "     43            0.9667        \u001b[32m0.2307\u001b[0m       0.3333            0.3333        1.8549  0.0006  0.1506\n",
      "     44            0.9667        \u001b[32m0.1935\u001b[0m       0.3438            0.3438        1.8659  0.0006  0.1681\n",
      "     45            0.9667        0.2280       0.3542            0.3542        1.8779  0.0005  0.1844\n",
      "     46            0.9667        \u001b[32m0.1752\u001b[0m       0.3542            0.3542        1.9006  0.0005  0.1504\n",
      "     47            0.9667        \u001b[32m0.1411\u001b[0m       0.3542            0.3542        1.9295  0.0004  0.1511\n",
      "     48            0.9667        0.1627       0.3229            0.3229        1.9693  0.0004  0.1374\n",
      "     49            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1276\u001b[0m       0.3021            0.3021        2.0135  0.0003  0.1540\n",
      "     50            1.0000        0.1613       0.2812            0.2812        2.0624  0.0003  0.1202\n",
      "Fine tuning model for subject 2 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3333        2.1538       0.3333            0.3333        1.7056  0.0004  0.1952\n",
      "     32            0.4667        2.0260       0.2917            0.2917        1.7443  0.0005  0.1449\n",
      "     33            0.6333        1.6706       0.2500            0.2500        1.7746  0.0005  0.1988\n",
      "     34            0.6667        1.2699       0.2604            0.2604        1.8700  0.0006  0.1459\n",
      "     35            0.7000        0.9736       0.2708            0.2708        2.0545  0.0006  0.1152\n",
      "     36            0.6333        0.8039       0.2396            0.2396        2.2755  0.0007  0.1198\n",
      "     37            0.5667        \u001b[32m0.5785\u001b[0m       0.2396            0.2396        2.4621  0.0007  0.1352\n",
      "     38            0.5333        \u001b[32m0.5745\u001b[0m       0.2188            0.2188        2.5607  0.0007  0.1339\n",
      "     39            0.5667        \u001b[32m0.3672\u001b[0m       0.2083            0.2083        2.5773  0.0007  0.1240\n",
      "     40            0.5667        \u001b[32m0.3126\u001b[0m       0.2083            0.2083        2.5519  0.0007  0.1333\n",
      "     41            0.6000        \u001b[32m0.2269\u001b[0m       0.2083            0.2083        2.5030  0.0007  0.1077\n",
      "     42            0.7000        \u001b[32m0.2215\u001b[0m       0.2188            0.2188        2.4460  0.0007  0.1148\n",
      "     43            0.8000        0.2218       0.1979            0.1979        2.3719  0.0006  0.1334\n",
      "     44            \u001b[36m0.8667\u001b[0m        \u001b[32m0.1623\u001b[0m       0.1875            0.1875        2.3076  0.0006  0.1340\n",
      "     45            \u001b[36m0.9667\u001b[0m        \u001b[32m0.1446\u001b[0m       0.2083            0.2083        2.2485  0.0005  0.1192\n",
      "     46            0.9667        0.1650       0.2188            0.2188        2.2022  0.0005  0.1209\n",
      "     47            0.9667        0.2054       0.2188            0.2188        2.1640  0.0004  0.1512\n",
      "     48            0.9667        0.1695       0.2083            0.2083        2.1393  0.0004  0.1289\n",
      "     49            0.9667        0.1713       0.1979            0.1979        2.1186  0.0003  0.1260\n",
      "     50            \u001b[36m1.0000\u001b[0m        0.1568       0.2083            0.2083        2.1055  0.0003  0.1242\n",
      "Fine tuning model for subject 2 with 30 = 30 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.5329       0.3021            0.3021        1.7213  0.0004  0.1235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5000        1.6583       0.2917            0.2917        1.7911  0.0005  0.1469\n",
      "     33            0.6000        1.2155       0.3021            0.3021        1.8721  0.0005  0.1178\n",
      "     34            0.6333        1.1182       0.2708            0.2708        1.9214  0.0006  0.1198\n",
      "     35            0.7333        0.9040       0.2188            0.2188        1.9393  0.0006  0.1185\n",
      "     36            0.8000        0.7518       0.2396            0.2396        1.9427  0.0007  0.1291\n",
      "     37            \u001b[36m0.8333\u001b[0m        \u001b[32m0.6278\u001b[0m       0.2500            0.2500        1.9523  0.0007  0.1221\n",
      "     38            0.8000        \u001b[32m0.5160\u001b[0m       0.2500            0.2500        1.9762  0.0007  0.1223\n",
      "     39            0.8333        \u001b[32m0.3824\u001b[0m       0.2188            0.2188        1.9976  0.0007  0.1059\n",
      "     40            0.8333        \u001b[32m0.3674\u001b[0m       0.2188            0.2188        2.0460  0.0007  0.1123\n",
      "     41            \u001b[36m0.8667\u001b[0m        \u001b[32m0.3309\u001b[0m       0.2292            0.2292        2.0807  0.0007  0.1188\n",
      "     42            0.8667        \u001b[32m0.2149\u001b[0m       0.2396            0.2396        2.0905  0.0007  0.1324\n",
      "     43            0.8667        \u001b[32m0.1689\u001b[0m       0.2292            0.2292        2.0831  0.0006  0.1182\n",
      "     44            0.8333        0.2289       0.2188            0.2188        2.0725  0.0006  0.1189\n",
      "     45            0.8333        \u001b[32m0.1300\u001b[0m       0.2292            0.2292        2.0588  0.0005  0.1207\n",
      "     46            \u001b[36m0.9000\u001b[0m        0.1966       0.2292            0.2292        2.0404  0.0005  0.1346\n",
      "     47            0.9000        \u001b[32m0.1109\u001b[0m       0.2292            0.2292        2.0143  0.0004  0.1295\n",
      "     48            0.9000        0.1371       0.2396            0.2396        1.9910  0.0004  0.1125\n",
      "     49            \u001b[36m0.9333\u001b[0m        0.1907       0.2396            0.2396        1.9714  0.0003  0.1214\n",
      "     50            \u001b[36m0.9667\u001b[0m        0.1277       0.2292            0.2292        1.9527  0.0003  0.1291\n",
      "Fine tuning model for subject 2 with 35 = 35 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2286        2.0976       0.3438            0.3438        1.6636  0.0004  0.1179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.3429        2.0022       0.3438            0.3438        1.6722  0.0005  0.1280\n",
      "     33            0.4000        1.5458       0.3021            0.3021        1.7276  0.0005  0.1311\n",
      "     34            0.5143        1.3722       0.2500            0.2500        1.8012  0.0006  0.1195\n",
      "     35            0.6286        1.0304       0.2396            0.2396        1.8395  0.0006  0.1214\n",
      "     36            0.7429        0.6393       0.1979            0.1979        1.8221  0.0007  0.1198\n",
      "     37            0.8000        0.7157       0.2604            0.2604        1.8442  0.0007  0.1311\n",
      "     38            0.8000        \u001b[32m0.5870\u001b[0m       0.2292            0.2292        1.9604  0.0007  0.1367\n",
      "     39            0.7714        \u001b[32m0.3803\u001b[0m       0.2396            0.2396        2.1711  0.0007  0.1425\n",
      "     40            0.7429        0.3888       0.2083            0.2083        2.4296  0.0007  0.1281\n",
      "     41            0.6857        \u001b[32m0.3384\u001b[0m       0.2083            0.2083        2.6497  0.0007  0.1146\n",
      "     42            0.6571        \u001b[32m0.3056\u001b[0m       0.1979            0.1979        2.8213  0.0007  0.1238\n",
      "     43            0.6286        \u001b[32m0.2417\u001b[0m       0.1979            0.1979        2.9323  0.0006  0.1100\n",
      "     44            0.6286        0.2558       0.1979            0.1979        2.9949  0.0006  0.1182\n",
      "     45            0.6286        0.2422       0.2083            0.2083        2.9961  0.0005  0.1189\n",
      "     46            0.6571        \u001b[32m0.1880\u001b[0m       0.2188            0.2188        2.9547  0.0005  0.1205\n",
      "     47            0.7143        0.1920       0.2188            0.2188        2.8838  0.0004  0.1199\n",
      "     48            0.7429        \u001b[32m0.1776\u001b[0m       0.2188            0.2188        2.8009  0.0004  0.1313\n",
      "     49            0.7714        \u001b[32m0.1484\u001b[0m       0.2188            0.2188        2.7150  0.0003  0.1375\n",
      "     50            0.7714        0.1682       0.2292            0.2292        2.6337  0.0003  0.1365\n",
      "Fine tuning model for subject 2 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3143        1.7280       0.3125            0.3125        1.7055  0.0004  0.1227\n",
      "     32            0.4571        1.7397       0.3125            0.3125        1.7491  0.0005  0.1230\n",
      "     33            0.5429        1.3709       0.3021            0.3021        1.7738  0.0005  0.1515\n",
      "     34            0.5714        1.1951       0.3333            0.3333        1.8053  0.0006  0.1313\n",
      "     35            0.6000        1.0332       0.3542            0.3542        1.8616  0.0006  0.1349\n",
      "     36            0.5429        0.9023       0.3542            0.3542        1.9882  0.0007  0.1275\n",
      "     37            0.5143        0.8014       0.3542            0.3542        2.1494  0.0007  0.1178\n",
      "     38            0.5429        0.6689       0.3229            0.3229        2.2712  0.0007  0.1280\n",
      "     39            0.5143        \u001b[32m0.5585\u001b[0m       0.3125            0.3125        2.3600  0.0007  0.1293\n",
      "     40            0.5143        \u001b[32m0.4829\u001b[0m       0.3125            0.3125        2.4472  0.0007  0.1297\n",
      "     41            0.5143        \u001b[32m0.3810\u001b[0m       0.3229            0.3229        2.4944  0.0007  0.1342\n",
      "     42            0.5429        \u001b[32m0.3715\u001b[0m       0.3125            0.3125        2.5127  0.0007  0.1254\n",
      "     43            0.6000        0.3800       0.3125            0.3125        2.4860  0.0006  0.1160\n",
      "     44            0.6286        \u001b[32m0.2752\u001b[0m       0.3229            0.3229        2.4284  0.0006  0.1101\n",
      "     45            0.6286        \u001b[32m0.2585\u001b[0m       0.3229            0.3229        2.3525  0.0005  0.1162\n",
      "     46            0.7429        \u001b[32m0.2008\u001b[0m       0.3333            0.3333        2.2796  0.0005  0.1131\n",
      "     47            0.8000        \u001b[32m0.1820\u001b[0m       0.3542            0.3542        2.2136  0.0004  0.1317\n",
      "     48            0.8286        0.2241       0.3333            0.3333        2.1578  0.0004  0.1356\n",
      "     49            \u001b[36m0.8571\u001b[0m        0.2098       0.2917            0.2917        2.1064  0.0003  0.1220\n",
      "     50            \u001b[36m0.9429\u001b[0m        0.2111       0.2917            0.2917        2.0630  0.0003  0.1146\n",
      "Fine tuning model for subject 2 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4286        1.8318       0.3333            0.3333        1.6747  0.0004  0.1349\n",
      "     32            0.4571        1.9726       0.3125            0.3125        1.6499  0.0005  0.1120\n",
      "     33            0.4286        1.4261       0.3125            0.3125        1.7229  0.0005  0.1380\n",
      "     34            0.5143        1.4005       0.2812            0.2812        1.8610  0.0006  0.1403\n",
      "     35            0.5714        1.0425       0.2917            0.2917        1.9506  0.0006  0.1295\n",
      "     36            0.6000        0.7435       0.3021            0.3021        1.9761  0.0007  0.1338\n",
      "     37            0.6286        0.7418       0.2812            0.2812        1.9355  0.0007  0.1367\n",
      "     38            0.7143        \u001b[32m0.5275\u001b[0m       0.2708            0.2708        1.8444  0.0007  0.1826\n",
      "     39            0.7714        \u001b[32m0.4583\u001b[0m       0.2812            0.2812        1.7362  0.0007  0.1342\n",
      "     40            0.7714        \u001b[32m0.4152\u001b[0m       0.3021            0.3021        1.6442  0.0007  0.1363\n",
      "     41            \u001b[36m0.8571\u001b[0m        \u001b[32m0.3317\u001b[0m       0.3021            0.3021        1.5920  0.0007  0.1432\n",
      "     42            \u001b[36m0.8857\u001b[0m        0.3956       0.3021            0.3021        1.5804  0.0007  0.1540\n",
      "     43            0.8571        \u001b[32m0.3287\u001b[0m       0.3229            0.3229        1.5827  0.0006  0.1425\n",
      "     44            0.8571        \u001b[32m0.2954\u001b[0m       0.3333            0.3333        1.5964  0.0006  0.1828\n",
      "     45            0.8571        \u001b[32m0.2746\u001b[0m       0.3333            0.3333        1.6114  0.0005  0.1521\n",
      "     46            \u001b[36m0.9143\u001b[0m        \u001b[32m0.2626\u001b[0m       0.3542            0.3542        1.6258  0.0005  0.1506\n",
      "     47            \u001b[36m0.9429\u001b[0m        \u001b[32m0.2123\u001b[0m       0.3438            0.3438        1.6411  0.0004  0.1347\n",
      "     48            0.9143        \u001b[32m0.1960\u001b[0m       0.3542            0.3542        1.6567  0.0004  0.1509\n",
      "     49            0.9143        0.2286       0.3542            0.3542        1.6685  0.0003  0.1397\n",
      "     50            0.9143        \u001b[32m0.1907\u001b[0m       0.3646            0.3646        1.6751  0.0003  0.1547\n",
      "Fine tuning model for subject 2 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4857        1.7831       0.3438            0.3438        1.6822  0.0004  0.1484\n",
      "     32            0.4286        1.7636       0.3125            0.3125        1.6813  0.0005  0.1530\n",
      "     33            0.5143        1.5454       0.2708            0.2708        1.7009  0.0005  0.1457\n",
      "     34            0.5714        1.2722       0.2812            0.2812        1.7747  0.0006  0.1419\n",
      "     35            0.6000        1.1173       0.2500            0.2500        1.8600  0.0006  0.1536\n",
      "     36            0.6857        0.8888       0.2500            0.2500        1.8957  0.0007  0.1347\n",
      "     37            0.8000        0.8114       0.2500            0.2500        1.8708  0.0007  0.1215\n",
      "     38            0.8286        \u001b[32m0.6028\u001b[0m       0.2188            0.2188        1.8439  0.0007  0.1202\n",
      "     39            \u001b[36m0.8857\u001b[0m        \u001b[32m0.5346\u001b[0m       0.2604            0.2604        1.8398  0.0007  0.1190\n",
      "     40            \u001b[36m0.9143\u001b[0m        \u001b[32m0.4142\u001b[0m       0.2812            0.2812        1.8450  0.0007  0.1185\n",
      "     41            \u001b[36m0.9714\u001b[0m        \u001b[32m0.3676\u001b[0m       0.2708            0.2708        1.8540  0.0007  0.1191\n",
      "     42            0.9714        \u001b[32m0.2998\u001b[0m       0.2708            0.2708        1.8588  0.0007  0.1197\n",
      "     43            \u001b[36m1.0000\u001b[0m        0.3641       0.2917            0.2917        1.8628  0.0006  0.1500\n",
      "     44            1.0000        \u001b[32m0.2482\u001b[0m       0.2604            0.2604        1.8704  0.0006  0.1311\n",
      "     45            1.0000        \u001b[32m0.2165\u001b[0m       0.2604            0.2604        1.8863  0.0005  0.1336\n",
      "     46            1.0000        0.2744       0.2604            0.2604        1.9060  0.0005  0.1345\n",
      "     47            1.0000        0.2395       0.2396            0.2396        1.9290  0.0004  0.1352\n",
      "     48            1.0000        \u001b[32m0.1655\u001b[0m       0.2396            0.2396        1.9542  0.0004  0.1208\n",
      "     49            1.0000        \u001b[32m0.1516\u001b[0m       0.2500            0.2500        1.9796  0.0003  0.1198\n",
      "     50            1.0000        0.1577       0.2396            0.2396        2.0026  0.0003  0.1360\n",
      "Fine tuning model for subject 2 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2857        1.9326       0.3646            0.3646        1.6573  0.0004  0.1310\n",
      "     32            0.3714        1.8089       0.3438            0.3438        1.6585  0.0005  0.1242\n",
      "     33            0.4571        1.5636       0.3229            0.3229        1.7084  0.0005  0.1354\n",
      "     34            0.5429        1.4305       0.2917            0.2917        1.7475  0.0006  0.1193\n",
      "     35            0.6286        1.1920       0.2708            0.2708        1.7628  0.0006  0.1504\n",
      "     36            0.6857        1.0392       0.2396            0.2396        1.7354  0.0007  0.1209\n",
      "     37            0.8000        0.9324       0.2708            0.2708        1.6926  0.0007  0.1168\n",
      "     38            \u001b[36m0.9429\u001b[0m        0.6461       0.3333            0.3333        1.6787  0.0007  0.1206\n",
      "     39            \u001b[36m0.9714\u001b[0m        \u001b[32m0.4540\u001b[0m       0.3125            0.3125        1.6904  0.0007  0.1326\n",
      "     40            0.9429        0.4725       0.3021            0.3021        1.7188  0.0007  0.1391\n",
      "     41            0.9429        0.4607       0.2812            0.2812        1.7545  0.0007  0.1152\n",
      "     42            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3905\u001b[0m       0.2812            0.2812        1.7983  0.0007  0.1212\n",
      "     43            1.0000        \u001b[32m0.3834\u001b[0m       0.2812            0.2812        1.8535  0.0006  0.1198\n",
      "     44            1.0000        \u001b[32m0.3018\u001b[0m       0.3021            0.3021        1.9095  0.0006  0.1355\n",
      "     45            1.0000        \u001b[32m0.2561\u001b[0m       0.3021            0.3021        1.9666  0.0005  0.1387\n",
      "     46            1.0000        0.2835       0.3125            0.3125        2.0100  0.0005  0.1415\n",
      "     47            1.0000        0.2823       0.3333            0.3333        2.0400  0.0004  0.1316\n",
      "     48            1.0000        0.2630       0.3229            0.3229        2.0568  0.0004  0.1361\n",
      "     49            1.0000        \u001b[32m0.2120\u001b[0m       0.3125            0.3125        2.0596  0.0003  0.1617\n",
      "     50            0.9714        \u001b[32m0.1797\u001b[0m       0.3125            0.3125        2.0508  0.0003  0.1302\n",
      "Fine tuning model for subject 2 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.7748       0.3229            0.3229        1.7296  0.0004  0.1158\n",
      "     32            0.4857        1.6635       0.3021            0.3021        1.8392  0.0005  0.1391\n",
      "     33            0.5714        1.4393       0.2396            0.2396        1.9639  0.0005  0.1297\n",
      "     34            0.6286        1.0071       0.2292            0.2292        2.0745  0.0006  0.1206\n",
      "     35            0.6857        0.9331       0.2292            0.2292        2.1627  0.0006  0.1200\n",
      "     36            0.6857        \u001b[32m0.6016\u001b[0m       0.2396            0.2396        2.2310  0.0007  0.1357\n",
      "     37            0.6857        \u001b[32m0.4445\u001b[0m       0.2604            0.2604        2.2781  0.0007  0.1347\n",
      "     38            0.6857        \u001b[32m0.4252\u001b[0m       0.2500            0.2500        2.2798  0.0007  0.1402\n",
      "     39            0.7429        \u001b[32m0.3560\u001b[0m       0.2708            0.2708        2.2596  0.0007  0.1455\n",
      "     40            0.7714        0.3670       0.2604            0.2604        2.2074  0.0007  0.1327\n",
      "     41            \u001b[36m0.8571\u001b[0m        \u001b[32m0.3078\u001b[0m       0.2604            0.2604        2.1574  0.0007  0.1373\n",
      "     42            \u001b[36m0.9143\u001b[0m        \u001b[32m0.2481\u001b[0m       0.2500            0.2500        2.1205  0.0007  0.1410\n",
      "     43            \u001b[36m0.9714\u001b[0m        \u001b[32m0.2105\u001b[0m       0.2604            0.2604        2.0988  0.0006  0.1303\n",
      "     44            0.9714        \u001b[32m0.1427\u001b[0m       0.2812            0.2812        2.0878  0.0006  0.1380\n",
      "     45            \u001b[36m1.0000\u001b[0m        0.1833       0.2708            0.2708        2.0842  0.0005  0.1427\n",
      "     46            1.0000        0.2124       0.2708            0.2708        2.0846  0.0005  0.1099\n",
      "     47            1.0000        0.1488       0.2812            0.2812        2.0884  0.0004  0.1251\n",
      "     48            1.0000        0.1589       0.2812            0.2812        2.0986  0.0004  0.1257\n",
      "     49            1.0000        0.1448       0.2917            0.2917        2.1110  0.0003  0.1230\n",
      "     50            1.0000        0.1628       0.2917            0.2917        2.1249  0.0003  0.1310\n",
      "Fine tuning model for subject 2 with 35 = 35 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5143        1.8239       0.3021            0.3021        1.6891  0.0004  0.1229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.4821       0.2812            0.2812        1.7257  0.0005  0.1159\n",
      "     33            0.6286        1.4095       0.2500            0.2500        1.7929  0.0005  0.1344\n",
      "     34            0.6571        1.0198       0.2292            0.2292        1.8787  0.0006  0.1365\n",
      "     35            0.7143        0.9833       0.2500            0.2500        1.9266  0.0006  0.1323\n",
      "     36            0.7714        0.7740       0.2396            0.2396        1.9511  0.0007  0.1198\n",
      "     37            \u001b[36m0.8857\u001b[0m        0.7463       0.2708            0.2708        1.9659  0.0007  0.1188\n",
      "     38            0.8857        \u001b[32m0.5657\u001b[0m       0.2812            0.2812        1.9780  0.0007  0.1106\n",
      "     39            0.8857        \u001b[32m0.4594\u001b[0m       0.2396            0.2396        1.9798  0.0007  0.1202\n",
      "     40            \u001b[36m0.9429\u001b[0m        \u001b[32m0.3898\u001b[0m       0.2188            0.2188        1.9644  0.0007  0.1198\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3732\u001b[0m       0.2604            0.2604        1.9306  0.0007  0.1359\n",
      "     42            1.0000        \u001b[32m0.2399\u001b[0m       0.2708            0.2708        1.9011  0.0007  0.1279\n",
      "     43            1.0000        0.2609       0.2812            0.2812        1.8784  0.0006  0.1289\n",
      "     44            1.0000        \u001b[32m0.2115\u001b[0m       0.2708            0.2708        1.8619  0.0006  0.1346\n",
      "     45            1.0000        \u001b[32m0.1827\u001b[0m       0.2708            0.2708        1.8549  0.0005  0.1373\n",
      "     46            1.0000        \u001b[32m0.1447\u001b[0m       0.2812            0.2812        1.8567  0.0005  0.1164\n",
      "     47            1.0000        \u001b[32m0.1263\u001b[0m       0.2812            0.2812        1.8625  0.0004  0.1114\n",
      "     48            1.0000        0.1376       0.2812            0.2812        1.8702  0.0004  0.1241\n",
      "     49            1.0000        0.1529       0.2812            0.2812        1.8788  0.0003  0.1339\n",
      "     50            1.0000        \u001b[32m0.1049\u001b[0m       0.2812            0.2812        1.8854  0.0003  0.1197\n",
      "Fine tuning model for subject 2 with 35 = 35 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4857        1.7116       0.3333            0.3333        1.6840  0.0004  0.1128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4857        1.6858       0.3125            0.3125        1.6940  0.0005  0.1207\n",
      "     33            0.5714        1.5274       0.2604            0.2604        1.7627  0.0005  0.1336\n",
      "     34            0.5714        1.1827       0.2500            0.2500        1.8758  0.0006  0.1356\n",
      "     35            0.6000        0.9769       0.2500            0.2500        1.9608  0.0006  0.1512\n",
      "     36            0.6571        0.7684       0.1979            0.1979        1.9925  0.0007  0.1636\n",
      "     37            0.7429        \u001b[32m0.5421\u001b[0m       0.2188            0.2188        1.9959  0.0007  0.1334\n",
      "     38            0.8000        \u001b[32m0.4652\u001b[0m       0.2083            0.2083        1.9696  0.0007  0.1615\n",
      "     39            \u001b[36m0.8857\u001b[0m        \u001b[32m0.3765\u001b[0m       0.1875            0.1875        1.9384  0.0007  0.1513\n",
      "     40            \u001b[36m0.9714\u001b[0m        \u001b[32m0.3190\u001b[0m       0.2292            0.2292        1.9174  0.0007  0.1837\n",
      "     41            0.9714        0.3211       0.2500            0.2500        1.9151  0.0007  0.1500\n",
      "     42            0.9714        \u001b[32m0.2628\u001b[0m       0.2188            0.2188        1.9398  0.0007  0.1470\n",
      "     43            \u001b[36m1.0000\u001b[0m        0.2945       0.2083            0.2083        1.9797  0.0006  0.1357\n",
      "     44            1.0000        \u001b[32m0.2368\u001b[0m       0.2083            0.2083        2.0290  0.0006  0.1663\n",
      "     45            1.0000        \u001b[32m0.2182\u001b[0m       0.2083            0.2083        2.0779  0.0005  0.1484\n",
      "     46            1.0000        \u001b[32m0.1723\u001b[0m       0.2083            0.2083        2.1232  0.0005  0.1416\n",
      "     47            1.0000        0.1818       0.2083            0.2083        2.1594  0.0004  0.1492\n",
      "     48            1.0000        \u001b[32m0.1652\u001b[0m       0.2188            0.2188        2.1871  0.0004  0.1416\n",
      "     49            1.0000        \u001b[32m0.1365\u001b[0m       0.2188            0.2188        2.2072  0.0003  0.1811\n",
      "     50            1.0000        \u001b[32m0.1210\u001b[0m       0.2188            0.2188        2.2221  0.0003  0.1387\n",
      "Fine tuning model for subject 2 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.9581       0.3021            0.3021        1.6996  0.0004  0.1851\n",
      "     32            0.4857        1.7566       0.3438            0.3438        1.6927  0.0005  0.1456\n",
      "     33            0.4571        1.7407       0.3646            0.3646        1.6950  0.0005  0.1138\n",
      "     34            0.4571        1.4704       0.3646            0.3646        1.7578  0.0006  0.1171\n",
      "     35            0.3429        1.1129       0.3125            0.3125        1.8557  0.0006  0.1187\n",
      "     36            0.3714        0.9214       0.3021            0.3021        1.9299  0.0007  0.1358\n",
      "     37            0.4286        0.7676       0.2917            0.2917        1.9423  0.0007  0.1354\n",
      "     38            0.5143        0.7312       0.2708            0.2708        1.9131  0.0007  0.1376\n",
      "     39            0.5714        \u001b[32m0.5739\u001b[0m       0.2188            0.2188        1.8825  0.0007  0.1345\n",
      "     40            0.5714        \u001b[32m0.4403\u001b[0m       0.2708            0.2708        1.8771  0.0007  0.1413\n",
      "     41            0.6000        \u001b[32m0.4343\u001b[0m       0.2812            0.2812        1.8982  0.0007  0.1278\n",
      "     42            0.6286        \u001b[32m0.3152\u001b[0m       0.3021            0.3021        1.9427  0.0007  0.1318\n",
      "     43            0.6571        \u001b[32m0.3127\u001b[0m       0.2604            0.2604        2.0026  0.0006  0.1375\n",
      "     44            0.7143        \u001b[32m0.2807\u001b[0m       0.2500            0.2500        2.0648  0.0006  0.1157\n",
      "     45            0.6857        \u001b[32m0.2247\u001b[0m       0.2604            0.2604        2.1145  0.0005  0.1237\n",
      "     46            0.6857        0.2662       0.2396            0.2396        2.1436  0.0005  0.1198\n",
      "     47            \u001b[36m0.8571\u001b[0m        \u001b[32m0.2044\u001b[0m       0.2396            0.2396        2.1489  0.0004  0.1185\n",
      "     48            0.8571        0.2432       0.2396            0.2396        2.1386  0.0004  0.1179\n",
      "     49            \u001b[36m0.9143\u001b[0m        \u001b[32m0.1720\u001b[0m       0.2292            0.2292        2.1244  0.0003  0.1362\n",
      "     50            0.9143        0.2063       0.2188            0.2188        2.1113  0.0003  0.1332\n",
      "Fine tuning model for subject 2 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3714        1.8573       0.3438            0.3438        1.6766  0.0004  0.1136\n",
      "     32            0.4000        1.7889       0.3125            0.3125        1.7099  0.0005  0.1272\n",
      "     33            0.4286        1.4026       0.2708            0.2708        1.7775  0.0005  0.1353\n",
      "     34            0.6000        1.2284       0.2604            0.2604        1.7935  0.0006  0.1376\n",
      "     35            0.7714        1.0949       0.2292            0.2292        1.8055  0.0006  0.1360\n",
      "     36            0.8000        0.8732       0.1562            0.1562        1.8947  0.0007  0.1394\n",
      "     37            0.7714        0.6671       0.1667            0.1667        2.0295  0.0007  0.1195\n",
      "     38            0.8000        \u001b[32m0.5860\u001b[0m       0.1979            0.1979        2.1736  0.0007  0.1303\n",
      "     39            \u001b[36m0.8571\u001b[0m        \u001b[32m0.4661\u001b[0m       0.2188            0.2188        2.3072  0.0007  0.1341\n",
      "     40            \u001b[36m0.9429\u001b[0m        \u001b[32m0.3889\u001b[0m       0.2188            0.2188        2.4417  0.0007  0.1173\n",
      "     41            \u001b[36m0.9714\u001b[0m        0.4195       0.2083            0.2083        2.5602  0.0007  0.1155\n",
      "     42            0.9714        0.3900       0.1979            0.1979        2.6526  0.0007  0.1136\n",
      "     43            0.9714        \u001b[32m0.3023\u001b[0m       0.1979            0.1979        2.7179  0.0006  0.1186\n",
      "     44            0.9143        \u001b[32m0.2795\u001b[0m       0.1979            0.1979        2.7657  0.0006  0.1354\n",
      "     45            0.8857        \u001b[32m0.2492\u001b[0m       0.2083            0.2083        2.7898  0.0005  0.1340\n",
      "     46            0.9429        0.2765       0.2083            0.2083        2.7937  0.0005  0.1201\n",
      "     47            0.9429        \u001b[32m0.2056\u001b[0m       0.2188            0.2188        2.7905  0.0004  0.1213\n",
      "     48            0.9714        0.2488       0.1979            0.1979        2.7868  0.0004  0.1262\n",
      "     49            0.9714        \u001b[32m0.1752\u001b[0m       0.1979            0.1979        2.7747  0.0003  0.1354\n",
      "     50            \u001b[36m1.0000\u001b[0m        0.1826       0.1979            0.1979        2.7593  0.0003  0.1222\n",
      "Fine tuning model for subject 2 with 40 = 40 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3500        1.8174       0.3646            0.3646        1.6560  0.0004  0.1142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.3000        1.7696       0.3125            0.3125        1.6611  0.0005  0.1502\n",
      "     33            0.5000        1.4456       0.3125            0.3125        1.7480  0.0005  0.1332\n",
      "     34            0.5000        1.2157       0.2917            0.2917        1.8895  0.0006  0.1358\n",
      "     35            0.5500        1.1161       0.2188            0.2188        2.0548  0.0006  0.1291\n",
      "     36            0.6500        1.0185       0.2188            0.2188        2.2286  0.0007  0.1336\n",
      "     37            0.6750        0.9773       0.2708            0.2708        2.3614  0.0007  0.1318\n",
      "     38            0.6500        0.7340       0.2812            0.2812        2.4068  0.0007  0.1336\n",
      "     39            0.6750        0.7817       0.2917            0.2917        2.3679  0.0007  0.1354\n",
      "     40            0.7500        \u001b[32m0.6142\u001b[0m       0.3125            0.3125        2.2766  0.0007  0.1345\n",
      "     41            0.8000        \u001b[32m0.5258\u001b[0m       0.2604            0.2604        2.1694  0.0007  0.1361\n",
      "     42            0.8000        \u001b[32m0.5101\u001b[0m       0.2812            0.2812        2.0661  0.0007  0.1515\n",
      "     43            0.7500        0.5410       0.2812            0.2812        1.9967  0.0006  0.1303\n",
      "     44            0.7000        0.5211       0.3229            0.3229        1.9520  0.0006  0.1354\n",
      "     45            0.7000        \u001b[32m0.3913\u001b[0m       0.3125            0.3125        1.9133  0.0005  0.1352\n",
      "     46            0.7250        \u001b[32m0.3331\u001b[0m       0.3125            0.3125        1.8717  0.0005  0.1344\n",
      "     47            0.7750        0.3528       0.3125            0.3125        1.8337  0.0004  0.1362\n",
      "     48            0.8000        0.3540       0.3021            0.3021        1.8033  0.0004  0.1231\n",
      "     49            0.8000        0.3404       0.3125            0.3125        1.7816  0.0003  0.1296\n",
      "     50            \u001b[36m0.8500\u001b[0m        \u001b[32m0.3244\u001b[0m       0.2812            0.2812        1.7665  0.0003  0.1353\n",
      "Fine tuning model for subject 2 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        1.8435       0.3438            0.3438        1.6900  0.0004  0.1448\n",
      "     32            0.4250        1.5917       0.3229            0.3229        1.7122  0.0005  0.1316\n",
      "     33            0.4250        1.6663       0.2812            0.2812        1.7652  0.0005  0.1193\n",
      "     34            0.5500        1.1781       0.2604            0.2604        1.8562  0.0006  0.1354\n",
      "     35            0.5750        1.0743       0.2292            0.2292        1.9561  0.0006  0.1344\n",
      "     36            0.6750        0.8352       0.2292            0.2292        2.0452  0.0007  0.1350\n",
      "     37            0.7500        0.6805       0.2500            0.2500        2.1340  0.0007  0.1203\n",
      "     38            \u001b[36m0.8500\u001b[0m        \u001b[32m0.5379\u001b[0m       0.2292            0.2292        2.1999  0.0007  0.1509\n",
      "     39            0.8500        0.5702       0.2188            0.2188        2.2578  0.0007  0.1602\n",
      "     40            0.8500        \u001b[32m0.4494\u001b[0m       0.2292            0.2292        2.3112  0.0007  0.1350\n",
      "     41            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3669\u001b[0m       0.2188            0.2188        2.3315  0.0007  0.1516\n",
      "     42            \u001b[36m0.9250\u001b[0m        0.3877       0.2188            0.2188        2.3220  0.0007  0.1292\n",
      "     43            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3267\u001b[0m       0.2083            0.2083        2.3098  0.0006  0.1359\n",
      "     44            0.9750        \u001b[32m0.2696\u001b[0m       0.2083            0.2083        2.3064  0.0006  0.1460\n",
      "     45            0.9750        0.3023       0.2083            0.2083        2.3101  0.0005  0.1507\n",
      "     46            0.9750        0.2775       0.2188            0.2188        2.2983  0.0005  0.1370\n",
      "     47            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2307\u001b[0m       0.2188            0.2188        2.2845  0.0004  0.1339\n",
      "     48            1.0000        \u001b[32m0.1932\u001b[0m       0.2188            0.2188        2.2726  0.0004  0.1177\n",
      "     49            1.0000        \u001b[32m0.1867\u001b[0m       0.2292            0.2292        2.2595  0.0003  0.1391\n",
      "     50            1.0000        0.2024       0.2396            0.2396        2.2459  0.0003  0.1562\n",
      "Fine tuning model for subject 2 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4250        1.7064       0.3646            0.3646        1.6508  0.0004  0.1593\n",
      "     32            0.5250        1.6229       0.3646            0.3646        1.6450  0.0005  0.1485\n",
      "     33            0.5250        1.5443       0.3750            0.3750        1.6953  0.0005  0.1425\n",
      "     34            0.6750        1.1701       0.3438            0.3438        1.7303  0.0006  0.1900\n",
      "     35            0.7750        1.0833       0.3333            0.3333        1.7402  0.0006  0.1490\n",
      "     36            0.7000        0.8132       0.3125            0.3125        1.7725  0.0007  0.1609\n",
      "     37            0.7250        0.6805       0.2708            0.2708        1.8224  0.0007  0.1572\n",
      "     38            0.7750        0.6783       0.2500            0.2500        1.8808  0.0007  0.1506\n",
      "     39            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5413\u001b[0m       0.2396            0.2396        1.9398  0.0007  0.1517\n",
      "     40            0.9000        \u001b[32m0.4826\u001b[0m       0.2396            0.2396        1.9959  0.0007  0.1504\n",
      "     41            \u001b[36m0.9250\u001b[0m        \u001b[32m0.4035\u001b[0m       0.2708            0.2708        2.0441  0.0007  0.1706\n",
      "     42            0.8750        \u001b[32m0.3249\u001b[0m       0.2500            0.2500        2.1003  0.0007  0.1638\n",
      "     43            0.9000        0.3454       0.2396            0.2396        2.1491  0.0006  0.1510\n",
      "     44            \u001b[36m0.9500\u001b[0m        \u001b[32m0.3202\u001b[0m       0.2500            0.2500        2.1803  0.0006  0.1511\n",
      "     45            0.9500        \u001b[32m0.3192\u001b[0m       0.2604            0.2604        2.2044  0.0005  0.2283\n",
      "     46            0.9500        \u001b[32m0.2150\u001b[0m       0.2708            0.2708        2.2224  0.0005  0.1453\n",
      "     47            0.9500        0.2947       0.2708            0.2708        2.2299  0.0004  0.1172\n",
      "     48            \u001b[36m0.9750\u001b[0m        0.2218       0.2812            0.2812        2.2288  0.0004  0.1222\n",
      "     49            0.9750        \u001b[32m0.2136\u001b[0m       0.2812            0.2812        2.2211  0.0003  0.1198\n",
      "     50            0.9750        0.2141       0.2708            0.2708        2.2079  0.0003  0.1202\n",
      "Fine tuning model for subject 2 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3500        1.9828       0.3125            0.3125        1.6616  0.0004  0.1571\n",
      "     32            0.3750        1.9509       0.3229            0.3229        1.6393  0.0005  0.1172\n",
      "     33            0.4500        1.6994       0.3125            0.3125        1.6503  0.0005  0.1355\n",
      "     34            0.5500        1.4874       0.2812            0.2812        1.6875  0.0006  0.1337\n",
      "     35            0.5750        1.1694       0.2500            0.2500        1.7164  0.0006  0.1368\n",
      "     36            0.7500        1.0472       0.1667            0.1667        1.7510  0.0007  0.1335\n",
      "     37            \u001b[36m0.9000\u001b[0m        0.8601       0.2396            0.2396        1.8023  0.0007  0.1352\n",
      "     38            0.8500        0.6733       0.2396            0.2396        1.8554  0.0007  0.1306\n",
      "     39            0.8500        \u001b[32m0.6007\u001b[0m       0.2500            0.2500        1.8640  0.0007  0.1351\n",
      "     40            0.8000        \u001b[32m0.5451\u001b[0m       0.3021            0.3021        1.8754  0.0007  0.1345\n",
      "     41            0.8000        \u001b[32m0.4494\u001b[0m       0.3021            0.3021        1.9230  0.0007  0.1375\n",
      "     42            0.8250        \u001b[32m0.4074\u001b[0m       0.2917            0.2917        1.9815  0.0007  0.1333\n",
      "     43            0.8250        \u001b[32m0.3848\u001b[0m       0.2917            0.2917        2.0382  0.0006  0.1404\n",
      "     44            0.9000        \u001b[32m0.3385\u001b[0m       0.3021            0.3021        2.0793  0.0006  0.1312\n",
      "     45            \u001b[36m0.9250\u001b[0m        0.3471       0.2917            0.2917        2.1052  0.0005  0.1347\n",
      "     46            0.9250        \u001b[32m0.3034\u001b[0m       0.2917            0.2917        2.1130  0.0005  0.1365\n",
      "     47            \u001b[36m0.9500\u001b[0m        0.3334       0.2917            0.2917        2.1051  0.0004  0.1197\n",
      "     48            0.9500        \u001b[32m0.2337\u001b[0m       0.2917            0.2917        2.0899  0.0004  0.1513\n",
      "     49            0.9500        0.3212       0.2917            0.2917        2.0700  0.0003  0.1409\n",
      "     50            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2171\u001b[0m       0.2708            0.2708        2.0488  0.0003  0.1459\n",
      "Fine tuning model for subject 2 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        2.0407       0.3333            0.3333        1.6959  0.0004  0.1137\n",
      "     32            0.3250        2.0106       0.3021            0.3021        1.7196  0.0005  0.1294\n",
      "     33            0.4000        1.7770       0.2708            0.2708        1.7665  0.0005  0.1347\n",
      "     34            0.4750        1.6035       0.2708            0.2708        1.8680  0.0006  0.1360\n",
      "     35            0.4500        1.2138       0.2604            0.2604        2.0144  0.0006  0.1370\n",
      "     36            0.5250        1.0544       0.2708            0.2708        2.1425  0.0007  0.1338\n",
      "     37            0.6250        0.8552       0.2812            0.2812        2.1627  0.0007  0.1355\n",
      "     38            0.7250        0.8717       0.2604            0.2604        2.1024  0.0007  0.1357\n",
      "     39            0.7750        0.7236       0.2292            0.2292        2.0129  0.0007  0.1340\n",
      "     40            \u001b[36m0.8500\u001b[0m        0.6807       0.2396            0.2396        1.9229  0.0007  0.1348\n",
      "     41            \u001b[36m0.9250\u001b[0m        \u001b[32m0.5923\u001b[0m       0.2708            0.2708        1.8590  0.0007  0.1381\n",
      "     42            \u001b[36m0.9750\u001b[0m        \u001b[32m0.5852\u001b[0m       0.2396            0.2396        1.8349  0.0007  0.1623\n",
      "     43            0.9500        \u001b[32m0.4687\u001b[0m       0.2604            0.2604        1.8281  0.0006  0.1359\n",
      "     44            0.9250        \u001b[32m0.3860\u001b[0m       0.2396            0.2396        1.8314  0.0006  0.1353\n",
      "     45            0.9250        \u001b[32m0.3622\u001b[0m       0.2500            0.2500        1.8354  0.0005  0.1350\n",
      "     46            0.9250        \u001b[32m0.3479\u001b[0m       0.2604            0.2604        1.8414  0.0005  0.1358\n",
      "     47            0.9500        \u001b[32m0.3205\u001b[0m       0.2708            0.2708        1.8476  0.0004  0.1392\n",
      "     48            0.9750        \u001b[32m0.3030\u001b[0m       0.2708            0.2708        1.8558  0.0004  0.1329\n",
      "     49            0.9750        \u001b[32m0.2770\u001b[0m       0.2500            0.2500        1.8646  0.0003  0.1483\n",
      "     50            0.9750        \u001b[32m0.2429\u001b[0m       0.2500            0.2500        1.8765  0.0003  0.1355\n",
      "Fine tuning model for subject 2 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3500        2.0095       0.3229            0.3229        1.6749  0.0004  0.1439\n",
      "     32            0.4250        1.6927       0.3229            0.3229        1.6704  0.0005  0.1300\n",
      "     33            0.5000        1.6018       0.2604            0.2604        1.7063  0.0005  0.1327\n",
      "     34            0.5750        1.3604       0.2708            0.2708        1.7733  0.0006  0.1356\n",
      "     35            0.6000        0.9766       0.2604            0.2604        1.8658  0.0006  0.1343\n",
      "     36            0.5750        0.8771       0.2812            0.2812        1.9519  0.0007  0.1359\n",
      "     37            0.5750        0.7419       0.2604            0.2604        2.0196  0.0007  0.1191\n",
      "     38            0.6000        \u001b[32m0.5845\u001b[0m       0.2500            0.2500        2.0650  0.0007  0.1354\n",
      "     39            0.6500        \u001b[32m0.4958\u001b[0m       0.2396            0.2396        2.1065  0.0007  0.1357\n",
      "     40            0.6750        \u001b[32m0.4848\u001b[0m       0.2292            0.2292        2.1474  0.0007  0.1341\n",
      "     41            0.6750        \u001b[32m0.3001\u001b[0m       0.2292            0.2292        2.1893  0.0007  0.1356\n",
      "     42            0.6500        0.3179       0.2188            0.2188        2.2176  0.0007  0.1353\n",
      "     43            0.6750        0.3320       0.2396            0.2396        2.2281  0.0006  0.1501\n",
      "     44            0.6750        0.3074       0.2500            0.2500        2.2291  0.0006  0.1366\n",
      "     45            0.7750        \u001b[32m0.2805\u001b[0m       0.2500            0.2500        2.2273  0.0005  0.1519\n",
      "     46            0.7750        \u001b[32m0.2744\u001b[0m       0.2396            0.2396        2.2248  0.0005  0.1183\n",
      "     47            0.8000        \u001b[32m0.2739\u001b[0m       0.2292            0.2292        2.2192  0.0004  0.1343\n",
      "     48            0.8250        \u001b[32m0.2356\u001b[0m       0.2396            0.2396        2.2109  0.0004  0.1313\n",
      "     49            \u001b[36m0.8500\u001b[0m        0.2708       0.2292            0.2292        2.1951  0.0003  0.1275\n",
      "     50            \u001b[36m0.9000\u001b[0m        \u001b[32m0.2217\u001b[0m       0.2292            0.2292        2.1794  0.0003  0.1290\n",
      "Fine tuning model for subject 2 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3250        2.1820       0.3229            0.3229        1.7078  0.0004  0.1360\n",
      "     32            0.4250        1.8069       0.3125            0.3125        1.7459  0.0005  0.1250\n",
      "     33            0.5000        1.6128       0.2812            0.2812        1.7812  0.0005  0.1344\n",
      "     34            0.6000        1.2412       0.3229            0.3229        1.8250  0.0006  0.1220\n",
      "     35            0.7000        1.0271       0.3021            0.3021        1.8851  0.0006  0.1333\n",
      "     36            0.7750        0.8877       0.3021            0.3021        1.9442  0.0007  0.1480\n",
      "     37            0.8000        \u001b[32m0.5983\u001b[0m       0.2917            0.2917        1.9790  0.0007  0.1291\n",
      "     38            0.8250        0.6535       0.2812            0.2812        1.9922  0.0007  0.1358\n",
      "     39            0.8250        \u001b[32m0.5577\u001b[0m       0.2708            0.2708        1.9930  0.0007  0.1339\n",
      "     40            \u001b[36m0.8500\u001b[0m        \u001b[32m0.4443\u001b[0m       0.2500            0.2500        1.9947  0.0007  0.1514\n",
      "     41            0.8500        \u001b[32m0.4057\u001b[0m       0.2812            0.2812        2.0077  0.0007  0.1491\n",
      "     42            0.8250        0.4474       0.3125            0.3125        2.0214  0.0007  0.1526\n",
      "     43            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3568\u001b[0m       0.3125            0.3125        2.0349  0.0006  0.1667\n",
      "     44            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2696\u001b[0m       0.3333            0.3333        2.0433  0.0006  0.1667\n",
      "     45            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2302\u001b[0m       0.3229            0.3229        2.0501  0.0005  0.1678\n",
      "     46            \u001b[36m1.0000\u001b[0m        0.2741       0.3229            0.3229        2.0476  0.0005  0.1659\n",
      "     47            1.0000        \u001b[32m0.1730\u001b[0m       0.3125            0.3125        2.0428  0.0004  0.1656\n",
      "     48            1.0000        0.2316       0.3125            0.3125        2.0349  0.0004  0.1647\n",
      "     49            1.0000        0.2375       0.3125            0.3125        2.0263  0.0003  0.1670\n",
      "     50            1.0000        \u001b[32m0.1377\u001b[0m       0.3021            0.3021        2.0197  0.0003  0.1521\n",
      "Fine tuning model for subject 2 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        2.3158       0.3229            0.3229        1.6840  0.0004  0.1766\n",
      "     32            0.3500        1.9324       0.3021            0.3021        1.6877  0.0005  0.1462\n",
      "     33            0.3500        1.7597       0.3229            0.3229        1.6885  0.0005  0.1759\n",
      "     34            0.4500        1.4913       0.2812            0.2812        1.7133  0.0006  0.1511\n",
      "     35            0.6250        1.2477       0.2812            0.2812        1.7342  0.0006  0.1510\n",
      "     36            0.8000        0.9143       0.2708            0.2708        1.7594  0.0007  0.1896\n",
      "     37            \u001b[36m0.9000\u001b[0m        0.7747       0.2604            0.2604        1.7965  0.0007  0.1374\n",
      "     38            0.9000        0.6741       0.2396            0.2396        1.8386  0.0007  0.1268\n",
      "     39            0.8750        \u001b[32m0.5889\u001b[0m       0.2292            0.2292        1.8935  0.0007  0.1310\n",
      "     40            0.8750        \u001b[32m0.4130\u001b[0m       0.2188            0.2188        1.9453  0.0007  0.1211\n",
      "     41            0.9000        0.5129       0.2292            0.2292        1.9813  0.0007  0.1388\n",
      "     42            0.9000        0.4495       0.2292            0.2292        2.0158  0.0007  0.1194\n",
      "     43            0.8750        0.4446       0.2188            0.2188        2.0411  0.0006  0.1636\n",
      "     44            0.9000        \u001b[32m0.3598\u001b[0m       0.2188            0.2188        2.0714  0.0006  0.1357\n",
      "     45            0.9000        0.3972       0.2083            0.2083        2.1015  0.0005  0.1338\n",
      "     46            \u001b[36m0.9250\u001b[0m        \u001b[32m0.3385\u001b[0m       0.2083            0.2083        2.1210  0.0005  0.1181\n",
      "     47            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2407\u001b[0m       0.2083            0.2083        2.1351  0.0004  0.1214\n",
      "     48            \u001b[36m0.9750\u001b[0m        0.2962       0.2292            0.2292        2.1415  0.0004  0.1244\n",
      "     49            \u001b[36m1.0000\u001b[0m        0.2816       0.2188            0.2188        2.1458  0.0003  0.1213\n",
      "     50            1.0000        0.2500       0.2188            0.2188        2.1486  0.0003  0.1239\n",
      "Fine tuning model for subject 2 with 40 = 40 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        1.7077       0.3438            0.3438        1.6826  0.0004  0.1306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4000        1.6731       0.2917            0.2917        1.7082  0.0005  0.1471\n",
      "     33            0.4750        1.4409       0.2708            0.2708        1.8053  0.0005  0.1345\n",
      "     34            0.4750        1.0722       0.2396            0.2396        1.9366  0.0006  0.1404\n",
      "     35            0.5750        0.9170       0.2396            0.2396        2.0456  0.0006  0.1311\n",
      "     36            0.6000        0.9011       0.2500            0.2500        2.0967  0.0007  0.1346\n",
      "     37            0.7000        0.7120       0.2604            0.2604        2.1042  0.0007  0.1200\n",
      "     38            0.8000        \u001b[32m0.6089\u001b[0m       0.2708            0.2708        2.0737  0.0007  0.1355\n",
      "     39            \u001b[36m0.8750\u001b[0m        0.6295       0.2708            0.2708        2.0158  0.0007  0.1343\n",
      "     40            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5033\u001b[0m       0.2708            0.2708        1.9608  0.0007  0.1415\n",
      "     41            0.9000        \u001b[32m0.4496\u001b[0m       0.2708            0.2708        1.9138  0.0007  0.1314\n",
      "     42            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3361\u001b[0m       0.3021            0.3021        1.8841  0.0007  0.1495\n",
      "     43            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2545\u001b[0m       0.2708            0.2708        1.8754  0.0006  0.1340\n",
      "     44            1.0000        0.2910       0.2708            0.2708        1.8787  0.0006  0.1357\n",
      "     45            1.0000        0.3392       0.2812            0.2812        1.8827  0.0005  0.1509\n",
      "     46            1.0000        0.2664       0.2604            0.2604        1.8920  0.0005  0.1237\n",
      "     47            1.0000        \u001b[32m0.2530\u001b[0m       0.2500            0.2500        1.9005  0.0004  0.1477\n",
      "     48            1.0000        0.2820       0.2604            0.2604        1.9048  0.0004  0.1260\n",
      "     49            1.0000        \u001b[32m0.2420\u001b[0m       0.2396            0.2396        1.9086  0.0003  0.1332\n",
      "     50            1.0000        \u001b[32m0.1796\u001b[0m       0.2500            0.2500        1.9141  0.0003  0.1357\n",
      "Fine tuning model for subject 2 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        1.8673       0.3542            0.3542        1.6852  0.0004  0.1421\n",
      "     32            0.3750        1.8936       0.3229            0.3229        1.7136  0.0005  0.1346\n",
      "     33            0.4000        1.5716       0.3125            0.3125        1.7925  0.0005  0.1200\n",
      "     34            0.5000        1.3266       0.3021            0.3021        1.9289  0.0006  0.1353\n",
      "     35            0.5250        1.2371       0.3333            0.3333        2.0844  0.0006  0.1352\n",
      "     36            0.6500        0.8546       0.3021            0.3021        2.2124  0.0007  0.1341\n",
      "     37            0.7750        0.7825       0.2812            0.2812        2.3132  0.0007  0.1375\n",
      "     38            0.7500        0.6750       0.2500            0.2500        2.3701  0.0007  0.1293\n",
      "     39            0.6250        \u001b[32m0.4972\u001b[0m       0.2396            0.2396        2.3866  0.0007  0.1364\n",
      "     40            0.6250        0.5980       0.2396            0.2396        2.3680  0.0007  0.1330\n",
      "     41            0.6250        \u001b[32m0.4408\u001b[0m       0.2292            0.2292        2.3504  0.0007  0.1353\n",
      "     42            0.6500        \u001b[32m0.4061\u001b[0m       0.2292            0.2292        2.3672  0.0007  0.1358\n",
      "     43            0.7500        \u001b[32m0.3831\u001b[0m       0.2396            0.2396        2.4101  0.0006  0.1346\n",
      "     44            0.8250        \u001b[32m0.3397\u001b[0m       0.2604            0.2604        2.4757  0.0006  0.1370\n",
      "     45            0.8250        \u001b[32m0.2984\u001b[0m       0.2604            0.2604        2.5503  0.0005  0.1274\n",
      "     46            \u001b[36m0.8500\u001b[0m        \u001b[32m0.2919\u001b[0m       0.2604            0.2604        2.6218  0.0005  0.1241\n",
      "     47            0.8500        \u001b[32m0.2359\u001b[0m       0.2604            0.2604        2.6637  0.0004  0.1227\n",
      "     48            \u001b[36m0.9000\u001b[0m        0.2458       0.2604            0.2604        2.6765  0.0004  0.1250\n",
      "     49            \u001b[36m0.9500\u001b[0m        \u001b[32m0.1929\u001b[0m       0.2500            0.2500        2.6699  0.0003  0.1250\n",
      "     50            \u001b[36m1.0000\u001b[0m        0.2044       0.2708            0.2708        2.6459  0.0003  0.1256\n",
      "Fine tuning model for subject 2 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2889        1.9970       0.3125            0.3125        1.7116  0.0004  0.1370\n",
      "     32            0.3556        1.9926       0.3125            0.3125        1.7951  0.0005  0.1490\n",
      "     33            0.3778        1.6119       0.2604            0.2604        1.9250  0.0005  0.1353\n",
      "     34            0.4222        1.2308       0.2500            0.2500        2.0840  0.0006  0.1281\n",
      "     35            0.4889        1.0058       0.2500            0.2500        2.2132  0.0006  0.1556\n",
      "     36            0.5333        0.9634       0.2188            0.2188        2.2978  0.0007  0.1300\n",
      "     37            0.6222        0.7047       0.2396            0.2396        2.2806  0.0007  0.1389\n",
      "     38            0.6444        0.6330       0.2396            0.2396        2.1816  0.0007  0.1290\n",
      "     39            0.8222        \u001b[32m0.6194\u001b[0m       0.2604            0.2604        2.0500  0.0007  0.1455\n",
      "     40            \u001b[36m0.9111\u001b[0m        \u001b[32m0.5165\u001b[0m       0.2708            0.2708        1.9205  0.0007  0.1667\n",
      "     41            \u001b[36m0.9556\u001b[0m        \u001b[32m0.3920\u001b[0m       0.2708            0.2708        1.8264  0.0007  0.1698\n",
      "     42            0.9556        0.4574       0.2917            0.2917        1.7787  0.0007  0.1512\n",
      "     43            \u001b[36m0.9778\u001b[0m        0.3953       0.3125            0.3125        1.7677  0.0006  0.1354\n",
      "     44            0.9778        \u001b[32m0.3223\u001b[0m       0.2708            0.2708        1.7806  0.0006  0.1508\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2403\u001b[0m       0.2812            0.2812        1.8039  0.0005  0.1356\n",
      "     46            1.0000        0.2770       0.2708            0.2708        1.8316  0.0005  0.1527\n",
      "     47            0.9778        \u001b[32m0.1874\u001b[0m       0.2917            0.2917        1.8523  0.0004  0.1391\n",
      "     48            1.0000        0.2680       0.2708            0.2708        1.8636  0.0004  0.1343\n",
      "     49            1.0000        \u001b[32m0.1832\u001b[0m       0.2812            0.2812        1.8684  0.0003  0.1511\n",
      "     50            1.0000        0.2296       0.3021            0.3021        1.8685  0.0003  0.1584\n",
      "Fine tuning model for subject 2 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4222        1.8012       0.3125            0.3125        1.7083  0.0004  0.1832\n",
      "     32            0.4667        1.7362       0.3333            0.3333        1.7536  0.0005  0.1460\n",
      "     33            0.4889        1.3984       0.3229            0.3229        1.7914  0.0005  0.1684\n",
      "     34            0.5333        1.2445       0.2812            0.2812        1.8080  0.0006  0.1761\n",
      "     35            0.6000        1.0133       0.2188            0.2188        1.8426  0.0006  0.1763\n",
      "     36            0.6667        0.8637       0.2500            0.2500        1.8977  0.0007  0.1928\n",
      "     37            0.7778        0.7553       0.2604            0.2604        1.9527  0.0007  0.1543\n",
      "     38            \u001b[36m0.8667\u001b[0m        0.6836       0.2500            0.2500        1.9886  0.0007  0.1834\n",
      "     39            \u001b[36m0.9111\u001b[0m        \u001b[32m0.5816\u001b[0m       0.2708            0.2708        2.0073  0.0007  0.1667\n",
      "     40            0.9111        \u001b[32m0.4771\u001b[0m       0.2292            0.2292        2.0255  0.0007  0.1669\n",
      "     41            0.8667        \u001b[32m0.4000\u001b[0m       0.2500            0.2500        2.0480  0.0007  0.1791\n",
      "     42            0.8667        0.4218       0.2708            0.2708        2.0716  0.0007  0.1655\n",
      "     43            0.9111        0.4184       0.2604            0.2604        2.0756  0.0006  0.1987\n",
      "     44            0.9111        \u001b[32m0.3509\u001b[0m       0.2396            0.2396        2.0684  0.0006  0.1641\n",
      "     45            0.8444        0.3561       0.2396            0.2396        2.0523  0.0005  0.1758\n",
      "     46            0.8667        \u001b[32m0.2854\u001b[0m       0.2500            0.2500        2.0317  0.0005  0.1701\n",
      "     47            0.8889        \u001b[32m0.2729\u001b[0m       0.2500            0.2500        2.0102  0.0004  0.1532\n",
      "     48            \u001b[36m0.9556\u001b[0m        0.3128       0.2500            0.2500        1.9937  0.0004  0.1661\n",
      "     49            0.9556        \u001b[32m0.2520\u001b[0m       0.2500            0.2500        1.9754  0.0003  0.1341\n",
      "     50            \u001b[36m0.9778\u001b[0m        0.2531       0.2708            0.2708        1.9621  0.0003  0.1508\n",
      "Fine tuning model for subject 2 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4444        1.5040       0.3333            0.3333        1.6847  0.0004  0.1399\n",
      "     32            0.5111        1.3551       0.3333            0.3333        1.7002  0.0005  0.1421\n",
      "     33            0.5333        1.2245       0.3021            0.3021        1.7445  0.0005  0.1302\n",
      "     34            0.6000        1.1117       0.2917            0.2917        1.8215  0.0006  0.1514\n",
      "     35            0.6667        1.0274       0.2604            0.2604        1.9102  0.0006  0.1356\n",
      "     36            0.6889        0.7119       0.2396            0.2396        1.9900  0.0007  0.1295\n",
      "     37            0.7333        \u001b[32m0.6275\u001b[0m       0.2188            0.2188        2.0371  0.0007  0.1341\n",
      "     38            0.7556        0.6834       0.2292            0.2292        2.0489  0.0007  0.1375\n",
      "     39            0.7333        \u001b[32m0.4267\u001b[0m       0.2396            0.2396        2.0596  0.0007  0.1272\n",
      "     40            0.7333        0.4804       0.2396            0.2396        2.0919  0.0007  0.1447\n",
      "     41            0.7778        \u001b[32m0.3885\u001b[0m       0.2708            0.2708        2.1332  0.0007  0.1521\n",
      "     42            0.7556        \u001b[32m0.3357\u001b[0m       0.2604            0.2604        2.1750  0.0007  0.1460\n",
      "     43            0.7556        \u001b[32m0.3201\u001b[0m       0.2500            0.2500        2.2096  0.0006  0.1511\n",
      "     44            0.7556        0.3270       0.2500            0.2500        2.2356  0.0006  0.1365\n",
      "     45            0.7778        \u001b[32m0.2636\u001b[0m       0.2500            0.2500        2.2494  0.0005  0.1515\n",
      "     46            0.8000        \u001b[32m0.2549\u001b[0m       0.2500            0.2500        2.2528  0.0005  0.1354\n",
      "     47            0.8222        \u001b[32m0.2038\u001b[0m       0.2500            0.2500        2.2465  0.0004  0.1550\n",
      "     48            \u001b[36m0.8444\u001b[0m        \u001b[32m0.1817\u001b[0m       0.2188            0.2188        2.2326  0.0004  0.1531\n",
      "     49            \u001b[36m0.8667\u001b[0m        0.2534       0.2188            0.2188        2.2162  0.0003  0.1480\n",
      "     50            \u001b[36m0.9111\u001b[0m        \u001b[32m0.1724\u001b[0m       0.2083            0.2083        2.1972  0.0003  0.1366\n",
      "Fine tuning model for subject 2 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        2.0626       0.3438            0.3438        1.6716  0.0004  0.1428\n",
      "     32            0.4667        1.9851       0.2917            0.2917        1.7175  0.0005  0.1305\n",
      "     33            0.4444        1.5100       0.2708            0.2708        1.8235  0.0005  0.1446\n",
      "     34            0.4222        1.3168       0.2292            0.2292        1.9229  0.0006  0.1280\n",
      "     35            0.5778        1.1957       0.2188            0.2188        1.9889  0.0006  0.1528\n",
      "     36            0.6889        0.9965       0.1979            0.1979        2.0204  0.0007  0.1269\n",
      "     37            0.6889        0.9509       0.1979            0.1979        2.0548  0.0007  0.1539\n",
      "     38            0.7333        0.7506       0.2188            0.2188        2.0894  0.0007  0.1318\n",
      "     39            0.7778        0.6507       0.2083            0.2083        2.1183  0.0007  0.1536\n",
      "     40            0.7778        0.6377       0.2188            0.2188        2.1406  0.0007  0.1253\n",
      "     41            0.7333        \u001b[32m0.6236\u001b[0m       0.2396            0.2396        2.1588  0.0007  0.1357\n",
      "     42            0.7333        \u001b[32m0.5025\u001b[0m       0.2188            0.2188        2.1853  0.0007  0.1513\n",
      "     43            0.7778        \u001b[32m0.5011\u001b[0m       0.2396            0.2396        2.2086  0.0006  0.1422\n",
      "     44            0.7778        \u001b[32m0.3587\u001b[0m       0.2500            0.2500        2.2274  0.0006  0.1747\n",
      "     45            \u001b[36m0.8444\u001b[0m        \u001b[32m0.3259\u001b[0m       0.2604            0.2604        2.2355  0.0005  0.1432\n",
      "     46            \u001b[36m0.8667\u001b[0m        0.3335       0.2604            0.2604        2.2354  0.0005  0.1473\n",
      "     47            0.8667        0.3284       0.2604            0.2604        2.2211  0.0004  0.1498\n",
      "     48            \u001b[36m0.9556\u001b[0m        \u001b[32m0.2739\u001b[0m       0.2604            0.2604        2.2085  0.0004  0.1482\n",
      "     49            0.9556        0.3159       0.2708            0.2708        2.1882  0.0003  0.1613\n",
      "     50            \u001b[36m0.9778\u001b[0m        \u001b[32m0.2206\u001b[0m       0.2604            0.2604        2.1690  0.0003  0.1498\n",
      "Fine tuning model for subject 2 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3556        1.8094       0.3125            0.3125        1.6827  0.0004  0.1378\n",
      "     32            0.4222        1.5815       0.3125            0.3125        1.6974  0.0005  0.1475\n",
      "     33            0.4444        1.4976       0.2917            0.2917        1.7267  0.0005  0.1333\n",
      "     34            0.4889        1.2975       0.2812            0.2812        1.7706  0.0006  0.1397\n",
      "     35            0.6000        1.1426       0.2812            0.2812        1.7805  0.0006  0.1348\n",
      "     36            0.6889        1.0399       0.2812            0.2812        1.7540  0.0007  0.1557\n",
      "     37            \u001b[36m0.8444\u001b[0m        0.8239       0.2917            0.2917        1.7304  0.0007  0.1516\n",
      "     38            0.8444        0.7877       0.3125            0.3125        1.7283  0.0007  0.1441\n",
      "     39            \u001b[36m0.8667\u001b[0m        \u001b[32m0.5490\u001b[0m       0.2812            0.2812        1.7305  0.0007  0.1495\n",
      "     40            \u001b[36m0.8889\u001b[0m        \u001b[32m0.4864\u001b[0m       0.3021            0.3021        1.7235  0.0007  0.1471\n",
      "     41            \u001b[36m0.9111\u001b[0m        0.5133       0.2917            0.2917        1.7130  0.0007  0.1511\n",
      "     42            0.8889        \u001b[32m0.4641\u001b[0m       0.2812            0.2812        1.7131  0.0007  0.1509\n",
      "     43            0.8889        \u001b[32m0.4199\u001b[0m       0.2708            0.2708        1.7213  0.0006  0.1351\n",
      "     44            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3783\u001b[0m       0.2917            0.2917        1.7359  0.0006  0.1519\n",
      "     45            0.9333        \u001b[32m0.3053\u001b[0m       0.2917            0.2917        1.7470  0.0005  0.1515\n",
      "     46            0.9333        \u001b[32m0.2526\u001b[0m       0.3021            0.3021        1.7569  0.0005  0.1511\n",
      "     47            \u001b[36m0.9778\u001b[0m        \u001b[32m0.2516\u001b[0m       0.3125            0.3125        1.7631  0.0004  0.1495\n",
      "     48            0.9778        0.2577       0.3021            0.3021        1.7680  0.0004  0.1359\n",
      "     49            0.9778        \u001b[32m0.2380\u001b[0m       0.3021            0.3021        1.7718  0.0003  0.1523\n",
      "     50            \u001b[36m1.0000\u001b[0m        0.2661       0.3021            0.3021        1.7752  0.0003  0.1281\n",
      "Fine tuning model for subject 2 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2222        2.5472       0.3333            0.3333        1.6856  0.0004  0.1388\n",
      "     32            0.3778        2.2024       0.2917            0.2917        1.7376  0.0005  0.1423\n",
      "     33            0.4222        1.8730       0.3125            0.3125        1.7668  0.0005  0.1504\n",
      "     34            0.5333        1.6826       0.3125            0.3125        1.7593  0.0006  0.1343\n",
      "     35            0.5778        1.3997       0.2917            0.2917        1.7491  0.0006  0.1754\n",
      "     36            0.6889        1.2248       0.2708            0.2708        1.7424  0.0007  0.1758\n",
      "     37            0.7556        1.0628       0.2708            0.2708        1.7567  0.0007  0.1553\n",
      "     38            0.7333        0.7905       0.2708            0.2708        1.8212  0.0007  0.1666\n",
      "     39            0.6889        0.7057       0.2604            0.2604        1.9386  0.0007  0.1526\n",
      "     40            0.6889        0.7161       0.2708            0.2708        2.0243  0.0007  0.1824\n",
      "     41            0.6889        \u001b[32m0.5296\u001b[0m       0.2708            0.2708        2.0834  0.0007  0.1692\n",
      "     42            0.7333        \u001b[32m0.5199\u001b[0m       0.3021            0.3021        2.1427  0.0007  0.1597\n",
      "     43            0.7333        \u001b[32m0.4603\u001b[0m       0.3125            0.3125        2.1591  0.0006  0.1652\n",
      "     44            0.7556        \u001b[32m0.4295\u001b[0m       0.3125            0.3125        2.1741  0.0006  0.1672\n",
      "     45            0.7778        \u001b[32m0.3385\u001b[0m       0.3125            0.3125        2.1683  0.0005  0.1565\n",
      "     46            0.8000        \u001b[32m0.3378\u001b[0m       0.3021            0.3021        2.1639  0.0005  0.1866\n",
      "     47            \u001b[36m0.8667\u001b[0m        \u001b[32m0.3171\u001b[0m       0.2812            0.2812        2.1500  0.0004  0.1513\n",
      "     48            \u001b[36m0.9111\u001b[0m        0.3199       0.2812            0.2812        2.1299  0.0004  0.2157\n",
      "     49            \u001b[36m0.9333\u001b[0m        0.3308       0.2917            0.2917        2.1063  0.0003  0.1576\n",
      "     50            \u001b[36m0.9556\u001b[0m        \u001b[32m0.3171\u001b[0m       0.2708            0.2708        2.0816  0.0003  0.2137\n",
      "Fine tuning model for subject 2 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2667        2.1959       0.3333            0.3333        1.7088  0.0004  0.1369\n",
      "     32            0.3333        2.0005       0.3125            0.3125        1.7324  0.0005  0.1396\n",
      "     33            0.3333        1.7856       0.3125            0.3125        1.7300  0.0005  0.1481\n",
      "     34            0.3778        1.3489       0.3021            0.3021        1.7714  0.0006  0.1300\n",
      "     35            0.4444        1.5601       0.2917            0.2917        1.8072  0.0006  0.1524\n",
      "     36            0.5556        1.1435       0.3125            0.3125        1.8280  0.0007  0.1335\n",
      "     37            0.6000        1.0089       0.2812            0.2812        1.8136  0.0007  0.1453\n",
      "     38            0.7333        0.8941       0.2708            0.2708        1.7935  0.0007  0.1475\n",
      "     39            0.8222        0.6624       0.2604            0.2604        1.7868  0.0007  0.1572\n",
      "     40            0.8222        0.6592       0.2917            0.2917        1.7984  0.0007  0.1487\n",
      "     41            0.7778        \u001b[32m0.6068\u001b[0m       0.2812            0.2812        1.8251  0.0007  0.1545\n",
      "     42            0.7333        \u001b[32m0.5198\u001b[0m       0.3021            0.3021        1.8653  0.0007  0.1421\n",
      "     43            0.7111        0.5491       0.2812            0.2812        1.9038  0.0006  0.1369\n",
      "     44            0.7111        \u001b[32m0.4938\u001b[0m       0.2812            0.2812        1.9344  0.0006  0.1507\n",
      "     45            0.8000        \u001b[32m0.4599\u001b[0m       0.2812            0.2812        1.9500  0.0005  0.1349\n",
      "     46            0.8222        \u001b[32m0.4427\u001b[0m       0.2708            0.2708        1.9558  0.0005  0.1357\n",
      "     47            \u001b[36m0.8889\u001b[0m        \u001b[32m0.3830\u001b[0m       0.2812            0.2812        1.9587  0.0004  0.1838\n",
      "     48            \u001b[36m0.9333\u001b[0m        0.4375       0.2708            0.2708        1.9583  0.0004  0.1419\n",
      "     49            \u001b[36m0.9556\u001b[0m        \u001b[32m0.3784\u001b[0m       0.2812            0.2812        1.9552  0.0003  0.1478\n",
      "     50            0.9556        \u001b[32m0.3461\u001b[0m       0.2708            0.2708        1.9510  0.0003  0.1452\n",
      "Fine tuning model for subject 2 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5556        1.4431       0.3438            0.3438        1.6806  0.0004  0.1436\n",
      "     32            0.5333        1.3715       0.3229            0.3229        1.6679  0.0005  0.1310\n",
      "     33            0.5333        1.2352       0.3125            0.3125        1.6712  0.0005  0.1490\n",
      "     34            0.6889        1.1440       0.2708            0.2708        1.7224  0.0006  0.1438\n",
      "     35            0.8222        0.9744       0.2500            0.2500        1.7991  0.0006  0.1349\n",
      "     36            \u001b[36m0.8444\u001b[0m        0.8423       0.2500            0.2500        1.8779  0.0007  0.1263\n",
      "     37            0.8444        0.6806       0.2396            0.2396        1.9474  0.0007  0.1518\n",
      "     38            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6186\u001b[0m       0.2292            0.2292        1.9862  0.0007  0.1449\n",
      "     39            \u001b[36m0.9111\u001b[0m        \u001b[32m0.5745\u001b[0m       0.2396            0.2396        2.0036  0.0007  0.1343\n",
      "     40            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3859\u001b[0m       0.2396            0.2396        2.0072  0.0007  0.1427\n",
      "     41            0.9111        0.4457       0.2500            0.2500        1.9995  0.0007  0.1442\n",
      "     42            \u001b[36m0.9556\u001b[0m        \u001b[32m0.3803\u001b[0m       0.2396            0.2396        1.9791  0.0007  0.1500\n",
      "     43            0.9556        0.4393       0.2396            0.2396        1.9665  0.0006  0.1474\n",
      "     44            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3544\u001b[0m       0.2604            0.2604        1.9656  0.0006  0.1554\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3064\u001b[0m       0.2604            0.2604        1.9757  0.0005  0.1499\n",
      "     46            1.0000        0.3811       0.2708            0.2708        1.9864  0.0005  0.1519\n",
      "     47            1.0000        0.3129       0.2708            0.2708        1.9971  0.0004  0.1351\n",
      "     48            1.0000        \u001b[32m0.2668\u001b[0m       0.2708            0.2708        2.0055  0.0004  0.1359\n",
      "     49            1.0000        \u001b[32m0.2265\u001b[0m       0.2604            0.2604        2.0163  0.0003  0.1677\n",
      "     50            1.0000        \u001b[32m0.2155\u001b[0m       0.2604            0.2604        2.0271  0.0003  0.1364\n",
      "Fine tuning model for subject 2 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.8647       0.3438            0.3438        1.6926  0.0004  0.1316\n",
      "     32            0.4000        1.5586       0.3542            0.3542        1.7017  0.0005  0.1428\n",
      "     33            0.5111        1.4044       0.3750            0.3750        1.7009  0.0005  0.1361\n",
      "     34            0.4889        1.4110       0.3646            0.3646        1.7109  0.0006  0.1398\n",
      "     35            0.6222        0.9577       0.3438            0.3438        1.7551  0.0006  0.1301\n",
      "     36            0.5778        0.9811       0.3021            0.3021        1.8316  0.0007  0.1415\n",
      "     37            0.5556        0.7786       0.2917            0.2917        1.8949  0.0007  0.1321\n",
      "     38            0.5778        0.7191       0.2812            0.2812        1.9123  0.0007  0.1412\n",
      "     39            0.5778        \u001b[32m0.6274\u001b[0m       0.2708            0.2708        1.9166  0.0007  0.1428\n",
      "     40            0.6444        \u001b[32m0.5600\u001b[0m       0.2500            0.2500        1.9165  0.0007  0.1545\n",
      "     41            0.7333        \u001b[32m0.5297\u001b[0m       0.2604            0.2604        1.9075  0.0007  0.1484\n",
      "     42            0.7556        \u001b[32m0.4410\u001b[0m       0.2500            0.2500        1.9004  0.0007  0.1573\n",
      "     43            0.7111        \u001b[32m0.3721\u001b[0m       0.2708            0.2708        1.8911  0.0006  0.1504\n",
      "     44            0.7111        0.4013       0.2604            0.2604        1.8723  0.0006  0.1367\n",
      "     45            \u001b[36m0.8444\u001b[0m        \u001b[32m0.3596\u001b[0m       0.2604            0.2604        1.8600  0.0005  0.1509\n",
      "     46            \u001b[36m0.8667\u001b[0m        \u001b[32m0.3217\u001b[0m       0.2500            0.2500        1.8496  0.0005  0.1340\n",
      "     47            \u001b[36m0.8889\u001b[0m        \u001b[32m0.2993\u001b[0m       0.2604            0.2604        1.8442  0.0004  0.1521\n",
      "     48            \u001b[36m0.9111\u001b[0m        0.3805       0.2604            0.2604        1.8424  0.0004  0.1530\n",
      "     49            \u001b[36m0.9333\u001b[0m        \u001b[32m0.2630\u001b[0m       0.2917            0.2917        1.8418  0.0003  0.1323\n",
      "     50            0.9333        \u001b[32m0.2435\u001b[0m       0.3021            0.3021        1.8422  0.0003  0.1365\n",
      "Fine tuning model for subject 2 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.5764       0.3438            0.3438        1.6642  0.0004  0.1550\n",
      "     32            0.4667        1.6519       0.3542            0.3542        1.6310  0.0005  0.1342\n",
      "     33            0.4889        1.4464       0.3229            0.3229        1.6298  0.0005  0.1368\n",
      "     34            0.4667        1.3870       0.3125            0.3125        1.6647  0.0006  0.1503\n",
      "     35            0.5556        1.0943       0.2917            0.2917        1.7195  0.0006  0.1262\n",
      "     36            0.5333        0.9452       0.2812            0.2812        1.7664  0.0007  0.1450\n",
      "     37            0.6889        0.7635       0.3125            0.3125        1.7862  0.0007  0.1424\n",
      "     38            0.6222        0.7676       0.2708            0.2708        1.8043  0.0007  0.1330\n",
      "     39            0.5556        \u001b[32m0.6069\u001b[0m       0.3021            0.3021        1.8098  0.0007  0.1427\n",
      "     40            0.5778        \u001b[32m0.5363\u001b[0m       0.2812            0.2812        1.8048  0.0007  0.1531\n",
      "     41            0.6222        \u001b[32m0.4575\u001b[0m       0.2917            0.2917        1.8022  0.0007  0.1672\n",
      "     42            0.7556        \u001b[32m0.4387\u001b[0m       0.3021            0.3021        1.7900  0.0007  0.1502\n",
      "     43            0.8000        \u001b[32m0.4128\u001b[0m       0.3125            0.3125        1.7694  0.0006  0.1905\n",
      "     44            \u001b[36m0.8667\u001b[0m        \u001b[32m0.3501\u001b[0m       0.3229            0.3229        1.7392  0.0006  0.1534\n",
      "     45            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3296\u001b[0m       0.3229            0.3229        1.7098  0.0005  0.1769\n",
      "     46            0.9333        \u001b[32m0.2372\u001b[0m       0.3229            0.3229        1.6863  0.0005  0.2018\n",
      "     47            0.9333        0.2416       0.3021            0.3021        1.6652  0.0004  0.1666\n",
      "     48            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1940\u001b[0m       0.3125            0.3125        1.6511  0.0004  0.1667\n",
      "     49            1.0000        0.2747       0.3229            0.3229        1.6416  0.0003  0.1683\n",
      "     50            1.0000        0.2058       0.3333            0.3333        1.6374  0.0003  0.1606\n",
      "Hold out data from subject 3\n",
      "Pre-training model with data from all subjects but subject 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3953\u001b[0m        \u001b[32m1.6115\u001b[0m       \u001b[35m0.3151\u001b[0m            \u001b[31m0.3151\u001b[0m        \u001b[94m1.4335\u001b[0m  0.0007  6.8700\n",
      "      2            \u001b[36m0.4573\u001b[0m        \u001b[32m1.4340\u001b[0m       \u001b[35m0.3698\u001b[0m            \u001b[31m0.3698\u001b[0m        \u001b[94m1.3322\u001b[0m  0.0007  7.0015\n",
      "      3            \u001b[36m0.5164\u001b[0m        \u001b[32m1.3537\u001b[0m       \u001b[35m0.4180\u001b[0m            \u001b[31m0.4180\u001b[0m        \u001b[94m1.2964\u001b[0m  0.0007  6.6769\n",
      "      4            \u001b[36m0.5341\u001b[0m        \u001b[32m1.2572\u001b[0m       0.4141            0.4141        \u001b[94m1.2761\u001b[0m  0.0007  7.3911\n",
      "      5            \u001b[36m0.5708\u001b[0m        \u001b[32m1.2038\u001b[0m       \u001b[35m0.4271\u001b[0m            \u001b[31m0.4271\u001b[0m        \u001b[94m1.2576\u001b[0m  0.0007  6.5629\n",
      "      6            \u001b[36m0.5885\u001b[0m        \u001b[32m1.1719\u001b[0m       0.4193            0.4193        1.2618  0.0006  7.0207\n",
      "      7            \u001b[36m0.5940\u001b[0m        \u001b[32m1.1224\u001b[0m       \u001b[35m0.4388\u001b[0m            \u001b[31m0.4388\u001b[0m        \u001b[94m1.2450\u001b[0m  0.0006  6.7583\n",
      "      8            \u001b[36m0.6331\u001b[0m        \u001b[32m1.0696\u001b[0m       \u001b[35m0.4609\u001b[0m            \u001b[31m0.4609\u001b[0m        \u001b[94m1.1948\u001b[0m  0.0006  6.6089\n",
      "      9            0.6279        \u001b[32m1.0541\u001b[0m       0.4557            0.4557        1.2206  0.0006  7.2022\n",
      "     10            \u001b[36m0.6464\u001b[0m        \u001b[32m1.0198\u001b[0m       \u001b[35m0.4818\u001b[0m            \u001b[31m0.4818\u001b[0m        1.2030  0.0005  6.5629\n",
      "     11            \u001b[36m0.6591\u001b[0m        \u001b[32m0.9695\u001b[0m       0.4792            0.4792        1.2036  0.0005  7.2814\n",
      "     12            \u001b[36m0.6909\u001b[0m        0.9833       0.4740            0.4740        1.2126  0.0005  6.5698\n",
      "     13            \u001b[36m0.7021\u001b[0m        \u001b[32m0.9448\u001b[0m       \u001b[35m0.5039\u001b[0m            \u001b[31m0.5039\u001b[0m        \u001b[94m1.1751\u001b[0m  0.0004  7.0325\n",
      "     14            0.7016        \u001b[32m0.9223\u001b[0m       0.4896            0.4896        1.1907  0.0004  7.0177\n",
      "     15            0.6919        \u001b[32m0.9109\u001b[0m       0.4909            0.4909        1.1913  0.0004  6.5283\n",
      "     16            \u001b[36m0.7448\u001b[0m        \u001b[32m0.8895\u001b[0m       \u001b[35m0.5052\u001b[0m            \u001b[31m0.5052\u001b[0m        \u001b[94m1.1631\u001b[0m  0.0003  7.3752\n",
      "     17            \u001b[36m0.7534\u001b[0m        \u001b[32m0.8417\u001b[0m       0.5039            0.5039        1.1646  0.0003  6.5907\n",
      "     18            \u001b[36m0.7648\u001b[0m        \u001b[32m0.8296\u001b[0m       0.4961            0.4961        1.1689  0.0003  7.0220\n",
      "     19            0.7615        \u001b[32m0.8252\u001b[0m       \u001b[35m0.5078\u001b[0m            \u001b[31m0.5078\u001b[0m        1.1640  0.0002  6.9291\n",
      "     20            \u001b[36m0.7716\u001b[0m        \u001b[32m0.7987\u001b[0m       0.5078            0.5078        \u001b[94m1.1544\u001b[0m  0.0002  6.8349\n",
      "     21            \u001b[36m0.7831\u001b[0m        \u001b[32m0.7778\u001b[0m       \u001b[35m0.5130\u001b[0m            \u001b[31m0.5130\u001b[0m        \u001b[94m1.1401\u001b[0m  0.0002  8.9267\n",
      "     22            0.7799        \u001b[32m0.7744\u001b[0m       0.5065            0.5065        1.1441  0.0001  7.2078\n",
      "     23            0.7831        \u001b[32m0.7618\u001b[0m       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        1.1424  0.0001  9.0013\n",
      "     24            \u001b[36m0.7909\u001b[0m        \u001b[32m0.7609\u001b[0m       0.5143            0.5143        1.1412  0.0001  8.2496\n",
      "     25            \u001b[36m0.7958\u001b[0m        \u001b[32m0.7365\u001b[0m       0.5182            0.5182        \u001b[94m1.1347\u001b[0m  0.0001  7.2877\n",
      "     26            \u001b[36m0.7969\u001b[0m        \u001b[32m0.7344\u001b[0m       0.5143            0.5143        1.1363  0.0000  7.4610\n",
      "     27            \u001b[36m0.7982\u001b[0m        0.7442       0.5156            0.5156        \u001b[94m1.1332\u001b[0m  0.0000  7.8001\n",
      "     28            0.7977        0.7392       0.5156            0.5156        1.1336  0.0000  6.7535\n",
      "     29            0.7979        \u001b[32m0.7318\u001b[0m       0.5143            0.5143        1.1337  0.0000  10.3228\n",
      "     30            0.7979        \u001b[32m0.7245\u001b[0m       0.5130            0.5130        1.1333  0.0000  9.9845\n",
      "Before finetuning for subject 3, the baseline accuracy is 0.6458333333333334\n",
      "Fine tuning model for subject 3 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m1.0000\u001b[0m        1.0992       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8801\u001b[0m  0.0004  0.1143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            1.0000        \u001b[32m0.4135\u001b[0m       \u001b[35m0.6771\u001b[0m            \u001b[31m0.6771\u001b[0m        0.8943  0.0005  0.1344\n",
      "     33            1.0000        \u001b[32m0.1999\u001b[0m       0.6250            0.6250        0.9346  0.0005  0.1161\n",
      "     34            1.0000        \u001b[32m0.1232\u001b[0m       0.5521            0.5521        0.9965  0.0006  0.1361\n",
      "     35            1.0000        \u001b[32m0.0418\u001b[0m       0.5208            0.5208        1.0660  0.0006  0.1349\n",
      "     36            1.0000        \u001b[32m0.0380\u001b[0m       0.4896            0.4896        1.1413  0.0007  0.1266\n",
      "     37            1.0000        \u001b[32m0.0317\u001b[0m       0.4792            0.4792        1.2195  0.0007  0.1132\n",
      "     38            1.0000        \u001b[32m0.0287\u001b[0m       0.4375            0.4375        1.2992  0.0007  0.1171\n",
      "     39            1.0000        \u001b[32m0.0172\u001b[0m       0.4479            0.4479        1.3769  0.0007  0.1196\n",
      "     40            1.0000        \u001b[32m0.0119\u001b[0m       0.4271            0.4271        1.4486  0.0007  0.1249\n",
      "     41            1.0000        \u001b[32m0.0022\u001b[0m       0.3958            0.3958        1.5102  0.0007  0.1172\n",
      "     42            1.0000        0.0034       0.3854            0.3854        1.5593  0.0007  0.1041\n",
      "     43            1.0000        0.0119       0.3854            0.3854        1.5953  0.0006  0.1169\n",
      "     44            1.0000        0.0027       0.3854            0.3854        1.6183  0.0006  0.1209\n",
      "     45            1.0000        0.0081       0.3854            0.3854        1.6305  0.0005  0.1443\n",
      "     46            1.0000        0.0047       0.3854            0.3854        1.6333  0.0005  0.1092\n",
      "     47            1.0000        0.0022       0.3958            0.3958        1.6294  0.0004  0.1169\n",
      "     48            1.0000        0.0052       0.3854            0.3854        1.6212  0.0004  0.1197\n",
      "     49            1.0000        \u001b[32m0.0018\u001b[0m       0.3854            0.3854        1.6107  0.0003  0.1196\n",
      "     50            1.0000        0.0081       0.3854            0.3854        1.5994  0.0003  0.1030\n",
      "Fine tuning model for subject 3 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.2360       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8903\u001b[0m  0.0004  0.1114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        1.0816       0.6042            0.6042        0.9190  0.0005  0.1327\n",
      "     33            0.8000        \u001b[32m0.4535\u001b[0m       0.5625            0.5625        0.9848  0.0005  0.1614\n",
      "     34            0.8000        \u001b[32m0.2329\u001b[0m       0.5000            0.5000        1.0908  0.0006  0.1511\n",
      "     35            0.8000        \u001b[32m0.0630\u001b[0m       0.4479            0.4479        1.2202  0.0006  0.1472\n",
      "     36            0.8000        \u001b[32m0.0137\u001b[0m       0.4062            0.4062        1.3552  0.0007  0.1500\n",
      "     37            0.8000        0.0160       0.3750            0.3750        1.4776  0.0007  0.1500\n",
      "     38            \u001b[36m1.0000\u001b[0m        0.0221       0.3750            0.3750        1.5794  0.0007  0.1327\n",
      "     39            1.0000        \u001b[32m0.0115\u001b[0m       0.3646            0.3646        1.6580  0.0007  0.1918\n",
      "     40            1.0000        \u001b[32m0.0111\u001b[0m       0.3333            0.3333        1.7143  0.0007  0.1472\n",
      "     41            1.0000        0.0120       0.3229            0.3229        1.7520  0.0007  0.1530\n",
      "     42            1.0000        \u001b[32m0.0036\u001b[0m       0.3333            0.3333        1.7741  0.0007  0.1324\n",
      "     43            1.0000        0.0125       0.3333            0.3333        1.7814  0.0006  0.1510\n",
      "     44            1.0000        0.0051       0.3438            0.3438        1.7782  0.0006  0.1501\n",
      "     45            1.0000        \u001b[32m0.0034\u001b[0m       0.3021            0.3021        1.7674  0.0005  0.1667\n",
      "     46            1.0000        0.0048       0.3229            0.3229        1.7520  0.0005  0.1516\n",
      "     47            1.0000        0.0043       0.3229            0.3229        1.7350  0.0004  0.1688\n",
      "     48            1.0000        0.0042       0.3125            0.3125        1.7189  0.0004  0.1350\n",
      "     49            1.0000        \u001b[32m0.0027\u001b[0m       0.3438            0.3438        1.7058  0.0003  0.1186\n",
      "     50            1.0000        0.0032       0.3542            0.3542        1.6969  0.0003  0.1193\n",
      "Fine tuning model for subject 3 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        0.9258       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.9006\u001b[0m  0.0004  0.1289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        0.7741       0.6146            0.6146        0.9436  0.0005  0.1168\n",
      "     33            1.0000        \u001b[32m0.3289\u001b[0m       0.5833            0.5833        0.9858  0.0005  0.1146\n",
      "     34            1.0000        \u001b[32m0.0890\u001b[0m       0.5729            0.5729        1.0216  0.0006  0.1208\n",
      "     35            1.0000        \u001b[32m0.0183\u001b[0m       0.5312            0.5312        1.0509  0.0006  0.1387\n",
      "     36            1.0000        \u001b[32m0.0139\u001b[0m       0.5417            0.5417        1.0818  0.0007  0.1009\n",
      "     37            1.0000        \u001b[32m0.0045\u001b[0m       0.5208            0.5208        1.1219  0.0007  0.1090\n",
      "     38            1.0000        0.0097       0.5521            0.5521        1.1750  0.0007  0.1181\n",
      "     39            1.0000        0.0117       0.5208            0.5208        1.2384  0.0007  0.1510\n",
      "     40            1.0000        0.0072       0.5208            0.5208        1.3066  0.0007  0.1332\n",
      "     41            1.0000        0.0160       0.5000            0.5000        1.3698  0.0007  0.1050\n",
      "     42            1.0000        0.0049       0.5000            0.5000        1.4282  0.0007  0.1129\n",
      "     43            1.0000        0.0047       0.5000            0.5000        1.4787  0.0006  0.1170\n",
      "     44            1.0000        \u001b[32m0.0033\u001b[0m       0.4792            0.4792        1.5211  0.0006  0.1198\n",
      "     45            1.0000        0.0112       0.4896            0.4896        1.5520  0.0005  0.1268\n",
      "     46            1.0000        0.0072       0.4792            0.4792        1.5734  0.0005  0.1111\n",
      "     47            1.0000        0.0042       0.4688            0.4688        1.5880  0.0004  0.1173\n",
      "     48            1.0000        0.0038       0.4688            0.4688        1.5970  0.0004  0.1174\n",
      "     49            1.0000        \u001b[32m0.0032\u001b[0m       0.4688            0.4688        1.6018  0.0003  0.1214\n",
      "     50            1.0000        \u001b[32m0.0027\u001b[0m       0.4583            0.4583        1.6036  0.0003  0.1375\n",
      "Fine tuning model for subject 3 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.4242       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9276\u001b[0m  0.0004  0.1221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        1.4080       0.5938            0.5938        1.0382  0.0005  0.1313\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5995\u001b[0m       0.5417            0.5417        1.2103  0.0005  0.1313\n",
      "     34            1.0000        \u001b[32m0.2267\u001b[0m       0.4583            0.4583        1.3921  0.0006  0.1207\n",
      "     35            1.0000        \u001b[32m0.0770\u001b[0m       0.4375            0.4375        1.5394  0.0006  0.1313\n",
      "     36            1.0000        \u001b[32m0.0404\u001b[0m       0.4062            0.4062        1.6443  0.0007  0.1235\n",
      "     37            1.0000        \u001b[32m0.0258\u001b[0m       0.3854            0.3854        1.7185  0.0007  0.1076\n",
      "     38            1.0000        0.0265       0.3750            0.3750        1.7776  0.0007  0.1169\n",
      "     39            1.0000        \u001b[32m0.0244\u001b[0m       0.3646            0.3646        1.8305  0.0007  0.1043\n",
      "     40            1.0000        \u001b[32m0.0181\u001b[0m       0.3646            0.3646        1.8772  0.0007  0.1273\n",
      "     41            1.0000        0.0228       0.3750            0.3750        1.9145  0.0007  0.1099\n",
      "     42            1.0000        \u001b[32m0.0104\u001b[0m       0.4062            0.4062        1.9418  0.0007  0.1181\n",
      "     43            1.0000        0.0143       0.3854            0.3854        1.9579  0.0006  0.1170\n",
      "     44            1.0000        \u001b[32m0.0099\u001b[0m       0.3750            0.3750        1.9630  0.0006  0.1366\n",
      "     45            1.0000        0.0129       0.3750            0.3750        1.9578  0.0005  0.1316\n",
      "     46            1.0000        \u001b[32m0.0066\u001b[0m       0.3646            0.3646        1.9437  0.0005  0.1060\n",
      "     47            1.0000        0.0093       0.3646            0.3646        1.9231  0.0004  0.1408\n",
      "     48            1.0000        0.0083       0.3646            0.3646        1.8984  0.0004  0.1164\n",
      "     49            1.0000        0.0195       0.3646            0.3646        1.8711  0.0003  0.1336\n",
      "     50            1.0000        0.0103       0.3646            0.3646        1.8434  0.0003  0.1198\n",
      "Fine tuning model for subject 3 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        0.8169       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.8774\u001b[0m  0.0004  0.1162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        0.8884       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        0.8804  0.0005  0.1314\n",
      "     33            1.0000        \u001b[32m0.3343\u001b[0m       0.6354            0.6354        0.9155  0.0005  0.1166\n",
      "     34            1.0000        \u001b[32m0.2623\u001b[0m       0.5729            0.5729        0.9755  0.0006  0.1197\n",
      "     35            1.0000        \u001b[32m0.0389\u001b[0m       0.4896            0.4896        1.0440  0.0006  0.1298\n",
      "     36            1.0000        0.0565       0.4896            0.4896        1.1085  0.0007  0.1242\n",
      "     37            1.0000        \u001b[32m0.0217\u001b[0m       0.4896            0.4896        1.1587  0.0007  0.1057\n",
      "     38            1.0000        \u001b[32m0.0033\u001b[0m       0.4688            0.4688        1.1931  0.0007  0.1160\n",
      "     39            1.0000        0.0045       0.4792            0.4792        1.2169  0.0007  0.1208\n",
      "     40            1.0000        0.0059       0.4479            0.4479        1.2360  0.0007  0.1209\n",
      "     41            1.0000        0.0055       0.4479            0.4479        1.2535  0.0007  0.1020\n",
      "     42            1.0000        0.0054       0.4688            0.4688        1.2708  0.0007  0.1161\n",
      "     43            1.0000        0.0052       0.4583            0.4583        1.2874  0.0006  0.1208\n",
      "     44            1.0000        0.0034       0.4375            0.4375        1.3034  0.0006  0.1271\n",
      "     45            1.0000        \u001b[32m0.0019\u001b[0m       0.4375            0.4375        1.3187  0.0005  0.1109\n",
      "     46            1.0000        0.0023       0.4375            0.4375        1.3333  0.0005  0.1333\n",
      "     47            1.0000        0.0039       0.4271            0.4271        1.3475  0.0004  0.1337\n",
      "     48            1.0000        0.0025       0.4062            0.4062        1.3611  0.0004  0.1198\n",
      "     49            1.0000        \u001b[32m0.0013\u001b[0m       0.3958            0.3958        1.3742  0.0003  0.1237\n",
      "     50            1.0000        0.0028       0.4167            0.4167        1.3868  0.0003  0.1141\n",
      "Fine tuning model for subject 3 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m1.0000\u001b[0m        1.7045       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.9122\u001b[0m  0.0004  0.1311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            1.0000        \u001b[32m0.6671\u001b[0m       0.5729            0.5729        1.0249  0.0005  0.1338\n",
      "     33            1.0000        \u001b[32m0.1631\u001b[0m       0.4896            0.4896        1.2422  0.0005  0.1136\n",
      "     34            1.0000        \u001b[32m0.0251\u001b[0m       0.4375            0.4375        1.4812  0.0006  0.1279\n",
      "     35            1.0000        \u001b[32m0.0106\u001b[0m       0.4062            0.4062        1.6617  0.0006  0.1166\n",
      "     36            1.0000        \u001b[32m0.0060\u001b[0m       0.4167            0.4167        1.7620  0.0007  0.1052\n",
      "     37            1.0000        0.0068       0.4271            0.4271        1.7990  0.0007  0.1130\n",
      "     38            1.0000        \u001b[32m0.0039\u001b[0m       0.4271            0.4271        1.7976  0.0007  0.1169\n",
      "     39            1.0000        \u001b[32m0.0030\u001b[0m       0.4167            0.4167        1.7779  0.0007  0.1199\n",
      "     40            1.0000        0.0041       0.3958            0.3958        1.7531  0.0007  0.1200\n",
      "     41            1.0000        0.0032       0.4167            0.4167        1.7311  0.0007  0.1021\n",
      "     42            1.0000        \u001b[32m0.0024\u001b[0m       0.4062            0.4062        1.7145  0.0007  0.1200\n",
      "     43            1.0000        0.0028       0.4062            0.4062        1.7039  0.0006  0.1167\n",
      "     44            1.0000        0.0027       0.3958            0.3958        1.6985  0.0006  0.1309\n",
      "     45            1.0000        0.0029       0.3958            0.3958        1.6971  0.0005  0.1080\n",
      "     46            1.0000        0.0078       0.3958            0.3958        1.6982  0.0005  0.1157\n",
      "     47            1.0000        0.0032       0.3958            0.3958        1.7008  0.0004  0.1170\n",
      "     48            1.0000        0.0050       0.3750            0.3750        1.7042  0.0004  0.1280\n",
      "     49            1.0000        0.0037       0.3854            0.3854        1.7079  0.0003  0.1179\n",
      "     50            1.0000        \u001b[32m0.0021\u001b[0m       0.3750            0.3750        1.7115  0.0003  0.1049\n",
      "Fine tuning model for subject 3 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        \u001b[32m0.7093\u001b[0m       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8746\u001b[0m  0.0004  0.1133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        0.7226       \u001b[35m0.6771\u001b[0m            \u001b[31m0.6771\u001b[0m        \u001b[94m0.8586\u001b[0m  0.0005  0.1326\n",
      "     33            1.0000        \u001b[32m0.3338\u001b[0m       0.6667            0.6667        \u001b[94m0.8548\u001b[0m  0.0005  0.1157\n",
      "     34            1.0000        \u001b[32m0.2116\u001b[0m       0.6458            0.6458        0.8765  0.0006  0.1192\n",
      "     35            1.0000        \u001b[32m0.0640\u001b[0m       0.6354            0.6354        0.9255  0.0006  0.1186\n",
      "     36            1.0000        \u001b[32m0.0426\u001b[0m       0.6250            0.6250        0.9934  0.0007  0.1051\n",
      "     37            1.0000        \u001b[32m0.0348\u001b[0m       0.6146            0.6146        1.0675  0.0007  0.1153\n",
      "     38            1.0000        \u001b[32m0.0177\u001b[0m       0.5417            0.5417        1.1423  0.0007  0.1192\n",
      "     39            1.0000        0.0283       0.5312            0.5312        1.2010  0.0007  0.1198\n",
      "     40            1.0000        \u001b[32m0.0123\u001b[0m       0.5312            0.5312        1.2479  0.0007  0.1511\n",
      "     41            1.0000        0.1197       0.5417            0.5417        1.2494  0.0007  0.1306\n",
      "     42            1.0000        \u001b[32m0.0028\u001b[0m       0.5625            0.5625        1.2519  0.0007  0.1069\n",
      "     43            1.0000        0.0075       0.5417            0.5417        1.2536  0.0006  0.1186\n",
      "     44            1.0000        0.0092       0.5208            0.5208        1.2549  0.0006  0.1183\n",
      "     45            1.0000        0.0050       0.5521            0.5521        1.2560  0.0005  0.1261\n",
      "     46            1.0000        0.0031       0.5417            0.5417        1.2566  0.0005  0.1143\n",
      "     47            1.0000        0.0045       0.5417            0.5417        1.2564  0.0004  0.1159\n",
      "     48            1.0000        \u001b[32m0.0019\u001b[0m       0.5417            0.5417        1.2555  0.0004  0.1194\n",
      "     49            1.0000        0.0040       0.5417            0.5417        1.2538  0.0003  0.1193\n",
      "     50            1.0000        \u001b[32m0.0014\u001b[0m       0.5417            0.5417        1.2515  0.0003  0.1345\n",
      "Fine tuning model for subject 3 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.4977       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8914\u001b[0m  0.0004  0.1583\n",
      "     32            \u001b[36m1.0000\u001b[0m        1.0587       0.6146            0.6146        0.9164  0.0005  0.1396\n",
      "     33            1.0000        0.9743       0.5833            0.5833        1.0095  0.0005  0.1490\n",
      "     34            1.0000        \u001b[32m0.1474\u001b[0m       0.5104            0.5104        1.1540  0.0006  0.1511\n",
      "     35            1.0000        \u001b[32m0.0498\u001b[0m       0.4479            0.4479        1.3068  0.0006  0.1354\n",
      "     36            1.0000        \u001b[32m0.0204\u001b[0m       0.4062            0.4062        1.4433  0.0007  0.1354\n",
      "     37            1.0000        0.0204       0.3958            0.3958        1.5535  0.0007  0.1499\n",
      "     38            1.0000        0.0352       0.4062            0.4062        1.6388  0.0007  0.1381\n",
      "     39            1.0000        \u001b[32m0.0161\u001b[0m       0.4479            0.4479        1.7084  0.0007  0.1473\n",
      "     40            1.0000        0.0205       0.4688            0.4688        1.7660  0.0007  0.1512\n",
      "     41            1.0000        \u001b[32m0.0089\u001b[0m       0.4688            0.4688        1.8146  0.0007  0.1354\n",
      "     42            1.0000        0.0113       0.4688            0.4688        1.8537  0.0007  0.1500\n",
      "     43            1.0000        \u001b[32m0.0048\u001b[0m       0.4688            0.4688        1.8835  0.0006  0.1511\n",
      "     44            1.0000        0.0051       0.4583            0.4583        1.9038  0.0006  0.1563\n",
      "     45            1.0000        0.0112       0.4479            0.4479        1.9155  0.0005  0.1308\n",
      "     46            1.0000        0.0124       0.4479            0.4479        1.9202  0.0005  0.1186\n",
      "     47            1.0000        0.0058       0.4479            0.4479        1.9204  0.0004  0.1209\n",
      "     48            1.0000        0.0053       0.4375            0.4375        1.9180  0.0004  0.1276\n",
      "     49            1.0000        0.0049       0.4062            0.4062        1.9146  0.0003  0.1237\n",
      "     50            1.0000        0.0097       0.3958            0.3958        1.9111  0.0003  0.1210\n",
      "Fine tuning model for subject 3 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m1.0000\u001b[0m        0.7482       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8866\u001b[0m  0.0004  0.1042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            1.0000        \u001b[32m0.5869\u001b[0m       0.6354            0.6354        0.9183  0.0005  0.1099\n",
      "     33            1.0000        \u001b[32m0.3541\u001b[0m       0.5729            0.5729        0.9681  0.0005  0.1162\n",
      "     34            1.0000        \u001b[32m0.1578\u001b[0m       0.5521            0.5521        1.0344  0.0006  0.1198\n",
      "     35            1.0000        \u001b[32m0.0325\u001b[0m       0.5417            0.5417        1.1069  0.0006  0.1277\n",
      "     36            1.0000        0.0381       0.5208            0.5208        1.1835  0.0007  0.1111\n",
      "     37            1.0000        0.0614       0.5000            0.5000        1.2531  0.0007  0.1142\n",
      "     38            1.0000        \u001b[32m0.0177\u001b[0m       0.4479            0.4479        1.3201  0.0007  0.1482\n",
      "     39            1.0000        0.0344       0.4167            0.4167        1.3745  0.0007  0.1187\n",
      "     40            1.0000        0.0312       0.4167            0.4167        1.4151  0.0007  0.1354\n",
      "     41            1.0000        \u001b[32m0.0112\u001b[0m       0.3958            0.3958        1.4471  0.0007  0.1338\n",
      "     42            1.0000        0.0128       0.3854            0.3854        1.4696  0.0007  0.1051\n",
      "     43            1.0000        \u001b[32m0.0100\u001b[0m       0.3646            0.3646        1.4827  0.0006  0.1128\n",
      "     44            1.0000        \u001b[32m0.0092\u001b[0m       0.3646            0.3646        1.4882  0.0006  0.1014\n",
      "     45            1.0000        \u001b[32m0.0066\u001b[0m       0.3646            0.3646        1.4866  0.0005  0.1198\n",
      "     46            1.0000        \u001b[32m0.0035\u001b[0m       0.3958            0.3958        1.4805  0.0005  0.1210\n",
      "     47            1.0000        \u001b[32m0.0026\u001b[0m       0.3958            0.3958        1.4710  0.0004  0.1164\n",
      "     48            1.0000        0.0051       0.3854            0.3854        1.4590  0.0004  0.1199\n",
      "     49            1.0000        0.0042       0.4062            0.4062        1.4461  0.0003  0.1025\n",
      "     50            1.0000        0.0038       0.4167            0.4167        1.4332  0.0003  0.1277\n",
      "Fine tuning model for subject 3 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        \u001b[32m0.6734\u001b[0m       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8934\u001b[0m  0.0004  0.1190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        1.1528       0.6146            0.6146        0.9305  0.0005  0.1243\n",
      "     33            1.0000        \u001b[32m0.4086\u001b[0m       0.5312            0.5312        1.0823  0.0005  0.1135\n",
      "     34            1.0000        \u001b[32m0.0950\u001b[0m       0.4375            0.4375        1.3295  0.0006  0.1208\n",
      "     35            1.0000        \u001b[32m0.0481\u001b[0m       0.4062            0.4062        1.6187  0.0006  0.1286\n",
      "     36            1.0000        \u001b[32m0.0143\u001b[0m       0.3854            0.3854        1.8937  0.0007  0.1259\n",
      "     37            1.0000        \u001b[32m0.0100\u001b[0m       0.3542            0.3542        2.1285  0.0007  0.1059\n",
      "     38            1.0000        0.0131       0.3438            0.3438        2.3149  0.0007  0.1170\n",
      "     39            1.0000        \u001b[32m0.0040\u001b[0m       0.3438            0.3438        2.4513  0.0007  0.1199\n",
      "     40            1.0000        0.0043       0.3438            0.3438        2.5429  0.0007  0.1220\n",
      "     41            1.0000        0.0043       0.3542            0.3542        2.5963  0.0007  0.1151\n",
      "     42            1.0000        \u001b[32m0.0022\u001b[0m       0.3542            0.3542        2.6187  0.0007  0.1196\n",
      "     43            1.0000        0.0025       0.3542            0.3542        2.6166  0.0006  0.1177\n",
      "     44            1.0000        0.0030       0.3542            0.3542        2.5958  0.0006  0.1299\n",
      "     45            1.0000        0.0033       0.3333            0.3333        2.5611  0.0005  0.1073\n",
      "     46            1.0000        0.0031       0.3646            0.3646        2.5168  0.0005  0.1154\n",
      "     47            1.0000        \u001b[32m0.0018\u001b[0m       0.3542            0.3542        2.4664  0.0004  0.1176\n",
      "     48            1.0000        \u001b[32m0.0015\u001b[0m       0.3542            0.3542        2.4126  0.0004  0.1203\n",
      "     49            1.0000        0.0038       0.3646            0.3646        2.3578  0.0003  0.1199\n",
      "     50            1.0000        0.0027       0.3750            0.3750        2.3038  0.0003  0.1040\n",
      "Fine tuning model for subject 3 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        1.0245       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.9196\u001b[0m  0.0004  0.1043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        0.7918       0.5625            0.5625        1.0187  0.0005  0.0989\n",
      "     33            0.8000        \u001b[32m0.6284\u001b[0m       0.5312            0.5312        1.1655  0.0005  0.0991\n",
      "     34            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3058\u001b[0m       0.5104            0.5104        1.2812  0.0006  0.1198\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2208\u001b[0m       0.5104            0.5104        1.3208  0.0006  0.0987\n",
      "     36            1.0000        \u001b[32m0.1623\u001b[0m       0.5208            0.5208        1.3038  0.0007  0.0906\n",
      "     37            1.0000        \u001b[32m0.1008\u001b[0m       0.5417            0.5417        1.2631  0.0007  0.1014\n",
      "     38            1.0000        \u001b[32m0.0415\u001b[0m       0.5417            0.5417        1.2267  0.0007  0.1132\n",
      "     39            1.0000        0.0623       0.5208            0.5208        1.2058  0.0007  0.0997\n",
      "     40            1.0000        \u001b[32m0.0369\u001b[0m       0.5104            0.5104        1.1983  0.0007  0.0970\n",
      "     41            1.0000        0.0398       0.5000            0.5000        1.2019  0.0007  0.1086\n",
      "     42            1.0000        \u001b[32m0.0164\u001b[0m       0.5000            0.5000        1.2100  0.0007  0.0836\n",
      "     43            1.0000        0.0182       0.5104            0.5104        1.2197  0.0006  0.0993\n",
      "     44            1.0000        0.0257       0.5104            0.5104        1.2270  0.0006  0.1142\n",
      "     45            1.0000        0.0182       0.5104            0.5104        1.2317  0.0005  0.0998\n",
      "     46            1.0000        \u001b[32m0.0084\u001b[0m       0.5000            0.5000        1.2345  0.0005  0.0939\n",
      "     47            1.0000        0.0147       0.5104            0.5104        1.2358  0.0004  0.1001\n",
      "     48            1.0000        0.0114       0.5104            0.5104        1.2363  0.0004  0.1179\n",
      "     49            1.0000        0.0162       0.5000            0.5000        1.2369  0.0003  0.0974\n",
      "     50            1.0000        0.0223       0.5000            0.5000        1.2380  0.0003  0.1025\n",
      "Fine tuning model for subject 3 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        0.7306       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.8854\u001b[0m  0.0004  0.1092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        \u001b[32m0.5099\u001b[0m       0.6354            0.6354        0.8916  0.0005  0.1392\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4573\u001b[0m       0.6146            0.6146        0.9104  0.0005  0.0979\n",
      "     34            1.0000        \u001b[32m0.2302\u001b[0m       0.6042            0.6042        0.9453  0.0006  0.1182\n",
      "     35            1.0000        \u001b[32m0.1812\u001b[0m       0.5521            0.5521        0.9882  0.0006  0.0987\n",
      "     36            1.0000        \u001b[32m0.0791\u001b[0m       0.5000            0.5000        1.0350  0.0007  0.0904\n",
      "     37            1.0000        \u001b[32m0.0272\u001b[0m       0.4896            0.4896        1.0799  0.0007  0.1024\n",
      "     38            1.0000        0.0861       0.4583            0.4583        1.1176  0.0007  0.1060\n",
      "     39            1.0000        0.0399       0.4583            0.4583        1.1513  0.0007  0.0976\n",
      "     40            1.0000        0.0486       0.4583            0.4583        1.1812  0.0007  0.1168\n",
      "     41            1.0000        \u001b[32m0.0143\u001b[0m       0.4479            0.4479        1.2088  0.0007  0.1190\n",
      "     42            1.0000        0.0213       0.4375            0.4375        1.2322  0.0007  0.0856\n",
      "     43            1.0000        \u001b[32m0.0085\u001b[0m       0.4375            0.4375        1.2524  0.0006  0.1122\n",
      "     44            1.0000        0.0096       0.4479            0.4479        1.2683  0.0006  0.1043\n",
      "     45            1.0000        \u001b[32m0.0057\u001b[0m       0.4375            0.4375        1.2804  0.0005  0.0834\n",
      "     46            1.0000        0.0079       0.4271            0.4271        1.2889  0.0005  0.1149\n",
      "     47            1.0000        0.0067       0.4271            0.4271        1.2944  0.0004  0.1031\n",
      "     48            1.0000        0.0103       0.4375            0.4375        1.2977  0.0004  0.1089\n",
      "     49            1.0000        0.0083       0.4479            0.4479        1.2993  0.0003  0.0965\n",
      "     50            1.0000        0.0107       0.4688            0.4688        1.2999  0.0003  0.1145\n",
      "Fine tuning model for subject 3 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        1.0426       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8838\u001b[0m  0.0004  0.0885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        0.9083       0.6250            0.6250        0.9170  0.0005  0.0948\n",
      "     33            1.0000        \u001b[32m0.5267\u001b[0m       0.5729            0.5729        1.0061  0.0005  0.1011\n",
      "     34            1.0000        \u001b[32m0.2399\u001b[0m       0.5312            0.5312        1.1640  0.0006  0.1084\n",
      "     35            1.0000        \u001b[32m0.1657\u001b[0m       0.4896            0.4896        1.3557  0.0006  0.0986\n",
      "     36            1.0000        \u001b[32m0.1104\u001b[0m       0.4375            0.4375        1.5418  0.0007  0.0993\n",
      "     37            1.0000        \u001b[32m0.0625\u001b[0m       0.4271            0.4271        1.7009  0.0007  0.1133\n",
      "     38            1.0000        \u001b[32m0.0606\u001b[0m       0.4271            0.4271        1.8317  0.0007  0.0986\n",
      "     39            1.0000        \u001b[32m0.0327\u001b[0m       0.4167            0.4167        1.9397  0.0007  0.0983\n",
      "     40            1.0000        0.0817       0.4062            0.4062        2.0066  0.0007  0.1047\n",
      "     41            1.0000        \u001b[32m0.0285\u001b[0m       0.4062            0.4062        2.0557  0.0007  0.0991\n",
      "     42            1.0000        \u001b[32m0.0269\u001b[0m       0.3958            0.3958        2.0915  0.0007  0.0890\n",
      "     43            1.0000        \u001b[32m0.0133\u001b[0m       0.3542            0.3542        2.1189  0.0006  0.1024\n",
      "     44            1.0000        0.0204       0.3646            0.3646        2.1388  0.0006  0.1090\n",
      "     45            1.0000        \u001b[32m0.0127\u001b[0m       0.3542            0.3542        2.1515  0.0005  0.0943\n",
      "     46            1.0000        \u001b[32m0.0098\u001b[0m       0.3958            0.3958        2.1583  0.0005  0.1013\n",
      "     47            1.0000        0.0099       0.3854            0.3854        2.1604  0.0004  0.1085\n",
      "     48            1.0000        \u001b[32m0.0057\u001b[0m       0.3958            0.3958        2.1588  0.0004  0.0973\n",
      "     49            1.0000        \u001b[32m0.0049\u001b[0m       0.3958            0.3958        2.1539  0.0003  0.1001\n",
      "     50            1.0000        0.0062       0.4167            0.4167        2.1467  0.0003  0.1087\n",
      "Fine tuning model for subject 3 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        0.9441       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.9058\u001b[0m  0.0004  0.1046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        1.0200       0.5833            0.5833        0.9939  0.0005  0.1167\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5429\u001b[0m       0.5729            0.5729        1.1279  0.0005  0.0979\n",
      "     34            1.0000        \u001b[32m0.3213\u001b[0m       0.5521            0.5521        1.2623  0.0006  0.1196\n",
      "     35            1.0000        \u001b[32m0.1462\u001b[0m       0.5312            0.5312        1.3700  0.0006  0.1355\n",
      "     36            1.0000        \u001b[32m0.0781\u001b[0m       0.5417            0.5417        1.4358  0.0007  0.1345\n",
      "     37            1.0000        \u001b[32m0.0750\u001b[0m       0.5521            0.5521        1.4728  0.0007  0.1256\n",
      "     38            1.0000        \u001b[32m0.0504\u001b[0m       0.5729            0.5729        1.4918  0.0007  0.1071\n",
      "     39            1.0000        \u001b[32m0.0341\u001b[0m       0.5521            0.5521        1.5034  0.0007  0.1489\n",
      "     40            1.0000        0.0378       0.5208            0.5208        1.5121  0.0007  0.1493\n",
      "     41            1.0000        0.0383       0.5000            0.5000        1.5222  0.0007  0.1356\n",
      "     42            1.0000        \u001b[32m0.0207\u001b[0m       0.4792            0.4792        1.5324  0.0007  0.1362\n",
      "     43            1.0000        \u001b[32m0.0175\u001b[0m       0.4896            0.4896        1.5409  0.0006  0.1219\n",
      "     44            1.0000        0.0255       0.4896            0.4896        1.5461  0.0006  0.1319\n",
      "     45            1.0000        0.0326       0.4792            0.4792        1.5489  0.0005  0.1188\n",
      "     46            1.0000        \u001b[32m0.0137\u001b[0m       0.4792            0.4792        1.5508  0.0005  0.1094\n",
      "     47            1.0000        \u001b[32m0.0113\u001b[0m       0.4896            0.4896        1.5520  0.0004  0.1650\n",
      "     48            1.0000        0.0226       0.5000            0.5000        1.5536  0.0004  0.1500\n",
      "     49            1.0000        0.0115       0.5000            0.5000        1.5559  0.0003  0.1198\n",
      "     50            1.0000        \u001b[32m0.0088\u001b[0m       0.5312            0.5312        1.5595  0.0003  0.1198\n",
      "Fine tuning model for subject 3 with 10 = 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        \u001b[32m0.5798\u001b[0m       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8736\u001b[0m  0.0004  0.1483\n",
      "     32            \u001b[36m1.0000\u001b[0m        0.8430       0.6458            0.6458        \u001b[94m0.8700\u001b[0m  0.0005  0.0980\n",
      "     33            1.0000        0.6613       0.6458            0.6458        0.9059  0.0005  0.1122\n",
      "     34            1.0000        \u001b[32m0.2379\u001b[0m       0.5729            0.5729        0.9825  0.0006  0.1147\n",
      "     35            1.0000        \u001b[32m0.1179\u001b[0m       0.5521            0.5521        1.0775  0.0006  0.0867\n",
      "     36            1.0000        0.1947       0.5312            0.5312        1.1673  0.0007  0.1021\n",
      "     37            1.0000        \u001b[32m0.0675\u001b[0m       0.4896            0.4896        1.2543  0.0007  0.1030\n",
      "     38            1.0000        \u001b[32m0.0423\u001b[0m       0.4688            0.4688        1.3368  0.0007  0.0890\n",
      "     39            1.0000        \u001b[32m0.0184\u001b[0m       0.4688            0.4688        1.4128  0.0007  0.1014\n",
      "     40            1.0000        0.0206       0.4583            0.4583        1.4769  0.0007  0.1092\n",
      "     41            1.0000        \u001b[32m0.0162\u001b[0m       0.4479            0.4479        1.5291  0.0007  0.0942\n",
      "     42            1.0000        0.0165       0.4479            0.4479        1.5673  0.0007  0.1004\n",
      "     43            1.0000        \u001b[32m0.0098\u001b[0m       0.4375            0.4375        1.5928  0.0006  0.1032\n",
      "     44            1.0000        0.0234       0.4271            0.4271        1.6078  0.0006  0.0990\n",
      "     45            1.0000        0.0114       0.4479            0.4479        1.6129  0.0005  0.0883\n",
      "     46            1.0000        0.0321       0.4479            0.4479        1.6108  0.0005  0.1130\n",
      "     47            1.0000        0.0261       0.4688            0.4688        1.6029  0.0004  0.1077\n",
      "     48            1.0000        \u001b[32m0.0078\u001b[0m       0.4688            0.4688        1.5905  0.0004  0.0961\n",
      "     49            1.0000        \u001b[32m0.0065\u001b[0m       0.4583            0.4583        1.5750  0.0003  0.1181\n",
      "     50            1.0000        0.0100       0.4688            0.4688        1.5578  0.0003  0.0986\n",
      "Fine tuning model for subject 3 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        1.3832       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8923\u001b[0m  0.0004  0.1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        1.0986       0.6250            0.6250        0.9158  0.0005  0.1150\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6965\u001b[0m       0.5938            0.5938        0.9621  0.0005  0.0989\n",
      "     34            1.0000        \u001b[32m0.5388\u001b[0m       0.5312            0.5312        1.0198  0.0006  0.1158\n",
      "     35            1.0000        \u001b[32m0.1965\u001b[0m       0.5312            0.5312        1.0907  0.0006  0.0971\n",
      "     36            1.0000        \u001b[32m0.1554\u001b[0m       0.5104            0.5104        1.1573  0.0007  0.0905\n",
      "     37            1.0000        \u001b[32m0.1344\u001b[0m       0.5000            0.5000        1.2073  0.0007  0.1025\n",
      "     38            1.0000        \u001b[32m0.0812\u001b[0m       0.5000            0.5000        1.2601  0.0007  0.1061\n",
      "     39            1.0000        \u001b[32m0.0498\u001b[0m       0.4688            0.4688        1.3126  0.0007  0.0974\n",
      "     40            1.0000        \u001b[32m0.0294\u001b[0m       0.4583            0.4583        1.3684  0.0007  0.1181\n",
      "     41            1.0000        \u001b[32m0.0151\u001b[0m       0.4479            0.4479        1.4223  0.0007  0.1142\n",
      "     42            1.0000        0.0179       0.4375            0.4375        1.4664  0.0007  0.0895\n",
      "     43            1.0000        \u001b[32m0.0121\u001b[0m       0.4375            0.4375        1.4992  0.0006  0.1106\n",
      "     44            1.0000        0.0370       0.4375            0.4375        1.5168  0.0006  0.1053\n",
      "     45            1.0000        0.0157       0.4271            0.4271        1.5232  0.0005  0.0965\n",
      "     46            1.0000        \u001b[32m0.0113\u001b[0m       0.4583            0.4583        1.5197  0.0005  0.1020\n",
      "     47            1.0000        0.0123       0.4688            0.4688        1.5084  0.0004  0.1156\n",
      "     48            1.0000        \u001b[32m0.0107\u001b[0m       0.4167            0.4167        1.4919  0.0004  0.0986\n",
      "     49            1.0000        0.0151       0.4167            0.4167        1.4727  0.0003  0.0879\n",
      "     50            1.0000        0.0153       0.4167            0.4167        1.4527  0.0003  0.1180\n",
      "Fine tuning model for subject 3 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.7507       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.9042\u001b[0m  0.0004  0.1173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.9000\u001b[0m        1.0214       0.6042            0.6042        0.9597  0.0005  0.1155\n",
      "     33            0.9000        \u001b[32m0.4775\u001b[0m       0.6146            0.6146        1.0218  0.0005  0.0990\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2879\u001b[0m       0.5417            0.5417        1.0723  0.0006  0.1042\n",
      "     35            1.0000        \u001b[32m0.1701\u001b[0m       0.5208            0.5208        1.1136  0.0006  0.1500\n",
      "     36            1.0000        \u001b[32m0.0813\u001b[0m       0.4688            0.4688        1.1580  0.0007  0.0987\n",
      "     37            1.0000        \u001b[32m0.0811\u001b[0m       0.4375            0.4375        1.2149  0.0007  0.1006\n",
      "     38            1.0000        \u001b[32m0.0652\u001b[0m       0.4375            0.4375        1.2827  0.0007  0.1028\n",
      "     39            1.0000        0.0842       0.4375            0.4375        1.3486  0.0007  0.0988\n",
      "     40            1.0000        \u001b[32m0.0536\u001b[0m       0.4375            0.4375        1.4134  0.0007  0.0987\n",
      "     41            1.0000        \u001b[32m0.0418\u001b[0m       0.4479            0.4479        1.4692  0.0007  0.1025\n",
      "     42            1.0000        \u001b[32m0.0244\u001b[0m       0.4479            0.4479        1.5153  0.0007  0.1079\n",
      "     43            1.0000        0.0297       0.4375            0.4375        1.5487  0.0006  0.0953\n",
      "     44            1.0000        0.0297       0.4375            0.4375        1.5730  0.0006  0.1014\n",
      "     45            1.0000        0.0337       0.4271            0.4271        1.5899  0.0005  0.1168\n",
      "     46            1.0000        0.0322       0.4375            0.4375        1.5986  0.0005  0.0885\n",
      "     47            1.0000        \u001b[32m0.0162\u001b[0m       0.4375            0.4375        1.6037  0.0004  0.1021\n",
      "     48            1.0000        0.0206       0.4375            0.4375        1.6072  0.0004  0.1026\n",
      "     49            1.0000        \u001b[32m0.0122\u001b[0m       0.4167            0.4167        1.6097  0.0003  0.0934\n",
      "     50            1.0000        0.0147       0.4062            0.4062        1.6113  0.0003  0.0921\n",
      "Fine tuning model for subject 3 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        1.4014       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8938\u001b[0m  0.0004  0.1057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        0.8773       0.6458            0.6458        0.9414  0.0005  0.1087\n",
      "     33            1.0000        \u001b[32m0.6441\u001b[0m       0.5833            0.5833        1.0473  0.0005  0.0955\n",
      "     34            1.0000        \u001b[32m0.3442\u001b[0m       0.5625            0.5625        1.2123  0.0006  0.1042\n",
      "     35            1.0000        \u001b[32m0.2812\u001b[0m       0.5000            0.5000        1.4095  0.0006  0.1049\n",
      "     36            1.0000        \u001b[32m0.1907\u001b[0m       0.4896            0.4896        1.6082  0.0007  0.0996\n",
      "     37            1.0000        \u001b[32m0.1507\u001b[0m       0.4479            0.4479        1.7816  0.0007  0.1012\n",
      "     38            1.0000        \u001b[32m0.1244\u001b[0m       0.3854            0.3854        1.9222  0.0007  0.1136\n",
      "     39            1.0000        \u001b[32m0.0751\u001b[0m       0.3646            0.3646        2.0287  0.0007  0.0915\n",
      "     40            1.0000        \u001b[32m0.0420\u001b[0m       0.3542            0.3542        2.0996  0.0007  0.0999\n",
      "     41            1.0000        \u001b[32m0.0312\u001b[0m       0.3542            0.3542        2.1360  0.0007  0.1068\n",
      "     42            1.0000        0.0762       0.3438            0.3438        2.1361  0.0007  0.1002\n",
      "     43            1.0000        0.0440       0.3333            0.3333        2.1131  0.0006  0.0993\n",
      "     44            1.0000        \u001b[32m0.0220\u001b[0m       0.3333            0.3333        2.0788  0.0006  0.1133\n",
      "     45            1.0000        0.0489       0.3750            0.3750        2.0299  0.0005  0.1112\n",
      "     46            1.0000        0.0244       0.3750            0.3750        1.9783  0.0005  0.0883\n",
      "     47            1.0000        \u001b[32m0.0216\u001b[0m       0.3958            0.3958        1.9273  0.0004  0.1024\n",
      "     48            1.0000        0.0344       0.4062            0.4062        1.8777  0.0004  0.1088\n",
      "     49            1.0000        0.0260       0.4167            0.4167        1.8315  0.0003  0.0943\n",
      "     50            1.0000        \u001b[32m0.0151\u001b[0m       0.4167            0.4167        1.7906  0.0003  0.1014\n",
      "Fine tuning model for subject 3 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        1.0753       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8880\u001b[0m  0.0004  0.0910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        1.1235       0.6250            0.6250        \u001b[94m0.8878\u001b[0m  0.0005  0.1047\n",
      "     33            1.0000        \u001b[32m0.4796\u001b[0m       0.6042            0.6042        0.8955  0.0005  0.1018\n",
      "     34            1.0000        \u001b[32m0.3398\u001b[0m       0.5938            0.5938        0.9213  0.0006  0.1004\n",
      "     35            1.0000        0.3411       0.5521            0.5521        0.9606  0.0006  0.0914\n",
      "     36            1.0000        \u001b[32m0.0904\u001b[0m       0.5625            0.5625        1.0086  0.0007  0.0994\n",
      "     37            1.0000        0.0976       0.5625            0.5625        1.0532  0.0007  0.1132\n",
      "     38            1.0000        \u001b[32m0.0488\u001b[0m       0.5312            0.5312        1.0929  0.0007  0.0986\n",
      "     39            1.0000        \u001b[32m0.0399\u001b[0m       0.5417            0.5417        1.1263  0.0007  0.0976\n",
      "     40            1.0000        \u001b[32m0.0390\u001b[0m       0.5312            0.5312        1.1546  0.0007  0.1025\n",
      "     41            1.0000        \u001b[32m0.0155\u001b[0m       0.4896            0.4896        1.1801  0.0007  0.0997\n",
      "     42            1.0000        0.0197       0.4896            0.4896        1.2021  0.0007  0.1039\n",
      "     43            1.0000        0.0265       0.4792            0.4792        1.2191  0.0006  0.0990\n",
      "     44            1.0000        0.0163       0.4792            0.4792        1.2312  0.0006  0.1116\n",
      "     45            1.0000        \u001b[32m0.0140\u001b[0m       0.4688            0.4688        1.2395  0.0005  0.0916\n",
      "     46            1.0000        \u001b[32m0.0126\u001b[0m       0.4583            0.4583        1.2433  0.0005  0.1012\n",
      "     47            1.0000        \u001b[32m0.0116\u001b[0m       0.4583            0.4583        1.2441  0.0004  0.1074\n",
      "     48            1.0000        0.0181       0.4688            0.4688        1.2422  0.0004  0.1010\n",
      "     49            1.0000        0.0164       0.4583            0.4583        1.2385  0.0003  0.1149\n",
      "     50            1.0000        0.0122       0.4792            0.4792        1.2340  0.0003  0.1353\n",
      "Fine tuning model for subject 3 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.9000\u001b[0m        0.7270       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8895\u001b[0m  0.0004  0.1032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.9000        0.7654       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8893\u001b[0m  0.0005  0.1164\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4967\u001b[0m       0.6250            0.6250        0.8975  0.0005  0.0980\n",
      "     34            1.0000        \u001b[32m0.3798\u001b[0m       0.6146            0.6146        0.9108  0.0006  0.1131\n",
      "     35            1.0000        \u001b[32m0.1709\u001b[0m       0.5938            0.5938        0.9314  0.0006  0.0988\n",
      "     36            1.0000        \u001b[32m0.1384\u001b[0m       0.5521            0.5521        0.9677  0.0007  0.0928\n",
      "     37            1.0000        \u001b[32m0.0821\u001b[0m       0.5104            0.5104        1.0199  0.0007  0.1024\n",
      "     38            1.0000        \u001b[32m0.0743\u001b[0m       0.5000            0.5000        1.0804  0.0007  0.0986\n",
      "     39            1.0000        \u001b[32m0.0581\u001b[0m       0.5000            0.5000        1.1433  0.0007  0.1050\n",
      "     40            1.0000        \u001b[32m0.0570\u001b[0m       0.5000            0.5000        1.2008  0.0007  0.0883\n",
      "     41            1.0000        \u001b[32m0.0361\u001b[0m       0.4583            0.4583        1.2524  0.0007  0.1121\n",
      "     42            1.0000        \u001b[32m0.0318\u001b[0m       0.4375            0.4375        1.2939  0.0007  0.0923\n",
      "     43            1.0000        \u001b[32m0.0295\u001b[0m       0.4375            0.4375        1.3267  0.0006  0.1003\n",
      "     44            1.0000        \u001b[32m0.0201\u001b[0m       0.4479            0.4479        1.3510  0.0006  0.1102\n",
      "     45            1.0000        0.0262       0.4583            0.4583        1.3675  0.0005  0.1324\n",
      "     46            1.0000        \u001b[32m0.0170\u001b[0m       0.4583            0.4583        1.3781  0.0005  0.1187\n",
      "     47            1.0000        0.0232       0.4479            0.4479        1.3843  0.0004  0.1156\n",
      "     48            1.0000        \u001b[32m0.0134\u001b[0m       0.4479            0.4479        1.3868  0.0004  0.1312\n",
      "     49            1.0000        \u001b[32m0.0103\u001b[0m       0.4479            0.4479        1.3869  0.0003  0.1364\n",
      "     50            1.0000        0.0155       0.4479            0.4479        1.3847  0.0003  0.1356\n",
      "Fine tuning model for subject 3 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        1.1823       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.8844\u001b[0m  0.0004  0.1147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6667        0.9154       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8684\u001b[0m  0.0005  0.1344\n",
      "     33            \u001b[36m0.8667\u001b[0m        0.9197       0.6354            0.6354        \u001b[94m0.8483\u001b[0m  0.0005  0.1354\n",
      "     34            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5956\u001b[0m       0.5938            0.5938        0.8745  0.0006  0.1484\n",
      "     35            0.9333        \u001b[32m0.4811\u001b[0m       0.5729            0.5729        0.9862  0.0006  0.1182\n",
      "     36            0.9333        \u001b[32m0.1394\u001b[0m       0.4896            0.4896        1.1554  0.0007  0.1192\n",
      "     37            \u001b[36m1.0000\u001b[0m        0.2164       0.4583            0.4583        1.3409  0.0007  0.1361\n",
      "     38            1.0000        \u001b[32m0.0619\u001b[0m       0.4271            0.4271        1.5229  0.0007  0.1345\n",
      "     39            1.0000        0.0811       0.3958            0.3958        1.6754  0.0007  0.1275\n",
      "     40            1.0000        0.0708       0.3542            0.3542        1.7959  0.0007  0.1509\n",
      "     41            1.0000        \u001b[32m0.0290\u001b[0m       0.3542            0.3542        1.8857  0.0007  0.1303\n",
      "     42            1.0000        0.0348       0.3542            0.3542        1.9473  0.0007  0.1674\n",
      "     43            1.0000        0.0481       0.3646            0.3646        1.9829  0.0006  0.1456\n",
      "     44            1.0000        0.0315       0.3646            0.3646        2.0000  0.0006  0.1038\n",
      "     45            1.0000        \u001b[32m0.0193\u001b[0m       0.3646            0.3646        2.0012  0.0005  0.1004\n",
      "     46            1.0000        0.0250       0.3438            0.3438        1.9925  0.0005  0.1123\n",
      "     47            1.0000        0.0222       0.3438            0.3438        1.9752  0.0004  0.0987\n",
      "     48            1.0000        \u001b[32m0.0147\u001b[0m       0.3333            0.3333        1.9534  0.0004  0.1052\n",
      "     49            1.0000        \u001b[32m0.0137\u001b[0m       0.3542            0.3542        1.9298  0.0003  0.1141\n",
      "     50            1.0000        0.0206       0.3542            0.3542        1.9051  0.0003  0.1067\n",
      "Fine tuning model for subject 3 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        1.1225       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.9176\u001b[0m  0.0004  0.1027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6667        0.9354       0.5625            0.5625        0.9948  0.0005  0.1156\n",
      "     33            0.7333        0.7489       0.5417            0.5417        1.1142  0.0005  0.0855\n",
      "     34            \u001b[36m0.8667\u001b[0m        \u001b[32m0.4353\u001b[0m       0.5521            0.5521        1.2565  0.0006  0.1035\n",
      "     35            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3019\u001b[0m       0.5208            0.5208        1.3918  0.0006  0.1148\n",
      "     36            0.9333        0.3702       0.5104            0.5104        1.4936  0.0007  0.0975\n",
      "     37            0.9333        \u001b[32m0.1356\u001b[0m       0.5312            0.5312        1.5487  0.0007  0.1012\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1043\u001b[0m       0.5312            0.5312        1.5814  0.0007  0.1142\n",
      "     39            1.0000        \u001b[32m0.0857\u001b[0m       0.5208            0.5208        1.5980  0.0007  0.1021\n",
      "     40            1.0000        \u001b[32m0.0502\u001b[0m       0.5104            0.5104        1.6059  0.0007  0.1004\n",
      "     41            1.0000        \u001b[32m0.0308\u001b[0m       0.5104            0.5104        1.6067  0.0007  0.1042\n",
      "     42            1.0000        0.0490       0.5104            0.5104        1.6036  0.0007  0.1345\n",
      "     43            1.0000        \u001b[32m0.0283\u001b[0m       0.5000            0.5000        1.5995  0.0006  0.0997\n",
      "     44            1.0000        0.0568       0.5104            0.5104        1.5940  0.0006  0.0937\n",
      "     45            1.0000        \u001b[32m0.0197\u001b[0m       0.5104            0.5104        1.5881  0.0005  0.0924\n",
      "     46            1.0000        0.0252       0.5312            0.5312        1.5819  0.0005  0.1152\n",
      "     47            1.0000        0.0365       0.5208            0.5208        1.5763  0.0004  0.0883\n",
      "     48            1.0000        0.0204       0.5104            0.5104        1.5723  0.0004  0.1013\n",
      "     49            1.0000        0.0255       0.5208            0.5208        1.5681  0.0003  0.1146\n",
      "     50            1.0000        0.0223       0.5208            0.5208        1.5648  0.0003  0.0894\n",
      "Fine tuning model for subject 3 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.0932       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8879\u001b[0m  0.0004  0.1184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7333        0.8149       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8846\u001b[0m  0.0005  0.1001\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5387\u001b[0m       \u001b[35m0.6771\u001b[0m            \u001b[31m0.6771\u001b[0m        \u001b[94m0.8738\u001b[0m  0.0005  0.1010\n",
      "     34            1.0000        \u001b[32m0.3612\u001b[0m       0.6562            0.6562        0.8769  0.0006  0.1113\n",
      "     35            1.0000        \u001b[32m0.2450\u001b[0m       0.6250            0.6250        0.9099  0.0006  0.0988\n",
      "     36            1.0000        \u001b[32m0.2064\u001b[0m       0.5938            0.5938        0.9803  0.0007  0.0924\n",
      "     37            1.0000        \u001b[32m0.1943\u001b[0m       0.6042            0.6042        1.0751  0.0007  0.1013\n",
      "     38            0.9333        \u001b[32m0.1470\u001b[0m       0.6042            0.6042        1.1879  0.0007  0.1092\n",
      "     39            0.9333        \u001b[32m0.1398\u001b[0m       0.5729            0.5729        1.2958  0.0007  0.0935\n",
      "     40            0.9333        \u001b[32m0.1241\u001b[0m       0.5417            0.5417        1.3794  0.0007  0.1015\n",
      "     41            1.0000        \u001b[32m0.0814\u001b[0m       0.5208            0.5208        1.4533  0.0007  0.1124\n",
      "     42            1.0000        \u001b[32m0.0723\u001b[0m       0.5104            0.5104        1.4947  0.0007  0.0895\n",
      "     43            1.0000        \u001b[32m0.0482\u001b[0m       0.5104            0.5104        1.5208  0.0006  0.1004\n",
      "     44            1.0000        0.0601       0.5104            0.5104        1.5182  0.0006  0.1122\n",
      "     45            1.0000        \u001b[32m0.0430\u001b[0m       0.5208            0.5208        1.4982  0.0005  0.0985\n",
      "     46            1.0000        \u001b[32m0.0199\u001b[0m       0.5208            0.5208        1.4690  0.0005  0.0874\n",
      "     47            1.0000        0.0272       0.5521            0.5521        1.4332  0.0004  0.1014\n",
      "     48            1.0000        \u001b[32m0.0160\u001b[0m       0.5833            0.5833        1.3946  0.0004  0.1141\n",
      "     49            1.0000        0.0268       0.5729            0.5729        1.3573  0.0003  0.0981\n",
      "     50            1.0000        0.0218       0.5729            0.5729        1.3227  0.0003  0.1014\n",
      "Fine tuning model for subject 3 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        0.9890       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8834\u001b[0m  0.0004  0.1034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7333        0.9277       0.6667            0.6667        \u001b[94m0.8773\u001b[0m  0.0005  0.1163\n",
      "     33            \u001b[36m0.9333\u001b[0m        0.8056       0.6562            0.6562        \u001b[94m0.8720\u001b[0m  0.0005  0.1003\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4936\u001b[0m       0.6458            0.6458        0.8858  0.0006  0.1156\n",
      "     35            1.0000        \u001b[32m0.1980\u001b[0m       0.6354            0.6354        0.9229  0.0006  0.0870\n",
      "     36            1.0000        \u001b[32m0.1894\u001b[0m       0.5833            0.5833        0.9697  0.0007  0.0996\n",
      "     37            1.0000        \u001b[32m0.1119\u001b[0m       0.5938            0.5938        1.0210  0.0007  0.1036\n",
      "     38            1.0000        \u001b[32m0.0616\u001b[0m       0.5729            0.5729        1.0716  0.0007  0.1011\n",
      "     39            1.0000        \u001b[32m0.0430\u001b[0m       0.5521            0.5521        1.1183  0.0007  0.0862\n",
      "     40            1.0000        0.0906       0.5521            0.5521        1.1575  0.0007  0.1014\n",
      "     41            1.0000        0.0852       0.5312            0.5312        1.1868  0.0007  0.1155\n",
      "     42            1.0000        0.0627       0.5000            0.5000        1.2089  0.0007  0.1011\n",
      "     43            1.0000        \u001b[32m0.0312\u001b[0m       0.4792            0.4792        1.2240  0.0006  0.1159\n",
      "     44            1.0000        0.0314       0.5000            0.5000        1.2337  0.0006  0.1208\n",
      "     45            1.0000        0.0510       0.4896            0.4896        1.2372  0.0005  0.1096\n",
      "     46            1.0000        \u001b[32m0.0176\u001b[0m       0.4896            0.4896        1.2368  0.0005  0.0922\n",
      "     47            1.0000        0.0210       0.4792            0.4792        1.2343  0.0004  0.1003\n",
      "     48            1.0000        0.0320       0.4896            0.4896        1.2307  0.0004  0.1112\n",
      "     49            1.0000        0.0289       0.5000            0.5000        1.2268  0.0003  0.0986\n",
      "     50            1.0000        0.0228       0.5000            0.5000        1.2235  0.0003  0.1003\n",
      "Fine tuning model for subject 3 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.3456       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.8873\u001b[0m  0.0004  0.0851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        1.2272       0.6354            0.6354        0.8890  0.0005  0.1460\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.9502       0.6250            0.6250        0.9081  0.0005  0.0863\n",
      "     34            1.0000        \u001b[32m0.5612\u001b[0m       0.6146            0.6146        0.9871  0.0006  0.1036\n",
      "     35            1.0000        \u001b[32m0.2841\u001b[0m       0.5521            0.5521        1.1357  0.0006  0.0997\n",
      "     36            1.0000        \u001b[32m0.1867\u001b[0m       0.4792            0.4792        1.3330  0.0007  0.0911\n",
      "     37            1.0000        \u001b[32m0.1591\u001b[0m       0.4375            0.4375        1.5453  0.0007  0.1012\n",
      "     38            0.9333        \u001b[32m0.1183\u001b[0m       0.3750            0.3750        1.7495  0.0007  0.1112\n",
      "     39            0.9333        \u001b[32m0.0862\u001b[0m       0.3646            0.3646        1.9340  0.0007  0.0911\n",
      "     40            0.9333        \u001b[32m0.0692\u001b[0m       0.3542            0.3542        2.0939  0.0007  0.1014\n",
      "     41            0.9333        \u001b[32m0.0455\u001b[0m       0.3333            0.3333        2.2264  0.0007  0.1148\n",
      "     42            0.9333        \u001b[32m0.0349\u001b[0m       0.3333            0.3333        2.3295  0.0007  0.0874\n",
      "     43            0.9333        0.0450       0.3333            0.3333        2.3962  0.0006  0.1006\n",
      "     44            0.9333        0.0408       0.3333            0.3333        2.4280  0.0006  0.1140\n",
      "     45            1.0000        0.0359       0.3333            0.3333        2.4302  0.0005  0.1140\n",
      "     46            1.0000        \u001b[32m0.0215\u001b[0m       0.3229            0.3229        2.4082  0.0005  0.1015\n",
      "     47            1.0000        0.0287       0.3229            0.3229        2.3687  0.0004  0.1021\n",
      "     48            1.0000        0.0390       0.3438            0.3438        2.3129  0.0004  0.1189\n",
      "     49            1.0000        0.0227       0.3438            0.3438        2.2484  0.0003  0.1021\n",
      "     50            1.0000        \u001b[32m0.0183\u001b[0m       0.3438            0.3438        2.1824  0.0003  0.0997\n",
      "Fine tuning model for subject 3 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        0.9186       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8890\u001b[0m  0.0004  0.0951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        0.8557       0.6458            0.6458        0.9029  0.0005  0.1099\n",
      "     33            \u001b[36m0.8667\u001b[0m        0.7272       0.5938            0.5938        0.9205  0.0005  0.0978\n",
      "     34            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4985\u001b[0m       0.5938            0.5938        0.9211  0.0006  0.1084\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3238\u001b[0m       0.5833            0.5833        0.9214  0.0006  0.0987\n",
      "     36            1.0000        \u001b[32m0.1929\u001b[0m       0.5729            0.5729        0.9284  0.0007  0.0970\n",
      "     37            1.0000        \u001b[32m0.1442\u001b[0m       0.5729            0.5729        0.9436  0.0007  0.1025\n",
      "     38            1.0000        \u001b[32m0.1316\u001b[0m       0.5729            0.5729        0.9601  0.0007  0.1041\n",
      "     39            1.0000        \u001b[32m0.0641\u001b[0m       0.5729            0.5729        0.9752  0.0007  0.0965\n",
      "     40            1.0000        \u001b[32m0.0375\u001b[0m       0.5625            0.5625        0.9893  0.0007  0.1013\n",
      "     41            1.0000        0.0532       0.5625            0.5625        1.0040  0.0007  0.1065\n",
      "     42            1.0000        \u001b[32m0.0297\u001b[0m       0.5417            0.5417        1.0231  0.0007  0.0986\n",
      "     43            1.0000        \u001b[32m0.0198\u001b[0m       0.5208            0.5208        1.0460  0.0006  0.0987\n",
      "     44            1.0000        0.0224       0.5417            0.5417        1.0714  0.0006  0.1024\n",
      "     45            1.0000        0.0223       0.5208            0.5208        1.0972  0.0005  0.1058\n",
      "     46            1.0000        \u001b[32m0.0189\u001b[0m       0.5000            0.5000        1.1218  0.0005  0.1010\n",
      "     47            1.0000        0.0221       0.5000            0.5000        1.1440  0.0004  0.0980\n",
      "     48            1.0000        \u001b[32m0.0090\u001b[0m       0.5104            0.5104        1.1631  0.0004  0.1082\n",
      "     49            1.0000        0.0245       0.5104            0.5104        1.1791  0.0003  0.0987\n",
      "     50            1.0000        0.0126       0.4792            0.4792        1.1920  0.0003  0.0968\n",
      "Fine tuning model for subject 3 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5333        1.6481       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8969\u001b[0m  0.0004  0.0997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.3856       0.6354            0.6354        0.9129  0.0005  0.1482\n",
      "     33            0.7333        1.3566       0.6250            0.6250        0.9465  0.0005  0.1168\n",
      "     34            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6733\u001b[0m       0.6042            0.6042        1.0113  0.0006  0.1353\n",
      "     35            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4221\u001b[0m       0.5625            0.5625        1.1011  0.0006  0.1198\n",
      "     36            0.9333        \u001b[32m0.2006\u001b[0m       0.5312            0.5312        1.2197  0.0007  0.1259\n",
      "     37            0.9333        \u001b[32m0.1989\u001b[0m       0.4792            0.4792        1.3446  0.0007  0.1407\n",
      "     38            0.9333        0.2128       0.4583            0.4583        1.4568  0.0007  0.1339\n",
      "     39            0.9333        \u001b[32m0.0916\u001b[0m       0.4375            0.4375        1.5516  0.0007  0.1253\n",
      "     40            0.9333        \u001b[32m0.0793\u001b[0m       0.4583            0.4583        1.6247  0.0007  0.1174\n",
      "     41            0.9333        \u001b[32m0.0609\u001b[0m       0.4688            0.4688        1.6705  0.0007  0.1655\n",
      "     42            0.9333        \u001b[32m0.0396\u001b[0m       0.4688            0.4688        1.6967  0.0007  0.1164\n",
      "     43            0.9333        0.0589       0.4688            0.4688        1.7033  0.0006  0.1323\n",
      "     44            0.9333        0.0468       0.4688            0.4688        1.6951  0.0006  0.1198\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0301\u001b[0m       0.4688            0.4688        1.6754  0.0005  0.1510\n",
      "     46            1.0000        \u001b[32m0.0173\u001b[0m       0.4688            0.4688        1.6491  0.0005  0.1353\n",
      "     47            1.0000        0.0387       0.4688            0.4688        1.6196  0.0004  0.1511\n",
      "     48            1.0000        0.0411       0.4583            0.4583        1.5871  0.0004  0.1199\n",
      "     49            1.0000        0.0356       0.4167            0.4167        1.5557  0.0003  0.1355\n",
      "     50            1.0000        0.0392       0.4167            0.4167        1.5269  0.0003  0.1355\n",
      "Fine tuning model for subject 3 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        1.3969       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.9123\u001b[0m  0.0004  0.1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        1.2441       0.5833            0.5833        0.9801  0.0005  0.1010\n",
      "     33            0.7333        0.7359       0.5417            0.5417        1.1104  0.0005  0.0999\n",
      "     34            0.7333        \u001b[32m0.4969\u001b[0m       0.5000            0.5000        1.2891  0.0006  0.1032\n",
      "     35            \u001b[36m0.8667\u001b[0m        \u001b[32m0.3408\u001b[0m       0.4583            0.4583        1.4568  0.0006  0.0998\n",
      "     36            0.8667        \u001b[32m0.2057\u001b[0m       0.4375            0.4375        1.5792  0.0007  0.0913\n",
      "     37            \u001b[36m0.9333\u001b[0m        0.2310       0.4062            0.4062        1.6536  0.0007  0.1014\n",
      "     38            0.9333        \u001b[32m0.1545\u001b[0m       0.3854            0.3854        1.7080  0.0007  0.1082\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1175\u001b[0m       0.3750            0.3750        1.7496  0.0007  0.0937\n",
      "     40            1.0000        \u001b[32m0.0985\u001b[0m       0.3646            0.3646        1.7901  0.0007  0.0996\n",
      "     41            1.0000        0.1097       0.3542            0.3542        1.8219  0.0007  0.1206\n",
      "     42            1.0000        \u001b[32m0.0952\u001b[0m       0.3438            0.3438        1.8454  0.0007  0.1050\n",
      "     43            1.0000        \u001b[32m0.0684\u001b[0m       0.3229            0.3229        1.8580  0.0006  0.0966\n",
      "     44            1.0000        \u001b[32m0.0478\u001b[0m       0.3229            0.3229        1.8579  0.0006  0.1013\n",
      "     45            1.0000        0.0712       0.3333            0.3333        1.8461  0.0005  0.1042\n",
      "     46            1.0000        0.0728       0.3333            0.3333        1.8277  0.0005  0.1002\n",
      "     47            1.0000        0.0571       0.3333            0.3333        1.8010  0.0004  0.0989\n",
      "     48            1.0000        \u001b[32m0.0462\u001b[0m       0.3438            0.3438        1.7700  0.0004  0.1093\n",
      "     49            1.0000        \u001b[32m0.0419\u001b[0m       0.3438            0.3438        1.7366  0.0003  0.0987\n",
      "     50            1.0000        \u001b[32m0.0288\u001b[0m       0.3542            0.3542        1.7021  0.0003  0.0927\n",
      "Fine tuning model for subject 3 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7333        1.1247       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8864\u001b[0m  0.0004  0.0858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        0.9510       0.6354            0.6354        0.8910  0.0005  0.1170\n",
      "     33            \u001b[36m0.9333\u001b[0m        0.8751       0.6042            0.6042        0.9080  0.0005  0.0990\n",
      "     34            0.9333        \u001b[32m0.6645\u001b[0m       0.5938            0.5938        0.9430  0.0006  0.1041\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3935\u001b[0m       0.5625            0.5625        0.9981  0.0006  0.1061\n",
      "     36            1.0000        \u001b[32m0.3274\u001b[0m       0.5521            0.5521        1.0624  0.0007  0.0946\n",
      "     37            1.0000        \u001b[32m0.1627\u001b[0m       0.5417            0.5417        1.1251  0.0007  0.1002\n",
      "     38            1.0000        0.1677       0.5208            0.5208        1.1854  0.0007  0.1043\n",
      "     39            1.0000        \u001b[32m0.1050\u001b[0m       0.5104            0.5104        1.2377  0.0007  0.0964\n",
      "     40            1.0000        \u001b[32m0.0602\u001b[0m       0.5104            0.5104        1.2791  0.0007  0.0946\n",
      "     41            1.0000        0.0923       0.5104            0.5104        1.3096  0.0007  0.1025\n",
      "     42            1.0000        \u001b[32m0.0300\u001b[0m       0.5104            0.5104        1.3303  0.0007  0.1073\n",
      "     43            1.0000        \u001b[32m0.0298\u001b[0m       0.5104            0.5104        1.3420  0.0006  0.0957\n",
      "     44            1.0000        0.0516       0.5208            0.5208        1.3448  0.0006  0.0992\n",
      "     45            1.0000        \u001b[32m0.0296\u001b[0m       0.5104            0.5104        1.3410  0.0005  0.1088\n",
      "     46            1.0000        \u001b[32m0.0223\u001b[0m       0.4792            0.4792        1.3327  0.0005  0.0954\n",
      "     47            1.0000        \u001b[32m0.0123\u001b[0m       0.4792            0.4792        1.3210  0.0004  0.0996\n",
      "     48            1.0000        0.0246       0.4792            0.4792        1.3067  0.0004  0.1173\n",
      "     49            1.0000        0.0219       0.4688            0.4688        1.2907  0.0003  0.1191\n",
      "     50            1.0000        0.0198       0.4688            0.4688        1.2746  0.0003  0.0976\n",
      "Fine tuning model for subject 3 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        1.1781       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.8985\u001b[0m  0.0004  0.0864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        \u001b[32m0.6895\u001b[0m       0.5833            0.5833        0.9453  0.0005  0.1171\n",
      "     33            \u001b[36m0.8667\u001b[0m        0.7548       0.5521            0.5521        1.0309  0.0005  0.0989\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3249\u001b[0m       0.5104            0.5104        1.1480  0.0006  0.1112\n",
      "     35            1.0000        \u001b[32m0.2166\u001b[0m       0.4792            0.4792        1.2776  0.0006  0.1000\n",
      "     36            1.0000        \u001b[32m0.1703\u001b[0m       0.4688            0.4688        1.3931  0.0007  0.0893\n",
      "     37            1.0000        \u001b[32m0.1305\u001b[0m       0.4479            0.4479        1.4808  0.0007  0.1009\n",
      "     38            1.0000        \u001b[32m0.0540\u001b[0m       0.4583            0.4583        1.5397  0.0007  0.1097\n",
      "     39            1.0000        0.0618       0.4792            0.4792        1.5728  0.0007  0.0931\n",
      "     40            1.0000        \u001b[32m0.0497\u001b[0m       0.4688            0.4688        1.5850  0.0007  0.1493\n",
      "     41            1.0000        0.0771       0.4688            0.4688        1.5897  0.0007  0.1173\n",
      "     42            1.0000        \u001b[32m0.0327\u001b[0m       0.5000            0.5000        1.5916  0.0007  0.0986\n",
      "     43            1.0000        \u001b[32m0.0302\u001b[0m       0.4792            0.4792        1.5917  0.0006  0.1016\n",
      "     44            1.0000        \u001b[32m0.0251\u001b[0m       0.4792            0.4792        1.5930  0.0006  0.0989\n",
      "     45            1.0000        0.0272       0.4896            0.4896        1.5963  0.0005  0.1147\n",
      "     46            1.0000        0.0299       0.4896            0.4896        1.6015  0.0005  0.0875\n",
      "     47            1.0000        \u001b[32m0.0180\u001b[0m       0.4896            0.4896        1.6073  0.0004  0.1002\n",
      "     48            1.0000        0.0381       0.4896            0.4896        1.6137  0.0004  0.1199\n",
      "     49            1.0000        0.0252       0.4896            0.4896        1.6204  0.0003  0.1003\n",
      "     50            1.0000        0.0305       0.4792            0.4792        1.6272  0.0003  0.1005\n",
      "Fine tuning model for subject 3 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        1.0892       \u001b[35m0.6771\u001b[0m            \u001b[31m0.6771\u001b[0m        \u001b[94m0.8817\u001b[0m  0.0004  0.1070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        0.8506       0.6771            0.6771        \u001b[94m0.8713\u001b[0m  0.0005  0.0989\n",
      "     33            \u001b[36m0.9000\u001b[0m        0.7942       0.6562            0.6562        \u001b[94m0.8642\u001b[0m  0.0005  0.1163\n",
      "     34            \u001b[36m0.9500\u001b[0m        \u001b[32m0.6707\u001b[0m       0.6250            0.6250        0.8772  0.0006  0.1025\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4083\u001b[0m       0.6146            0.6146        0.9139  0.0006  0.1164\n",
      "     36            1.0000        \u001b[32m0.3115\u001b[0m       0.5938            0.5938        0.9555  0.0007  0.0995\n",
      "     37            1.0000        \u001b[32m0.2121\u001b[0m       0.5938            0.5938        1.0058  0.0007  0.1001\n",
      "     38            1.0000        \u001b[32m0.1793\u001b[0m       0.5833            0.5833        1.0563  0.0007  0.1201\n",
      "     39            1.0000        \u001b[32m0.1495\u001b[0m       0.5521            0.5521        1.1049  0.0007  0.1021\n",
      "     40            1.0000        \u001b[32m0.1224\u001b[0m       0.5208            0.5208        1.1519  0.0007  0.0933\n",
      "     41            1.0000        \u001b[32m0.0896\u001b[0m       0.5208            0.5208        1.1980  0.0007  0.1159\n",
      "     42            1.0000        \u001b[32m0.0688\u001b[0m       0.5000            0.5000        1.2360  0.0007  0.1123\n",
      "     43            1.0000        0.0862       0.5000            0.5000        1.2582  0.0006  0.1149\n",
      "     44            1.0000        0.0800       0.4896            0.4896        1.2676  0.0006  0.0991\n",
      "     45            1.0000        \u001b[32m0.0579\u001b[0m       0.4792            0.4792        1.2681  0.0005  0.1020\n",
      "     46            1.0000        \u001b[32m0.0532\u001b[0m       0.5000            0.5000        1.2625  0.0005  0.1083\n",
      "     47            1.0000        \u001b[32m0.0485\u001b[0m       0.5000            0.5000        1.2518  0.0004  0.1170\n",
      "     48            1.0000        0.0776       0.5104            0.5104        1.2409  0.0004  0.0971\n",
      "     49            1.0000        \u001b[32m0.0467\u001b[0m       0.5104            0.5104        1.2299  0.0003  0.1013\n",
      "     50            1.0000        \u001b[32m0.0403\u001b[0m       0.5208            0.5208        1.2184  0.0003  0.1089\n",
      "Fine tuning model for subject 3 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6500        1.0534       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.8931\u001b[0m  0.0004  0.1045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        0.8408       0.6146            0.6146        0.9236  0.0005  0.1238\n",
      "     33            \u001b[36m0.8500\u001b[0m        \u001b[32m0.6594\u001b[0m       0.5729            0.5729        0.9647  0.0005  0.0980\n",
      "     34            0.8500        \u001b[32m0.6294\u001b[0m       0.5521            0.5521        1.0023  0.0006  0.1183\n",
      "     35            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5123\u001b[0m       0.5417            0.5417        1.0336  0.0006  0.1071\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2578\u001b[0m       0.5312            0.5312        1.0604  0.0007  0.0921\n",
      "     37            1.0000        0.3283       0.5312            0.5312        1.0789  0.0007  0.1161\n",
      "     38            1.0000        0.2999       0.5104            0.5104        1.0826  0.0007  0.0998\n",
      "     39            1.0000        \u001b[32m0.1771\u001b[0m       0.5312            0.5312        1.0906  0.0007  0.1171\n",
      "     40            1.0000        \u001b[32m0.1204\u001b[0m       0.5521            0.5521        1.1030  0.0007  0.0985\n",
      "     41            1.0000        \u001b[32m0.0867\u001b[0m       0.5625            0.5625        1.1171  0.0007  0.0935\n",
      "     42            1.0000        \u001b[32m0.0641\u001b[0m       0.5729            0.5729        1.1320  0.0007  0.1160\n",
      "     43            1.0000        0.0793       0.5729            0.5729        1.1431  0.0006  0.1105\n",
      "     44            1.0000        \u001b[32m0.0606\u001b[0m       0.5729            0.5729        1.1514  0.0006  0.0987\n",
      "     45            1.0000        \u001b[32m0.0401\u001b[0m       0.5625            0.5625        1.1579  0.0005  0.1001\n",
      "     46            1.0000        \u001b[32m0.0355\u001b[0m       0.5417            0.5417        1.1617  0.0005  0.1011\n",
      "     47            1.0000        \u001b[32m0.0291\u001b[0m       0.5521            0.5521        1.1652  0.0004  0.1163\n",
      "     48            1.0000        0.0362       0.5521            0.5521        1.1676  0.0004  0.1012\n",
      "     49            1.0000        0.0309       0.5417            0.5417        1.1696  0.0003  0.0944\n",
      "     50            1.0000        \u001b[32m0.0276\u001b[0m       0.5417            0.5417        1.1718  0.0003  0.1480\n",
      "Fine tuning model for subject 3 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        0.9246       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.8916\u001b[0m  0.0004  0.1028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7500        0.8555       0.6042            0.6042        0.9113  0.0005  0.1178\n",
      "     33            \u001b[36m0.8500\u001b[0m        \u001b[32m0.6811\u001b[0m       0.6042            0.6042        0.9303  0.0005  0.0988\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5887\u001b[0m       0.6146            0.6146        0.9478  0.0006  0.1347\n",
      "     35            1.0000        \u001b[32m0.3698\u001b[0m       0.5729            0.5729        0.9632  0.0006  0.1354\n",
      "     36            1.0000        \u001b[32m0.3067\u001b[0m       0.5833            0.5833        0.9775  0.0007  0.1977\n",
      "     37            1.0000        \u001b[32m0.2324\u001b[0m       0.5938            0.5938        0.9989  0.0007  0.1967\n",
      "     38            1.0000        \u001b[32m0.1362\u001b[0m       0.5729            0.5729        1.0273  0.0007  0.1743\n",
      "     39            1.0000        0.1696       0.5521            0.5521        1.0599  0.0007  0.1820\n",
      "     40            1.0000        \u001b[32m0.1088\u001b[0m       0.5521            0.5521        1.0921  0.0007  0.1322\n",
      "     41            1.0000        \u001b[32m0.0946\u001b[0m       0.5312            0.5312        1.1226  0.0007  0.1199\n",
      "     42            1.0000        \u001b[32m0.0669\u001b[0m       0.5208            0.5208        1.1505  0.0007  0.1533\n",
      "     43            1.0000        \u001b[32m0.0608\u001b[0m       0.5208            0.5208        1.1725  0.0006  0.1323\n",
      "     44            1.0000        \u001b[32m0.0604\u001b[0m       0.5000            0.5000        1.1863  0.0006  0.1355\n",
      "     45            1.0000        \u001b[32m0.0503\u001b[0m       0.5104            0.5104        1.1919  0.0005  0.1345\n",
      "     46            1.0000        0.0553       0.5104            0.5104        1.1916  0.0005  0.1326\n",
      "     47            1.0000        0.0638       0.5208            0.5208        1.1828  0.0004  0.1150\n",
      "     48            1.0000        0.0656       0.5312            0.5312        1.1701  0.0004  0.1333\n",
      "     49            1.0000        \u001b[32m0.0360\u001b[0m       0.5312            0.5312        1.1552  0.0003  0.1237\n",
      "     50            1.0000        0.0397       0.5417            0.5417        1.1416  0.0003  0.1157\n",
      "Fine tuning model for subject 3 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        0.9316       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.9045\u001b[0m  0.0004  0.1446\n",
      "     32            0.6000        0.8672       0.6146            0.6146        0.9771  0.0005  0.0989\n",
      "     33            0.7500        0.8843       0.6250            0.6250        1.1158  0.0005  0.1203\n",
      "     34            0.7500        0.7563       0.5729            0.5729        1.2879  0.0006  0.1135\n",
      "     35            \u001b[36m0.8000\u001b[0m        \u001b[32m0.4869\u001b[0m       0.5208            0.5208        1.4524  0.0006  0.1134\n",
      "     36            \u001b[36m0.8500\u001b[0m        \u001b[32m0.2978\u001b[0m       0.5000            0.5000        1.5653  0.0007  0.0999\n",
      "     37            0.8500        \u001b[32m0.1950\u001b[0m       0.4792            0.4792        1.6271  0.0007  0.0999\n",
      "     38            \u001b[36m0.9000\u001b[0m        0.2058       0.5208            0.5208        1.6251  0.0007  0.1200\n",
      "     39            \u001b[36m0.9500\u001b[0m        \u001b[32m0.1126\u001b[0m       0.4792            0.4792        1.5993  0.0007  0.1121\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0804\u001b[0m       0.4688            0.4688        1.5693  0.0007  0.0992\n",
      "     41            1.0000        \u001b[32m0.0713\u001b[0m       0.4479            0.4479        1.5423  0.0007  0.1019\n",
      "     42            1.0000        \u001b[32m0.0538\u001b[0m       0.4583            0.4583        1.5231  0.0007  0.1072\n",
      "     43            1.0000        0.0570       0.4479            0.4479        1.5043  0.0006  0.0986\n",
      "     44            1.0000        \u001b[32m0.0445\u001b[0m       0.4583            0.4583        1.4891  0.0006  0.0986\n",
      "     45            1.0000        \u001b[32m0.0330\u001b[0m       0.4583            0.4583        1.4740  0.0005  0.1025\n",
      "     46            1.0000        0.0345       0.4583            0.4583        1.4595  0.0005  0.1175\n",
      "     47            1.0000        0.0333       0.4583            0.4583        1.4464  0.0004  0.1000\n",
      "     48            1.0000        \u001b[32m0.0284\u001b[0m       0.4375            0.4375        1.4343  0.0004  0.0985\n",
      "     49            1.0000        0.0358       0.4375            0.4375        1.4249  0.0003  0.1166\n",
      "     50            1.0000        0.0293       0.4479            0.4479        1.4176  0.0003  0.1153\n",
      "Fine tuning model for subject 3 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5500        1.2065       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8856\u001b[0m  0.0004  0.1038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        1.2314       0.6458            0.6458        \u001b[94m0.8808\u001b[0m  0.0005  0.1272\n",
      "     33            \u001b[36m0.9500\u001b[0m        0.8181       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        0.8828  0.0005  0.0980\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5561\u001b[0m       0.6458            0.6458        0.9178  0.0006  0.1192\n",
      "     35            1.0000        \u001b[32m0.2447\u001b[0m       0.6354            0.6354        0.9921  0.0006  0.1351\n",
      "     36            1.0000        \u001b[32m0.1836\u001b[0m       0.5521            0.5521        1.1010  0.0007  0.1190\n",
      "     37            1.0000        \u001b[32m0.1624\u001b[0m       0.4896            0.4896        1.2309  0.0007  0.0986\n",
      "     38            1.0000        \u001b[32m0.0766\u001b[0m       0.4375            0.4375        1.3634  0.0007  0.1007\n",
      "     39            1.0000        0.1044       0.4271            0.4271        1.4901  0.0007  0.1177\n",
      "     40            1.0000        \u001b[32m0.0531\u001b[0m       0.3958            0.3958        1.6014  0.0007  0.1104\n",
      "     41            1.0000        0.0541       0.3958            0.3958        1.6957  0.0007  0.0987\n",
      "     42            1.0000        \u001b[32m0.0324\u001b[0m       0.3958            0.3958        1.7724  0.0007  0.0997\n",
      "     43            1.0000        0.0479       0.4062            0.4062        1.8323  0.0006  0.1021\n",
      "     44            1.0000        \u001b[32m0.0274\u001b[0m       0.3958            0.3958        1.8794  0.0006  0.1148\n",
      "     45            1.0000        0.0282       0.3958            0.3958        1.9146  0.0005  0.1009\n",
      "     46            1.0000        0.0351       0.3958            0.3958        1.9400  0.0005  0.0966\n",
      "     47            1.0000        \u001b[32m0.0202\u001b[0m       0.3958            0.3958        1.9565  0.0004  0.1181\n",
      "     48            1.0000        0.0248       0.3854            0.3854        1.9645  0.0004  0.1172\n",
      "     49            1.0000        \u001b[32m0.0197\u001b[0m       0.3854            0.3854        1.9675  0.0003  0.0981\n",
      "     50            1.0000        0.0240       0.3958            0.3958        1.9649  0.0003  0.1009\n",
      "Fine tuning model for subject 3 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4500        1.1537       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8808\u001b[0m  0.0004  0.1004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6500        1.1064       0.6562            0.6562        \u001b[94m0.8756\u001b[0m  0.0005  0.1510\n",
      "     33            \u001b[36m0.8500\u001b[0m        \u001b[32m0.6625\u001b[0m       0.6562            0.6562        \u001b[94m0.8753\u001b[0m  0.0005  0.1010\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5190\u001b[0m       0.6250            0.6250        0.8774  0.0006  0.1174\n",
      "     35            1.0000        \u001b[32m0.4076\u001b[0m       0.5938            0.5938        0.9076  0.0006  0.1199\n",
      "     36            1.0000        \u001b[32m0.2215\u001b[0m       0.5938            0.5938        0.9529  0.0007  0.0938\n",
      "     37            1.0000        0.2264       0.5625            0.5625        0.9873  0.0007  0.1161\n",
      "     38            1.0000        \u001b[32m0.2032\u001b[0m       0.5417            0.5417        1.0090  0.0007  0.1087\n",
      "     39            1.0000        \u001b[32m0.1113\u001b[0m       0.5625            0.5625        1.0230  0.0007  0.0996\n",
      "     40            1.0000        \u001b[32m0.0676\u001b[0m       0.5521            0.5521        1.0370  0.0007  0.0992\n",
      "     41            1.0000        0.0753       0.5104            0.5104        1.0494  0.0007  0.1013\n",
      "     42            1.0000        \u001b[32m0.0528\u001b[0m       0.5208            0.5208        1.0636  0.0007  0.1190\n",
      "     43            1.0000        \u001b[32m0.0444\u001b[0m       0.5104            0.5104        1.0773  0.0006  0.1335\n",
      "     44            1.0000        \u001b[32m0.0335\u001b[0m       0.5104            0.5104        1.0904  0.0006  0.0960\n",
      "     45            1.0000        0.0364       0.5104            0.5104        1.1017  0.0005  0.1017\n",
      "     46            1.0000        0.0352       0.5208            0.5208        1.1117  0.0005  0.1200\n",
      "     47            1.0000        0.0431       0.5208            0.5208        1.1197  0.0004  0.1135\n",
      "     48            1.0000        0.0386       0.5208            0.5208        1.1272  0.0004  0.1022\n",
      "     49            1.0000        \u001b[32m0.0320\u001b[0m       0.5104            0.5104        1.1353  0.0003  0.1002\n",
      "     50            1.0000        0.0457       0.5104            0.5104        1.1435  0.0003  0.1230\n",
      "Fine tuning model for subject 3 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        1.0943       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.8964\u001b[0m  0.0004  0.1134\n",
      "     32            0.7500        1.0982       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        0.9198  0.0005  0.1120\n",
      "     33            \u001b[36m0.8000\u001b[0m        0.9648       0.6042            0.6042        0.9556  0.0005  0.1113\n",
      "     34            \u001b[36m0.8500\u001b[0m        \u001b[32m0.6075\u001b[0m       0.6042            0.6042        0.9874  0.0006  0.1201\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4572\u001b[0m       0.6042            0.6042        0.9948  0.0006  0.0929\n",
      "     36            1.0000        \u001b[32m0.3159\u001b[0m       0.6146            0.6146        0.9865  0.0007  0.1015\n",
      "     37            1.0000        \u001b[32m0.2358\u001b[0m       0.6354            0.6354        0.9724  0.0007  0.1047\n",
      "     38            1.0000        \u001b[32m0.1440\u001b[0m       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        0.9487  0.0007  0.1096\n",
      "     39            1.0000        0.1668       0.6667            0.6667        0.9364  0.0007  0.1027\n",
      "     40            1.0000        \u001b[32m0.0952\u001b[0m       0.6354            0.6354        0.9310  0.0007  0.1316\n",
      "     41            1.0000        \u001b[32m0.0758\u001b[0m       \u001b[35m0.6771\u001b[0m            \u001b[31m0.6771\u001b[0m        0.9327  0.0007  0.1041\n",
      "     42            1.0000        0.0792       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        0.9416  0.0007  0.1036\n",
      "     43            1.0000        \u001b[32m0.0730\u001b[0m       0.6771            0.6771        0.9610  0.0006  0.1216\n",
      "     44            1.0000        \u001b[32m0.0558\u001b[0m       0.6562            0.6562        0.9831  0.0006  0.0947\n",
      "     45            1.0000        \u001b[32m0.0366\u001b[0m       0.6354            0.6354        1.0044  0.0005  0.1202\n",
      "     46            1.0000        \u001b[32m0.0308\u001b[0m       0.6250            0.6250        1.0204  0.0005  0.0969\n",
      "     47            1.0000        0.0360       0.6146            0.6146        1.0316  0.0004  0.1108\n",
      "     48            1.0000        0.0479       0.6042            0.6042        1.0395  0.0004  0.1052\n",
      "     49            1.0000        0.0314       0.6042            0.6042        1.0454  0.0003  0.0966\n",
      "     50            1.0000        0.0468       0.6042            0.6042        1.0491  0.0003  0.1198\n",
      "Fine tuning model for subject 3 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5500        1.2354       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m0.8951\u001b[0m  0.0004  0.1055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6500        1.1552       0.6250            0.6250        0.9300  0.0005  0.1211\n",
      "     33            0.7500        0.7635       0.6042            0.6042        0.9760  0.0005  0.0990\n",
      "     34            \u001b[36m0.9500\u001b[0m        0.8207       0.6146            0.6146        1.0184  0.0006  0.1171\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4699\u001b[0m       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        1.0588  0.0006  0.1045\n",
      "     36            1.0000        \u001b[32m0.4164\u001b[0m       0.5625            0.5625        1.1214  0.0007  0.0944\n",
      "     37            1.0000        \u001b[32m0.2641\u001b[0m       0.5208            0.5208        1.2253  0.0007  0.1004\n",
      "     38            1.0000        \u001b[32m0.2599\u001b[0m       0.4271            0.4271        1.3573  0.0007  0.1183\n",
      "     39            1.0000        \u001b[32m0.1457\u001b[0m       0.3854            0.3854        1.5020  0.0007  0.1102\n",
      "     40            0.9500        \u001b[32m0.1302\u001b[0m       0.3854            0.3854        1.6351  0.0007  0.0905\n",
      "     41            0.9000        0.1324       0.3958            0.3958        1.7438  0.0007  0.1190\n",
      "     42            0.9000        \u001b[32m0.0588\u001b[0m       0.3958            0.3958        1.8217  0.0007  0.1013\n",
      "     43            0.8500        0.0644       0.3958            0.3958        1.8651  0.0006  0.1143\n",
      "     44            0.8500        \u001b[32m0.0480\u001b[0m       0.4062            0.4062        1.8769  0.0006  0.1006\n",
      "     45            0.9000        0.0559       0.4062            0.4062        1.8571  0.0005  0.1002\n",
      "     46            0.9500        0.0657       0.4271            0.4271        1.8195  0.0005  0.1183\n",
      "     47            1.0000        \u001b[32m0.0362\u001b[0m       0.4271            0.4271        1.7700  0.0004  0.1199\n",
      "     48            1.0000        0.0534       0.4167            0.4167        1.7120  0.0004  0.0928\n",
      "     49            1.0000        \u001b[32m0.0281\u001b[0m       0.4167            0.4167        1.6525  0.0003  0.1217\n",
      "     50            1.0000        0.0393       0.4062            0.4062        1.5953  0.0003  0.1092\n",
      "Fine tuning model for subject 3 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        0.8363       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8879\u001b[0m  0.0004  0.1045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        0.8106       0.6667            0.6667        0.8914  0.0005  0.1011\n",
      "     33            \u001b[36m0.8000\u001b[0m        0.9577       0.6354            0.6354        0.8966  0.0005  0.1173\n",
      "     34            \u001b[36m0.9500\u001b[0m        \u001b[32m0.6822\u001b[0m       0.6458            0.6458        0.9048  0.0006  0.1327\n",
      "     35            0.9500        \u001b[32m0.5353\u001b[0m       0.6250            0.6250        0.9220  0.0006  0.1209\n",
      "     36            0.9000        \u001b[32m0.4468\u001b[0m       0.6250            0.6250        0.9502  0.0007  0.1247\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2626\u001b[0m       0.6146            0.6146        0.9845  0.0007  0.1500\n",
      "     38            1.0000        \u001b[32m0.1916\u001b[0m       0.6250            0.6250        1.0141  0.0007  0.1394\n",
      "     39            1.0000        \u001b[32m0.1595\u001b[0m       0.6146            0.6146        1.0399  0.0007  0.1147\n",
      "     40            1.0000        \u001b[32m0.1509\u001b[0m       0.6146            0.6146        1.0641  0.0007  0.1604\n",
      "     41            1.0000        \u001b[32m0.1299\u001b[0m       0.6250            0.6250        1.0810  0.0007  0.1313\n",
      "     42            1.0000        \u001b[32m0.0645\u001b[0m       0.6354            0.6354        1.0928  0.0007  0.1257\n",
      "     43            1.0000        0.0728       0.6458            0.6458        1.1018  0.0006  0.1178\n",
      "     44            1.0000        \u001b[32m0.0574\u001b[0m       0.6354            0.6354        1.1086  0.0006  0.1332\n",
      "     45            1.0000        \u001b[32m0.0358\u001b[0m       0.6354            0.6354        1.1136  0.0005  0.1356\n",
      "     46            1.0000        0.0362       0.6250            0.6250        1.1164  0.0005  0.1208\n",
      "     47            1.0000        0.0552       0.6250            0.6250        1.1164  0.0004  0.1353\n",
      "     48            1.0000        0.0380       0.6042            0.6042        1.1160  0.0004  0.1358\n",
      "     49            1.0000        0.0409       0.6042            0.6042        1.1151  0.0003  0.1990\n",
      "     50            1.0000        0.0629       0.6146            0.6146        1.1142  0.0003  0.1356\n",
      "Fine tuning model for subject 3 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        1.1985       \u001b[35m0.6771\u001b[0m            \u001b[31m0.6771\u001b[0m        \u001b[94m0.9035\u001b[0m  0.0004  0.1384\n",
      "     32            0.7500        0.9758       0.6146            0.6146        0.9432  0.0005  0.1131\n",
      "     33            0.8000        \u001b[32m0.7079\u001b[0m       0.6146            0.6146        0.9932  0.0005  0.1209\n",
      "     34            \u001b[36m0.8500\u001b[0m        \u001b[32m0.5280\u001b[0m       0.6146            0.6146        1.0548  0.0006  0.1171\n",
      "     35            \u001b[36m0.9500\u001b[0m        \u001b[32m0.3911\u001b[0m       0.5625            0.5625        1.0989  0.0006  0.0981\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1987\u001b[0m       0.5417            0.5417        1.1348  0.0007  0.0939\n",
      "     37            1.0000        \u001b[32m0.1720\u001b[0m       0.5312            0.5312        1.1484  0.0007  0.1327\n",
      "     38            1.0000        \u001b[32m0.1121\u001b[0m       0.5000            0.5000        1.1519  0.0007  0.1203\n",
      "     39            1.0000        \u001b[32m0.0840\u001b[0m       0.4896            0.4896        1.1529  0.0007  0.0930\n",
      "     40            1.0000        \u001b[32m0.0612\u001b[0m       0.5000            0.5000        1.1547  0.0007  0.1170\n",
      "     41            1.0000        \u001b[32m0.0598\u001b[0m       0.5000            0.5000        1.1584  0.0007  0.1036\n",
      "     42            1.0000        0.0714       0.4583            0.4583        1.1641  0.0007  0.1127\n",
      "     43            1.0000        \u001b[32m0.0476\u001b[0m       0.4688            0.4688        1.1721  0.0006  0.1024\n",
      "     44            1.0000        0.0726       0.4688            0.4688        1.1797  0.0006  0.0963\n",
      "     45            1.0000        \u001b[32m0.0416\u001b[0m       0.4896            0.4896        1.1876  0.0005  0.1031\n",
      "     46            1.0000        \u001b[32m0.0285\u001b[0m       0.4896            0.4896        1.1956  0.0005  0.1118\n",
      "     47            1.0000        0.0413       0.4896            0.4896        1.2043  0.0004  0.0896\n",
      "     48            1.0000        \u001b[32m0.0183\u001b[0m       0.4896            0.4896        1.2128  0.0004  0.0997\n",
      "     49            1.0000        0.0215       0.4792            0.4792        1.2213  0.0003  0.1188\n",
      "     50            1.0000        0.0221       0.4688            0.4688        1.2298  0.0003  0.1117\n",
      "Fine tuning model for subject 3 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        \u001b[32m0.6607\u001b[0m       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8826\u001b[0m  0.0004  0.1146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        0.7670       \u001b[35m0.6771\u001b[0m            \u001b[31m0.6771\u001b[0m        \u001b[94m0.8809\u001b[0m  0.0005  0.1313\n",
      "     33            \u001b[36m0.8400\u001b[0m        \u001b[32m0.6415\u001b[0m       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        \u001b[94m0.8750\u001b[0m  0.0005  0.1168\n",
      "     34            \u001b[36m0.9600\u001b[0m        \u001b[32m0.4950\u001b[0m       0.6458            0.6458        \u001b[94m0.8733\u001b[0m  0.0006  0.1180\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4046\u001b[0m       0.6146            0.6146        0.8876  0.0006  0.1250\n",
      "     36            1.0000        \u001b[32m0.2601\u001b[0m       0.5833            0.5833        0.9230  0.0007  0.1291\n",
      "     37            1.0000        \u001b[32m0.2138\u001b[0m       0.5938            0.5938        0.9722  0.0007  0.0983\n",
      "     38            1.0000        \u001b[32m0.1891\u001b[0m       0.5729            0.5729        1.0190  0.0007  0.0998\n",
      "     39            1.0000        \u001b[32m0.0857\u001b[0m       0.5729            0.5729        1.0594  0.0007  0.1120\n",
      "     40            1.0000        0.1010       0.5625            0.5625        1.0890  0.0007  0.1203\n",
      "     41            1.0000        \u001b[32m0.0835\u001b[0m       0.5729            0.5729        1.1084  0.0007  0.1250\n",
      "     42            1.0000        \u001b[32m0.0555\u001b[0m       0.5729            0.5729        1.1211  0.0007  0.1271\n",
      "     43            1.0000        0.0598       0.5729            0.5729        1.1260  0.0006  0.1144\n",
      "     44            1.0000        \u001b[32m0.0345\u001b[0m       0.5729            0.5729        1.1274  0.0006  0.1173\n",
      "     45            1.0000        0.0500       0.5729            0.5729        1.1262  0.0005  0.1144\n",
      "     46            1.0000        0.0372       0.5729            0.5729        1.1235  0.0005  0.1168\n",
      "     47            1.0000        \u001b[32m0.0300\u001b[0m       0.5833            0.5833        1.1191  0.0004  0.1356\n",
      "     48            1.0000        0.0368       0.5938            0.5938        1.1128  0.0004  0.1236\n",
      "     49            1.0000        0.0406       0.5833            0.5833        1.1064  0.0003  0.1227\n",
      "     50            1.0000        \u001b[32m0.0246\u001b[0m       0.5729            0.5729        1.0997  0.0003  0.1258\n",
      "Fine tuning model for subject 3 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7200        0.9777       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8957\u001b[0m  0.0004  0.1549\n",
      "     32            0.7600        0.8911       0.6562            0.6562        0.9068  0.0005  0.1190\n",
      "     33            \u001b[36m0.8800\u001b[0m        0.7409       0.6354            0.6354        0.9296  0.0005  0.1218\n",
      "     34            \u001b[36m0.9600\u001b[0m        \u001b[32m0.6275\u001b[0m       0.6042            0.6042        0.9619  0.0006  0.1243\n",
      "     35            0.9600        \u001b[32m0.5373\u001b[0m       0.5625            0.5625        1.0114  0.0006  0.1239\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3574\u001b[0m       0.5521            0.5521        1.0733  0.0007  0.1835\n",
      "     37            1.0000        \u001b[32m0.3110\u001b[0m       0.5417            0.5417        1.1446  0.0007  0.1683\n",
      "     38            0.9600        \u001b[32m0.2338\u001b[0m       0.5625            0.5625        1.2082  0.0007  0.1256\n",
      "     39            0.9600        \u001b[32m0.2234\u001b[0m       0.5729            0.5729        1.2532  0.0007  0.1226\n",
      "     40            0.9600        \u001b[32m0.1135\u001b[0m       0.5521            0.5521        1.2886  0.0007  0.1265\n",
      "     41            1.0000        0.1448       0.5521            0.5521        1.3065  0.0007  0.1191\n",
      "     42            1.0000        0.1348       0.5521            0.5521        1.3101  0.0007  0.1199\n",
      "     43            1.0000        0.1593       0.5625            0.5625        1.3021  0.0006  0.1393\n",
      "     44            1.0000        \u001b[32m0.0798\u001b[0m       0.5625            0.5625        1.2893  0.0006  0.1101\n",
      "     45            1.0000        0.0860       0.5833            0.5833        1.2733  0.0005  0.1104\n",
      "     46            1.0000        \u001b[32m0.0717\u001b[0m       0.5833            0.5833        1.2581  0.0005  0.1196\n",
      "     47            1.0000        \u001b[32m0.0505\u001b[0m       0.5833            0.5833        1.2441  0.0004  0.1156\n",
      "     48            1.0000        0.0632       0.5729            0.5729        1.2313  0.0004  0.1215\n",
      "     49            1.0000        0.0678       0.5833            0.5833        1.2191  0.0003  0.1165\n",
      "     50            1.0000        \u001b[32m0.0483\u001b[0m       0.5833            0.5833        1.2089  0.0003  0.1164\n",
      "Fine tuning model for subject 3 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7600        0.7416       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.8932\u001b[0m  0.0004  0.1279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8400\u001b[0m        1.0312       0.6250            0.6250        0.9118  0.0005  0.1292\n",
      "     33            0.8400        0.7611       0.6250            0.6250        0.9446  0.0005  0.1148\n",
      "     34            \u001b[36m0.9600\u001b[0m        \u001b[32m0.6338\u001b[0m       0.5833            0.5833        0.9987  0.0006  0.1032\n",
      "     35            0.9600        \u001b[32m0.5970\u001b[0m       0.5312            0.5312        1.0740  0.0006  0.1198\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4964\u001b[0m       0.5312            0.5312        1.1760  0.0007  0.1347\n",
      "     37            1.0000        \u001b[32m0.3409\u001b[0m       0.5312            0.5312        1.2851  0.0007  0.1157\n",
      "     38            0.9200        \u001b[32m0.2419\u001b[0m       0.5208            0.5208        1.3900  0.0007  0.1228\n",
      "     39            0.9200        \u001b[32m0.2019\u001b[0m       0.5104            0.5104        1.4606  0.0007  0.1053\n",
      "     40            0.9600        0.3448       0.5104            0.5104        1.4761  0.0007  0.1395\n",
      "     41            0.9600        0.2326       0.5104            0.5104        1.4656  0.0007  0.1326\n",
      "     42            0.9600        \u001b[32m0.1069\u001b[0m       0.5208            0.5208        1.4495  0.0007  0.1187\n",
      "     43            0.9600        \u001b[32m0.0912\u001b[0m       0.5208            0.5208        1.4297  0.0006  0.1293\n",
      "     44            0.9600        \u001b[32m0.0728\u001b[0m       0.5208            0.5208        1.4108  0.0006  0.1217\n",
      "     45            1.0000        0.1286       0.5000            0.5000        1.3928  0.0005  0.1382\n",
      "     46            1.0000        0.0876       0.5000            0.5000        1.3781  0.0005  0.1543\n",
      "     47            1.0000        \u001b[32m0.0567\u001b[0m       0.4896            0.4896        1.3648  0.0004  0.1016\n",
      "     48            1.0000        0.0581       0.4792            0.4792        1.3512  0.0004  0.0975\n",
      "     49            1.0000        0.0642       0.5000            0.5000        1.3385  0.0003  0.1190\n",
      "     50            1.0000        0.0592       0.5000            0.5000        1.3278  0.0003  0.1179\n",
      "Fine tuning model for subject 3 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8400\u001b[0m        0.8233       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8827\u001b[0m  0.0004  0.1170\n",
      "     32            0.8400        0.7810       0.6354            0.6354        0.8933  0.0005  0.1198\n",
      "     33            \u001b[36m0.8800\u001b[0m        \u001b[32m0.6456\u001b[0m       0.6250            0.6250        0.9154  0.0005  0.1200\n",
      "     34            \u001b[36m0.9600\u001b[0m        \u001b[32m0.5797\u001b[0m       0.6146            0.6146        0.9331  0.0006  0.1552\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4984\u001b[0m       0.6146            0.6146        0.9612  0.0006  0.1195\n",
      "     36            1.0000        \u001b[32m0.3565\u001b[0m       0.5625            0.5625        1.0050  0.0007  0.1211\n",
      "     37            1.0000        \u001b[32m0.2047\u001b[0m       0.5104            0.5104        1.0622  0.0007  0.1067\n",
      "     38            1.0000        \u001b[32m0.1580\u001b[0m       0.5208            0.5208        1.0950  0.0007  0.1062\n",
      "     39            1.0000        \u001b[32m0.1415\u001b[0m       0.5312            0.5312        1.1143  0.0007  0.1162\n",
      "     40            1.0000        \u001b[32m0.0902\u001b[0m       0.5417            0.5417        1.1294  0.0007  0.1035\n",
      "     41            1.0000        0.1530       0.5417            0.5417        1.1330  0.0007  0.1176\n",
      "     42            1.0000        \u001b[32m0.0742\u001b[0m       0.5417            0.5417        1.1358  0.0007  0.1172\n",
      "     43            1.0000        0.0787       0.5208            0.5208        1.1368  0.0006  0.1443\n",
      "     44            1.0000        0.0763       0.5208            0.5208        1.1383  0.0006  0.1432\n",
      "     45            1.0000        \u001b[32m0.0539\u001b[0m       0.5104            0.5104        1.1419  0.0005  0.1380\n",
      "     46            1.0000        \u001b[32m0.0415\u001b[0m       0.5312            0.5312        1.1451  0.0005  0.1436\n",
      "     47            1.0000        \u001b[32m0.0356\u001b[0m       0.5312            0.5312        1.1469  0.0004  0.1150\n",
      "     48            1.0000        \u001b[32m0.0287\u001b[0m       0.5312            0.5312        1.1473  0.0004  0.1546\n",
      "     49            1.0000        \u001b[32m0.0257\u001b[0m       0.5417            0.5417        1.1467  0.0003  0.1434\n",
      "     50            1.0000        0.0355       0.5417            0.5417        1.1452  0.0003  0.1140\n",
      "Fine tuning model for subject 3 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7200        0.9658       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.9028\u001b[0m  0.0004  0.1289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        1.0734       0.5938            0.5938        0.9820  0.0005  0.1469\n",
      "     33            \u001b[36m0.8400\u001b[0m        \u001b[32m0.6506\u001b[0m       0.5833            0.5833        1.1196  0.0005  0.1345\n",
      "     34            \u001b[36m0.9600\u001b[0m        \u001b[32m0.6453\u001b[0m       0.5625            0.5625        1.2897  0.0006  0.1353\n",
      "     35            0.9600        \u001b[32m0.3801\u001b[0m       0.5417            0.5417        1.4554  0.0006  0.1355\n",
      "     36            0.9600        \u001b[32m0.3417\u001b[0m       0.4896            0.4896        1.6080  0.0007  0.1136\n",
      "     37            0.9600        \u001b[32m0.2001\u001b[0m       0.4583            0.4583        1.7611  0.0007  0.1353\n",
      "     38            0.9600        \u001b[32m0.1932\u001b[0m       0.4271            0.4271        1.9018  0.0007  0.1345\n",
      "     39            0.9200        \u001b[32m0.1531\u001b[0m       0.4167            0.4167        2.0178  0.0007  0.1344\n",
      "     40            0.9200        \u001b[32m0.1224\u001b[0m       0.3958            0.3958        2.1076  0.0007  0.1511\n",
      "     41            0.9200        \u001b[32m0.0983\u001b[0m       0.3854            0.3854        2.1679  0.0007  0.1849\n",
      "     42            0.9600        0.0994       0.3750            0.3750        2.1996  0.0007  0.1130\n",
      "     43            0.9600        \u001b[32m0.0768\u001b[0m       0.3646            0.3646        2.2081  0.0006  0.1202\n",
      "     44            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0598\u001b[0m       0.3958            0.3958        2.1949  0.0006  0.1270\n",
      "     45            1.0000        \u001b[32m0.0597\u001b[0m       0.4167            0.4167        2.1673  0.0005  0.1289\n",
      "     46            1.0000        \u001b[32m0.0350\u001b[0m       0.4271            0.4271        2.1349  0.0005  0.1141\n",
      "     47            1.0000        0.0592       0.4583            0.4583        2.0989  0.0004  0.1064\n",
      "     48            1.0000        0.0505       0.4688            0.4688        2.0635  0.0004  0.1166\n",
      "     49            1.0000        \u001b[32m0.0346\u001b[0m       0.4688            0.4688        2.0309  0.0003  0.1196\n",
      "     50            1.0000        0.0353       0.4479            0.4479        2.0016  0.0003  0.1282\n",
      "Fine tuning model for subject 3 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.0553       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8825\u001b[0m  0.0004  0.1135\n",
      "     32            0.7600        0.8296       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        0.8841  0.0005  0.1143\n",
      "     33            \u001b[36m0.8400\u001b[0m        0.7991       0.6667            0.6667        0.8880  0.0005  0.1041\n",
      "     34            \u001b[36m0.9600\u001b[0m        \u001b[32m0.6466\u001b[0m       0.6458            0.6458        0.9096  0.0006  0.1179\n",
      "     35            0.9600        \u001b[32m0.5328\u001b[0m       0.6354            0.6354        0.9559  0.0006  0.1230\n",
      "     36            0.9600        \u001b[32m0.3818\u001b[0m       0.5729            0.5729        1.0254  0.0007  0.1030\n",
      "     37            0.9600        \u001b[32m0.3570\u001b[0m       0.5417            0.5417        1.1192  0.0007  0.1014\n",
      "     38            0.9600        \u001b[32m0.2168\u001b[0m       0.5104            0.5104        1.2254  0.0007  0.1034\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1557\u001b[0m       0.4688            0.4688        1.3324  0.0007  0.1027\n",
      "     40            1.0000        \u001b[32m0.1233\u001b[0m       0.3958            0.3958        1.4277  0.0007  0.1173\n",
      "     41            1.0000        0.1362       0.3854            0.3854        1.4979  0.0007  0.1153\n",
      "     42            1.0000        \u001b[32m0.1125\u001b[0m       0.3854            0.3854        1.5522  0.0007  0.1093\n",
      "     43            1.0000        \u001b[32m0.0833\u001b[0m       0.3854            0.3854        1.5843  0.0006  0.1508\n",
      "     44            1.0000        0.1102       0.3958            0.3958        1.5983  0.0006  0.1199\n",
      "     45            1.0000        \u001b[32m0.0787\u001b[0m       0.4062            0.4062        1.5936  0.0005  0.1198\n",
      "     46            1.0000        0.0947       0.4167            0.4167        1.5741  0.0005  0.1189\n",
      "     47            1.0000        \u001b[32m0.0697\u001b[0m       0.4375            0.4375        1.5458  0.0004  0.1180\n",
      "     48            1.0000        \u001b[32m0.0635\u001b[0m       0.4375            0.4375        1.5113  0.0004  0.1072\n",
      "     49            1.0000        \u001b[32m0.0614\u001b[0m       0.4583            0.4583        1.4753  0.0003  0.1175\n",
      "     50            1.0000        \u001b[32m0.0466\u001b[0m       0.4896            0.4896        1.4387  0.0003  0.1208\n",
      "Fine tuning model for subject 3 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5600        1.2349       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8726\u001b[0m  0.0004  0.1057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7600        1.1799       0.6354            0.6354        \u001b[94m0.8649\u001b[0m  0.0005  0.1343\n",
      "     33            \u001b[36m0.8400\u001b[0m        \u001b[32m0.6735\u001b[0m       0.5833            0.5833        0.8911  0.0005  0.1334\n",
      "     34            \u001b[36m0.8800\u001b[0m        0.7475       0.5521            0.5521        0.9702  0.0006  0.1145\n",
      "     35            \u001b[36m0.9200\u001b[0m        \u001b[32m0.4874\u001b[0m       0.5625            0.5625        1.0650  0.0006  0.1208\n",
      "     36            0.9200        \u001b[32m0.2988\u001b[0m       0.5521            0.5521        1.1376  0.0007  0.1274\n",
      "     37            \u001b[36m0.9600\u001b[0m        \u001b[32m0.2623\u001b[0m       0.5417            0.5417        1.1870  0.0007  0.1148\n",
      "     38            0.9600        \u001b[32m0.2185\u001b[0m       0.5417            0.5417        1.2133  0.0007  0.0999\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1565\u001b[0m       0.5104            0.5104        1.2234  0.0007  0.1145\n",
      "     40            1.0000        \u001b[32m0.1418\u001b[0m       0.5208            0.5208        1.2291  0.0007  0.1177\n",
      "     41            1.0000        \u001b[32m0.1280\u001b[0m       0.5208            0.5208        1.2319  0.0007  0.1204\n",
      "     42            1.0000        0.1298       0.5208            0.5208        1.2316  0.0007  0.1299\n",
      "     43            1.0000        \u001b[32m0.0898\u001b[0m       0.5312            0.5312        1.2310  0.0006  0.1155\n",
      "     44            1.0000        0.0911       0.5312            0.5312        1.2276  0.0006  0.1113\n",
      "     45            1.0000        0.1099       0.5417            0.5417        1.2277  0.0005  0.1289\n",
      "     46            1.0000        \u001b[32m0.0691\u001b[0m       0.5312            0.5312        1.2281  0.0005  0.1460\n",
      "     47            1.0000        0.0701       0.5208            0.5208        1.2299  0.0004  0.1202\n",
      "     48            1.0000        \u001b[32m0.0629\u001b[0m       0.5208            0.5208        1.2307  0.0004  0.1205\n",
      "     49            1.0000        \u001b[32m0.0551\u001b[0m       0.5104            0.5104        1.2304  0.0003  0.1157\n",
      "     50            1.0000        \u001b[32m0.0457\u001b[0m       0.5104            0.5104        1.2296  0.0003  0.1164\n",
      "Fine tuning model for subject 3 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.2832       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8869\u001b[0m  0.0004  0.1240\n",
      "     32            0.7200        1.0106       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        0.8909  0.0005  0.1146\n",
      "     33            \u001b[36m0.8000\u001b[0m        0.8589       0.6250            0.6250        0.9036  0.0005  0.1042\n",
      "     34            \u001b[36m0.8400\u001b[0m        0.8309       0.6042            0.6042        0.9437  0.0006  0.1136\n",
      "     35            \u001b[36m0.9600\u001b[0m        \u001b[32m0.5354\u001b[0m       0.5625            0.5625        1.0223  0.0006  0.0986\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4922\u001b[0m       0.5312            0.5312        1.1306  0.0007  0.1123\n",
      "     37            1.0000        \u001b[32m0.3591\u001b[0m       0.5104            0.5104        1.2598  0.0007  0.1007\n",
      "     38            1.0000        \u001b[32m0.2233\u001b[0m       0.4792            0.4792        1.3979  0.0007  0.1022\n",
      "     39            0.9200        0.2494       0.4792            0.4792        1.5195  0.0007  0.1047\n",
      "     40            0.9600        \u001b[32m0.1482\u001b[0m       0.4688            0.4688        1.6168  0.0007  0.1153\n",
      "     41            0.9600        \u001b[32m0.1150\u001b[0m       0.4792            0.4792        1.6796  0.0007  0.0987\n",
      "     42            1.0000        \u001b[32m0.1031\u001b[0m       0.4583            0.4583        1.7192  0.0007  0.1114\n",
      "     43            1.0000        \u001b[32m0.0950\u001b[0m       0.4583            0.4583        1.7332  0.0006  0.1165\n",
      "     44            1.0000        \u001b[32m0.0937\u001b[0m       0.4688            0.4688        1.7296  0.0006  0.1375\n",
      "     45            1.0000        \u001b[32m0.0805\u001b[0m       0.4583            0.4583        1.7051  0.0005  0.1290\n",
      "     46            1.0000        0.0823       0.4479            0.4479        1.6669  0.0005  0.1374\n",
      "     47            1.0000        0.0835       0.4479            0.4479        1.6259  0.0004  0.1656\n",
      "     48            1.0000        \u001b[32m0.0600\u001b[0m       0.4479            0.4479        1.5796  0.0004  0.1230\n",
      "     49            1.0000        0.0833       0.4583            0.4583        1.5328  0.0003  0.1143\n",
      "     50            1.0000        \u001b[32m0.0354\u001b[0m       0.4688            0.4688        1.4885  0.0003  0.1010\n",
      "Fine tuning model for subject 3 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4800        1.2326       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8811\u001b[0m  0.0004  0.1074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5200        1.0837       \u001b[35m0.6771\u001b[0m            \u001b[31m0.6771\u001b[0m        \u001b[94m0.8721\u001b[0m  0.0005  0.1170\n",
      "     33            0.7600        0.9644       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        \u001b[94m0.8719\u001b[0m  0.0005  0.1176\n",
      "     34            \u001b[36m0.8800\u001b[0m        0.8702       0.6562            0.6562        0.8848  0.0006  0.1177\n",
      "     35            \u001b[36m0.9200\u001b[0m        \u001b[32m0.5364\u001b[0m       0.6354            0.6354        0.9220  0.0006  0.1042\n",
      "     36            \u001b[36m1.0000\u001b[0m        0.5865       0.6146            0.6146        0.9716  0.0007  0.1250\n",
      "     37            1.0000        \u001b[32m0.3913\u001b[0m       0.6250            0.6250        1.0219  0.0007  0.1300\n",
      "     38            1.0000        \u001b[32m0.2657\u001b[0m       0.5938            0.5938        1.0661  0.0007  0.1116\n",
      "     39            1.0000        \u001b[32m0.1713\u001b[0m       0.5521            0.5521        1.1045  0.0007  0.1103\n",
      "     40            1.0000        \u001b[32m0.1657\u001b[0m       0.5521            0.5521        1.1340  0.0007  0.1200\n",
      "     41            1.0000        0.1822       0.5312            0.5312        1.1528  0.0007  0.1177\n",
      "     42            1.0000        \u001b[32m0.1446\u001b[0m       0.5000            0.5000        1.1629  0.0007  0.1162\n",
      "     43            1.0000        \u001b[32m0.1277\u001b[0m       0.5104            0.5104        1.1666  0.0006  0.1152\n",
      "     44            1.0000        \u001b[32m0.1022\u001b[0m       0.5208            0.5208        1.1664  0.0006  0.1086\n",
      "     45            1.0000        \u001b[32m0.0588\u001b[0m       0.5104            0.5104        1.1661  0.0005  0.1190\n",
      "     46            1.0000        0.0642       0.5208            0.5208        1.1644  0.0005  0.1209\n",
      "     47            1.0000        0.0626       0.5104            0.5104        1.1616  0.0004  0.1299\n",
      "     48            1.0000        0.0603       0.5104            0.5104        1.1578  0.0004  0.1152\n",
      "     49            1.0000        0.0704       0.5208            0.5208        1.1544  0.0003  0.1094\n",
      "     50            1.0000        0.0707       0.5208            0.5208        1.1510  0.0003  0.1193\n",
      "Fine tuning model for subject 3 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        1.0552       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8832\u001b[0m  0.0004  0.1053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        1.0218       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8671\u001b[0m  0.0005  0.1532\n",
      "     33            \u001b[36m0.8800\u001b[0m        0.8008       0.6458            0.6458        \u001b[94m0.8502\u001b[0m  0.0005  0.1313\n",
      "     34            \u001b[36m0.9200\u001b[0m        \u001b[32m0.5816\u001b[0m       0.6458            0.6458        0.8551  0.0006  0.1381\n",
      "     35            \u001b[36m0.9600\u001b[0m        \u001b[32m0.4601\u001b[0m       0.6562            0.6562        0.8923  0.0006  0.1317\n",
      "     36            0.9200        \u001b[32m0.3857\u001b[0m       0.6250            0.6250        0.9454  0.0007  0.1495\n",
      "     37            0.9200        \u001b[32m0.2961\u001b[0m       0.5833            0.5833        1.0133  0.0007  0.1368\n",
      "     38            0.9200        \u001b[32m0.2186\u001b[0m       0.5521            0.5521        1.0823  0.0007  0.1806\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1959\u001b[0m       0.5312            0.5312        1.1475  0.0007  0.1226\n",
      "     40            1.0000        \u001b[32m0.1541\u001b[0m       0.5312            0.5312        1.2110  0.0007  0.1375\n",
      "     41            1.0000        \u001b[32m0.1199\u001b[0m       0.5312            0.5312        1.2644  0.0007  0.1177\n",
      "     42            1.0000        \u001b[32m0.0698\u001b[0m       0.5312            0.5312        1.3078  0.0007  0.1500\n",
      "     43            1.0000        \u001b[32m0.0584\u001b[0m       0.5417            0.5417        1.3416  0.0006  0.1243\n",
      "     44            1.0000        0.0827       0.5417            0.5417        1.3666  0.0006  0.1349\n",
      "     45            1.0000        \u001b[32m0.0543\u001b[0m       0.5417            0.5417        1.3841  0.0005  0.1192\n",
      "     46            1.0000        0.0670       0.5625            0.5625        1.3965  0.0005  0.1221\n",
      "     47            1.0000        0.0637       0.5625            0.5625        1.4038  0.0004  0.1326\n",
      "     48            1.0000        0.0771       0.5625            0.5625        1.4052  0.0004  0.1499\n",
      "     49            1.0000        \u001b[32m0.0361\u001b[0m       0.5625            0.5625        1.4051  0.0003  0.1355\n",
      "     50            1.0000        \u001b[32m0.0323\u001b[0m       0.5625            0.5625        1.4040  0.0003  0.1844\n",
      "Fine tuning model for subject 3 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7333        0.8001       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8771\u001b[0m  0.0004  0.1173\n",
      "     32            \u001b[36m0.8000\u001b[0m        0.7435       0.6667            0.6667        \u001b[94m0.8633\u001b[0m  0.0005  0.1072\n",
      "     33            0.8000        \u001b[32m0.4915\u001b[0m       0.6667            0.6667        \u001b[94m0.8545\u001b[0m  0.0005  0.1190\n",
      "     34            \u001b[36m0.9000\u001b[0m        0.5746       0.6354            0.6354        0.8588  0.0006  0.1190\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4174\u001b[0m       0.6042            0.6042        0.8853  0.0006  0.1325\n",
      "     36            1.0000        \u001b[32m0.3134\u001b[0m       0.5833            0.5833        0.9347  0.0007  0.1271\n",
      "     37            1.0000        \u001b[32m0.2838\u001b[0m       0.5312            0.5312        0.9981  0.0007  0.1252\n",
      "     38            1.0000        \u001b[32m0.1875\u001b[0m       0.5000            0.5000        1.0593  0.0007  0.1680\n",
      "     39            1.0000        \u001b[32m0.1701\u001b[0m       0.4896            0.4896        1.1126  0.0007  0.1334\n",
      "     40            1.0000        \u001b[32m0.1552\u001b[0m       0.4688            0.4688        1.1550  0.0007  0.1142\n",
      "     41            1.0000        \u001b[32m0.1372\u001b[0m       0.4688            0.4688        1.1887  0.0007  0.1109\n",
      "     42            1.0000        \u001b[32m0.0597\u001b[0m       0.4583            0.4583        1.2158  0.0007  0.1159\n",
      "     43            1.0000        0.0747       0.4583            0.4583        1.2342  0.0006  0.1342\n",
      "     44            1.0000        \u001b[32m0.0593\u001b[0m       0.4688            0.4688        1.2463  0.0006  0.1344\n",
      "     45            1.0000        0.0644       0.4583            0.4583        1.2528  0.0005  0.1208\n",
      "     46            1.0000        0.0642       0.4375            0.4375        1.2557  0.0005  0.1201\n",
      "     47            1.0000        0.0759       0.4583            0.4583        1.2562  0.0004  0.1282\n",
      "     48            1.0000        \u001b[32m0.0418\u001b[0m       0.4792            0.4792        1.2532  0.0004  0.1240\n",
      "     49            1.0000        \u001b[32m0.0354\u001b[0m       0.4896            0.4896        1.2484  0.0003  0.1391\n",
      "     50            1.0000        0.0486       0.5208            0.5208        1.2427  0.0003  0.1171\n",
      "Fine tuning model for subject 3 with 30 = 30 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5667        1.1437       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8768\u001b[0m  0.0004  0.1235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6333        1.0448       \u001b[35m0.6771\u001b[0m            \u001b[31m0.6771\u001b[0m        \u001b[94m0.8682\u001b[0m  0.0005  0.1389\n",
      "     33            0.7000        0.9215       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        \u001b[94m0.8605\u001b[0m  0.0005  0.1179\n",
      "     34            0.7667        0.8572       0.6562            0.6562        0.8699  0.0006  0.1188\n",
      "     35            \u001b[36m0.8000\u001b[0m        \u001b[32m0.6273\u001b[0m       0.5729            0.5729        0.9048  0.0006  0.1209\n",
      "     36            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5877\u001b[0m       0.5417            0.5417        0.9700  0.0007  0.1309\n",
      "     37            0.9000        \u001b[32m0.5382\u001b[0m       0.5208            0.5208        1.0417  0.0007  0.1242\n",
      "     38            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3740\u001b[0m       0.5000            0.5000        1.1081  0.0007  0.1393\n",
      "     39            \u001b[36m0.9667\u001b[0m        \u001b[32m0.2135\u001b[0m       0.5104            0.5104        1.1571  0.0007  0.1163\n",
      "     40            0.9667        0.2141       0.5000            0.5000        1.1913  0.0007  0.1062\n",
      "     41            0.9667        \u001b[32m0.1611\u001b[0m       0.5000            0.5000        1.2153  0.0007  0.1176\n",
      "     42            0.9667        \u001b[32m0.1449\u001b[0m       0.4688            0.4688        1.2258  0.0007  0.1197\n",
      "     43            0.9667        \u001b[32m0.1435\u001b[0m       0.4688            0.4688        1.2267  0.0006  0.1209\n",
      "     44            0.9667        0.1728       0.4688            0.4688        1.2184  0.0006  0.1353\n",
      "     45            0.9667        \u001b[32m0.1175\u001b[0m       0.4688            0.4688        1.2017  0.0005  0.1344\n",
      "     46            0.9667        0.1179       0.4688            0.4688        1.1794  0.0005  0.1397\n",
      "     47            0.9667        \u001b[32m0.0856\u001b[0m       0.4792            0.4792        1.1559  0.0004  0.1372\n",
      "     48            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0673\u001b[0m       0.4896            0.4896        1.1327  0.0004  0.1204\n",
      "     49            1.0000        0.0952       0.4896            0.4896        1.1116  0.0003  0.1080\n",
      "     50            1.0000        0.0821       0.4792            0.4792        1.0913  0.0003  0.1165\n",
      "Fine tuning model for subject 3 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        0.9285       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8906\u001b[0m  0.0004  0.1295\n",
      "     32            \u001b[36m0.8333\u001b[0m        0.9130       0.6354            0.6354        0.8966  0.0005  0.1258\n",
      "     33            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6417\u001b[0m       0.6146            0.6146        0.9058  0.0005  0.1195\n",
      "     34            \u001b[36m0.9667\u001b[0m        0.6732       0.6042            0.6042        0.9191  0.0006  0.1199\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5125\u001b[0m       0.6042            0.6042        0.9303  0.0006  0.1354\n",
      "     36            1.0000        \u001b[32m0.3996\u001b[0m       0.5938            0.5938        0.9402  0.0007  0.1323\n",
      "     37            1.0000        \u001b[32m0.2429\u001b[0m       0.6042            0.6042        0.9528  0.0007  0.1133\n",
      "     38            1.0000        \u001b[32m0.2326\u001b[0m       0.5833            0.5833        0.9644  0.0007  0.1124\n",
      "     39            1.0000        0.2475       0.5938            0.5938        0.9802  0.0007  0.1149\n",
      "     40            1.0000        \u001b[32m0.1303\u001b[0m       0.5938            0.5938        0.9966  0.0007  0.1170\n",
      "     41            1.0000        0.2354       0.5833            0.5833        1.0149  0.0007  0.1669\n",
      "     42            1.0000        0.1469       0.5729            0.5729        1.0312  0.0007  0.1562\n",
      "     43            1.0000        0.1623       0.5729            0.5729        1.0474  0.0006  0.1120\n",
      "     44            1.0000        \u001b[32m0.0892\u001b[0m       0.5729            0.5729        1.0614  0.0006  0.1217\n",
      "     45            1.0000        0.1186       0.5521            0.5521        1.0695  0.0005  0.1167\n",
      "     46            1.0000        \u001b[32m0.0758\u001b[0m       0.5417            0.5417        1.0759  0.0005  0.1365\n",
      "     47            1.0000        \u001b[32m0.0680\u001b[0m       0.5417            0.5417        1.0788  0.0004  0.1442\n",
      "     48            1.0000        \u001b[32m0.0519\u001b[0m       0.5521            0.5521        1.0790  0.0004  0.1259\n",
      "     49            1.0000        0.0631       0.5521            0.5521        1.0766  0.0003  0.1251\n",
      "     50            1.0000        0.0871       0.5521            0.5521        1.0732  0.0003  0.1171\n",
      "Fine tuning model for subject 3 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7667        0.7344       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8810\u001b[0m  0.0004  0.1270\n",
      "     32            \u001b[36m0.8000\u001b[0m        0.8322       0.6562            0.6562        \u001b[94m0.8743\u001b[0m  0.0005  0.1116\n",
      "     33            \u001b[36m0.8667\u001b[0m        0.9436       0.6562            0.6562        \u001b[94m0.8562\u001b[0m  0.0005  0.1206\n",
      "     34            \u001b[36m0.9000\u001b[0m        0.7288       0.6354            0.6354        \u001b[94m0.8417\u001b[0m  0.0006  0.1354\n",
      "     35            0.9000        \u001b[32m0.5244\u001b[0m       0.6458            0.6458        \u001b[94m0.8388\u001b[0m  0.0006  0.1367\n",
      "     36            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3468\u001b[0m       0.6354            0.6354        0.8582  0.0007  0.1329\n",
      "     37            0.9667        \u001b[32m0.2598\u001b[0m       0.6250            0.6250        0.8923  0.0007  0.1338\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2397\u001b[0m       0.6146            0.6146        0.9265  0.0007  0.1187\n",
      "     39            1.0000        \u001b[32m0.2116\u001b[0m       0.6146            0.6146        0.9554  0.0007  0.1069\n",
      "     40            1.0000        \u001b[32m0.1385\u001b[0m       0.6042            0.6042        0.9777  0.0007  0.1161\n",
      "     41            1.0000        \u001b[32m0.1321\u001b[0m       0.5833            0.5833        0.9947  0.0007  0.1198\n",
      "     42            1.0000        \u001b[32m0.1298\u001b[0m       0.6042            0.6042        1.0038  0.0007  0.1210\n",
      "     43            1.0000        \u001b[32m0.0960\u001b[0m       0.5938            0.5938        1.0100  0.0006  0.1300\n",
      "     44            1.0000        \u001b[32m0.0578\u001b[0m       0.5938            0.5938        1.0148  0.0006  0.1281\n",
      "     45            1.0000        \u001b[32m0.0522\u001b[0m       0.6146            0.6146        1.0182  0.0005  0.1234\n",
      "     46            1.0000        0.0580       0.6042            0.6042        1.0202  0.0005  0.1170\n",
      "     47            1.0000        \u001b[32m0.0509\u001b[0m       0.6042            0.6042        1.0212  0.0004  0.1190\n",
      "     48            1.0000        0.0513       0.6042            0.6042        1.0213  0.0004  0.1332\n",
      "     49            1.0000        0.0565       0.6042            0.6042        1.0212  0.0003  0.1197\n",
      "     50            1.0000        \u001b[32m0.0494\u001b[0m       0.6146            0.6146        1.0204  0.0003  0.1200\n",
      "Fine tuning model for subject 3 with 30 = 30 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        1.0544       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.8881\u001b[0m  0.0004  0.1161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7667        0.8934       0.6146            0.6146        0.8989  0.0005  0.1198\n",
      "     33            \u001b[36m0.8000\u001b[0m        0.8741       0.6146            0.6146        0.8970  0.0005  0.1158\n",
      "     34            \u001b[36m0.9000\u001b[0m        0.7646       0.6250            0.6250        0.8951  0.0006  0.1181\n",
      "     35            \u001b[36m0.9333\u001b[0m        \u001b[32m0.6917\u001b[0m       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        0.8943  0.0006  0.1527\n",
      "     36            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4516\u001b[0m       0.6042            0.6042        0.9121  0.0007  0.1197\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3920\u001b[0m       0.5729            0.5729        0.9516  0.0007  0.1520\n",
      "     38            1.0000        \u001b[32m0.3459\u001b[0m       0.5312            0.5312        1.0020  0.0007  0.1522\n",
      "     39            0.9667        \u001b[32m0.2561\u001b[0m       0.5208            0.5208        1.0561  0.0007  0.1323\n",
      "     40            0.9667        \u001b[32m0.2022\u001b[0m       0.5000            0.5000        1.0993  0.0007  0.1669\n",
      "     41            0.9667        \u001b[32m0.1387\u001b[0m       0.4896            0.4896        1.1301  0.0007  0.1450\n",
      "     42            0.9667        0.1625       0.5000            0.5000        1.1457  0.0007  0.1361\n",
      "     43            0.9667        \u001b[32m0.1297\u001b[0m       0.5000            0.5000        1.1533  0.0006  0.1357\n",
      "     44            1.0000        \u001b[32m0.1283\u001b[0m       0.5208            0.5208        1.1554  0.0006  0.1345\n",
      "     45            1.0000        \u001b[32m0.0895\u001b[0m       0.5208            0.5208        1.1570  0.0005  0.1510\n",
      "     46            1.0000        0.1043       0.5104            0.5104        1.1560  0.0005  0.1375\n",
      "     47            1.0000        \u001b[32m0.0869\u001b[0m       0.5104            0.5104        1.1520  0.0004  0.1345\n",
      "     48            1.0000        0.0894       0.5104            0.5104        1.1445  0.0004  0.1822\n",
      "     49            1.0000        0.0991       0.5104            0.5104        1.1369  0.0003  0.1287\n",
      "     50            1.0000        \u001b[32m0.0646\u001b[0m       0.5208            0.5208        1.1296  0.0003  0.1596\n",
      "Fine tuning model for subject 3 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        1.0474       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8845\u001b[0m  0.0004  0.1318\n",
      "     32            0.7667        1.0416       0.6458            0.6458        0.8919  0.0005  0.1690\n",
      "     33            \u001b[36m0.8000\u001b[0m        0.7917       0.6458            0.6458        0.9026  0.0005  0.1161\n",
      "     34            \u001b[36m0.8333\u001b[0m        \u001b[32m0.6839\u001b[0m       0.6250            0.6250        0.9063  0.0006  0.1219\n",
      "     35            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5839\u001b[0m       0.6458            0.6458        0.8992  0.0006  0.1177\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3865\u001b[0m       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        0.8858  0.0007  0.1196\n",
      "     37            1.0000        \u001b[32m0.3635\u001b[0m       \u001b[35m0.6771\u001b[0m            \u001b[31m0.6771\u001b[0m        \u001b[94m0.8737\u001b[0m  0.0007  0.1377\n",
      "     38            1.0000        0.3818       0.6458            0.6458        0.8786  0.0007  0.1367\n",
      "     39            1.0000        \u001b[32m0.2293\u001b[0m       0.6146            0.6146        0.9030  0.0007  0.1381\n",
      "     40            1.0000        \u001b[32m0.1613\u001b[0m       0.6146            0.6146        0.9410  0.0007  0.1181\n",
      "     41            1.0000        0.2375       0.6146            0.6146        0.9830  0.0007  0.1173\n",
      "     42            1.0000        \u001b[32m0.1156\u001b[0m       0.6250            0.6250        1.0256  0.0007  0.1373\n",
      "     43            1.0000        0.1257       0.6042            0.6042        1.0622  0.0006  0.1143\n",
      "     44            1.0000        0.1428       0.5833            0.5833        1.0883  0.0006  0.1206\n",
      "     45            1.0000        \u001b[32m0.1104\u001b[0m       0.5833            0.5833        1.1036  0.0005  0.1284\n",
      "     46            1.0000        0.1334       0.5729            0.5729        1.1092  0.0005  0.1236\n",
      "     47            1.0000        \u001b[32m0.0677\u001b[0m       0.5833            0.5833        1.1046  0.0004  0.1441\n",
      "     48            1.0000        0.0970       0.5938            0.5938        1.0911  0.0004  0.1142\n",
      "     49            1.0000        0.0777       0.5938            0.5938        1.0740  0.0003  0.1167\n",
      "     50            1.0000        0.0684       0.5938            0.5938        1.0557  0.0003  0.1336\n",
      "Fine tuning model for subject 3 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        0.9457       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8869\u001b[0m  0.0004  0.1293\n",
      "     32            0.7333        0.9661       0.6667            0.6667        \u001b[94m0.8821\u001b[0m  0.0005  0.1268\n",
      "     33            \u001b[36m0.8333\u001b[0m        0.8679       0.6667            0.6667        \u001b[94m0.8761\u001b[0m  0.0005  0.1196\n",
      "     34            \u001b[36m0.9333\u001b[0m        0.7272       0.6354            0.6354        0.8812  0.0006  0.1136\n",
      "     35            \u001b[36m0.9667\u001b[0m        \u001b[32m0.6982\u001b[0m       0.6146            0.6146        0.9137  0.0006  0.1288\n",
      "     36            0.9667        \u001b[32m0.4017\u001b[0m       0.5625            0.5625        0.9672  0.0007  0.1218\n",
      "     37            0.9667        \u001b[32m0.2911\u001b[0m       0.5521            0.5521        1.0292  0.0007  0.1220\n",
      "     38            \u001b[36m1.0000\u001b[0m        0.3008       0.5208            0.5208        1.0940  0.0007  0.1067\n",
      "     39            1.0000        \u001b[32m0.1715\u001b[0m       0.5312            0.5312        1.1465  0.0007  0.1113\n",
      "     40            1.0000        0.1746       0.5208            0.5208        1.1867  0.0007  0.1210\n",
      "     41            1.0000        \u001b[32m0.1356\u001b[0m       0.5208            0.5208        1.2154  0.0007  0.1158\n",
      "     42            1.0000        0.1465       0.5000            0.5000        1.2327  0.0007  0.1196\n",
      "     43            1.0000        \u001b[32m0.0947\u001b[0m       0.5104            0.5104        1.2392  0.0006  0.1366\n",
      "     44            1.0000        \u001b[32m0.0834\u001b[0m       0.5104            0.5104        1.2387  0.0006  0.1333\n",
      "     45            1.0000        \u001b[32m0.0765\u001b[0m       0.5417            0.5417        1.2335  0.0005  0.1310\n",
      "     46            1.0000        \u001b[32m0.0701\u001b[0m       0.5521            0.5521        1.2242  0.0005  0.1137\n",
      "     47            1.0000        \u001b[32m0.0588\u001b[0m       0.5521            0.5521        1.2118  0.0004  0.1135\n",
      "     48            1.0000        0.0685       0.5729            0.5729        1.1963  0.0004  0.1143\n",
      "     49            1.0000        0.0590       0.5833            0.5833        1.1803  0.0003  0.1363\n",
      "     50            1.0000        \u001b[32m0.0491\u001b[0m       0.5833            0.5833        1.1646  0.0003  0.1189\n",
      "Fine tuning model for subject 3 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        0.9800       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8860\u001b[0m  0.0004  0.1645\n",
      "     32            0.7000        1.1022       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8857\u001b[0m  0.0005  0.1115\n",
      "     33            \u001b[36m0.8333\u001b[0m        0.7787       0.6562            0.6562        \u001b[94m0.8831\u001b[0m  0.0005  0.1219\n",
      "     34            \u001b[36m0.9000\u001b[0m        0.7256       0.6562            0.6562        0.8890  0.0006  0.1260\n",
      "     35            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5703\u001b[0m       0.6250            0.6250        0.9179  0.0006  0.1354\n",
      "     36            0.9333        \u001b[32m0.3576\u001b[0m       0.6042            0.6042        0.9760  0.0007  0.1328\n",
      "     37            \u001b[36m0.9667\u001b[0m        \u001b[32m0.2789\u001b[0m       0.5729            0.5729        1.0485  0.0007  0.1147\n",
      "     38            0.9667        0.2976       0.5417            0.5417        1.1276  0.0007  0.1118\n",
      "     39            0.9667        \u001b[32m0.2009\u001b[0m       0.5208            0.5208        1.1930  0.0007  0.1170\n",
      "     40            0.9333        \u001b[32m0.1984\u001b[0m       0.5000            0.5000        1.2379  0.0007  0.1215\n",
      "     41            0.9333        \u001b[32m0.1524\u001b[0m       0.5104            0.5104        1.2656  0.0007  0.1161\n",
      "     42            0.9667        \u001b[32m0.1249\u001b[0m       0.5104            0.5104        1.2706  0.0007  0.1208\n",
      "     43            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1089\u001b[0m       0.5208            0.5208        1.2662  0.0006  0.1188\n",
      "     44            1.0000        \u001b[32m0.0718\u001b[0m       0.5312            0.5312        1.2493  0.0006  0.1395\n",
      "     45            1.0000        0.0986       0.5208            0.5208        1.2295  0.0005  0.1355\n",
      "     46            1.0000        \u001b[32m0.0635\u001b[0m       0.5208            0.5208        1.2101  0.0005  0.1319\n",
      "     47            1.0000        0.0650       0.5208            0.5208        1.1887  0.0004  0.1100\n",
      "     48            1.0000        \u001b[32m0.0489\u001b[0m       0.5312            0.5312        1.1665  0.0004  0.1121\n",
      "     49            1.0000        0.0687       0.5521            0.5521        1.1439  0.0003  0.1193\n",
      "     50            1.0000        \u001b[32m0.0439\u001b[0m       0.5312            0.5312        1.1234  0.0003  0.1336\n",
      "Fine tuning model for subject 3 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        0.7929       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        \u001b[94m0.8820\u001b[0m  0.0004  0.1211\n",
      "     32            0.7667        0.7514       0.6875            0.6875        \u001b[94m0.8711\u001b[0m  0.0005  0.1128\n",
      "     33            \u001b[36m0.8333\u001b[0m        \u001b[32m0.6339\u001b[0m       0.6771            0.6771        \u001b[94m0.8569\u001b[0m  0.0005  0.1202\n",
      "     34            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5810\u001b[0m       0.6667            0.6667        \u001b[94m0.8547\u001b[0m  0.0006  0.1269\n",
      "     35            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4593\u001b[0m       0.6354            0.6354        0.8717  0.0006  0.1332\n",
      "     36            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3726\u001b[0m       0.6042            0.6042        0.8942  0.0007  0.1282\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3011\u001b[0m       0.5938            0.5938        0.9212  0.0007  0.1377\n",
      "     38            1.0000        \u001b[32m0.2184\u001b[0m       0.5833            0.5833        0.9464  0.0007  0.1321\n",
      "     39            1.0000        0.2219       0.5833            0.5833        0.9648  0.0007  0.1097\n",
      "     40            1.0000        \u001b[32m0.1879\u001b[0m       0.5625            0.5625        0.9797  0.0007  0.1128\n",
      "     41            1.0000        \u001b[32m0.1654\u001b[0m       0.5417            0.5417        0.9820  0.0007  0.1131\n",
      "     42            1.0000        \u001b[32m0.1112\u001b[0m       0.5312            0.5312        0.9804  0.0007  0.1352\n",
      "     43            1.0000        \u001b[32m0.0825\u001b[0m       0.5521            0.5521        0.9774  0.0006  0.1199\n",
      "     44            1.0000        0.0903       0.5729            0.5729        0.9740  0.0006  0.1197\n",
      "     45            1.0000        0.1005       0.5833            0.5833        0.9670  0.0005  0.1187\n",
      "     46            1.0000        0.0916       0.5938            0.5938        0.9606  0.0005  0.1239\n",
      "     47            1.0000        0.0884       0.6042            0.6042        0.9542  0.0004  0.1216\n",
      "     48            1.0000        \u001b[32m0.0741\u001b[0m       0.6042            0.6042        0.9474  0.0004  0.1204\n",
      "     49            1.0000        \u001b[32m0.0644\u001b[0m       0.6042            0.6042        0.9424  0.0003  0.1209\n",
      "     50            1.0000        \u001b[32m0.0562\u001b[0m       0.5833            0.5833        0.9385  0.0003  0.1203\n",
      "Fine tuning model for subject 3 with 30 = 30 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        0.9727       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8884\u001b[0m  0.0004  0.1089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7667        0.7683       0.6562            0.6562        0.9002  0.0005  0.1375\n",
      "     33            \u001b[36m0.9000\u001b[0m        0.8152       0.6458            0.6458        0.9223  0.0005  0.1333\n",
      "     34            \u001b[36m0.9333\u001b[0m        0.7458       0.6250            0.6250        0.9687  0.0006  0.1179\n",
      "     35            0.9000        \u001b[32m0.7025\u001b[0m       0.6146            0.6146        1.0501  0.0006  0.1206\n",
      "     36            0.9000        \u001b[32m0.5001\u001b[0m       0.5938            0.5938        1.1564  0.0007  0.1319\n",
      "     37            0.9000        \u001b[32m0.2579\u001b[0m       0.5521            0.5521        1.2621  0.0007  0.1823\n",
      "     38            0.8667        \u001b[32m0.1951\u001b[0m       0.5208            0.5208        1.3526  0.0007  0.1355\n",
      "     39            0.8333        0.2252       0.5104            0.5104        1.4248  0.0007  0.1517\n",
      "     40            0.8333        \u001b[32m0.1608\u001b[0m       0.5000            0.5000        1.4752  0.0007  0.1493\n",
      "     41            0.8667        0.1729       0.5000            0.5000        1.4999  0.0007  0.1656\n",
      "     42            0.8667        \u001b[32m0.1438\u001b[0m       0.5104            0.5104        1.5059  0.0007  0.1615\n",
      "     43            \u001b[36m0.9667\u001b[0m        0.1447       0.5208            0.5208        1.4947  0.0006  0.1518\n",
      "     44            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0709\u001b[0m       0.5208            0.5208        1.4738  0.0006  0.1315\n",
      "     45            1.0000        0.0865       0.5104            0.5104        1.4457  0.0005  0.1524\n",
      "     46            1.0000        0.0807       0.5208            0.5208        1.4121  0.0005  0.1485\n",
      "     47            1.0000        0.0788       0.5312            0.5312        1.3760  0.0004  0.1732\n",
      "     48            1.0000        0.0843       0.5833            0.5833        1.3390  0.0004  0.1552\n",
      "     49            1.0000        0.0723       0.5729            0.5729        1.3051  0.0003  0.1501\n",
      "     50            1.0000        \u001b[32m0.0615\u001b[0m       0.5833            0.5833        1.2751  0.0003  0.1521\n",
      "Fine tuning model for subject 3 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7143        1.1860       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8845\u001b[0m  0.0004  0.1756\n",
      "     32            0.7429        1.1365       \u001b[35m0.6771\u001b[0m            \u001b[31m0.6771\u001b[0m        0.8864  0.0005  0.1476\n",
      "     33            0.7714        0.8931       0.6771            0.6771        0.8894  0.0005  0.1857\n",
      "     34            \u001b[36m0.8286\u001b[0m        0.8060       0.6458            0.6458        0.9081  0.0006  0.1145\n",
      "     35            \u001b[36m0.8571\u001b[0m        \u001b[32m0.6444\u001b[0m       0.6042            0.6042        0.9503  0.0006  0.1361\n",
      "     36            \u001b[36m0.9429\u001b[0m        \u001b[32m0.4729\u001b[0m       0.6042            0.6042        1.0131  0.0007  0.1327\n",
      "     37            \u001b[36m0.9714\u001b[0m        \u001b[32m0.3686\u001b[0m       0.5729            0.5729        1.1019  0.0007  0.1191\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3055\u001b[0m       0.5208            0.5208        1.2146  0.0007  0.1341\n",
      "     39            1.0000        \u001b[32m0.2196\u001b[0m       0.4896            0.4896        1.3287  0.0007  0.1357\n",
      "     40            1.0000        0.2292       0.4583            0.4583        1.4285  0.0007  0.1368\n",
      "     41            1.0000        \u001b[32m0.1855\u001b[0m       0.4583            0.4583        1.5086  0.0007  0.1357\n",
      "     42            1.0000        \u001b[32m0.1773\u001b[0m       0.4271            0.4271        1.5716  0.0007  0.1297\n",
      "     43            1.0000        \u001b[32m0.1697\u001b[0m       0.4271            0.4271        1.6111  0.0006  0.1345\n",
      "     44            1.0000        \u001b[32m0.1206\u001b[0m       0.4375            0.4375        1.6309  0.0006  0.1415\n",
      "     45            1.0000        0.1363       0.4375            0.4375        1.6374  0.0005  0.1309\n",
      "     46            1.0000        0.1484       0.4479            0.4479        1.6270  0.0005  0.1354\n",
      "     47            1.0000        \u001b[32m0.1013\u001b[0m       0.4271            0.4271        1.6091  0.0004  0.1184\n",
      "     48            1.0000        \u001b[32m0.0812\u001b[0m       0.4271            0.4271        1.5871  0.0004  0.1280\n",
      "     49            1.0000        0.1036       0.4375            0.4375        1.5617  0.0003  0.1113\n",
      "     50            1.0000        0.1103       0.4375            0.4375        1.5326  0.0003  0.1101\n",
      "Fine tuning model for subject 3 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7143        0.9535       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8812\u001b[0m  0.0004  0.1254\n",
      "     32            0.7429        0.9372       0.6562            0.6562        0.8838  0.0005  0.1190\n",
      "     33            \u001b[36m0.8000\u001b[0m        1.0329       0.6562            0.6562        0.8858  0.0005  0.1186\n",
      "     34            \u001b[36m0.8571\u001b[0m        0.7699       0.6354            0.6354        0.8899  0.0006  0.1363\n",
      "     35            \u001b[36m0.8857\u001b[0m        0.8031       0.6562            0.6562        0.8885  0.0006  0.1190\n",
      "     36            \u001b[36m0.9143\u001b[0m        \u001b[32m0.5306\u001b[0m       0.6354            0.6354        0.9057  0.0007  0.1333\n",
      "     37            \u001b[36m0.9429\u001b[0m        \u001b[32m0.3483\u001b[0m       0.6042            0.6042        0.9441  0.0007  0.1364\n",
      "     38            \u001b[36m0.9714\u001b[0m        0.3986       0.6146            0.6146        1.0063  0.0007  0.1178\n",
      "     39            0.9714        0.4019       0.5938            0.5938        1.0689  0.0007  0.1207\n",
      "     40            0.9714        \u001b[32m0.3069\u001b[0m       0.5729            0.5729        1.1205  0.0007  0.1366\n",
      "     41            0.9714        \u001b[32m0.2371\u001b[0m       0.5625            0.5625        1.1598  0.0007  0.1232\n",
      "     42            0.9714        \u001b[32m0.2196\u001b[0m       0.5729            0.5729        1.1802  0.0007  0.1329\n",
      "     43            0.9714        \u001b[32m0.1588\u001b[0m       0.5938            0.5938        1.1864  0.0006  0.1370\n",
      "     44            \u001b[36m1.0000\u001b[0m        0.1954       0.5833            0.5833        1.1874  0.0006  0.1650\n",
      "     45            1.0000        \u001b[32m0.1470\u001b[0m       0.5833            0.5833        1.1842  0.0005  0.1371\n",
      "     46            1.0000        \u001b[32m0.1334\u001b[0m       0.5938            0.5938        1.1711  0.0005  0.1396\n",
      "     47            1.0000        \u001b[32m0.1247\u001b[0m       0.5938            0.5938        1.1553  0.0004  0.1298\n",
      "     48            1.0000        0.1249       0.5729            0.5729        1.1386  0.0004  0.1330\n",
      "     49            1.0000        \u001b[32m0.1120\u001b[0m       0.5729            0.5729        1.1202  0.0003  0.1408\n",
      "     50            1.0000        \u001b[32m0.1118\u001b[0m       0.5625            0.5625        1.1028  0.0003  0.1140\n",
      "Fine tuning model for subject 3 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6857        1.0588       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8917\u001b[0m  0.0004  0.1317\n",
      "     32            0.6857        0.9306       0.6354            0.6354        0.9081  0.0005  0.1251\n",
      "     33            0.7429        1.1213       0.6354            0.6354        0.9488  0.0005  0.1364\n",
      "     34            0.7714        \u001b[32m0.6743\u001b[0m       0.6354            0.6354        1.0015  0.0006  0.1200\n",
      "     35            0.7714        0.7059       0.5938            0.5938        1.0635  0.0006  0.1344\n",
      "     36            \u001b[36m0.8857\u001b[0m        \u001b[32m0.4582\u001b[0m       0.5729            0.5729        1.1206  0.0007  0.1343\n",
      "     37            0.8857        0.4857       0.5417            0.5417        1.1628  0.0007  0.1255\n",
      "     38            \u001b[36m0.9143\u001b[0m        \u001b[32m0.3080\u001b[0m       0.5104            0.5104        1.1918  0.0007  0.1385\n",
      "     39            0.9143        \u001b[32m0.2881\u001b[0m       0.4896            0.4896        1.2147  0.0007  0.1471\n",
      "     40            0.8857        \u001b[32m0.2183\u001b[0m       0.4792            0.4792        1.2351  0.0007  0.1301\n",
      "     41            0.8857        \u001b[32m0.1536\u001b[0m       0.4583            0.4583        1.2483  0.0007  0.1153\n",
      "     42            0.8857        0.2137       0.4479            0.4479        1.2500  0.0007  0.1146\n",
      "     43            0.8857        0.1573       0.4375            0.4375        1.2380  0.0006  0.1302\n",
      "     44            \u001b[36m0.9714\u001b[0m        0.1857       0.4375            0.4375        1.2120  0.0006  0.1191\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1220\u001b[0m       0.4583            0.4583        1.1861  0.0005  0.1355\n",
      "     46            1.0000        0.1465       0.4688            0.4688        1.1609  0.0005  0.1313\n",
      "     47            1.0000        \u001b[32m0.0839\u001b[0m       0.4688            0.4688        1.1401  0.0004  0.1208\n",
      "     48            1.0000        0.1007       0.4479            0.4479        1.1234  0.0004  0.1198\n",
      "     49            1.0000        0.1368       0.4479            0.4479        1.1103  0.0003  0.1354\n",
      "     50            1.0000        0.0929       0.4479            0.4479        1.0995  0.0003  0.1309\n",
      "Fine tuning model for subject 3 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4857        1.3661       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8909\u001b[0m  0.0004  0.1096\n",
      "     32            0.6286        1.2049       0.6354            0.6354        0.9133  0.0005  0.1241\n",
      "     33            0.6857        1.0309       0.6146            0.6146        0.9517  0.0005  0.1196\n",
      "     34            0.7714        0.9070       0.5833            0.5833        1.0239  0.0006  0.1185\n",
      "     35            \u001b[36m0.8286\u001b[0m        \u001b[32m0.6061\u001b[0m       0.5417            0.5417        1.1410  0.0006  0.1354\n",
      "     36            \u001b[36m0.8571\u001b[0m        \u001b[32m0.5048\u001b[0m       0.4896            0.4896        1.2921  0.0007  0.1185\n",
      "     37            0.8286        \u001b[32m0.3652\u001b[0m       0.4375            0.4375        1.4396  0.0007  0.1347\n",
      "     38            0.8286        0.3758       0.3646            0.3646        1.5407  0.0007  0.1363\n",
      "     39            \u001b[36m0.8857\u001b[0m        \u001b[32m0.3497\u001b[0m       0.3646            0.3646        1.5753  0.0007  0.1171\n",
      "     40            \u001b[36m0.9429\u001b[0m        \u001b[32m0.2552\u001b[0m       0.3750            0.3750        1.5662  0.0007  0.1213\n",
      "     41            \u001b[36m0.9714\u001b[0m        \u001b[32m0.2100\u001b[0m       0.3750            0.3750        1.5316  0.0007  0.1356\n",
      "     42            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1885\u001b[0m       0.3750            0.3750        1.4784  0.0007  0.1353\n",
      "     43            1.0000        \u001b[32m0.1684\u001b[0m       0.4062            0.4062        1.4195  0.0006  0.1188\n",
      "     44            1.0000        0.1846       0.4479            0.4479        1.3658  0.0006  0.1380\n",
      "     45            1.0000        \u001b[32m0.1674\u001b[0m       0.4583            0.4583        1.3218  0.0005  0.1415\n",
      "     46            1.0000        \u001b[32m0.1093\u001b[0m       0.4583            0.4583        1.2892  0.0005  0.1325\n",
      "     47            1.0000        0.1306       0.4688            0.4688        1.2687  0.0004  0.1375\n",
      "     48            1.0000        0.1628       0.4688            0.4688        1.2555  0.0004  0.1308\n",
      "     49            1.0000        \u001b[32m0.0661\u001b[0m       0.4688            0.4688        1.2492  0.0003  0.1243\n",
      "     50            1.0000        0.0785       0.4688            0.4688        1.2466  0.0003  0.1206\n",
      "Fine tuning model for subject 3 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6857        1.0867       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8805\u001b[0m  0.0004  0.1305\n",
      "     32            0.7143        0.9910       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8711\u001b[0m  0.0005  0.1222\n",
      "     33            0.7429        0.8964       0.6458            0.6458        \u001b[94m0.8634\u001b[0m  0.0005  0.1198\n",
      "     34            \u001b[36m0.8000\u001b[0m        0.7793       0.6354            0.6354        0.8641  0.0006  0.1342\n",
      "     35            \u001b[36m0.8857\u001b[0m        0.7798       0.6354            0.6354        0.8803  0.0006  0.1359\n",
      "     36            \u001b[36m0.9714\u001b[0m        \u001b[32m0.5261\u001b[0m       0.6042            0.6042        0.9123  0.0007  0.1412\n",
      "     37            0.9714        \u001b[32m0.3918\u001b[0m       0.5938            0.5938        0.9567  0.0007  0.1350\n",
      "     38            0.9714        0.3937       0.5833            0.5833        1.0067  0.0007  0.1511\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3269\u001b[0m       0.5521            0.5521        1.0619  0.0007  0.1547\n",
      "     40            1.0000        \u001b[32m0.2308\u001b[0m       0.5104            0.5104        1.1151  0.0007  0.1822\n",
      "     41            1.0000        0.2557       0.4896            0.4896        1.1648  0.0007  0.1611\n",
      "     42            1.0000        \u001b[32m0.1911\u001b[0m       0.4583            0.4583        1.2063  0.0007  0.1940\n",
      "     43            1.0000        \u001b[32m0.1450\u001b[0m       0.4583            0.4583        1.2356  0.0006  0.1475\n",
      "     44            1.0000        \u001b[32m0.1171\u001b[0m       0.4688            0.4688        1.2544  0.0006  0.1447\n",
      "     45            1.0000        0.1599       0.4688            0.4688        1.2606  0.0005  0.1660\n",
      "     46            1.0000        0.1289       0.4792            0.4792        1.2639  0.0005  0.1319\n",
      "     47            1.0000        \u001b[32m0.0905\u001b[0m       0.4792            0.4792        1.2651  0.0004  0.1659\n",
      "     48            1.0000        \u001b[32m0.0867\u001b[0m       0.4792            0.4792        1.2632  0.0004  0.1633\n",
      "     49            1.0000        \u001b[32m0.0860\u001b[0m       0.4896            0.4896        1.2591  0.0003  0.1522\n",
      "     50            1.0000        0.1054       0.5104            0.5104        1.2534  0.0003  0.1472\n",
      "Fine tuning model for subject 3 with 35 = 35 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6857        0.8632       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8889\u001b[0m  0.0004  0.1273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7714        0.8715       0.6354            0.6354        0.9054  0.0005  0.1301\n",
      "     33            \u001b[36m0.8000\u001b[0m        0.7919       0.6354            0.6354        0.9228  0.0005  0.1335\n",
      "     34            \u001b[36m0.8857\u001b[0m        \u001b[32m0.6334\u001b[0m       0.6458            0.6458        0.9439  0.0006  0.1199\n",
      "     35            \u001b[36m0.9429\u001b[0m        \u001b[32m0.5591\u001b[0m       0.6042            0.6042        0.9618  0.0006  0.1258\n",
      "     36            \u001b[36m0.9714\u001b[0m        \u001b[32m0.4120\u001b[0m       0.5938            0.5938        0.9895  0.0007  0.1379\n",
      "     37            0.9714        \u001b[32m0.3446\u001b[0m       0.5729            0.5729        1.0236  0.0007  0.1320\n",
      "     38            0.9714        \u001b[32m0.3100\u001b[0m       0.5729            0.5729        1.0692  0.0007  0.1375\n",
      "     39            0.9714        \u001b[32m0.2028\u001b[0m       0.5729            0.5729        1.1129  0.0007  0.1180\n",
      "     40            0.9714        \u001b[32m0.1873\u001b[0m       0.5625            0.5625        1.1418  0.0007  0.1321\n",
      "     41            0.9714        \u001b[32m0.1621\u001b[0m       0.5521            0.5521        1.1581  0.0007  0.1367\n",
      "     42            0.9714        0.1709       0.5521            0.5521        1.1574  0.0007  0.1375\n",
      "     43            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1028\u001b[0m       0.5625            0.5625        1.1477  0.0006  0.1314\n",
      "     44            1.0000        0.1172       0.5625            0.5625        1.1371  0.0006  0.1370\n",
      "     45            1.0000        \u001b[32m0.0800\u001b[0m       0.5729            0.5729        1.1254  0.0005  0.1340\n",
      "     46            1.0000        0.0888       0.5729            0.5729        1.1126  0.0005  0.1198\n",
      "     47            1.0000        0.0899       0.5729            0.5729        1.0973  0.0004  0.1322\n",
      "     48            1.0000        \u001b[32m0.0655\u001b[0m       0.5729            0.5729        1.0819  0.0004  0.1398\n",
      "     49            1.0000        0.0936       0.5938            0.5938        1.0661  0.0003  0.1297\n",
      "     50            1.0000        \u001b[32m0.0529\u001b[0m       0.5938            0.5938        1.0516  0.0003  0.1330\n",
      "Fine tuning model for subject 3 with 35 = 35 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5714        1.1012       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8805\u001b[0m  0.0004  0.1208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.1580       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        \u001b[94m0.8672\u001b[0m  0.0005  0.1489\n",
      "     33            0.6571        0.9835       0.6667            0.6667        \u001b[94m0.8524\u001b[0m  0.0005  0.1180\n",
      "     34            0.7714        0.8337       0.6458            0.6458        \u001b[94m0.8471\u001b[0m  0.0006  0.1340\n",
      "     35            \u001b[36m0.8857\u001b[0m        0.7320       0.6458            0.6458        0.8596  0.0006  0.1202\n",
      "     36            \u001b[36m0.9429\u001b[0m        \u001b[32m0.5181\u001b[0m       0.6458            0.6458        0.8858  0.0007  0.1187\n",
      "     37            \u001b[36m0.9714\u001b[0m        \u001b[32m0.4022\u001b[0m       0.6146            0.6146        0.9193  0.0007  0.1355\n",
      "     38            0.9714        \u001b[32m0.2859\u001b[0m       0.6042            0.6042        0.9527  0.0007  0.1339\n",
      "     39            0.9714        \u001b[32m0.2711\u001b[0m       0.6042            0.6042        0.9831  0.0007  0.1187\n",
      "     40            0.9714        \u001b[32m0.2395\u001b[0m       0.6146            0.6146        1.0141  0.0007  0.1196\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1948\u001b[0m       0.5833            0.5833        1.0430  0.0007  0.2021\n",
      "     42            1.0000        \u001b[32m0.1777\u001b[0m       0.5521            0.5521        1.0731  0.0007  0.1287\n",
      "     43            1.0000        \u001b[32m0.1575\u001b[0m       0.5521            0.5521        1.1001  0.0006  0.1368\n",
      "     44            1.0000        \u001b[32m0.1223\u001b[0m       0.5521            0.5521        1.1233  0.0006  0.1332\n",
      "     45            1.0000        \u001b[32m0.0946\u001b[0m       0.5417            0.5417        1.1442  0.0005  0.1195\n",
      "     46            1.0000        0.1029       0.5521            0.5521        1.1613  0.0005  0.1352\n",
      "     47            1.0000        0.0986       0.5521            0.5521        1.1735  0.0004  0.1392\n",
      "     48            1.0000        \u001b[32m0.0937\u001b[0m       0.5521            0.5521        1.1826  0.0004  0.1626\n",
      "     49            1.0000        \u001b[32m0.0807\u001b[0m       0.5521            0.5521        1.1889  0.0003  0.1343\n",
      "     50            1.0000        0.0896       0.5521            0.5521        1.1932  0.0003  0.1495\n",
      "Fine tuning model for subject 3 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6286        1.0525       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8784\u001b[0m  0.0004  0.1443\n",
      "     32            0.6571        1.1196       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8626\u001b[0m  0.0005  0.1559\n",
      "     33            0.7143        0.9600       0.6667            0.6667        \u001b[94m0.8495\u001b[0m  0.0005  0.1411\n",
      "     34            \u001b[36m0.8286\u001b[0m        0.7874       0.6250            0.6250        0.8584  0.0006  0.1336\n",
      "     35            \u001b[36m0.8857\u001b[0m        \u001b[32m0.6355\u001b[0m       0.6042            0.6042        0.8899  0.0006  0.1468\n",
      "     36            \u001b[36m0.9429\u001b[0m        \u001b[32m0.4457\u001b[0m       0.6042            0.6042        0.9396  0.0007  0.1494\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4168\u001b[0m       0.5833            0.5833        0.9958  0.0007  0.1525\n",
      "     38            1.0000        \u001b[32m0.3430\u001b[0m       0.5521            0.5521        1.0447  0.0007  0.1397\n",
      "     39            1.0000        \u001b[32m0.2304\u001b[0m       0.5208            0.5208        1.0854  0.0007  0.1366\n",
      "     40            1.0000        \u001b[32m0.2165\u001b[0m       0.5000            0.5000        1.1164  0.0007  0.2905\n",
      "     41            1.0000        \u001b[32m0.1502\u001b[0m       0.4792            0.4792        1.1390  0.0007  0.2534\n",
      "     42            1.0000        \u001b[32m0.1414\u001b[0m       0.4688            0.4688        1.1547  0.0007  0.1485\n",
      "     43            1.0000        0.1569       0.4688            0.4688        1.1603  0.0006  0.1327\n",
      "     44            1.0000        \u001b[32m0.1113\u001b[0m       0.4896            0.4896        1.1608  0.0006  0.1940\n",
      "     45            1.0000        0.1221       0.4688            0.4688        1.1550  0.0005  0.1372\n",
      "     46            1.0000        \u001b[32m0.0973\u001b[0m       0.4583            0.4583        1.1443  0.0005  0.1347\n",
      "     47            1.0000        \u001b[32m0.0705\u001b[0m       0.4583            0.4583        1.1331  0.0004  0.1094\n",
      "     48            1.0000        \u001b[32m0.0647\u001b[0m       0.4688            0.4688        1.1222  0.0004  0.1384\n",
      "     49            1.0000        \u001b[32m0.0516\u001b[0m       0.4792            0.4792        1.1113  0.0003  0.1250\n",
      "     50            1.0000        0.0617       0.4792            0.4792        1.1010  0.0003  0.1407\n",
      "Fine tuning model for subject 3 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5714        1.0340       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8870\u001b[0m  0.0004  0.1435\n",
      "     32            0.6857        0.9119       \u001b[35m0.6771\u001b[0m            \u001b[31m0.6771\u001b[0m        0.9009  0.0005  0.1250\n",
      "     33            0.7429        \u001b[32m0.6956\u001b[0m       0.6042            0.6042        0.9292  0.0005  0.1250\n",
      "     34            \u001b[36m0.8857\u001b[0m        0.7395       0.5938            0.5938        0.9718  0.0006  0.1274\n",
      "     35            \u001b[36m0.9429\u001b[0m        \u001b[32m0.5786\u001b[0m       0.5625            0.5625        1.0407  0.0006  0.1407\n",
      "     36            0.9429        \u001b[32m0.4507\u001b[0m       0.5521            0.5521        1.1225  0.0007  0.1563\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3725\u001b[0m       0.5208            0.5208        1.2009  0.0007  0.1419\n",
      "     38            1.0000        \u001b[32m0.3390\u001b[0m       0.5104            0.5104        1.2669  0.0007  0.1406\n",
      "     39            1.0000        \u001b[32m0.2280\u001b[0m       0.4896            0.4896        1.3218  0.0007  0.1250\n",
      "     40            1.0000        \u001b[32m0.1820\u001b[0m       0.5104            0.5104        1.3576  0.0007  0.1423\n",
      "     41            1.0000        \u001b[32m0.1396\u001b[0m       0.5208            0.5208        1.3741  0.0007  0.1144\n",
      "     42            1.0000        0.1405       0.5312            0.5312        1.3822  0.0007  0.1243\n",
      "     43            1.0000        0.1516       0.5312            0.5312        1.3814  0.0006  0.1250\n",
      "     44            1.0000        \u001b[32m0.1210\u001b[0m       0.5417            0.5417        1.3786  0.0006  0.1904\n",
      "     45            1.0000        \u001b[32m0.1069\u001b[0m       0.5417            0.5417        1.3739  0.0005  0.1748\n",
      "     46            1.0000        \u001b[32m0.0679\u001b[0m       0.5312            0.5312        1.3711  0.0005  0.2203\n",
      "     47            1.0000        \u001b[32m0.0623\u001b[0m       0.5208            0.5208        1.3690  0.0004  0.1681\n",
      "     48            1.0000        \u001b[32m0.0592\u001b[0m       0.5104            0.5104        1.3676  0.0004  0.1406\n",
      "     49            1.0000        \u001b[32m0.0558\u001b[0m       0.5104            0.5104        1.3668  0.0003  0.1574\n",
      "     50            1.0000        0.0618       0.5208            0.5208        1.3659  0.0003  0.1719\n",
      "Fine tuning model for subject 3 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6286        0.9636       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8799\u001b[0m  0.0004  0.1542\n",
      "     32            0.6571        1.1109       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        0.8799  0.0005  0.1407\n",
      "     33            0.7429        0.8500       0.6458            0.6458        0.9135  0.0005  0.1562\n",
      "     34            \u001b[36m0.8286\u001b[0m        \u001b[32m0.6780\u001b[0m       0.5833            0.5833        0.9861  0.0006  0.1406\n",
      "     35            0.8000        \u001b[32m0.6104\u001b[0m       0.5104            0.5104        1.0797  0.0006  0.1311\n",
      "     36            0.8286        \u001b[32m0.4780\u001b[0m       0.5208            0.5208        1.1387  0.0007  0.1563\n",
      "     37            \u001b[36m0.8571\u001b[0m        \u001b[32m0.3374\u001b[0m       0.4896            0.4896        1.1454  0.0007  0.1406\n",
      "     38            \u001b[36m0.9714\u001b[0m        \u001b[32m0.2659\u001b[0m       0.4896            0.4896        1.1130  0.0007  0.1302\n",
      "     39            0.9714        \u001b[32m0.2546\u001b[0m       0.5208            0.5208        1.0962  0.0007  0.1406\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1488\u001b[0m       0.4792            0.4792        1.1011  0.0007  0.1406\n",
      "     41            1.0000        \u001b[32m0.1362\u001b[0m       0.4896            0.4896        1.1187  0.0007  0.1584\n",
      "     42            1.0000        \u001b[32m0.1307\u001b[0m       0.5000            0.5000        1.1448  0.0007  0.1719\n",
      "     43            1.0000        \u001b[32m0.1227\u001b[0m       0.4896            0.4896        1.1705  0.0006  0.1255\n",
      "     44            1.0000        \u001b[32m0.1109\u001b[0m       0.5000            0.5000        1.1940  0.0006  0.1147\n",
      "     45            1.0000        \u001b[32m0.0993\u001b[0m       0.5000            0.5000        1.2099  0.0005  0.1094\n",
      "     46            1.0000        0.1161       0.5000            0.5000        1.2233  0.0005  0.1251\n",
      "     47            1.0000        \u001b[32m0.0781\u001b[0m       0.4896            0.4896        1.2319  0.0004  0.1268\n",
      "     48            1.0000        \u001b[32m0.0718\u001b[0m       0.4896            0.4896        1.2367  0.0004  0.1250\n",
      "     49            1.0000        0.0765       0.4896            0.4896        1.2386  0.0003  0.1250\n",
      "     50            1.0000        0.0938       0.5000            0.5000        1.2368  0.0003  0.1250\n",
      "Fine tuning model for subject 3 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6500        0.9997       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8902\u001b[0m  0.0004  0.1250\n",
      "     32            0.6000        0.9636       0.6042            0.6042        0.9212  0.0005  0.1406\n",
      "     33            0.6500        0.9035       0.5833            0.5833        0.9909  0.0005  0.1250\n",
      "     34            0.7000        \u001b[32m0.6800\u001b[0m       0.5625            0.5625        1.0873  0.0006  0.1274\n",
      "     35            0.7750        \u001b[32m0.5688\u001b[0m       0.5625            0.5625        1.1833  0.0006  0.1254\n",
      "     36            \u001b[36m0.8250\u001b[0m        \u001b[32m0.4312\u001b[0m       0.5938            0.5938        1.2557  0.0007  0.1406\n",
      "     37            \u001b[36m0.8500\u001b[0m        \u001b[32m0.3443\u001b[0m       0.5938            0.5938        1.3010  0.0007  0.1422\n",
      "     38            \u001b[36m0.9000\u001b[0m        \u001b[32m0.2926\u001b[0m       0.5833            0.5833        1.3198  0.0007  0.1255\n",
      "     39            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2650\u001b[0m       0.5833            0.5833        1.3211  0.0007  0.1249\n",
      "     40            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2549\u001b[0m       0.5938            0.5938        1.3141  0.0007  0.1394\n",
      "     41            0.9750        \u001b[32m0.2238\u001b[0m       0.5938            0.5938        1.3057  0.0007  0.1407\n",
      "     42            \u001b[36m1.0000\u001b[0m        0.2947       0.5938            0.5938        1.2979  0.0007  0.1411\n",
      "     43            1.0000        \u001b[32m0.1977\u001b[0m       0.5729            0.5729        1.2871  0.0006  0.1406\n",
      "     44            1.0000        \u001b[32m0.1519\u001b[0m       0.5729            0.5729        1.2795  0.0006  0.1416\n",
      "     45            1.0000        0.1831       0.5833            0.5833        1.2696  0.0005  0.1250\n",
      "     46            1.0000        \u001b[32m0.1436\u001b[0m       0.5729            0.5729        1.2607  0.0005  0.1255\n",
      "     47            1.0000        \u001b[32m0.1190\u001b[0m       0.5625            0.5625        1.2508  0.0004  0.1264\n",
      "     48            1.0000        \u001b[32m0.0843\u001b[0m       0.5729            0.5729        1.2413  0.0004  0.1250\n",
      "     49            1.0000        0.0967       0.5521            0.5521        1.2330  0.0003  0.1250\n",
      "     50            1.0000        0.1120       0.5521            0.5521        1.2265  0.0003  0.1214\n",
      "Fine tuning model for subject 3 with 40 = 40 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        0.8448       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8926\u001b[0m  0.0004  0.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7750        0.9347       0.6250            0.6250        0.9198  0.0005  0.1250\n",
      "     33            \u001b[36m0.8250\u001b[0m        \u001b[32m0.6807\u001b[0m       0.6146            0.6146        0.9735  0.0005  0.1254\n",
      "     34            0.8250        \u001b[32m0.5148\u001b[0m       0.6146            0.6146        1.0293  0.0006  0.1137\n",
      "     35            \u001b[36m0.9000\u001b[0m        0.5310       0.5625            0.5625        1.0709  0.0006  0.1406\n",
      "     36            \u001b[36m0.9250\u001b[0m        \u001b[32m0.3945\u001b[0m       0.5312            0.5312        1.1088  0.0007  0.1250\n",
      "     37            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3644\u001b[0m       0.5312            0.5312        1.1411  0.0007  0.1276\n",
      "     38            0.9750        \u001b[32m0.3202\u001b[0m       0.4792            0.4792        1.1827  0.0007  0.1250\n",
      "     39            0.9750        \u001b[32m0.2198\u001b[0m       0.4688            0.4688        1.2336  0.0007  0.1250\n",
      "     40            0.9750        0.2346       0.4583            0.4583        1.2750  0.0007  0.1375\n",
      "     41            0.9500        \u001b[32m0.1416\u001b[0m       0.4271            0.4271        1.3046  0.0007  0.1250\n",
      "     42            0.9500        0.1421       0.4375            0.4375        1.3222  0.0007  0.1250\n",
      "     43            0.9250        \u001b[32m0.1405\u001b[0m       0.4375            0.4375        1.3231  0.0006  0.1406\n",
      "     44            0.9500        \u001b[32m0.1385\u001b[0m       0.4479            0.4479        1.3123  0.0006  0.1595\n",
      "     45            0.9750        \u001b[32m0.1077\u001b[0m       0.4688            0.4688        1.2929  0.0005  0.1406\n",
      "     46            0.9750        \u001b[32m0.0753\u001b[0m       0.4792            0.4792        1.2738  0.0005  0.1406\n",
      "     47            \u001b[36m1.0000\u001b[0m        0.0939       0.5000            0.5000        1.2546  0.0004  0.1265\n",
      "     48            1.0000        0.1574       0.5000            0.5000        1.2350  0.0004  0.1563\n",
      "     49            1.0000        0.1102       0.5000            0.5000        1.2194  0.0003  0.1406\n",
      "     50            1.0000        \u001b[32m0.0682\u001b[0m       0.5104            0.5104        1.2078  0.0003  0.1271\n",
      "Fine tuning model for subject 3 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6750        0.9978       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8883\u001b[0m  0.0004  0.1719\n",
      "     32            0.7500        0.9793       0.6562            0.6562        0.9004  0.0005  0.1275\n",
      "     33            \u001b[36m0.8250\u001b[0m        0.7778       \u001b[35m0.6771\u001b[0m            \u001b[31m0.6771\u001b[0m        0.9144  0.0005  0.1250\n",
      "     34            \u001b[36m0.8750\u001b[0m        \u001b[32m0.7184\u001b[0m       0.6562            0.6562        0.9388  0.0006  0.1250\n",
      "     35            \u001b[36m0.9250\u001b[0m        0.8011       0.6458            0.6458        0.9785  0.0006  0.1397\n",
      "     36            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4794\u001b[0m       0.6458            0.6458        1.0259  0.0007  0.1407\n",
      "     37            0.9500        0.5160       0.6354            0.6354        1.0735  0.0007  0.1250\n",
      "     38            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3209\u001b[0m       0.6354            0.6354        1.1040  0.0007  0.1250\n",
      "     39            \u001b[36m1.0000\u001b[0m        0.3766       0.6354            0.6354        1.1117  0.0007  0.1272\n",
      "     40            1.0000        0.3262       0.6146            0.6146        1.0997  0.0007  0.1250\n",
      "     41            1.0000        \u001b[32m0.2763\u001b[0m       0.6146            0.6146        1.0845  0.0007  0.1258\n",
      "     42            1.0000        \u001b[32m0.1939\u001b[0m       0.6042            0.6042        1.0686  0.0007  0.1409\n",
      "     43            1.0000        \u001b[32m0.1625\u001b[0m       0.6250            0.6250        1.0555  0.0006  0.1250\n",
      "     44            1.0000        \u001b[32m0.1364\u001b[0m       0.6146            0.6146        1.0471  0.0006  0.1249\n",
      "     45            1.0000        0.1439       0.6042            0.6042        1.0436  0.0005  0.1352\n",
      "     46            1.0000        \u001b[32m0.1322\u001b[0m       0.6354            0.6354        1.0422  0.0005  0.1251\n",
      "     47            1.0000        0.1369       0.6354            0.6354        1.0439  0.0004  0.1251\n",
      "     48            1.0000        \u001b[32m0.1072\u001b[0m       0.6250            0.6250        1.0465  0.0004  0.1406\n",
      "     49            1.0000        0.1251       0.6250            0.6250        1.0499  0.0003  0.1274\n",
      "     50            1.0000        \u001b[32m0.0816\u001b[0m       0.6250            0.6250        1.0522  0.0003  0.1406\n",
      "Fine tuning model for subject 3 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7250        0.8485       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8950\u001b[0m  0.0004  0.1250\n",
      "     32            0.7500        1.1310       0.5938            0.5938        0.9299  0.0005  0.1277\n",
      "     33            0.7500        0.8701       0.5417            0.5417        0.9799  0.0005  0.1250\n",
      "     34            0.7750        \u001b[32m0.6965\u001b[0m       0.5625            0.5625        1.0048  0.0006  0.1250\n",
      "     35            \u001b[36m0.8500\u001b[0m        \u001b[32m0.5977\u001b[0m       0.5625            0.5625        1.0019  0.0006  0.1583\n",
      "     36            \u001b[36m0.9250\u001b[0m        0.7004       0.6250            0.6250        0.9751  0.0007  0.1405\n",
      "     37            \u001b[36m0.9500\u001b[0m        \u001b[32m0.3846\u001b[0m       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        0.9423  0.0007  0.1250\n",
      "     38            \u001b[36m1.0000\u001b[0m        0.4351       0.6562            0.6562        0.9266  0.0007  0.1420\n",
      "     39            1.0000        \u001b[32m0.2847\u001b[0m       0.6562            0.6562        0.9317  0.0007  0.1250\n",
      "     40            0.9750        0.2889       0.6458            0.6458        0.9455  0.0007  0.1250\n",
      "     41            0.9750        \u001b[32m0.1675\u001b[0m       0.6354            0.6354        0.9616  0.0007  0.1635\n",
      "     42            0.9750        0.2480       0.6250            0.6250        0.9765  0.0007  0.1563\n",
      "     43            1.0000        0.1736       0.6250            0.6250        0.9871  0.0006  0.1406\n",
      "     44            1.0000        \u001b[32m0.1097\u001b[0m       0.6146            0.6146        0.9976  0.0006  0.1504\n",
      "     45            1.0000        0.1500       0.6250            0.6250        1.0062  0.0005  0.1562\n",
      "     46            1.0000        \u001b[32m0.0937\u001b[0m       0.6354            0.6354        1.0140  0.0005  0.1719\n",
      "     47            1.0000        0.1001       0.6458            0.6458        1.0203  0.0004  0.1737\n",
      "     48            1.0000        \u001b[32m0.0855\u001b[0m       0.6250            0.6250        1.0247  0.0004  0.1719\n",
      "     49            1.0000        0.1075       0.6146            0.6146        1.0278  0.0003  0.1407\n",
      "     50            1.0000        0.0995       0.6250            0.6250        1.0301  0.0003  0.1741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning model for subject 3 with 40 = 40 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6250        1.1115       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8884\u001b[0m  0.0004  0.1563\n",
      "     32            0.6750        1.1039       0.6354            0.6354        0.9051  0.0005  0.1451\n",
      "     33            0.7000        0.9315       0.6354            0.6354        0.9331  0.0005  0.1563\n",
      "     34            0.7750        0.8301       0.6146            0.6146        0.9767  0.0006  0.1406\n",
      "     35            \u001b[36m0.8750\u001b[0m        \u001b[32m0.6170\u001b[0m       0.5833            0.5833        1.0230  0.0006  0.1896\n",
      "     36            \u001b[36m0.9250\u001b[0m        \u001b[32m0.5667\u001b[0m       0.5521            0.5521        1.0631  0.0007  0.1874\n",
      "     37            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4326\u001b[0m       0.5521            0.5521        1.0896  0.0007  0.1874\n",
      "     38            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2843\u001b[0m       0.5521            0.5521        1.1098  0.0007  0.1251\n",
      "     39            \u001b[36m1.0000\u001b[0m        0.2845       0.5521            0.5521        1.1258  0.0007  0.1250\n",
      "     40            1.0000        \u001b[32m0.2399\u001b[0m       0.5417            0.5417        1.1422  0.0007  0.1250\n",
      "     41            1.0000        \u001b[32m0.2361\u001b[0m       0.5625            0.5625        1.1606  0.0007  0.1283\n",
      "     42            1.0000        \u001b[32m0.1716\u001b[0m       0.5312            0.5312        1.1849  0.0007  0.1407\n",
      "     43            1.0000        \u001b[32m0.1665\u001b[0m       0.5417            0.5417        1.2107  0.0006  0.1406\n",
      "     44            1.0000        0.1924       0.5208            0.5208        1.2374  0.0006  0.1268\n",
      "     45            1.0000        \u001b[32m0.1195\u001b[0m       0.5000            0.5000        1.2609  0.0005  0.1250\n",
      "     46            1.0000        0.1418       0.5104            0.5104        1.2775  0.0005  0.1250\n",
      "     47            1.0000        \u001b[32m0.1190\u001b[0m       0.5104            0.5104        1.2889  0.0004  0.1412\n",
      "     48            1.0000        \u001b[32m0.0851\u001b[0m       0.5104            0.5104        1.2955  0.0004  0.1251\n",
      "     49            1.0000        \u001b[32m0.0780\u001b[0m       0.5000            0.5000        1.2981  0.0003  0.1406\n",
      "     50            1.0000        0.0995       0.5000            0.5000        1.2960  0.0003  0.1406\n",
      "Fine tuning model for subject 3 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6500        1.0257       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8883\u001b[0m  0.0004  0.1251\n",
      "     32            0.6500        0.9536       0.6354            0.6354        \u001b[94m0.8876\u001b[0m  0.0005  0.1251\n",
      "     33            0.7000        0.7421       0.6458            0.6458        \u001b[94m0.8783\u001b[0m  0.0005  0.1406\n",
      "     34            0.7750        0.7394       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8580\u001b[0m  0.0006  0.1273\n",
      "     35            \u001b[36m0.8750\u001b[0m        \u001b[32m0.6876\u001b[0m       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        \u001b[94m0.8374\u001b[0m  0.0006  0.1250\n",
      "     36            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6459\u001b[0m       0.6771            0.6771        \u001b[94m0.8125\u001b[0m  0.0007  0.1250\n",
      "     37            \u001b[36m0.9250\u001b[0m        \u001b[32m0.4481\u001b[0m       0.6667            0.6667        \u001b[94m0.7932\u001b[0m  0.0007  0.1271\n",
      "     38            0.9250        \u001b[32m0.3860\u001b[0m       0.6771            0.6771        \u001b[94m0.7877\u001b[0m  0.0007  0.1250\n",
      "     39            0.9250        \u001b[32m0.3106\u001b[0m       0.6667            0.6667        0.7923  0.0007  0.1249\n",
      "     40            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2735\u001b[0m       0.6771            0.6771        0.8014  0.0007  0.1249\n",
      "     41            0.9750        0.2827       0.6667            0.6667        0.8133  0.0007  0.1145\n",
      "     42            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2135\u001b[0m       0.6562            0.6562        0.8264  0.0007  0.1250\n",
      "     43            1.0000        \u001b[32m0.1921\u001b[0m       0.6667            0.6667        0.8421  0.0006  0.1259\n",
      "     44            1.0000        0.2119       0.6771            0.6771        0.8578  0.0006  0.1425\n",
      "     45            1.0000        \u001b[32m0.1899\u001b[0m       0.6562            0.6562        0.8720  0.0005  0.1406\n",
      "     46            1.0000        \u001b[32m0.1280\u001b[0m       0.6667            0.6667        0.8837  0.0005  0.1719\n",
      "     47            1.0000        \u001b[32m0.1216\u001b[0m       0.6562            0.6562        0.8923  0.0004  0.1420\n",
      "     48            1.0000        \u001b[32m0.0878\u001b[0m       0.6458            0.6458        0.8984  0.0004  0.1250\n",
      "     49            1.0000        0.1227       0.6458            0.6458        0.9022  0.0003  0.1250\n",
      "     50            1.0000        0.1189       0.6354            0.6354        0.9045  0.0003  0.1336\n",
      "Fine tuning model for subject 3 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5750        1.3279       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8910\u001b[0m  0.0004  0.1251\n",
      "     32            0.6000        1.1763       0.6354            0.6354        0.9034  0.0005  0.1407\n",
      "     33            0.7000        0.9640       0.6042            0.6042        0.9203  0.0005  0.1346\n",
      "     34            0.7750        0.9105       0.6042            0.6042        0.9371  0.0006  0.1249\n",
      "     35            \u001b[36m0.9250\u001b[0m        0.8445       0.6250            0.6250        0.9496  0.0006  0.1406\n",
      "     36            \u001b[36m0.9750\u001b[0m        \u001b[32m0.5453\u001b[0m       0.5938            0.5938        0.9711  0.0007  0.1250\n",
      "     37            0.9250        \u001b[32m0.5223\u001b[0m       0.5938            0.5938        1.0082  0.0007  0.1308\n",
      "     38            0.9500        \u001b[32m0.3763\u001b[0m       0.5312            0.5312        1.0488  0.0007  0.1250\n",
      "     39            0.9500        \u001b[32m0.3693\u001b[0m       0.5208            0.5208        1.0856  0.0007  0.1405\n",
      "     40            0.9750        \u001b[32m0.2864\u001b[0m       0.5312            0.5312        1.1084  0.0007  0.1259\n",
      "     41            0.9750        \u001b[32m0.2467\u001b[0m       0.5312            0.5312        1.1179  0.0007  0.1255\n",
      "     42            0.9750        \u001b[32m0.2134\u001b[0m       0.5417            0.5417        1.1152  0.0007  0.1250\n",
      "     43            0.9750        0.2457       0.5521            0.5521        1.1071  0.0006  0.1281\n",
      "     44            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1474\u001b[0m       0.5417            0.5417        1.0956  0.0006  0.1250\n",
      "     45            1.0000        \u001b[32m0.1469\u001b[0m       0.5625            0.5625        1.0840  0.0005  0.1256\n",
      "     46            1.0000        \u001b[32m0.1304\u001b[0m       0.5417            0.5417        1.0748  0.0005  0.1250\n",
      "     47            1.0000        \u001b[32m0.1127\u001b[0m       0.5312            0.5312        1.0684  0.0004  0.1130\n",
      "     48            1.0000        0.1494       0.5208            0.5208        1.0630  0.0004  0.1406\n",
      "     49            1.0000        0.1262       0.5312            0.5312        1.0593  0.0003  0.1407\n",
      "     50            1.0000        \u001b[32m0.1121\u001b[0m       0.5312            0.5312        1.0553  0.0003  0.1427\n",
      "Fine tuning model for subject 3 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7250        0.8418       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8839\u001b[0m  0.0004  0.1250\n",
      "     32            0.7750        0.7442       \u001b[35m0.6771\u001b[0m            \u001b[31m0.6771\u001b[0m        \u001b[94m0.8808\u001b[0m  0.0005  0.1254\n",
      "     33            \u001b[36m0.8250\u001b[0m        \u001b[32m0.7009\u001b[0m       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        \u001b[94m0.8754\u001b[0m  0.0005  0.1270\n",
      "     34            \u001b[36m0.8750\u001b[0m        0.7226       0.6562            0.6562        \u001b[94m0.8729\u001b[0m  0.0006  0.1248\n",
      "     35            \u001b[36m0.9250\u001b[0m        \u001b[32m0.6266\u001b[0m       0.6042            0.6042        0.8880  0.0006  0.1250\n",
      "     36            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4228\u001b[0m       0.5729            0.5729        0.9366  0.0007  0.1439\n",
      "     37            0.9500        \u001b[32m0.3526\u001b[0m       0.5312            0.5312        1.0196  0.0007  0.1250\n",
      "     38            0.9000        \u001b[32m0.3435\u001b[0m       0.5000            0.5000        1.1197  0.0007  0.1250\n",
      "     39            0.8500        \u001b[32m0.2693\u001b[0m       0.4479            0.4479        1.2133  0.0007  0.1250\n",
      "     40            0.8750        \u001b[32m0.2474\u001b[0m       0.4479            0.4479        1.2867  0.0007  0.1274\n",
      "     41            0.9000        \u001b[32m0.1890\u001b[0m       0.4375            0.4375        1.3345  0.0007  0.1250\n",
      "     42            0.9000        \u001b[32m0.1605\u001b[0m       0.4479            0.4479        1.3561  0.0007  0.1250\n",
      "     43            \u001b[36m0.9750\u001b[0m        \u001b[32m0.1571\u001b[0m       0.4688            0.4688        1.3613  0.0006  0.1267\n",
      "     44            0.9750        0.1889       0.4792            0.4792        1.3405  0.0006  0.1251\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1364\u001b[0m       0.4792            0.4792        1.3082  0.0005  0.1251\n",
      "     46            1.0000        \u001b[32m0.1262\u001b[0m       0.4896            0.4896        1.2745  0.0005  0.1463\n",
      "     47            1.0000        \u001b[32m0.0750\u001b[0m       0.5104            0.5104        1.2418  0.0004  0.1250\n",
      "     48            1.0000        0.0852       0.5104            0.5104        1.2096  0.0004  0.1250\n",
      "     49            1.0000        0.0972       0.5208            0.5208        1.1851  0.0003  0.1406\n",
      "     50            1.0000        \u001b[32m0.0538\u001b[0m       0.5312            0.5312        1.1618  0.0003  0.1437\n",
      "Fine tuning model for subject 3 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7250        0.9927       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.8851\u001b[0m  0.0004  0.1406\n",
      "     32            0.7750        0.8890       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        0.8864  0.0005  0.1406\n",
      "     33            \u001b[36m0.8000\u001b[0m        0.8254       0.6354            0.6354        0.8888  0.0005  0.1224\n",
      "     34            \u001b[36m0.8750\u001b[0m        \u001b[32m0.6951\u001b[0m       0.6354            0.6354        \u001b[94m0.8847\u001b[0m  0.0006  0.1256\n",
      "     35            \u001b[36m0.9500\u001b[0m        \u001b[32m0.6006\u001b[0m       0.6146            0.6146        \u001b[94m0.8797\u001b[0m  0.0006  0.1406\n",
      "     36            0.9500        \u001b[32m0.5245\u001b[0m       0.6458            0.6458        0.8914  0.0007  0.1424\n",
      "     37            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3474\u001b[0m       0.6042            0.6042        0.9272  0.0007  0.1563\n",
      "     38            0.9750        \u001b[32m0.2886\u001b[0m       0.5729            0.5729        0.9705  0.0007  0.2583\n",
      "     39            0.9750        \u001b[32m0.2703\u001b[0m       0.5833            0.5833        1.0150  0.0007  0.1663\n",
      "     40            0.9750        \u001b[32m0.2277\u001b[0m       0.5625            0.5625        1.0426  0.0007  0.1929\n",
      "     41            0.9750        \u001b[32m0.1881\u001b[0m       0.5521            0.5521        1.0612  0.0007  0.1406\n",
      "     42            0.9750        0.2275       0.5417            0.5417        1.0661  0.0007  0.2166\n",
      "     43            0.9750        \u001b[32m0.1342\u001b[0m       0.5312            0.5312        1.0666  0.0006  0.2122\n",
      "     44            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1153\u001b[0m       0.5417            0.5417        1.0662  0.0006  0.2420\n",
      "     45            1.0000        0.1430       0.5208            0.5208        1.0628  0.0005  0.2496\n",
      "     46            1.0000        \u001b[32m0.1028\u001b[0m       0.5312            0.5312        1.0602  0.0005  0.2227\n",
      "     47            1.0000        \u001b[32m0.0904\u001b[0m       0.5312            0.5312        1.0587  0.0004  0.1875\n",
      "     48            1.0000        0.0952       0.5312            0.5312        1.0582  0.0004  0.1562\n",
      "     49            1.0000        0.1140       0.5312            0.5312        1.0583  0.0003  0.2187\n",
      "     50            1.0000        0.0992       0.5312            0.5312        1.0572  0.0003  0.1250\n",
      "Fine tuning model for subject 3 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7250        0.8733       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8853\u001b[0m  0.0004  0.1823\n",
      "     32            0.7250        0.9492       0.6667            0.6667        0.8922  0.0005  0.1496\n",
      "     33            0.7500        \u001b[32m0.6560\u001b[0m       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        0.9067  0.0005  0.1507\n",
      "     34            \u001b[36m0.8500\u001b[0m        \u001b[32m0.5874\u001b[0m       0.6250            0.6250        0.9314  0.0006  0.1388\n",
      "     35            0.8500        \u001b[32m0.5186\u001b[0m       0.5833            0.5833        0.9710  0.0006  0.1753\n",
      "     36            \u001b[36m0.9250\u001b[0m        \u001b[32m0.4634\u001b[0m       0.6042            0.6042        1.0220  0.0007  0.2073\n",
      "     37            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3296\u001b[0m       0.5938            0.5938        1.0715  0.0007  0.3325\n",
      "     38            0.9750        \u001b[32m0.2648\u001b[0m       0.5938            0.5938        1.1117  0.0007  0.2518\n",
      "     39            \u001b[36m1.0000\u001b[0m        0.3599       0.5729            0.5729        1.1306  0.0007  0.1988\n",
      "     40            1.0000        \u001b[32m0.2565\u001b[0m       0.5729            0.5729        1.1330  0.0007  0.1769\n",
      "     41            1.0000        \u001b[32m0.2157\u001b[0m       0.5729            0.5729        1.1219  0.0007  0.1630\n",
      "     42            1.0000        \u001b[32m0.2107\u001b[0m       0.5729            0.5729        1.1063  0.0007  0.1816\n",
      "     43            1.0000        \u001b[32m0.1514\u001b[0m       0.5729            0.5729        1.0862  0.0006  0.1656\n",
      "     44            1.0000        \u001b[32m0.1409\u001b[0m       0.5938            0.5938        1.0625  0.0006  0.1829\n",
      "     45            1.0000        \u001b[32m0.1283\u001b[0m       0.6042            0.6042        1.0396  0.0005  0.2174\n",
      "     46            1.0000        \u001b[32m0.1123\u001b[0m       0.6042            0.6042        1.0161  0.0005  0.2350\n",
      "     47            1.0000        \u001b[32m0.0731\u001b[0m       0.6042            0.6042        0.9977  0.0004  0.2454\n",
      "     48            1.0000        0.1042       0.6250            0.6250        0.9826  0.0004  0.1953\n",
      "     49            1.0000        \u001b[32m0.0674\u001b[0m       0.6250            0.6250        0.9703  0.0003  0.1723\n",
      "     50            1.0000        0.0850       0.6146            0.6146        0.9611  0.0003  0.2005\n",
      "Fine tuning model for subject 3 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6444        1.1778       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m0.8918\u001b[0m  0.0004  0.2017\n",
      "     32            0.6444        1.1496       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        0.9013  0.0005  0.1962\n",
      "     33            0.7333        0.9173       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        0.9105  0.0005  0.2273\n",
      "     34            0.7778        0.8491       0.6042            0.6042        0.9344  0.0006  0.1592\n",
      "     35            \u001b[36m0.8000\u001b[0m        0.8003       0.5729            0.5729        0.9819  0.0006  0.1655\n",
      "     36            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6212\u001b[0m       0.5417            0.5417        1.0481  0.0007  0.2110\n",
      "     37            0.8667        \u001b[32m0.6095\u001b[0m       0.5208            0.5208        1.1152  0.0007  0.1907\n",
      "     38            0.8667        \u001b[32m0.4719\u001b[0m       0.5312            0.5312        1.1639  0.0007  0.2174\n",
      "     39            \u001b[36m0.8889\u001b[0m        \u001b[32m0.4232\u001b[0m       0.5521            0.5521        1.1975  0.0007  0.2929\n",
      "     40            \u001b[36m0.9111\u001b[0m        \u001b[32m0.3134\u001b[0m       0.5312            0.5312        1.2129  0.0007  0.3155\n",
      "     41            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3092\u001b[0m       0.5312            0.5312        1.2163  0.0007  0.1973\n",
      "     42            0.9333        \u001b[32m0.2986\u001b[0m       0.5417            0.5417        1.2045  0.0007  0.2274\n",
      "     43            \u001b[36m0.9556\u001b[0m        \u001b[32m0.2172\u001b[0m       0.5521            0.5521        1.1884  0.0006  0.1660\n",
      "     44            0.9556        0.2411       0.5521            0.5521        1.1678  0.0006  0.3072\n",
      "     45            0.9556        \u001b[32m0.1940\u001b[0m       0.5521            0.5521        1.1494  0.0005  0.1885\n",
      "     46            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1905\u001b[0m       0.5521            0.5521        1.1300  0.0005  0.1980\n",
      "     47            1.0000        \u001b[32m0.1656\u001b[0m       0.5521            0.5521        1.1083  0.0004  0.1892\n",
      "     48            1.0000        \u001b[32m0.1372\u001b[0m       0.5625            0.5625        1.0863  0.0004  0.2180\n",
      "     49            1.0000        0.1772       0.5833            0.5833        1.0664  0.0003  0.2192\n",
      "     50            1.0000        0.1716       0.5833            0.5833        1.0477  0.0003  0.1863\n",
      "Fine tuning model for subject 3 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5111        1.1181       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8800\u001b[0m  0.0004  0.1785\n",
      "     32            0.5333        1.0825       0.6562            0.6562        \u001b[94m0.8698\u001b[0m  0.0005  0.2065\n",
      "     33            0.6444        1.0801       0.6562            0.6562        \u001b[94m0.8571\u001b[0m  0.0005  0.1763\n",
      "     34            0.7778        1.0425       0.6354            0.6354        0.8577  0.0006  0.2277\n",
      "     35            \u001b[36m0.8222\u001b[0m        0.8685       0.6354            0.6354        0.8894  0.0006  0.1973\n",
      "     36            0.8222        \u001b[32m0.7235\u001b[0m       0.5938            0.5938        0.9614  0.0007  0.1772\n",
      "     37            0.7556        \u001b[32m0.5728\u001b[0m       0.5208            0.5208        1.0527  0.0007  0.1869\n",
      "     38            0.7556        \u001b[32m0.3982\u001b[0m       0.5208            0.5208        1.1423  0.0007  0.1708\n",
      "     39            0.7778        0.4493       0.4896            0.4896        1.2085  0.0007  0.1895\n",
      "     40            0.8222        \u001b[32m0.3446\u001b[0m       0.5000            0.5000        1.2386  0.0007  0.2291\n",
      "     41            \u001b[36m0.8667\u001b[0m        \u001b[32m0.3058\u001b[0m       0.5104            0.5104        1.2505  0.0007  0.2493\n",
      "     42            \u001b[36m0.8889\u001b[0m        \u001b[32m0.2366\u001b[0m       0.5104            0.5104        1.2539  0.0007  0.2061\n",
      "     43            \u001b[36m0.9111\u001b[0m        \u001b[32m0.2051\u001b[0m       0.5208            0.5208        1.2497  0.0006  0.2002\n",
      "     44            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1616\u001b[0m       0.5208            0.5208        1.2399  0.0006  0.2116\n",
      "     45            1.0000        0.1887       0.5208            0.5208        1.2326  0.0005  0.2133\n",
      "     46            1.0000        \u001b[32m0.1562\u001b[0m       0.5417            0.5417        1.2279  0.0005  0.2571\n",
      "     47            1.0000        0.1616       0.5312            0.5312        1.2204  0.0004  0.2117\n",
      "     48            1.0000        \u001b[32m0.1391\u001b[0m       0.5417            0.5417        1.2103  0.0004  0.2083\n",
      "     49            1.0000        \u001b[32m0.1300\u001b[0m       0.5417            0.5417        1.2000  0.0003  0.2344\n",
      "     50            1.0000        \u001b[32m0.1243\u001b[0m       0.5417            0.5417        1.1883  0.0003  0.1986\n",
      "Fine tuning model for subject 3 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6222        1.0903       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8855\u001b[0m  0.0004  0.2109\n",
      "     32            0.6667        1.0700       0.6458            0.6458        \u001b[94m0.8769\u001b[0m  0.0005  0.2421\n",
      "     33            0.7333        0.9982       0.6562            0.6562        \u001b[94m0.8616\u001b[0m  0.0005  0.1993\n",
      "     34            \u001b[36m0.8000\u001b[0m        0.9168       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        \u001b[94m0.8494\u001b[0m  0.0006  0.2107\n",
      "     35            0.8000        \u001b[32m0.7111\u001b[0m       \u001b[35m0.6979\u001b[0m            \u001b[31m0.6979\u001b[0m        0.8508  0.0006  0.1652\n",
      "     36            \u001b[36m0.8444\u001b[0m        \u001b[32m0.5457\u001b[0m       0.6875            0.6875        0.8703  0.0007  0.1720\n",
      "     37            \u001b[36m0.8889\u001b[0m        \u001b[32m0.5206\u001b[0m       0.6562            0.6562        0.9027  0.0007  0.1615\n",
      "     38            0.8889        \u001b[32m0.4127\u001b[0m       0.6146            0.6146        0.9423  0.0007  0.1565\n",
      "     39            0.8889        \u001b[32m0.4054\u001b[0m       0.6042            0.6042        0.9825  0.0007  0.1600\n",
      "     40            \u001b[36m0.9333\u001b[0m        \u001b[32m0.2989\u001b[0m       0.5833            0.5833        1.0176  0.0007  0.1715\n",
      "     41            0.9333        \u001b[32m0.2820\u001b[0m       0.6042            0.6042        1.0381  0.0007  0.1732\n",
      "     42            \u001b[36m0.9556\u001b[0m        \u001b[32m0.2099\u001b[0m       0.5938            0.5938        1.0472  0.0007  0.1719\n",
      "     43            0.9556        0.2138       0.5833            0.5833        1.0465  0.0006  0.1718\n",
      "     44            0.9556        \u001b[32m0.1870\u001b[0m       0.5833            0.5833        1.0400  0.0006  0.1626\n",
      "     45            \u001b[36m0.9778\u001b[0m        \u001b[32m0.1652\u001b[0m       0.5833            0.5833        1.0274  0.0005  0.1647\n",
      "     46            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1617\u001b[0m       0.6250            0.6250        1.0128  0.0005  0.1776\n",
      "     47            1.0000        \u001b[32m0.1370\u001b[0m       0.6250            0.6250        0.9982  0.0004  0.1766\n",
      "     48            1.0000        0.1412       0.6250            0.6250        0.9834  0.0004  0.1784\n",
      "     49            1.0000        0.1421       0.6354            0.6354        0.9692  0.0003  0.1640\n",
      "     50            1.0000        \u001b[32m0.1049\u001b[0m       0.6250            0.6250        0.9568  0.0003  0.1706\n",
      "Fine tuning model for subject 3 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        0.9092       \u001b[35m0.6771\u001b[0m            \u001b[31m0.6771\u001b[0m        \u001b[94m0.8798\u001b[0m  0.0004  0.1590\n",
      "     32            0.6667        0.8895       0.6771            0.6771        \u001b[94m0.8652\u001b[0m  0.0005  0.1623\n",
      "     33            0.7778        0.8172       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        \u001b[94m0.8478\u001b[0m  0.0005  0.1676\n",
      "     34            \u001b[36m0.8444\u001b[0m        \u001b[32m0.6111\u001b[0m       0.6875            0.6875        \u001b[94m0.8359\u001b[0m  0.0006  0.1486\n",
      "     35            \u001b[36m0.8889\u001b[0m        \u001b[32m0.6079\u001b[0m       0.6562            0.6562        \u001b[94m0.8311\u001b[0m  0.0006  0.1672\n",
      "     36            \u001b[36m0.9556\u001b[0m        \u001b[32m0.4594\u001b[0m       0.6771            0.6771        0.8404  0.0007  0.1820\n",
      "     37            0.9556        \u001b[32m0.4125\u001b[0m       0.6667            0.6667        0.8636  0.0007  0.1750\n",
      "     38            0.9556        \u001b[32m0.3922\u001b[0m       0.6458            0.6458        0.8951  0.0007  0.1986\n",
      "     39            0.9556        \u001b[32m0.3275\u001b[0m       0.6458            0.6458        0.9261  0.0007  0.1745\n",
      "     40            0.9556        0.3394       0.6042            0.6042        0.9461  0.0007  0.2009\n",
      "     41            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3052\u001b[0m       0.6042            0.6042        0.9623  0.0007  0.1906\n",
      "     42            0.9778        \u001b[32m0.2305\u001b[0m       0.5938            0.5938        0.9723  0.0007  0.2412\n",
      "     43            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2274\u001b[0m       0.5938            0.5938        0.9762  0.0006  0.1832\n",
      "     44            1.0000        \u001b[32m0.1756\u001b[0m       0.5938            0.5938        0.9768  0.0006  0.1974\n",
      "     45            1.0000        \u001b[32m0.1564\u001b[0m       0.6042            0.6042        0.9738  0.0005  0.1665\n",
      "     46            1.0000        \u001b[32m0.1328\u001b[0m       0.5938            0.5938        0.9670  0.0005  0.1952\n",
      "     47            1.0000        0.1468       0.5833            0.5833        0.9570  0.0004  0.1687\n",
      "     48            1.0000        \u001b[32m0.1315\u001b[0m       0.5938            0.5938        0.9463  0.0004  0.1818\n",
      "     49            1.0000        \u001b[32m0.1071\u001b[0m       0.5938            0.5938        0.9369  0.0003  0.1973\n",
      "     50            1.0000        0.1284       0.6250            0.6250        0.9294  0.0003  0.1664\n",
      "Fine tuning model for subject 3 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        0.9033       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8938\u001b[0m  0.0004  0.1857\n",
      "     32            0.6222        0.9397       0.6562            0.6562        0.9036  0.0005  0.1693\n",
      "     33            0.7111        0.8300       0.6458            0.6458        0.9047  0.0005  0.2100\n",
      "     34            0.7778        0.8095       0.6458            0.6458        0.8983  0.0006  0.1878\n",
      "     35            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6275\u001b[0m       0.6458            0.6458        \u001b[94m0.8937\u001b[0m  0.0006  0.1799\n",
      "     36            \u001b[36m0.9556\u001b[0m        \u001b[32m0.4989\u001b[0m       0.6354            0.6354        0.9018  0.0007  0.1709\n",
      "     37            0.9556        \u001b[32m0.4425\u001b[0m       0.6250            0.6250        0.9256  0.0007  0.1586\n",
      "     38            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3427\u001b[0m       0.6042            0.6042        0.9597  0.0007  0.1798\n",
      "     39            0.9778        \u001b[32m0.3176\u001b[0m       0.6042            0.6042        0.9966  0.0007  0.1776\n",
      "     40            0.9778        \u001b[32m0.2355\u001b[0m       0.5833            0.5833        1.0317  0.0007  0.1599\n",
      "     41            0.9778        0.2712       0.5521            0.5521        1.0632  0.0007  0.1531\n",
      "     42            0.9778        \u001b[32m0.1681\u001b[0m       0.5417            0.5417        1.0876  0.0007  0.1796\n",
      "     43            0.9778        0.2076       0.5312            0.5312        1.1047  0.0006  0.1657\n",
      "     44            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1628\u001b[0m       0.5417            0.5417        1.1123  0.0006  0.1585\n",
      "     45            1.0000        \u001b[32m0.1415\u001b[0m       0.5521            0.5521        1.1138  0.0005  0.1543\n",
      "     46            1.0000        0.1698       0.5521            0.5521        1.1093  0.0005  0.2293\n",
      "     47            1.0000        0.1716       0.5625            0.5625        1.0986  0.0004  0.2011\n",
      "     48            1.0000        \u001b[32m0.1241\u001b[0m       0.5625            0.5625        1.0873  0.0004  0.1762\n",
      "     49            1.0000        \u001b[32m0.1066\u001b[0m       0.5833            0.5833        1.0772  0.0003  0.1980\n",
      "     50            1.0000        0.1144       0.5833            0.5833        1.0659  0.0003  0.1903\n",
      "Fine tuning model for subject 3 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7333        0.7337       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m0.8925\u001b[0m  0.0004  0.1858\n",
      "     32            0.7556        0.8913       0.6458            0.6458        0.9068  0.0005  0.1833\n",
      "     33            0.7778        \u001b[32m0.7099\u001b[0m       0.6042            0.6042        0.9203  0.0005  0.3349\n",
      "     34            0.7778        \u001b[32m0.6227\u001b[0m       0.6146            0.6146        0.9428  0.0006  0.3349\n",
      "     35            \u001b[36m0.8222\u001b[0m        \u001b[32m0.5385\u001b[0m       0.5938            0.5938        0.9611  0.0006  0.2505\n",
      "     36            \u001b[36m0.8667\u001b[0m        \u001b[32m0.4449\u001b[0m       0.5833            0.5833        0.9805  0.0007  0.3048\n",
      "     37            \u001b[36m0.9111\u001b[0m        \u001b[32m0.4074\u001b[0m       0.6042            0.6042        0.9949  0.0007  0.2666\n",
      "     38            0.9111        \u001b[32m0.3361\u001b[0m       0.6042            0.6042        1.0104  0.0007  0.2454\n",
      "     39            \u001b[36m0.9556\u001b[0m        \u001b[32m0.2404\u001b[0m       0.5938            0.5938        1.0333  0.0007  0.2209\n",
      "     40            \u001b[36m0.9778\u001b[0m        \u001b[32m0.2022\u001b[0m       0.5833            0.5833        1.0607  0.0007  0.2183\n",
      "     41            0.9778        \u001b[32m0.1587\u001b[0m       0.5938            0.5938        1.0849  0.0007  0.2350\n",
      "     42            0.9778        0.1737       0.5938            0.5938        1.1048  0.0007  0.2345\n",
      "     43            \u001b[36m1.0000\u001b[0m        0.1638       0.5729            0.5729        1.1227  0.0006  0.2895\n",
      "     44            1.0000        \u001b[32m0.1267\u001b[0m       0.5625            0.5625        1.1348  0.0006  0.1869\n",
      "     45            1.0000        0.1480       0.5625            0.5625        1.1455  0.0005  0.2072\n",
      "     46            1.0000        \u001b[32m0.1011\u001b[0m       0.5312            0.5312        1.1542  0.0005  0.2489\n",
      "     47            1.0000        0.1022       0.5312            0.5312        1.1589  0.0004  0.1774\n",
      "     48            1.0000        \u001b[32m0.0972\u001b[0m       0.5417            0.5417        1.1587  0.0004  0.1979\n",
      "     49            1.0000        \u001b[32m0.0944\u001b[0m       0.5417            0.5417        1.1546  0.0003  0.1827\n",
      "     50            1.0000        \u001b[32m0.0883\u001b[0m       0.5625            0.5625        1.1463  0.0003  0.1649\n",
      "Fine tuning model for subject 3 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6222        1.0086       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8836\u001b[0m  0.0004  0.1917\n",
      "     32            0.6444        1.0207       0.6458            0.6458        0.8860  0.0005  0.2186\n",
      "     33            0.7556        1.0459       0.6146            0.6146        \u001b[94m0.8819\u001b[0m  0.0005  0.2053\n",
      "     34            \u001b[36m0.8000\u001b[0m        0.9046       0.6354            0.6354        \u001b[94m0.8736\u001b[0m  0.0006  0.2279\n",
      "     35            0.8000        0.8390       0.6458            0.6458        \u001b[94m0.8718\u001b[0m  0.0006  0.2163\n",
      "     36            \u001b[36m0.8889\u001b[0m        0.7404       0.6458            0.6458        0.8778  0.0007  0.1757\n",
      "     37            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4982\u001b[0m       0.6250            0.6250        0.8937  0.0007  0.1992\n",
      "     38            0.9333        \u001b[32m0.4287\u001b[0m       0.6250            0.6250        0.9119  0.0007  0.1846\n",
      "     39            \u001b[36m0.9556\u001b[0m        \u001b[32m0.3718\u001b[0m       0.6458            0.6458        0.9306  0.0007  0.1815\n",
      "     40            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3336\u001b[0m       0.6458            0.6458        0.9433  0.0007  0.1780\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2493\u001b[0m       0.6250            0.6250        0.9523  0.0007  0.3125\n",
      "     42            1.0000        0.2669       0.6146            0.6146        0.9573  0.0007  0.1996\n",
      "     43            1.0000        \u001b[32m0.2075\u001b[0m       0.6354            0.6354        0.9614  0.0006  0.2419\n",
      "     44            1.0000        0.2117       0.6146            0.6146        0.9627  0.0006  0.1986\n",
      "     45            1.0000        \u001b[32m0.2066\u001b[0m       0.6042            0.6042        0.9594  0.0005  0.1929\n",
      "     46            1.0000        \u001b[32m0.1933\u001b[0m       0.5833            0.5833        0.9581  0.0005  0.2329\n",
      "     47            1.0000        \u001b[32m0.1594\u001b[0m       0.5833            0.5833        0.9556  0.0004  0.1872\n",
      "     48            1.0000        0.1692       0.5938            0.5938        0.9519  0.0004  0.2208\n",
      "     49            1.0000        \u001b[32m0.1471\u001b[0m       0.5938            0.5938        0.9476  0.0003  0.2183\n",
      "     50            1.0000        \u001b[32m0.1239\u001b[0m       0.6042            0.6042        0.9426  0.0003  0.1904\n",
      "Fine tuning model for subject 3 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5778        1.1898       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8776\u001b[0m  0.0004  0.1737\n",
      "     32            0.6889        1.1191       0.6458            0.6458        \u001b[94m0.8736\u001b[0m  0.0005  0.1941\n",
      "     33            0.7111        0.9584       0.6042            0.6042        0.8926  0.0005  0.2039\n",
      "     34            0.6889        0.7581       0.5521            0.5521        0.9581  0.0006  0.1821\n",
      "     35            0.7333        \u001b[32m0.6116\u001b[0m       0.5312            0.5312        1.0556  0.0006  0.2548\n",
      "     36            0.7111        \u001b[32m0.5784\u001b[0m       0.5208            0.5208        1.1475  0.0007  0.1752\n",
      "     37            0.7556        \u001b[32m0.4930\u001b[0m       0.5104            0.5104        1.2180  0.0007  0.2251\n",
      "     38            0.7778        \u001b[32m0.3807\u001b[0m       0.5104            0.5104        1.2696  0.0007  0.1872\n",
      "     39            \u001b[36m0.8667\u001b[0m        \u001b[32m0.3364\u001b[0m       0.5000            0.5000        1.2964  0.0007  0.1767\n",
      "     40            \u001b[36m0.8889\u001b[0m        \u001b[32m0.3034\u001b[0m       0.5104            0.5104        1.2998  0.0007  0.2111\n",
      "     41            \u001b[36m0.9556\u001b[0m        \u001b[32m0.2237\u001b[0m       0.5208            0.5208        1.2979  0.0007  0.2010\n",
      "     42            \u001b[36m0.9778\u001b[0m        0.2424       0.5208            0.5208        1.2928  0.0007  0.1803\n",
      "     43            \u001b[36m1.0000\u001b[0m        0.2503       0.5312            0.5312        1.2826  0.0006  0.2046\n",
      "     44            1.0000        \u001b[32m0.1925\u001b[0m       0.5312            0.5312        1.2715  0.0006  0.1862\n",
      "     45            1.0000        \u001b[32m0.1662\u001b[0m       0.5312            0.5312        1.2600  0.0005  0.1975\n",
      "     46            1.0000        \u001b[32m0.1334\u001b[0m       0.5312            0.5312        1.2499  0.0005  0.2061\n",
      "     47            1.0000        \u001b[32m0.1123\u001b[0m       0.5417            0.5417        1.2422  0.0004  0.1691\n",
      "     48            1.0000        \u001b[32m0.1057\u001b[0m       0.5417            0.5417        1.2339  0.0004  0.2108\n",
      "     49            1.0000        0.1401       0.5521            0.5521        1.2242  0.0003  0.2112\n",
      "     50            1.0000        0.1138       0.5729            0.5729        1.2146  0.0003  0.1827\n",
      "Fine tuning model for subject 3 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6222        1.0316       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8863\u001b[0m  0.0004  0.2394\n",
      "     32            0.7111        0.9495       0.6458            0.6458        0.8956  0.0005  0.2167\n",
      "     33            \u001b[36m0.8000\u001b[0m        1.0446       0.6250            0.6250        0.9132  0.0005  0.2668\n",
      "     34            \u001b[36m0.8889\u001b[0m        0.7917       0.5729            0.5729        0.9550  0.0006  0.1978\n",
      "     35            0.8889        \u001b[32m0.5610\u001b[0m       0.5625            0.5625        1.0272  0.0006  0.2061\n",
      "     36            \u001b[36m0.9111\u001b[0m        \u001b[32m0.5454\u001b[0m       0.5104            0.5104        1.1253  0.0007  0.2072\n",
      "     37            0.9111        \u001b[32m0.4327\u001b[0m       0.4688            0.4688        1.2429  0.0007  0.1977\n",
      "     38            0.8889        \u001b[32m0.3959\u001b[0m       0.4583            0.4583        1.3438  0.0007  0.2003\n",
      "     39            0.9111        \u001b[32m0.3407\u001b[0m       0.4271            0.4271        1.4065  0.0007  0.2468\n",
      "     40            0.9111        \u001b[32m0.2760\u001b[0m       0.4375            0.4375        1.4295  0.0007  0.2330\n",
      "     41            0.9111        \u001b[32m0.2673\u001b[0m       0.4583            0.4583        1.4242  0.0007  0.2085\n",
      "     42            0.9111        \u001b[32m0.2148\u001b[0m       0.4688            0.4688        1.3973  0.0007  0.2286\n",
      "     43            \u001b[36m0.9556\u001b[0m        \u001b[32m0.1751\u001b[0m       0.4896            0.4896        1.3624  0.0006  0.2603\n",
      "     44            0.9556        0.1800       0.4792            0.4792        1.3223  0.0006  0.2191\n",
      "     45            0.9556        \u001b[32m0.1560\u001b[0m       0.4896            0.4896        1.2830  0.0005  0.1762\n",
      "     46            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1487\u001b[0m       0.5208            0.5208        1.2450  0.0005  0.1727\n",
      "     47            1.0000        \u001b[32m0.1195\u001b[0m       0.5208            0.5208        1.2095  0.0004  0.2153\n",
      "     48            1.0000        \u001b[32m0.1093\u001b[0m       0.5208            0.5208        1.1777  0.0004  0.2144\n",
      "     49            1.0000        0.1172       0.5208            0.5208        1.1496  0.0003  0.1754\n",
      "     50            1.0000        \u001b[32m0.0991\u001b[0m       0.5312            0.5312        1.1260  0.0003  0.2024\n",
      "Fine tuning model for subject 3 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7333        1.0641       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m0.8936\u001b[0m  0.0004  0.1854\n",
      "     32            0.7556        1.0286       0.6250            0.6250        0.9083  0.0005  0.2109\n",
      "     33            0.7556        0.8857       0.6146            0.6146        0.9305  0.0005  0.1761\n",
      "     34            \u001b[36m0.8222\u001b[0m        \u001b[32m0.7227\u001b[0m       0.6250            0.6250        0.9500  0.0006  0.2149\n",
      "     35            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6714\u001b[0m       0.6250            0.6250        0.9612  0.0006  0.1786\n",
      "     36            \u001b[36m0.8889\u001b[0m        \u001b[32m0.5391\u001b[0m       0.6146            0.6146        0.9699  0.0007  0.1759\n",
      "     37            0.8889        \u001b[32m0.5289\u001b[0m       0.6042            0.6042        0.9932  0.0007  0.2133\n",
      "     38            0.8667        \u001b[32m0.3208\u001b[0m       0.5312            0.5312        1.0314  0.0007  0.1769\n",
      "     39            0.8667        \u001b[32m0.2735\u001b[0m       0.5208            0.5208        1.0676  0.0007  0.1730\n",
      "     40            0.8889        \u001b[32m0.2185\u001b[0m       0.5104            0.5104        1.0846  0.0007  0.2066\n",
      "     41            \u001b[36m0.9111\u001b[0m        0.2342       0.5104            0.5104        1.0883  0.0007  0.1727\n",
      "     42            \u001b[36m0.9556\u001b[0m        \u001b[32m0.1826\u001b[0m       0.5312            0.5312        1.0882  0.0007  0.1924\n",
      "     43            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1745\u001b[0m       0.5729            0.5729        1.0797  0.0006  0.2102\n",
      "     44            1.0000        \u001b[32m0.1423\u001b[0m       0.5729            0.5729        1.0727  0.0006  0.1811\n",
      "     45            1.0000        \u001b[32m0.1190\u001b[0m       0.6354            0.6354        1.0675  0.0005  0.1994\n",
      "     46            1.0000        \u001b[32m0.0720\u001b[0m       0.6354            0.6354        1.0662  0.0005  0.2322\n",
      "     47            1.0000        0.0844       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        1.0668  0.0004  0.1715\n",
      "     48            1.0000        0.1094       0.6562            0.6562        1.0665  0.0004  0.1618\n",
      "     49            1.0000        0.0856       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        1.0669  0.0003  0.2202\n",
      "     50            1.0000        \u001b[32m0.0683\u001b[0m       0.6458            0.6458        1.0676  0.0003  0.1790\n",
      "Hold out data from subject 4\n",
      "Pre-training model with data from all subjects but subject 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.4070\u001b[0m        \u001b[32m1.5943\u001b[0m       \u001b[35m0.3203\u001b[0m            \u001b[31m0.3203\u001b[0m        \u001b[94m1.4086\u001b[0m  0.0007  9.6948\n",
      "      2            \u001b[36m0.4849\u001b[0m        \u001b[32m1.4258\u001b[0m       \u001b[35m0.3737\u001b[0m            \u001b[31m0.3737\u001b[0m        \u001b[94m1.3331\u001b[0m  0.0007  8.1750\n",
      "      3            \u001b[36m0.5401\u001b[0m        \u001b[32m1.3237\u001b[0m       \u001b[35m0.4115\u001b[0m            \u001b[31m0.4115\u001b[0m        \u001b[94m1.2723\u001b[0m  0.0007  7.5864\n",
      "      4            \u001b[36m0.5578\u001b[0m        \u001b[32m1.2188\u001b[0m       0.4102            0.4102        \u001b[94m1.2353\u001b[0m  0.0007  8.0708\n",
      "      5            \u001b[36m0.5862\u001b[0m        \u001b[32m1.1677\u001b[0m       \u001b[35m0.4453\u001b[0m            \u001b[31m0.4453\u001b[0m        1.2504  0.0007  7.4509\n",
      "      6            \u001b[36m0.6078\u001b[0m        \u001b[32m1.1295\u001b[0m       \u001b[35m0.4570\u001b[0m            \u001b[31m0.4570\u001b[0m        \u001b[94m1.2213\u001b[0m  0.0006  7.6057\n",
      "      7            \u001b[36m0.6240\u001b[0m        \u001b[32m1.0744\u001b[0m       0.4453            0.4453        1.2250  0.0006  8.0858\n",
      "      8            \u001b[36m0.6477\u001b[0m        \u001b[32m1.0192\u001b[0m       \u001b[35m0.4766\u001b[0m            \u001b[31m0.4766\u001b[0m        \u001b[94m1.1826\u001b[0m  0.0006  7.3711\n",
      "      9            \u001b[36m0.6766\u001b[0m        \u001b[32m1.0133\u001b[0m       \u001b[35m0.4805\u001b[0m            \u001b[31m0.4805\u001b[0m        \u001b[94m1.1517\u001b[0m  0.0006  7.7889\n",
      "     10            \u001b[36m0.6857\u001b[0m        \u001b[32m0.9837\u001b[0m       \u001b[35m0.5052\u001b[0m            \u001b[31m0.5052\u001b[0m        1.1619  0.0005  7.8976\n",
      "     11            0.6773        \u001b[32m0.9410\u001b[0m       0.5013            0.5013        \u001b[94m1.1473\u001b[0m  0.0005  7.4458\n",
      "     12            \u001b[36m0.7167\u001b[0m        \u001b[32m0.9279\u001b[0m       \u001b[35m0.5078\u001b[0m            \u001b[31m0.5078\u001b[0m        1.1576  0.0005  7.8821\n",
      "     13            0.7141        \u001b[32m0.9132\u001b[0m       0.4935            0.4935        \u001b[94m1.1349\u001b[0m  0.0004  7.7142\n",
      "     14            \u001b[36m0.7247\u001b[0m        \u001b[32m0.8837\u001b[0m       \u001b[35m0.5182\u001b[0m            \u001b[31m0.5182\u001b[0m        1.1455  0.0004  7.3512\n",
      "     15            0.7214        \u001b[32m0.8748\u001b[0m       0.5065            0.5065        1.1511  0.0004  7.9489\n",
      "     16            \u001b[36m0.7518\u001b[0m        \u001b[32m0.8469\u001b[0m       0.5182            0.5182        1.1366  0.0003  7.7654\n",
      "     17            \u001b[36m0.7536\u001b[0m        \u001b[32m0.8164\u001b[0m       0.5104            0.5104        1.1569  0.0003  7.3514\n",
      "     18            \u001b[36m0.7651\u001b[0m        \u001b[32m0.7957\u001b[0m       \u001b[35m0.5273\u001b[0m            \u001b[31m0.5273\u001b[0m        \u001b[94m1.1247\u001b[0m  0.0003  8.0457\n",
      "     19            \u001b[36m0.7719\u001b[0m        \u001b[32m0.7936\u001b[0m       0.5221            0.5221        1.1372  0.0002  7.5282\n",
      "     20            0.7656        \u001b[32m0.7687\u001b[0m       \u001b[35m0.5352\u001b[0m            \u001b[31m0.5352\u001b[0m        1.1309  0.0002  7.3536\n",
      "     21            \u001b[36m0.7914\u001b[0m        \u001b[32m0.7651\u001b[0m       0.5286            0.5286        \u001b[94m1.1123\u001b[0m  0.0002  8.0008\n",
      "     22            0.7826        \u001b[32m0.7485\u001b[0m       \u001b[35m0.5443\u001b[0m            \u001b[31m0.5443\u001b[0m        \u001b[94m1.1040\u001b[0m  0.0001  7.3664\n",
      "     23            0.7888        \u001b[32m0.7419\u001b[0m       0.5391            0.5391        \u001b[94m1.1016\u001b[0m  0.0001  7.4631\n",
      "     24            \u001b[36m0.8005\u001b[0m        \u001b[32m0.7314\u001b[0m       0.5404            0.5404        1.1078  0.0001  8.0028\n",
      "     25            \u001b[36m0.8070\u001b[0m        \u001b[32m0.7116\u001b[0m       0.5391            0.5391        1.1024  0.0001  7.7178\n",
      "     26            0.8005        \u001b[32m0.7036\u001b[0m       0.5312            0.5312        1.1059  0.0000  7.5221\n",
      "     27            \u001b[36m0.8073\u001b[0m        0.7088       \u001b[35m0.5469\u001b[0m            \u001b[31m0.5469\u001b[0m        \u001b[94m1.1009\u001b[0m  0.0000  8.0218\n",
      "     28            \u001b[36m0.8089\u001b[0m        0.7057       0.5352            0.5352        1.1017  0.0000  7.5165\n",
      "     29            0.8081        0.7049       0.5417            0.5417        1.1012  0.0000  7.6951\n",
      "     30            0.8083        \u001b[32m0.6888\u001b[0m       0.5417            0.5417        \u001b[94m1.1008\u001b[0m  0.0000  8.1661\n",
      "Before finetuning for subject 4, the baseline accuracy is 0.5104166666666666\n",
      "Fine tuning model for subject 4 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.5521       0.5000            0.5000        1.4124  0.0004  0.1304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        1.0100       0.4688            0.4688        1.4525  0.0005  0.1357\n",
      "     33            1.0000        \u001b[32m0.2421\u001b[0m       0.4479            0.4479        1.4808  0.0005  0.1414\n",
      "     34            1.0000        \u001b[32m0.0655\u001b[0m       0.4583            0.4583        1.4956  0.0006  0.1364\n",
      "     35            1.0000        \u001b[32m0.0401\u001b[0m       0.4479            0.4479        1.5108  0.0006  0.1275\n",
      "     36            1.0000        \u001b[32m0.0084\u001b[0m       0.4271            0.4271        1.5334  0.0007  0.1287\n",
      "     37            1.0000        0.0097       0.4167            0.4167        1.5638  0.0007  0.1322\n",
      "     38            1.0000        0.0117       0.3854            0.3854        1.5987  0.0007  0.1251\n",
      "     39            1.0000        \u001b[32m0.0027\u001b[0m       0.3542            0.3542        1.6337  0.0007  0.1278\n",
      "     40            1.0000        0.0030       0.3542            0.3542        1.6660  0.0007  0.1274\n",
      "     41            1.0000        0.0121       0.3438            0.3438        1.6950  0.0007  0.1315\n",
      "     42            1.0000        0.0035       0.3542            0.3542        1.7201  0.0007  0.1339\n",
      "     43            1.0000        0.0070       0.3542            0.3542        1.7420  0.0006  0.1244\n",
      "     44            1.0000        0.0061       0.3333            0.3333        1.7615  0.0006  0.1252\n",
      "     45            1.0000        0.0078       0.3438            0.3438        1.7796  0.0005  0.1288\n",
      "     46            1.0000        0.0112       0.3542            0.3542        1.7972  0.0005  0.1281\n",
      "     47            1.0000        0.0086       0.3333            0.3333        1.8142  0.0004  0.1259\n",
      "     48            1.0000        0.0050       0.3333            0.3333        1.8308  0.0004  0.1294\n",
      "     49            1.0000        0.0042       0.3229            0.3229        1.8469  0.0003  0.1335\n",
      "     50            1.0000        0.0035       0.3333            0.3333        1.8627  0.0003  0.1305\n",
      "Fine tuning model for subject 4 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.2059       0.4792            0.4792        1.4020  0.0004  0.1282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        0.7496       0.4792            0.4792        1.4186  0.0005  0.1584\n",
      "     33            1.0000        0.8420       0.4479            0.4479        1.4685  0.0005  0.1318\n",
      "     34            1.0000        \u001b[32m0.1530\u001b[0m       0.4167            0.4167        1.5653  0.0006  0.1262\n",
      "     35            1.0000        \u001b[32m0.0312\u001b[0m       0.3750            0.3750        1.7148  0.0006  0.1330\n",
      "     36            1.0000        \u001b[32m0.0308\u001b[0m       0.3750            0.3750        1.8972  0.0007  0.1248\n",
      "     37            1.0000        \u001b[32m0.0099\u001b[0m       0.3542            0.3542        2.0778  0.0007  0.1254\n",
      "     38            1.0000        0.0164       0.3125            0.3125        2.2301  0.0007  0.1285\n",
      "     39            1.0000        \u001b[32m0.0099\u001b[0m       0.3125            0.3125        2.3427  0.0007  0.1291\n",
      "     40            1.0000        0.0198       0.3229            0.3229        2.4118  0.0007  0.1207\n",
      "     41            1.0000        0.0505       0.3125            0.3125        2.4455  0.0007  0.1347\n",
      "     42            1.0000        0.0247       0.2917            0.2917        2.4504  0.0007  0.1305\n",
      "     43            1.0000        0.0509       0.2917            0.2917        2.4274  0.0006  0.1250\n",
      "     44            1.0000        \u001b[32m0.0082\u001b[0m       0.2812            0.2812        2.3969  0.0006  0.1290\n",
      "     45            1.0000        \u001b[32m0.0056\u001b[0m       0.2917            0.2917        2.3638  0.0005  0.1446\n",
      "     46            1.0000        0.0089       0.2812            0.2812        2.3302  0.0005  0.1345\n",
      "     47            1.0000        0.0059       0.2917            0.2917        2.2987  0.0004  0.1330\n",
      "     48            1.0000        0.0085       0.2917            0.2917        2.2705  0.0004  0.1329\n",
      "     49            1.0000        0.0061       0.2812            0.2812        2.2447  0.0003  0.1305\n",
      "     50            1.0000        0.0076       0.3021            0.3021        2.2217  0.0003  0.1386\n",
      "Fine tuning model for subject 4 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.1609       0.5104            0.5104        1.3715  0.0004  0.1296\n",
      "     32            \u001b[36m1.0000\u001b[0m        1.0841       0.5104            0.5104        1.3267  0.0005  0.1272\n",
      "     33            1.0000        \u001b[32m0.2580\u001b[0m       0.5000            0.5000        1.2849  0.0005  0.1324\n",
      "     34            1.0000        \u001b[32m0.0955\u001b[0m       0.5000            0.5000        1.2513  0.0006  0.1364\n",
      "     35            1.0000        \u001b[32m0.0296\u001b[0m       0.4792            0.4792        1.2328  0.0006  0.1343\n",
      "     36            1.0000        0.0541       0.4479            0.4479        1.2354  0.0007  0.1208\n",
      "     37            1.0000        0.0494       0.4271            0.4271        1.2608  0.0007  0.1272\n",
      "     38            1.0000        0.0865       0.3542            0.3542        1.3024  0.0007  0.1370\n",
      "     39            1.0000        0.0693       0.3229            0.3229        1.3539  0.0007  0.1337\n",
      "     40            1.0000        \u001b[32m0.0161\u001b[0m       0.3333            0.3333        1.4097  0.0007  0.1251\n",
      "     41            1.0000        \u001b[32m0.0107\u001b[0m       0.3646            0.3646        1.4635  0.0007  0.1247\n",
      "     42            1.0000        \u001b[32m0.0076\u001b[0m       0.3646            0.3646        1.5101  0.0007  0.1281\n",
      "     43            1.0000        0.0094       0.3750            0.3750        1.5454  0.0006  0.1314\n",
      "     44            1.0000        \u001b[32m0.0053\u001b[0m       0.3646            0.3646        1.5676  0.0006  0.1279\n",
      "     45            1.0000        \u001b[32m0.0024\u001b[0m       0.3646            0.3646        1.5769  0.0005  0.1286\n",
      "     46            1.0000        0.0096       0.3646            0.3646        1.5736  0.0005  0.1280\n",
      "     47            1.0000        0.0025       0.3542            0.3542        1.5611  0.0004  0.1300\n",
      "     48            1.0000        \u001b[32m0.0010\u001b[0m       0.3438            0.3438        1.5423  0.0004  0.1254\n",
      "     49            1.0000        0.0033       0.3542            0.3542        1.5196  0.0003  0.1279\n",
      "     50            1.0000        0.0032       0.3646            0.3646        1.4954  0.0003  0.1285\n",
      "Fine tuning model for subject 4 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2000        2.5718       0.4792            0.4792        1.3904  0.0004  0.1697\n",
      "     32            0.6000        2.1772       0.4688            0.4688        1.3989  0.0005  0.1622\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.7809       0.4375            0.4375        1.4239  0.0005  0.1636\n",
      "     34            1.0000        \u001b[32m0.3389\u001b[0m       0.4062            0.4062        1.4468  0.0006  0.1586\n",
      "     35            1.0000        \u001b[32m0.0757\u001b[0m       0.3854            0.3854        1.4693  0.0006  0.1659\n",
      "     36            1.0000        \u001b[32m0.0686\u001b[0m       0.3646            0.3646        1.5036  0.0007  0.1668\n",
      "     37            1.0000        \u001b[32m0.0397\u001b[0m       0.3646            0.3646        1.5687  0.0007  0.1630\n",
      "     38            1.0000        \u001b[32m0.0106\u001b[0m       0.3229            0.3229        1.6682  0.0007  0.1644\n",
      "     39            1.0000        0.0136       0.3021            0.3021        1.7895  0.0007  0.1723\n",
      "     40            1.0000        \u001b[32m0.0089\u001b[0m       0.2604            0.2604        1.9134  0.0007  0.1706\n",
      "     41            1.0000        0.0184       0.2708            0.2708        2.0251  0.0007  0.1657\n",
      "     42            1.0000        0.0185       0.2604            0.2604        2.1103  0.0007  0.1658\n",
      "     43            1.0000        0.0144       0.2604            0.2604        2.1652  0.0006  0.1652\n",
      "     44            1.0000        0.0140       0.2604            0.2604        2.1895  0.0006  0.1721\n",
      "     45            1.0000        0.0096       0.2708            0.2708        2.1875  0.0005  0.1657\n",
      "     46            1.0000        0.0110       0.2812            0.2812        2.1638  0.0005  0.1328\n",
      "     47            1.0000        0.0456       0.2708            0.2708        2.1245  0.0004  0.1365\n",
      "     48            1.0000        0.0221       0.2708            0.2708        2.0752  0.0004  0.1259\n",
      "     49            1.0000        \u001b[32m0.0074\u001b[0m       0.2708            0.2708        2.0213  0.0003  0.1246\n",
      "     50            1.0000        0.0091       0.2917            0.2917        1.9669  0.0003  0.1233\n",
      "Fine tuning model for subject 4 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        2.8015       0.5104            0.5104        1.3970  0.0004  0.1257\n",
      "     32            0.6000        1.7438       0.4792            0.4792        1.3974  0.0005  0.1307\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6782\u001b[0m       0.4479            0.4479        1.4031  0.0005  0.1246\n",
      "     34            1.0000        \u001b[32m0.2081\u001b[0m       0.4896            0.4896        1.4074  0.0006  0.1274\n",
      "     35            1.0000        \u001b[32m0.0480\u001b[0m       0.4792            0.4792        1.4214  0.0006  0.1330\n",
      "     36            1.0000        \u001b[32m0.0197\u001b[0m       0.4271            0.4271        1.4475  0.0007  0.1323\n",
      "     37            1.0000        \u001b[32m0.0069\u001b[0m       0.4167            0.4167        1.4854  0.0007  0.1251\n",
      "     38            1.0000        0.0094       0.4271            0.4271        1.5341  0.0007  0.1265\n",
      "     39            1.0000        0.0081       0.3646            0.3646        1.5906  0.0007  0.1423\n",
      "     40            1.0000        \u001b[32m0.0028\u001b[0m       0.3542            0.3542        1.6511  0.0007  0.1562\n",
      "     41            1.0000        0.0089       0.3542            0.3542        1.7109  0.0007  0.1534\n",
      "     42            1.0000        0.0049       0.3333            0.3333        1.7662  0.0007  0.1492\n",
      "     43            1.0000        \u001b[32m0.0028\u001b[0m       0.3229            0.3229        1.8144  0.0006  0.1262\n",
      "     44            1.0000        0.0095       0.3333            0.3333        1.8544  0.0006  0.1303\n",
      "     45            1.0000        0.0086       0.3542            0.3542        1.8862  0.0005  0.1437\n",
      "     46            1.0000        0.0072       0.3542            0.3542        1.9105  0.0005  0.1250\n",
      "     47            1.0000        0.0065       0.3333            0.3333        1.9289  0.0004  0.1261\n",
      "     48            1.0000        0.0050       0.3333            0.3333        1.9427  0.0004  0.1239\n",
      "     49            1.0000        \u001b[32m0.0022\u001b[0m       0.3333            0.3333        1.9534  0.0003  0.1292\n",
      "     50            1.0000        0.0050       0.3333            0.3333        1.9618  0.0003  0.1263\n",
      "Fine tuning model for subject 4 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.8000        0.7165       0.5208            0.5208        1.4036  0.0004  0.1231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5186\u001b[0m       0.4583            0.4583        1.4185  0.0005  0.1318\n",
      "     33            1.0000        \u001b[32m0.1805\u001b[0m       0.4792            0.4792        1.4336  0.0005  0.1294\n",
      "     34            1.0000        \u001b[32m0.0870\u001b[0m       0.4167            0.4167        1.4589  0.0006  0.1270\n",
      "     35            1.0000        \u001b[32m0.0804\u001b[0m       0.4167            0.4167        1.4941  0.0006  0.1270\n",
      "     36            1.0000        \u001b[32m0.0650\u001b[0m       0.4167            0.4167        1.5390  0.0007  0.1274\n",
      "     37            1.0000        \u001b[32m0.0209\u001b[0m       0.4271            0.4271        1.5921  0.0007  0.1349\n",
      "     38            1.0000        0.0234       0.4062            0.4062        1.6467  0.0007  0.1290\n",
      "     39            1.0000        \u001b[32m0.0078\u001b[0m       0.4167            0.4167        1.7003  0.0007  0.1286\n",
      "     40            1.0000        0.0097       0.4167            0.4167        1.7500  0.0007  0.1407\n",
      "     41            1.0000        \u001b[32m0.0077\u001b[0m       0.4167            0.4167        1.7919  0.0007  0.1245\n",
      "     42            1.0000        \u001b[32m0.0053\u001b[0m       0.4167            0.4167        1.8265  0.0007  0.1253\n",
      "     43            1.0000        \u001b[32m0.0049\u001b[0m       0.4062            0.4062        1.8533  0.0006  0.1344\n",
      "     44            1.0000        0.0063       0.3854            0.3854        1.8726  0.0006  0.1623\n",
      "     45            1.0000        \u001b[32m0.0035\u001b[0m       0.3854            0.3854        1.8854  0.0005  0.1301\n",
      "     46            1.0000        \u001b[32m0.0030\u001b[0m       0.3854            0.3854        1.8923  0.0005  0.1310\n",
      "     47            1.0000        0.0106       0.3854            0.3854        1.8949  0.0004  0.1275\n",
      "     48            1.0000        0.0041       0.3854            0.3854        1.8940  0.0004  0.1298\n",
      "     49            1.0000        \u001b[32m0.0022\u001b[0m       0.3854            0.3854        1.8902  0.0003  0.1307\n",
      "     50            1.0000        0.0022       0.3854            0.3854        1.8842  0.0003  0.1239\n",
      "Fine tuning model for subject 4 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.8915       0.5104            0.5104        1.3929  0.0004  0.1256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        1.2798       0.5104            0.5104        1.3875  0.0005  0.1335\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.8875       0.5000            0.5000        1.4033  0.0005  0.1226\n",
      "     34            1.0000        \u001b[32m0.2076\u001b[0m       0.4583            0.4583        1.4568  0.0006  0.1217\n",
      "     35            1.0000        \u001b[32m0.0697\u001b[0m       0.4375            0.4375        1.5292  0.0006  0.1320\n",
      "     36            1.0000        \u001b[32m0.0461\u001b[0m       0.4479            0.4479        1.6145  0.0007  0.1282\n",
      "     37            1.0000        \u001b[32m0.0214\u001b[0m       0.4375            0.4375        1.7054  0.0007  0.1303\n",
      "     38            1.0000        0.0249       0.4062            0.4062        1.8052  0.0007  0.1356\n",
      "     39            1.0000        \u001b[32m0.0085\u001b[0m       0.3646            0.3646        1.9091  0.0007  0.1262\n",
      "     40            1.0000        \u001b[32m0.0072\u001b[0m       0.3646            0.3646        2.0089  0.0007  0.1239\n",
      "     41            1.0000        0.0128       0.3646            0.3646        2.0962  0.0007  0.1272\n",
      "     42            1.0000        0.0139       0.3750            0.3750        2.1616  0.0007  0.1270\n",
      "     43            1.0000        \u001b[32m0.0020\u001b[0m       0.3646            0.3646        2.2009  0.0006  0.1245\n",
      "     44            1.0000        0.0118       0.3438            0.3438        2.2148  0.0006  0.1257\n",
      "     45            1.0000        0.0051       0.3438            0.3438        2.2063  0.0005  0.1276\n",
      "     46            1.0000        0.0043       0.3646            0.3646        2.1799  0.0005  0.1223\n",
      "     47            1.0000        0.0036       0.3646            0.3646        2.1409  0.0004  0.1339\n",
      "     48            1.0000        0.0039       0.3542            0.3542        2.0943  0.0004  0.1342\n",
      "     49            1.0000        0.0071       0.3438            0.3438        2.0446  0.0003  0.1237\n",
      "     50            1.0000        0.0030       0.3542            0.3542        1.9951  0.0003  0.1268\n",
      "Fine tuning model for subject 4 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.3917       0.4583            0.4583        1.3763  0.0004  0.1234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        1.5268       0.3854            0.3854        1.3955  0.0005  0.1292\n",
      "     33            1.0000        \u001b[32m0.4431\u001b[0m       0.4375            0.4375        1.3908  0.0005  0.1308\n",
      "     34            1.0000        \u001b[32m0.1666\u001b[0m       0.4271            0.4271        1.3527  0.0006  0.1253\n",
      "     35            1.0000        \u001b[32m0.0979\u001b[0m       0.4167            0.4167        1.3558  0.0006  0.1245\n",
      "     36            1.0000        \u001b[32m0.0481\u001b[0m       0.3646            0.3646        1.3894  0.0007  0.1287\n",
      "     37            1.0000        \u001b[32m0.0237\u001b[0m       0.3854            0.3854        1.4207  0.0007  0.1332\n",
      "     38            1.0000        \u001b[32m0.0223\u001b[0m       0.3750            0.3750        1.4345  0.0007  0.1290\n",
      "     39            1.0000        \u001b[32m0.0209\u001b[0m       0.3542            0.3542        1.4330  0.0007  0.1258\n",
      "     40            1.0000        0.0256       0.3750            0.3750        1.4244  0.0007  0.1309\n",
      "     41            1.0000        \u001b[32m0.0083\u001b[0m       0.4062            0.4062        1.4161  0.0007  0.1272\n",
      "     42            1.0000        0.0125       0.4271            0.4271        1.4122  0.0007  0.1314\n",
      "     43            1.0000        0.0113       0.4271            0.4271        1.4131  0.0006  0.1272\n",
      "     44            1.0000        0.0097       0.4167            0.4167        1.4179  0.0006  0.1270\n",
      "     45            1.0000        0.0150       0.3854            0.3854        1.4252  0.0005  0.1172\n",
      "     46            1.0000        0.0192       0.3958            0.3958        1.4338  0.0005  0.1194\n",
      "     47            1.0000        \u001b[32m0.0071\u001b[0m       0.3854            0.3854        1.4424  0.0004  0.1246\n",
      "     48            1.0000        0.0098       0.3854            0.3854        1.4510  0.0004  0.1224\n",
      "     49            1.0000        \u001b[32m0.0070\u001b[0m       0.3750            0.3750        1.4591  0.0003  0.1260\n",
      "     50            1.0000        \u001b[32m0.0032\u001b[0m       0.3646            0.3646        1.4666  0.0003  0.1255\n",
      "Fine tuning model for subject 4 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2000        1.9081       0.5104            0.5104        1.4277  0.0004  0.1275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        1.5457       0.4583            0.4583        1.4721  0.0005  0.1337\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6735\u001b[0m       0.4167            0.4167        1.4841  0.0005  0.1231\n",
      "     34            1.0000        \u001b[32m0.1321\u001b[0m       0.3750            0.3750        1.5210  0.0006  0.1264\n",
      "     35            1.0000        \u001b[32m0.0840\u001b[0m       0.3646            0.3646        1.5879  0.0006  0.1272\n",
      "     36            1.0000        \u001b[32m0.0279\u001b[0m       0.3438            0.3438        1.6740  0.0007  0.1290\n",
      "     37            1.0000        \u001b[32m0.0095\u001b[0m       0.3125            0.3125        1.7645  0.0007  0.1278\n",
      "     38            1.0000        0.0832       0.3229            0.3229        1.8434  0.0007  0.1289\n",
      "     39            1.0000        0.0552       0.3021            0.3021        1.9013  0.0007  0.1584\n",
      "     40            1.0000        0.0137       0.3125            0.3125        1.9445  0.0007  0.1638\n",
      "     41            1.0000        \u001b[32m0.0075\u001b[0m       0.3021            0.3021        1.9735  0.0007  0.1562\n",
      "     42            1.0000        \u001b[32m0.0048\u001b[0m       0.2917            0.2917        1.9889  0.0007  0.1579\n",
      "     43            1.0000        \u001b[32m0.0025\u001b[0m       0.2708            0.2708        1.9917  0.0006  0.1636\n",
      "     44            1.0000        0.0030       0.2708            0.2708        1.9838  0.0006  0.1657\n",
      "     45            1.0000        \u001b[32m0.0013\u001b[0m       0.2396            0.2396        1.9678  0.0005  0.1667\n",
      "     46            1.0000        0.0062       0.2604            0.2604        1.9472  0.0005  0.1697\n",
      "     47            1.0000        0.0030       0.2812            0.2812        1.9249  0.0004  0.1631\n",
      "     48            1.0000        0.0019       0.2812            0.2812        1.9038  0.0004  0.1700\n",
      "     49            1.0000        0.0040       0.2917            0.2917        1.8862  0.0003  0.1661\n",
      "     50            1.0000        0.0078       0.2812            0.2812        1.8737  0.0003  0.1658\n",
      "Fine tuning model for subject 4 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        0.7962       0.4792            0.4792        1.4183  0.0004  0.1632\n",
      "     32            0.8000        \u001b[32m0.4449\u001b[0m       0.5104            0.5104        1.4689  0.0005  0.1652\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.4606       0.5208            0.5208        1.5260  0.0005  0.1591\n",
      "     34            1.0000        \u001b[32m0.2924\u001b[0m       0.5000            0.5000        1.5975  0.0006  0.1241\n",
      "     35            1.0000        \u001b[32m0.0913\u001b[0m       0.4688            0.4688        1.6656  0.0006  0.1404\n",
      "     36            1.0000        0.1213       0.4375            0.4375        1.7315  0.0007  0.1172\n",
      "     37            1.0000        \u001b[32m0.0492\u001b[0m       0.4375            0.4375        1.7901  0.0007  0.1241\n",
      "     38            1.0000        \u001b[32m0.0114\u001b[0m       0.4375            0.4375        1.8359  0.0007  0.1343\n",
      "     39            1.0000        0.0563       0.4271            0.4271        1.8758  0.0007  0.1223\n",
      "     40            1.0000        \u001b[32m0.0090\u001b[0m       0.4271            0.4271        1.9033  0.0007  0.1248\n",
      "     41            1.0000        \u001b[32m0.0043\u001b[0m       0.4167            0.4167        1.9202  0.0007  0.1278\n",
      "     42            1.0000        0.0062       0.4062            0.4062        1.9290  0.0007  0.1283\n",
      "     43            1.0000        \u001b[32m0.0041\u001b[0m       0.4062            0.4062        1.9307  0.0006  0.1252\n",
      "     44            1.0000        0.0067       0.4167            0.4167        1.9270  0.0006  0.1274\n",
      "     45            1.0000        \u001b[32m0.0015\u001b[0m       0.4167            0.4167        1.9186  0.0005  0.1227\n",
      "     46            1.0000        0.0023       0.4167            0.4167        1.9068  0.0005  0.1261\n",
      "     47            1.0000        0.0047       0.4167            0.4167        1.8926  0.0004  0.1266\n",
      "     48            1.0000        0.0029       0.4062            0.4062        1.8768  0.0004  0.1263\n",
      "     49            1.0000        0.0019       0.3854            0.3854        1.8601  0.0003  0.1323\n",
      "     50            1.0000        0.0026       0.3854            0.3854        1.8431  0.0003  0.1320\n",
      "Fine tuning model for subject 4 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        2.3471       0.4583            0.4583        1.3910  0.0004  0.1098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5000        1.9521       0.4896            0.4896        1.3848  0.0005  0.1067\n",
      "     33            0.8000        1.3633       0.5208            0.5208        1.3808  0.0005  0.1056\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6423\u001b[0m       0.4896            0.4896        1.3798  0.0006  0.1065\n",
      "     35            1.0000        \u001b[32m0.3255\u001b[0m       0.4896            0.4896        1.3870  0.0006  0.1082\n",
      "     36            1.0000        \u001b[32m0.1415\u001b[0m       0.4583            0.4583        1.4152  0.0007  0.1040\n",
      "     37            1.0000        \u001b[32m0.0952\u001b[0m       0.4688            0.4688        1.4713  0.0007  0.1081\n",
      "     38            1.0000        \u001b[32m0.0429\u001b[0m       0.4479            0.4479        1.5509  0.0007  0.1080\n",
      "     39            1.0000        \u001b[32m0.0348\u001b[0m       0.3958            0.3958        1.6411  0.0007  0.1088\n",
      "     40            1.0000        0.0387       0.3958            0.3958        1.7280  0.0007  0.1041\n",
      "     41            1.0000        \u001b[32m0.0302\u001b[0m       0.3542            0.3542        1.8008  0.0007  0.1081\n",
      "     42            1.0000        0.1047       0.3646            0.3646        1.8595  0.0007  0.1101\n",
      "     43            1.0000        \u001b[32m0.0233\u001b[0m       0.3646            0.3646        1.8918  0.0006  0.1092\n",
      "     44            1.0000        0.0285       0.3646            0.3646        1.8993  0.0006  0.1040\n",
      "     45            1.0000        \u001b[32m0.0187\u001b[0m       0.3646            0.3646        1.8846  0.0005  0.1058\n",
      "     46            1.0000        \u001b[32m0.0174\u001b[0m       0.3854            0.3854        1.8529  0.0005  0.1048\n",
      "     47            1.0000        0.0229       0.3854            0.3854        1.8087  0.0004  0.1096\n",
      "     48            1.0000        0.0288       0.3854            0.3854        1.7577  0.0004  0.1096\n",
      "     49            1.0000        0.0181       0.3854            0.3854        1.7035  0.0003  0.1005\n",
      "     50            1.0000        \u001b[32m0.0159\u001b[0m       0.3750            0.3750        1.6499  0.0003  0.1128\n",
      "Fine tuning model for subject 4 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.8408       0.5104            0.5104        1.3777  0.0004  0.1054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.7497       0.5104            0.5104        1.3341  0.0005  0.1107\n",
      "     33            0.8000        1.1979       0.5312            0.5312        1.2737  0.0005  0.1178\n",
      "     34            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6217\u001b[0m       0.5104            0.5104        1.2310  0.0006  0.1119\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2889\u001b[0m       0.5208            0.5208        1.2091  0.0006  0.1087\n",
      "     36            1.0000        \u001b[32m0.1914\u001b[0m       0.5104            0.5104        1.2137  0.0007  0.1054\n",
      "     37            1.0000        0.2304       0.5000            0.5000        1.2355  0.0007  0.1043\n",
      "     38            1.0000        \u001b[32m0.0686\u001b[0m       0.5208            0.5208        1.2656  0.0007  0.1056\n",
      "     39            1.0000        \u001b[32m0.0422\u001b[0m       0.5000            0.5000        1.3007  0.0007  0.1069\n",
      "     40            1.0000        0.1150       0.5104            0.5104        1.3435  0.0007  0.1057\n",
      "     41            1.0000        0.0634       0.4792            0.4792        1.3878  0.0007  0.1049\n",
      "     42            1.0000        0.0543       0.4688            0.4688        1.4319  0.0007  0.1092\n",
      "     43            1.0000        \u001b[32m0.0265\u001b[0m       0.4688            0.4688        1.4684  0.0006  0.1084\n",
      "     44            1.0000        \u001b[32m0.0223\u001b[0m       0.4688            0.4688        1.4939  0.0006  0.1111\n",
      "     45            1.0000        \u001b[32m0.0193\u001b[0m       0.4479            0.4479        1.5070  0.0005  0.1051\n",
      "     46            1.0000        \u001b[32m0.0171\u001b[0m       0.4375            0.4375        1.5104  0.0005  0.1678\n",
      "     47            1.0000        0.0401       0.4583            0.4583        1.5067  0.0004  0.1106\n",
      "     48            1.0000        0.0211       0.4479            0.4479        1.4996  0.0004  0.1065\n",
      "     49            1.0000        \u001b[32m0.0145\u001b[0m       0.4375            0.4375        1.4917  0.0003  0.1122\n",
      "     50            1.0000        0.0204       0.4479            0.4479        1.4841  0.0003  0.1110\n",
      "Fine tuning model for subject 4 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        2.1757       0.5104            0.5104        1.3980  0.0004  0.1049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        1.5786       0.4896            0.4896        1.3923  0.0005  0.1136\n",
      "     33            \u001b[36m1.0000\u001b[0m        1.2441       0.4792            0.4792        1.3666  0.0005  0.1069\n",
      "     34            1.0000        \u001b[32m0.5350\u001b[0m       0.4167            0.4167        1.4015  0.0006  0.1067\n",
      "     35            0.9000        \u001b[32m0.2014\u001b[0m       0.3438            0.3438        1.5797  0.0006  0.1099\n",
      "     36            0.9000        \u001b[32m0.0885\u001b[0m       0.3229            0.3229        1.8320  0.0007  0.1086\n",
      "     37            0.9000        \u001b[32m0.0589\u001b[0m       0.3229            0.3229        2.0702  0.0007  0.1172\n",
      "     38            0.9000        \u001b[32m0.0579\u001b[0m       0.2917            0.2917        2.2596  0.0007  0.1039\n",
      "     39            0.8000        \u001b[32m0.0394\u001b[0m       0.3021            0.3021        2.4021  0.0007  0.1096\n",
      "     40            0.8000        0.0630       0.2917            0.2917        2.4853  0.0007  0.1082\n",
      "     41            0.8000        \u001b[32m0.0215\u001b[0m       0.2917            0.2917        2.5334  0.0007  0.1124\n",
      "     42            0.8000        \u001b[32m0.0194\u001b[0m       0.3021            0.3021        2.5490  0.0007  0.1075\n",
      "     43            0.9000        \u001b[32m0.0167\u001b[0m       0.3125            0.3125        2.5351  0.0006  0.1172\n",
      "     44            0.9000        \u001b[32m0.0160\u001b[0m       0.3125            0.3125        2.4958  0.0006  0.1102\n",
      "     45            0.9000        \u001b[32m0.0118\u001b[0m       0.3125            0.3125        2.4386  0.0005  0.1187\n",
      "     46            1.0000        \u001b[32m0.0063\u001b[0m       0.3125            0.3125        2.3684  0.0005  0.1086\n",
      "     47            1.0000        0.0113       0.3125            0.3125        2.2897  0.0004  0.1095\n",
      "     48            1.0000        0.0211       0.3229            0.3229        2.2071  0.0004  0.1031\n",
      "     49            1.0000        0.0147       0.3229            0.3229        2.1248  0.0003  0.1085\n",
      "     50            1.0000        \u001b[32m0.0054\u001b[0m       0.3229            0.3229        2.0477  0.0003  0.1127\n",
      "Fine tuning model for subject 4 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        2.3284       0.4896            0.4896        1.4030  0.0004  0.1123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.4218       0.5000            0.5000        1.4213  0.0005  0.1093\n",
      "     33            0.7000        1.3352       0.5000            0.5000        1.4472  0.0005  0.1062\n",
      "     34            \u001b[36m1.0000\u001b[0m        0.7985       0.5000            0.5000        1.4658  0.0006  0.1078\n",
      "     35            1.0000        \u001b[32m0.2694\u001b[0m       0.4688            0.4688        1.4894  0.0006  0.1079\n",
      "     36            1.0000        \u001b[32m0.1881\u001b[0m       0.4375            0.4375        1.5417  0.0007  0.1060\n",
      "     37            1.0000        \u001b[32m0.1532\u001b[0m       0.3958            0.3958        1.6233  0.0007  0.1081\n",
      "     38            1.0000        \u001b[32m0.0793\u001b[0m       0.3958            0.3958        1.7131  0.0007  0.1127\n",
      "     39            1.0000        \u001b[32m0.0443\u001b[0m       0.3958            0.3958        1.8025  0.0007  0.1042\n",
      "     40            1.0000        0.0470       0.3854            0.3854        1.8808  0.0007  0.1065\n",
      "     41            1.0000        \u001b[32m0.0238\u001b[0m       0.3958            0.3958        1.9401  0.0007  0.1107\n",
      "     42            0.9000        0.0291       0.4062            0.4062        1.9777  0.0007  0.1107\n",
      "     43            0.9000        0.0374       0.4062            0.4062        1.9913  0.0006  0.1065\n",
      "     44            0.9000        0.0309       0.4062            0.4062        1.9845  0.0006  0.1092\n",
      "     45            1.0000        \u001b[32m0.0146\u001b[0m       0.3958            0.3958        1.9626  0.0005  0.1053\n",
      "     46            1.0000        \u001b[32m0.0139\u001b[0m       0.3854            0.3854        1.9296  0.0005  0.1069\n",
      "     47            1.0000        0.0331       0.3854            0.3854        1.8893  0.0004  0.1067\n",
      "     48            1.0000        \u001b[32m0.0127\u001b[0m       0.3646            0.3646        1.8450  0.0004  0.1077\n",
      "     49            1.0000        \u001b[32m0.0092\u001b[0m       0.3542            0.3542        1.7997  0.0003  0.1132\n",
      "     50            1.0000        0.0191       0.3438            0.3438        1.7554  0.0003  0.1066\n",
      "Fine tuning model for subject 4 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        1.6326       0.4896            0.4896        1.4198  0.0004  0.1098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        1.5109       0.4583            0.4583        1.4495  0.0005  0.1094\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.7745       0.4375            0.4375        1.4530  0.0005  0.1075\n",
      "     34            1.0000        \u001b[32m0.4799\u001b[0m       0.4479            0.4479        1.4368  0.0006  0.1111\n",
      "     35            1.0000        \u001b[32m0.3070\u001b[0m       0.4479            0.4479        1.4108  0.0006  0.1110\n",
      "     36            1.0000        \u001b[32m0.0820\u001b[0m       0.3854            0.3854        1.4030  0.0007  0.1121\n",
      "     37            1.0000        \u001b[32m0.0806\u001b[0m       0.3854            0.3854        1.4163  0.0007  0.1058\n",
      "     38            1.0000        \u001b[32m0.0465\u001b[0m       0.3854            0.3854        1.4448  0.0007  0.1412\n",
      "     39            1.0000        0.0573       0.3646            0.3646        1.4878  0.0007  0.1330\n",
      "     40            1.0000        \u001b[32m0.0438\u001b[0m       0.3854            0.3854        1.5383  0.0007  0.1329\n",
      "     41            1.0000        0.0526       0.3750            0.3750        1.5913  0.0007  0.1392\n",
      "     42            1.0000        \u001b[32m0.0286\u001b[0m       0.3646            0.3646        1.6401  0.0007  0.1318\n",
      "     43            1.0000        0.0521       0.3750            0.3750        1.6803  0.0006  0.1508\n",
      "     44            1.0000        0.0364       0.3750            0.3750        1.7128  0.0006  0.1438\n",
      "     45            1.0000        \u001b[32m0.0258\u001b[0m       0.3646            0.3646        1.7347  0.0005  0.1304\n",
      "     46            1.0000        \u001b[32m0.0180\u001b[0m       0.3438            0.3438        1.7455  0.0005  0.1376\n",
      "     47            1.0000        0.0251       0.3333            0.3333        1.7470  0.0004  0.1318\n",
      "     48            1.0000        0.0290       0.3646            0.3646        1.7414  0.0004  0.1336\n",
      "     49            1.0000        0.0268       0.3750            0.3750        1.7317  0.0003  0.1387\n",
      "     50            1.0000        0.0208       0.3750            0.3750        1.7202  0.0003  0.1429\n",
      "Fine tuning model for subject 4 with 10 = 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.5393       0.4792            0.4792        1.3932  0.0004  0.1519\n",
      "     32            0.7000        1.1723       0.4583            0.4583        1.3924  0.0005  0.1334\n",
      "     33            0.8000        0.7386       0.4375            0.4375        1.3883  0.0005  0.1343\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6642\u001b[0m       0.4375            0.4375        1.3903  0.0006  0.1569\n",
      "     35            1.0000        \u001b[32m0.5445\u001b[0m       0.4271            0.4271        1.4180  0.0006  0.1207\n",
      "     36            1.0000        \u001b[32m0.2012\u001b[0m       0.3854            0.3854        1.4485  0.0007  0.1162\n",
      "     37            1.0000        \u001b[32m0.1000\u001b[0m       0.3646            0.3646        1.4793  0.0007  0.1171\n",
      "     38            1.0000        \u001b[32m0.0891\u001b[0m       0.3646            0.3646        1.5081  0.0007  0.1113\n",
      "     39            1.0000        \u001b[32m0.0649\u001b[0m       0.3646            0.3646        1.5440  0.0007  0.1089\n",
      "     40            1.0000        \u001b[32m0.0645\u001b[0m       0.3542            0.3542        1.5879  0.0007  0.1010\n",
      "     41            1.0000        \u001b[32m0.0336\u001b[0m       0.3542            0.3542        1.6341  0.0007  0.1104\n",
      "     42            1.0000        \u001b[32m0.0310\u001b[0m       0.3646            0.3646        1.6772  0.0007  0.1083\n",
      "     43            1.0000        \u001b[32m0.0235\u001b[0m       0.3542            0.3542        1.7153  0.0006  0.1072\n",
      "     44            1.0000        0.0279       0.3438            0.3438        1.7469  0.0006  0.1173\n",
      "     45            1.0000        \u001b[32m0.0208\u001b[0m       0.3542            0.3542        1.7717  0.0005  0.1085\n",
      "     46            1.0000        \u001b[32m0.0147\u001b[0m       0.3542            0.3542        1.7909  0.0005  0.1063\n",
      "     47            1.0000        0.0336       0.3542            0.3542        1.8070  0.0004  0.1103\n",
      "     48            1.0000        0.0243       0.3542            0.3542        1.8211  0.0004  0.1075\n",
      "     49            1.0000        \u001b[32m0.0122\u001b[0m       0.3542            0.3542        1.8343  0.0003  0.1093\n",
      "     50            1.0000        0.0196       0.3542            0.3542        1.8474  0.0003  0.1069\n",
      "Fine tuning model for subject 4 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.1223       0.4688            0.4688        1.4047  0.0004  0.1089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        1.2861       0.4792            0.4792        1.4194  0.0005  0.1093\n",
      "     33            0.8000        1.0660       0.4792            0.4792        1.4357  0.0005  0.1180\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5229\u001b[0m       0.4375            0.4375        1.4618  0.0006  0.1102\n",
      "     35            1.0000        \u001b[32m0.3240\u001b[0m       0.4479            0.4479        1.5031  0.0006  0.1066\n",
      "     36            1.0000        \u001b[32m0.1083\u001b[0m       0.4271            0.4271        1.5468  0.0007  0.1015\n",
      "     37            1.0000        0.1342       0.4062            0.4062        1.6112  0.0007  0.1100\n",
      "     38            1.0000        \u001b[32m0.0468\u001b[0m       0.4375            0.4375        1.6889  0.0007  0.1039\n",
      "     39            1.0000        0.0686       0.4375            0.4375        1.7714  0.0007  0.1045\n",
      "     40            1.0000        \u001b[32m0.0321\u001b[0m       0.4688            0.4688        1.8560  0.0007  0.1016\n",
      "     41            1.0000        0.0349       0.4688            0.4688        1.9335  0.0007  0.1076\n",
      "     42            1.0000        \u001b[32m0.0250\u001b[0m       0.4375            0.4375        1.9965  0.0007  0.1082\n",
      "     43            1.0000        0.0256       0.4271            0.4271        2.0418  0.0006  0.1065\n",
      "     44            1.0000        \u001b[32m0.0141\u001b[0m       0.4167            0.4167        2.0699  0.0006  0.1058\n",
      "     45            1.0000        \u001b[32m0.0088\u001b[0m       0.4271            0.4271        2.0795  0.0005  0.1100\n",
      "     46            1.0000        0.0208       0.4375            0.4375        2.0720  0.0005  0.1067\n",
      "     47            1.0000        0.0191       0.4479            0.4479        2.0500  0.0004  0.1090\n",
      "     48            1.0000        0.0142       0.4479            0.4479        2.0183  0.0004  0.1010\n",
      "     49            1.0000        0.0109       0.4479            0.4479        1.9795  0.0003  0.1124\n",
      "     50            1.0000        0.0119       0.4583            0.4583        1.9369  0.0003  0.1010\n",
      "Fine tuning model for subject 4 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        0.8215       0.5000            0.5000        1.4132  0.0004  0.1066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        0.7068       0.4688            0.4688        1.4464  0.0005  0.1115\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4737\u001b[0m       0.4688            0.4688        1.4835  0.0005  0.0999\n",
      "     34            1.0000        \u001b[32m0.3053\u001b[0m       0.4479            0.4479        1.5145  0.0006  0.1106\n",
      "     35            1.0000        \u001b[32m0.1546\u001b[0m       0.4375            0.4375        1.5325  0.0006  0.1097\n",
      "     36            1.0000        \u001b[32m0.0902\u001b[0m       0.4271            0.4271        1.5436  0.0007  0.1120\n",
      "     37            1.0000        \u001b[32m0.0692\u001b[0m       0.4062            0.4062        1.5535  0.0007  0.1088\n",
      "     38            1.0000        \u001b[32m0.0321\u001b[0m       0.4062            0.4062        1.5616  0.0007  0.1078\n",
      "     39            1.0000        \u001b[32m0.0235\u001b[0m       0.4062            0.4062        1.5688  0.0007  0.1046\n",
      "     40            1.0000        0.0547       0.3646            0.3646        1.5822  0.0007  0.1083\n",
      "     41            1.0000        0.0252       0.3438            0.3438        1.5970  0.0007  0.1085\n",
      "     42            1.0000        0.0255       0.3542            0.3542        1.6087  0.0007  0.1120\n",
      "     43            1.0000        \u001b[32m0.0147\u001b[0m       0.3333            0.3333        1.6177  0.0006  0.1000\n",
      "     44            1.0000        \u001b[32m0.0143\u001b[0m       0.3438            0.3438        1.6246  0.0006  0.1058\n",
      "     45            1.0000        0.0377       0.3333            0.3333        1.6281  0.0005  0.1092\n",
      "     46            1.0000        \u001b[32m0.0131\u001b[0m       0.3333            0.3333        1.6288  0.0005  0.1285\n",
      "     47            1.0000        0.0140       0.3438            0.3438        1.6273  0.0004  0.1124\n",
      "     48            1.0000        0.0198       0.3646            0.3646        1.6253  0.0004  0.1059\n",
      "     49            1.0000        0.0305       0.3646            0.3646        1.6243  0.0003  0.1080\n",
      "     50            1.0000        0.0131       0.3646            0.3646        1.6226  0.0003  0.1121\n",
      "Fine tuning model for subject 4 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.5359       0.5104            0.5104        1.3969  0.0004  0.1010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.4791       0.5000            0.5000        1.3938  0.0005  0.1105\n",
      "     33            \u001b[36m0.9000\u001b[0m        1.1421       0.4792            0.4792        1.3949  0.0005  0.1036\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5292\u001b[0m       0.4479            0.4479        1.3987  0.0006  0.1100\n",
      "     35            1.0000        \u001b[32m0.2543\u001b[0m       0.4583            0.4583        1.3912  0.0006  0.1096\n",
      "     36            1.0000        \u001b[32m0.1765\u001b[0m       0.4479            0.4479        1.3747  0.0007  0.1083\n",
      "     37            1.0000        \u001b[32m0.0859\u001b[0m       0.4375            0.4375        1.3580  0.0007  0.1091\n",
      "     38            1.0000        0.1004       0.4479            0.4479        1.3465  0.0007  0.1024\n",
      "     39            1.0000        \u001b[32m0.0609\u001b[0m       0.4792            0.4792        1.3437  0.0007  0.1105\n",
      "     40            1.0000        \u001b[32m0.0301\u001b[0m       0.4688            0.4688        1.3495  0.0007  0.1191\n",
      "     41            1.0000        \u001b[32m0.0253\u001b[0m       0.4792            0.4792        1.3615  0.0007  0.1074\n",
      "     42            1.0000        \u001b[32m0.0173\u001b[0m       0.4792            0.4792        1.3749  0.0007  0.1084\n",
      "     43            1.0000        0.0561       0.5000            0.5000        1.3898  0.0006  0.1075\n",
      "     44            1.0000        0.0205       0.5104            0.5104        1.4012  0.0006  0.1139\n",
      "     45            1.0000        0.0187       0.5104            0.5104        1.4082  0.0005  0.1062\n",
      "     46            1.0000        \u001b[32m0.0158\u001b[0m       0.4896            0.4896        1.4116  0.0005  0.1085\n",
      "     47            1.0000        \u001b[32m0.0143\u001b[0m       0.5104            0.5104        1.4120  0.0004  0.1207\n",
      "     48            1.0000        0.0223       0.4896            0.4896        1.4102  0.0004  0.1093\n",
      "     49            1.0000        \u001b[32m0.0113\u001b[0m       0.4792            0.4792        1.4066  0.0003  0.1073\n",
      "     50            1.0000        0.0131       0.4792            0.4792        1.4017  0.0003  0.1130\n",
      "Fine tuning model for subject 4 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.8035       0.5104            0.5104        1.3632  0.0004  0.1066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.9000\u001b[0m        1.3577       0.5000            0.5000        1.3208  0.0005  0.1076\n",
      "     33            0.9000        1.1843       0.4583            0.4583        1.2691  0.0005  0.1107\n",
      "     34            0.9000        \u001b[32m0.5556\u001b[0m       0.4792            0.4792        1.2329  0.0006  0.1047\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2529\u001b[0m       0.4479            0.4479        1.2396  0.0006  0.1036\n",
      "     36            1.0000        \u001b[32m0.1231\u001b[0m       0.4792            0.4792        1.3035  0.0007  0.1053\n",
      "     37            1.0000        \u001b[32m0.0924\u001b[0m       0.4271            0.4271        1.4137  0.0007  0.1173\n",
      "     38            0.9000        \u001b[32m0.0798\u001b[0m       0.3958            0.3958        1.5459  0.0007  0.1105\n",
      "     39            0.9000        \u001b[32m0.0516\u001b[0m       0.3542            0.3542        1.6761  0.0007  0.1067\n",
      "     40            0.9000        0.0619       0.3229            0.3229        1.7872  0.0007  0.1045\n",
      "     41            0.9000        0.0630       0.3021            0.3021        1.8717  0.0007  0.1091\n",
      "     42            0.9000        0.0758       0.3021            0.3021        1.9228  0.0007  0.1125\n",
      "     43            1.0000        \u001b[32m0.0398\u001b[0m       0.3125            0.3125        1.9484  0.0006  0.1078\n",
      "     44            1.0000        \u001b[32m0.0290\u001b[0m       0.3229            0.3229        1.9534  0.0006  0.1096\n",
      "     45            1.0000        0.0376       0.3229            0.3229        1.9391  0.0005  0.1082\n",
      "     46            1.0000        \u001b[32m0.0183\u001b[0m       0.3229            0.3229        1.9138  0.0005  0.1003\n",
      "     47            1.0000        0.0196       0.3333            0.3333        1.8817  0.0004  0.0991\n",
      "     48            1.0000        0.0282       0.3333            0.3333        1.8471  0.0004  0.1074\n",
      "     49            1.0000        \u001b[32m0.0135\u001b[0m       0.3542            0.3542        1.8114  0.0003  0.1100\n",
      "     50            1.0000        0.0156       0.3542            0.3542        1.7762  0.0003  0.1041\n",
      "Fine tuning model for subject 4 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.7184       0.4792            0.4792        1.3838  0.0004  0.1172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6667        1.4409       0.4688            0.4688        1.3579  0.0005  0.1120\n",
      "     33            \u001b[36m0.9333\u001b[0m        0.9812       0.4688            0.4688        1.3364  0.0005  0.1058\n",
      "     34            0.9333        \u001b[32m0.5675\u001b[0m       0.4271            0.4271        1.3287  0.0006  0.1082\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4624\u001b[0m       0.4479            0.4479        1.3353  0.0006  0.1046\n",
      "     36            1.0000        \u001b[32m0.2676\u001b[0m       0.4271            0.4271        1.3519  0.0007  0.1042\n",
      "     37            1.0000        \u001b[32m0.2490\u001b[0m       0.4167            0.4167        1.3838  0.0007  0.1102\n",
      "     38            1.0000        \u001b[32m0.2134\u001b[0m       0.4062            0.4062        1.4175  0.0007  0.1137\n",
      "     39            1.0000        \u001b[32m0.1963\u001b[0m       0.3958            0.3958        1.4560  0.0007  0.1071\n",
      "     40            1.0000        \u001b[32m0.0865\u001b[0m       0.4062            0.4062        1.4947  0.0007  0.1171\n",
      "     41            1.0000        0.1189       0.4062            0.4062        1.5385  0.0007  0.1358\n",
      "     42            1.0000        \u001b[32m0.0678\u001b[0m       0.3958            0.3958        1.5823  0.0007  0.1232\n",
      "     43            1.0000        \u001b[32m0.0411\u001b[0m       0.4062            0.4062        1.6171  0.0006  0.1420\n",
      "     44            1.0000        0.0577       0.4062            0.4062        1.6460  0.0006  0.1280\n",
      "     45            1.0000        \u001b[32m0.0231\u001b[0m       0.4062            0.4062        1.6647  0.0005  0.1369\n",
      "     46            1.0000        \u001b[32m0.0212\u001b[0m       0.3958            0.3958        1.6754  0.0005  0.1436\n",
      "     47            1.0000        0.0345       0.3958            0.3958        1.6796  0.0004  0.1658\n",
      "     48            1.0000        0.0386       0.3958            0.3958        1.6764  0.0004  0.1497\n",
      "     49            1.0000        0.0246       0.3958            0.3958        1.6700  0.0003  0.1243\n",
      "     50            1.0000        0.0230       0.3854            0.3854        1.6609  0.0003  0.1327\n",
      "Fine tuning model for subject 4 with 15 = 15 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.6568       0.5000            0.5000        1.4131  0.0004  0.1306\n",
      "     32            0.5333        1.3948       0.4792            0.4792        1.4516  0.0005  0.1264\n",
      "     33            0.6000        1.0471       0.4479            0.4479        1.5002  0.0005  0.1332\n",
      "     34            \u001b[36m0.9333\u001b[0m        0.7088       0.4062            0.4062        1.5526  0.0006  0.1388\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4906\u001b[0m       0.3958            0.3958        1.6053  0.0006  0.1351\n",
      "     36            1.0000        \u001b[32m0.2944\u001b[0m       0.3854            0.3854        1.6568  0.0007  0.1658\n",
      "     37            1.0000        \u001b[32m0.2670\u001b[0m       0.3750            0.3750        1.7153  0.0007  0.1336\n",
      "     38            1.0000        \u001b[32m0.2016\u001b[0m       0.3438            0.3438        1.7719  0.0007  0.1422\n",
      "     39            1.0000        \u001b[32m0.1272\u001b[0m       0.3229            0.3229        1.8218  0.0007  0.1582\n",
      "     40            1.0000        \u001b[32m0.1155\u001b[0m       0.2917            0.2917        1.8687  0.0007  0.1066\n",
      "     41            1.0000        \u001b[32m0.0914\u001b[0m       0.3021            0.3021        1.9083  0.0007  0.1055\n",
      "     42            1.0000        \u001b[32m0.0454\u001b[0m       0.3021            0.3021        1.9398  0.0007  0.1076\n",
      "     43            1.0000        0.0739       0.3125            0.3125        1.9587  0.0006  0.1091\n",
      "     44            1.0000        0.0597       0.3229            0.3229        1.9686  0.0006  0.1050\n",
      "     45            1.0000        0.0484       0.3229            0.3229        1.9713  0.0005  0.1122\n",
      "     46            1.0000        0.0507       0.3229            0.3229        1.9680  0.0005  0.1069\n",
      "     47            1.0000        \u001b[32m0.0402\u001b[0m       0.3229            0.3229        1.9587  0.0004  0.1144\n",
      "     48            1.0000        0.0426       0.3333            0.3333        1.9461  0.0004  0.1080\n",
      "     49            1.0000        \u001b[32m0.0200\u001b[0m       0.3333            0.3333        1.9314  0.0003  0.1411\n",
      "     50            1.0000        0.0419       0.3333            0.3333        1.9161  0.0003  0.1083\n",
      "Fine tuning model for subject 4 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4667        1.5474       0.5000            0.5000        1.4004  0.0004  0.1102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6667        1.4867       0.4792            0.4792        1.4033  0.0005  0.1089\n",
      "     33            \u001b[36m0.8667\u001b[0m        0.9997       0.4375            0.4375        1.3935  0.0005  0.1089\n",
      "     34            \u001b[36m1.0000\u001b[0m        0.7345       0.4167            0.4167        1.3807  0.0006  0.1158\n",
      "     35            1.0000        \u001b[32m0.3232\u001b[0m       0.4583            0.4583        1.3768  0.0006  0.1213\n",
      "     36            1.0000        \u001b[32m0.2168\u001b[0m       0.4688            0.4688        1.3844  0.0007  0.1102\n",
      "     37            1.0000        \u001b[32m0.1226\u001b[0m       0.4688            0.4688        1.4084  0.0007  0.1104\n",
      "     38            1.0000        0.1236       0.4271            0.4271        1.4529  0.0007  0.1080\n",
      "     39            1.0000        \u001b[32m0.0507\u001b[0m       0.4167            0.4167        1.5142  0.0007  0.1081\n",
      "     40            1.0000        0.0723       0.4271            0.4271        1.5859  0.0007  0.1073\n",
      "     41            1.0000        0.1265       0.3958            0.3958        1.6546  0.0007  0.1099\n",
      "     42            1.0000        0.0690       0.3958            0.3958        1.7144  0.0007  0.1066\n",
      "     43            1.0000        0.0558       0.3854            0.3854        1.7603  0.0006  0.1085\n",
      "     44            1.0000        \u001b[32m0.0471\u001b[0m       0.3854            0.3854        1.7907  0.0006  0.1059\n",
      "     45            1.0000        \u001b[32m0.0338\u001b[0m       0.3854            0.3854        1.8080  0.0005  0.1100\n",
      "     46            1.0000        0.0483       0.3750            0.3750        1.8143  0.0005  0.1051\n",
      "     47            1.0000        0.0487       0.3854            0.3854        1.8116  0.0004  0.1070\n",
      "     48            1.0000        0.0345       0.3958            0.3958        1.8033  0.0004  0.1056\n",
      "     49            1.0000        \u001b[32m0.0230\u001b[0m       0.4062            0.4062        1.7918  0.0003  0.1048\n",
      "     50            1.0000        \u001b[32m0.0227\u001b[0m       0.4062            0.4062        1.7795  0.0003  0.1081\n",
      "Fine tuning model for subject 4 with 15 = 15 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.3996       0.4896            0.4896        1.3936  0.0004  0.1065\n",
      "     32            0.4667        1.4630       0.4583            0.4583        1.3797  0.0005  0.1074\n",
      "     33            0.6667        1.1773       0.4792            0.4792        1.3657  0.0005  0.1000\n",
      "     34            \u001b[36m0.8667\u001b[0m        0.9925       0.4792            0.4792        1.3605  0.0006  0.1115\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6235\u001b[0m       0.4583            0.4583        1.3921  0.0006  0.1054\n",
      "     36            1.0000        \u001b[32m0.3753\u001b[0m       0.3854            0.3854        1.4591  0.0007  0.1053\n",
      "     37            1.0000        \u001b[32m0.2078\u001b[0m       0.3854            0.3854        1.5489  0.0007  0.1356\n",
      "     38            1.0000        \u001b[32m0.1908\u001b[0m       0.3750            0.3750        1.6456  0.0007  0.1092\n",
      "     39            1.0000        \u001b[32m0.0881\u001b[0m       0.3854            0.3854        1.7349  0.0007  0.1053\n",
      "     40            1.0000        \u001b[32m0.0812\u001b[0m       0.3750            0.3750        1.8110  0.0007  0.1094\n",
      "     41            1.0000        0.0911       0.3646            0.3646        1.8601  0.0007  0.1096\n",
      "     42            1.0000        \u001b[32m0.0460\u001b[0m       0.3438            0.3438        1.8933  0.0007  0.1090\n",
      "     43            1.0000        \u001b[32m0.0204\u001b[0m       0.3438            0.3438        1.9092  0.0006  0.1121\n",
      "     44            1.0000        0.0617       0.3438            0.3438        1.9084  0.0006  0.1054\n",
      "     45            1.0000        0.0303       0.3333            0.3333        1.8943  0.0005  0.1107\n",
      "     46            1.0000        0.0263       0.3333            0.3333        1.8715  0.0005  0.1189\n",
      "     47            1.0000        \u001b[32m0.0180\u001b[0m       0.3333            0.3333        1.8436  0.0004  0.1010\n",
      "     48            1.0000        0.0411       0.3333            0.3333        1.8110  0.0004  0.1122\n",
      "     49            1.0000        0.0234       0.3438            0.3438        1.7772  0.0003  0.1130\n",
      "     50            1.0000        0.0189       0.3438            0.3438        1.7446  0.0003  0.1120\n",
      "Fine tuning model for subject 4 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4667        1.3587       0.4688            0.4688        1.3800  0.0004  0.1154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6667        1.2130       0.4792            0.4792        1.3539  0.0005  0.1032\n",
      "     33            \u001b[36m0.8667\u001b[0m        1.1209       0.5000            0.5000        1.3242  0.0005  0.1112\n",
      "     34            \u001b[36m1.0000\u001b[0m        0.9062       0.4479            0.4479        1.2949  0.0006  0.1117\n",
      "     35            1.0000        \u001b[32m0.4513\u001b[0m       0.4479            0.4479        1.2923  0.0006  0.1053\n",
      "     36            1.0000        \u001b[32m0.3106\u001b[0m       0.4167            0.4167        1.3206  0.0007  0.1130\n",
      "     37            1.0000        \u001b[32m0.2030\u001b[0m       0.4271            0.4271        1.3716  0.0007  0.1063\n",
      "     38            1.0000        \u001b[32m0.1840\u001b[0m       0.3854            0.3854        1.4240  0.0007  0.1062\n",
      "     39            1.0000        \u001b[32m0.1338\u001b[0m       0.3542            0.3542        1.4751  0.0007  0.1088\n",
      "     40            1.0000        0.1467       0.3646            0.3646        1.5235  0.0007  0.1067\n",
      "     41            1.0000        \u001b[32m0.0561\u001b[0m       0.3750            0.3750        1.5648  0.0007  0.1006\n",
      "     42            1.0000        0.0574       0.3750            0.3750        1.5942  0.0007  0.1097\n",
      "     43            1.0000        0.0587       0.3750            0.3750        1.6097  0.0006  0.1089\n",
      "     44            1.0000        \u001b[32m0.0388\u001b[0m       0.3542            0.3542        1.6137  0.0006  0.1131\n",
      "     45            1.0000        \u001b[32m0.0289\u001b[0m       0.3646            0.3646        1.6065  0.0005  0.1093\n",
      "     46            1.0000        0.0547       0.3646            0.3646        1.5893  0.0005  0.1079\n",
      "     47            1.0000        0.0472       0.3542            0.3542        1.5677  0.0004  0.1067\n",
      "     48            1.0000        0.0351       0.3958            0.3958        1.5453  0.0004  0.1017\n",
      "     49            1.0000        0.0405       0.4062            0.4062        1.5234  0.0003  0.1119\n",
      "     50            1.0000        0.0293       0.4167            0.4167        1.5036  0.0003  0.1131\n",
      "Fine tuning model for subject 4 with 15 = 15 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2000        2.0449       0.5104            0.5104        1.4010  0.0004  0.1070\n",
      "     32            0.5333        1.8080       0.4792            0.4792        1.4101  0.0005  0.0998\n",
      "     33            0.6667        1.2531       0.5104            0.5104        1.4089  0.0005  0.1081\n",
      "     34            \u001b[36m0.9333\u001b[0m        1.0567       0.4896            0.4896        1.4093  0.0006  0.1134\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6532\u001b[0m       0.4896            0.4896        1.4292  0.0006  0.1064\n",
      "     36            1.0000        \u001b[32m0.3803\u001b[0m       0.5208            0.5208        1.4688  0.0007  0.1095\n",
      "     37            1.0000        \u001b[32m0.3350\u001b[0m       0.5104            0.5104        1.5236  0.0007  0.1166\n",
      "     38            1.0000        \u001b[32m0.2110\u001b[0m       0.4583            0.4583        1.5957  0.0007  0.1082\n",
      "     39            1.0000        \u001b[32m0.1610\u001b[0m       0.4271            0.4271        1.6730  0.0007  0.1071\n",
      "     40            1.0000        \u001b[32m0.1082\u001b[0m       0.3854            0.3854        1.7540  0.0007  0.1104\n",
      "     41            1.0000        0.1116       0.3542            0.3542        1.8278  0.0007  0.1096\n",
      "     42            1.0000        \u001b[32m0.0783\u001b[0m       0.3333            0.3333        1.8880  0.0007  0.1082\n",
      "     43            1.0000        0.0879       0.3229            0.3229        1.9321  0.0006  0.1067\n",
      "     44            1.0000        0.1014       0.3125            0.3125        1.9565  0.0006  0.1127\n",
      "     45            1.0000        \u001b[32m0.0436\u001b[0m       0.3125            0.3125        1.9666  0.0005  0.1057\n",
      "     46            1.0000        0.0457       0.3125            0.3125        1.9621  0.0005  0.1098\n",
      "     47            1.0000        0.0458       0.3125            0.3125        1.9462  0.0004  0.1080\n",
      "     48            1.0000        \u001b[32m0.0257\u001b[0m       0.3229            0.3229        1.9227  0.0004  0.1101\n",
      "     49            1.0000        0.0355       0.3229            0.3229        1.8939  0.0003  0.1071\n",
      "     50            1.0000        0.0302       0.3333            0.3333        1.8616  0.0003  0.1152\n",
      "Fine tuning model for subject 4 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.9914       0.5000            0.5000        1.3821  0.0004  0.1081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.6530       0.4792            0.4792        1.3559  0.0005  0.1068\n",
      "     33            0.7333        1.4786       0.4479            0.4479        1.3350  0.0005  0.1124\n",
      "     34            0.7333        0.9661       0.4375            0.4375        1.3328  0.0006  0.1052\n",
      "     35            0.8000        \u001b[32m0.5798\u001b[0m       0.4271            0.4271        1.3687  0.0006  0.1070\n",
      "     36            \u001b[36m0.8667\u001b[0m        \u001b[32m0.5541\u001b[0m       0.4167            0.4167        1.4499  0.0007  0.1090\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2460\u001b[0m       0.3750            0.3750        1.5882  0.0007  0.1053\n",
      "     38            1.0000        \u001b[32m0.2034\u001b[0m       0.3542            0.3542        1.7535  0.0007  0.1099\n",
      "     39            1.0000        \u001b[32m0.1319\u001b[0m       0.3438            0.3438        1.9239  0.0007  0.1041\n",
      "     40            1.0000        \u001b[32m0.1110\u001b[0m       0.3021            0.3021        2.1050  0.0007  0.1086\n",
      "     41            1.0000        \u001b[32m0.0772\u001b[0m       0.2708            0.2708        2.2686  0.0007  0.1423\n",
      "     42            1.0000        \u001b[32m0.0597\u001b[0m       0.2604            0.2604        2.4034  0.0007  0.1398\n",
      "     43            1.0000        0.0628       0.2604            0.2604        2.5036  0.0006  0.1344\n",
      "     44            1.0000        0.0847       0.2604            0.2604        2.5655  0.0006  0.1387\n",
      "     45            1.0000        0.0609       0.2604            0.2604        2.5904  0.0005  0.1237\n",
      "     46            1.0000        0.0650       0.2604            0.2604        2.5832  0.0005  0.1393\n",
      "     47            1.0000        \u001b[32m0.0513\u001b[0m       0.2708            0.2708        2.5487  0.0004  0.1338\n",
      "     48            1.0000        \u001b[32m0.0454\u001b[0m       0.2708            0.2708        2.4950  0.0004  0.1274\n",
      "     49            1.0000        \u001b[32m0.0268\u001b[0m       0.2812            0.2812        2.4308  0.0003  0.1307\n",
      "     50            1.0000        0.0291       0.2812            0.2812        2.3603  0.0003  0.1359\n",
      "Fine tuning model for subject 4 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2000        2.1810       0.5208            0.5208        1.4159  0.0004  0.1284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4667        1.8322       0.4479            0.4479        1.4475  0.0005  0.1273\n",
      "     33            0.6000        1.4431       0.4583            0.4583        1.4742  0.0005  0.1457\n",
      "     34            0.8000        0.9898       0.4375            0.4375        1.5081  0.0006  0.1330\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6360\u001b[0m       0.4167            0.4167        1.5865  0.0006  0.1387\n",
      "     36            1.0000        \u001b[32m0.3583\u001b[0m       0.3125            0.3125        1.6737  0.0007  0.1388\n",
      "     37            1.0000        \u001b[32m0.3107\u001b[0m       0.2812            0.2812        1.7450  0.0007  0.1280\n",
      "     38            1.0000        \u001b[32m0.2962\u001b[0m       0.2812            0.2812        1.7795  0.0007  0.1543\n",
      "     39            1.0000        \u001b[32m0.1884\u001b[0m       0.2708            0.2708        1.8042  0.0007  0.1612\n",
      "     40            1.0000        \u001b[32m0.1648\u001b[0m       0.2604            0.2604        1.8268  0.0007  0.1098\n",
      "     41            1.0000        \u001b[32m0.0887\u001b[0m       0.2812            0.2812        1.8559  0.0007  0.1054\n",
      "     42            1.0000        0.1101       0.3125            0.3125        1.8852  0.0007  0.1139\n",
      "     43            1.0000        \u001b[32m0.0861\u001b[0m       0.2917            0.2917        1.9152  0.0006  0.1327\n",
      "     44            1.0000        0.0900       0.2812            0.2812        1.9400  0.0006  0.1067\n",
      "     45            1.0000        \u001b[32m0.0405\u001b[0m       0.2812            0.2812        1.9575  0.0005  0.1043\n",
      "     46            1.0000        \u001b[32m0.0331\u001b[0m       0.2812            0.2812        1.9657  0.0005  0.1191\n",
      "     47            1.0000        0.0511       0.2812            0.2812        1.9655  0.0004  0.1109\n",
      "     48            1.0000        \u001b[32m0.0253\u001b[0m       0.2812            0.2812        1.9592  0.0004  0.1059\n",
      "     49            1.0000        0.0442       0.2708            0.2708        1.9490  0.0003  0.1121\n",
      "     50            1.0000        0.0345       0.2604            0.2604        1.9377  0.0003  0.1021\n",
      "Fine tuning model for subject 4 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4667        1.3996       0.4479            0.4479        1.4055  0.0004  0.1055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4667        1.5298       0.4479            0.4479        1.4286  0.0005  0.1097\n",
      "     33            \u001b[36m0.8667\u001b[0m        1.2126       0.4167            0.4167        1.4824  0.0005  0.1095\n",
      "     34            \u001b[36m0.9333\u001b[0m        0.7980       0.4167            0.4167        1.5812  0.0006  0.1048\n",
      "     35            0.9333        \u001b[32m0.4521\u001b[0m       0.4167            0.4167        1.6926  0.0006  0.1068\n",
      "     36            0.9333        \u001b[32m0.4312\u001b[0m       0.3854            0.3854        1.8000  0.0007  0.1042\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1418\u001b[0m       0.3854            0.3854        1.9079  0.0007  0.1073\n",
      "     38            1.0000        0.1486       0.3542            0.3542        2.0132  0.0007  0.1099\n",
      "     39            1.0000        \u001b[32m0.0873\u001b[0m       0.3229            0.3229        2.1041  0.0007  0.1132\n",
      "     40            1.0000        \u001b[32m0.0489\u001b[0m       0.3021            0.3021        2.1778  0.0007  0.1071\n",
      "     41            1.0000        \u001b[32m0.0346\u001b[0m       0.2917            0.2917        2.2309  0.0007  0.1071\n",
      "     42            1.0000        0.0350       0.2812            0.2812        2.2653  0.0007  0.1053\n",
      "     43            1.0000        0.0369       0.2917            0.2917        2.2786  0.0006  0.1110\n",
      "     44            1.0000        0.0524       0.2917            0.2917        2.2712  0.0006  0.1052\n",
      "     45            1.0000        \u001b[32m0.0281\u001b[0m       0.2917            0.2917        2.2467  0.0005  0.1125\n",
      "     46            1.0000        \u001b[32m0.0213\u001b[0m       0.2917            0.2917        2.2113  0.0005  0.1057\n",
      "     47            1.0000        0.0373       0.2917            0.2917        2.1675  0.0004  0.1073\n",
      "     48            1.0000        0.0298       0.3229            0.3229        2.1171  0.0004  0.1114\n",
      "     49            1.0000        \u001b[32m0.0193\u001b[0m       0.3229            0.3229        2.0645  0.0003  0.1087\n",
      "     50            1.0000        \u001b[32m0.0179\u001b[0m       0.3333            0.3333        2.0108  0.0003  0.1032\n",
      "Fine tuning model for subject 4 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.2470       0.4792            0.4792        1.4028  0.0004  0.1088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7333        0.9912       0.5000            0.5000        1.4135  0.0005  0.1043\n",
      "     33            \u001b[36m0.8667\u001b[0m        1.0323       0.5208            0.5208        1.4176  0.0005  0.1082\n",
      "     34            \u001b[36m0.9333\u001b[0m        \u001b[32m0.6409\u001b[0m       0.5000            0.5000        1.4207  0.0006  0.1003\n",
      "     35            0.8667        \u001b[32m0.4569\u001b[0m       0.4896            0.4896        1.4190  0.0006  0.1042\n",
      "     36            0.9333        \u001b[32m0.3622\u001b[0m       0.5104            0.5104        1.4093  0.0007  0.1082\n",
      "     37            0.9333        \u001b[32m0.2362\u001b[0m       0.5417            0.5417        1.3920  0.0007  0.1126\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0913\u001b[0m       0.5417            0.5417        1.3821  0.0007  0.1256\n",
      "     39            1.0000        0.1563       0.5104            0.5104        1.3854  0.0007  0.1105\n",
      "     40            1.0000        0.0997       0.5000            0.5000        1.4031  0.0007  0.1176\n",
      "     41            1.0000        \u001b[32m0.0503\u001b[0m       0.4896            0.4896        1.4351  0.0007  0.1044\n",
      "     42            1.0000        0.0547       0.4479            0.4479        1.4761  0.0007  0.1098\n",
      "     43            1.0000        0.0657       0.4062            0.4062        1.5191  0.0006  0.1082\n",
      "     44            1.0000        \u001b[32m0.0279\u001b[0m       0.4062            0.4062        1.5576  0.0006  0.1061\n",
      "     45            1.0000        0.0399       0.3958            0.3958        1.5889  0.0005  0.1121\n",
      "     46            1.0000        0.0408       0.3958            0.3958        1.6134  0.0005  0.1239\n",
      "     47            1.0000        \u001b[32m0.0246\u001b[0m       0.3958            0.3958        1.6305  0.0004  0.1048\n",
      "     48            1.0000        0.0272       0.3958            0.3958        1.6408  0.0004  0.1124\n",
      "     49            1.0000        0.0310       0.3958            0.3958        1.6461  0.0003  0.1065\n",
      "     50            1.0000        \u001b[32m0.0224\u001b[0m       0.3958            0.3958        1.6474  0.0003  0.1097\n",
      "Fine tuning model for subject 4 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6500        1.1072       0.5104            0.5104        1.3985  0.0004  0.1147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        1.1838       0.5000            0.5000        1.4109  0.0005  0.1110\n",
      "     33            \u001b[36m0.9500\u001b[0m        0.9203       0.5104            0.5104        1.4323  0.0005  0.1114\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6276\u001b[0m       0.4896            0.4896        1.4636  0.0006  0.1067\n",
      "     35            1.0000        \u001b[32m0.2889\u001b[0m       0.4688            0.4688        1.4912  0.0006  0.1134\n",
      "     36            1.0000        \u001b[32m0.2154\u001b[0m       0.4896            0.4896        1.5070  0.0007  0.1153\n",
      "     37            1.0000        0.2171       0.4583            0.4583        1.5177  0.0007  0.1165\n",
      "     38            1.0000        \u001b[32m0.1572\u001b[0m       0.4583            0.4583        1.5251  0.0007  0.1176\n",
      "     39            1.0000        0.1586       0.4271            0.4271        1.5394  0.0007  0.1090\n",
      "     40            1.0000        \u001b[32m0.0919\u001b[0m       0.4375            0.4375        1.5606  0.0007  0.1139\n",
      "     41            1.0000        \u001b[32m0.0600\u001b[0m       0.3958            0.3958        1.5891  0.0007  0.1067\n",
      "     42            1.0000        0.0733       0.4062            0.4062        1.6162  0.0007  0.1123\n",
      "     43            1.0000        0.0784       0.4062            0.4062        1.6419  0.0006  0.1088\n",
      "     44            1.0000        \u001b[32m0.0428\u001b[0m       0.4062            0.4062        1.6619  0.0006  0.1102\n",
      "     45            1.0000        0.0483       0.4167            0.4167        1.6759  0.0005  0.1098\n",
      "     46            1.0000        \u001b[32m0.0381\u001b[0m       0.4167            0.4167        1.6838  0.0005  0.1057\n",
      "     47            1.0000        \u001b[32m0.0304\u001b[0m       0.4167            0.4167        1.6870  0.0004  0.1124\n",
      "     48            1.0000        \u001b[32m0.0267\u001b[0m       0.4062            0.4062        1.6857  0.0004  0.1123\n",
      "     49            1.0000        0.0331       0.4062            0.4062        1.6819  0.0003  0.1156\n",
      "     50            1.0000        0.0417       0.3854            0.3854        1.6755  0.0003  0.1074\n",
      "Fine tuning model for subject 4 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        1.5747       0.4792            0.4792        1.3766  0.0004  0.1082\n",
      "     32            0.4000        1.3895       0.4688            0.4688        1.3489  0.0005  0.1202\n",
      "     33            0.5500        1.2431       0.4688            0.4688        1.3154  0.0005  0.1152\n",
      "     34            0.7500        1.0411       0.4583            0.4583        1.2889  0.0006  0.1080\n",
      "     35            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6840\u001b[0m       0.4583            0.4583        1.2847  0.0006  0.1133\n",
      "     36            0.9000        \u001b[32m0.6274\u001b[0m       0.4896            0.4896        1.2940  0.0007  0.1287\n",
      "     37            0.9000        0.6307       0.4896            0.4896        1.3113  0.0007  0.1139\n",
      "     38            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2418\u001b[0m       0.4792            0.4792        1.3285  0.0007  0.1145\n",
      "     39            0.9500        \u001b[32m0.2152\u001b[0m       0.4792            0.4792        1.3403  0.0007  0.1142\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1564\u001b[0m       0.4688            0.4688        1.3496  0.0007  0.1173\n",
      "     41            1.0000        0.1688       0.4688            0.4688        1.3587  0.0007  0.1343\n",
      "     42            1.0000        \u001b[32m0.1048\u001b[0m       0.4688            0.4688        1.3675  0.0007  0.1648\n",
      "     43            1.0000        \u001b[32m0.0738\u001b[0m       0.4688            0.4688        1.3777  0.0006  0.1075\n",
      "     44            1.0000        \u001b[32m0.0564\u001b[0m       0.4896            0.4896        1.3876  0.0006  0.1444\n",
      "     45            1.0000        \u001b[32m0.0372\u001b[0m       0.4896            0.4896        1.3970  0.0005  0.1138\n",
      "     46            1.0000        0.0399       0.4583            0.4583        1.4053  0.0005  0.1167\n",
      "     47            1.0000        0.0487       0.4583            0.4583        1.4136  0.0004  0.1069\n",
      "     48            1.0000        0.0567       0.4583            0.4583        1.4216  0.0004  0.1203\n",
      "     49            1.0000        0.0415       0.4583            0.4583        1.4291  0.0003  0.1216\n",
      "     50            1.0000        \u001b[32m0.0272\u001b[0m       0.4688            0.4688        1.4355  0.0003  0.1102\n",
      "Fine tuning model for subject 4 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2500        1.3615       0.4688            0.4688        1.4009  0.0004  0.1066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.3500        1.4264       0.4896            0.4896        1.4045  0.0005  0.1590\n",
      "     33            0.5500        1.4814       0.4896            0.4896        1.4011  0.0005  0.1110\n",
      "     34            0.7500        1.2291       0.4896            0.4896        1.3841  0.0006  0.1110\n",
      "     35            \u001b[36m0.9500\u001b[0m        \u001b[32m0.6129\u001b[0m       0.4479            0.4479        1.3525  0.0006  0.1157\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4595\u001b[0m       0.4375            0.4375        1.3263  0.0007  0.1131\n",
      "     37            1.0000        \u001b[32m0.3272\u001b[0m       0.4688            0.4688        1.3160  0.0007  0.1425\n",
      "     38            1.0000        \u001b[32m0.2563\u001b[0m       0.4375            0.4375        1.3235  0.0007  0.1381\n",
      "     39            1.0000        \u001b[32m0.1609\u001b[0m       0.4479            0.4479        1.3454  0.0007  0.1403\n",
      "     40            1.0000        0.1667       0.4479            0.4479        1.3748  0.0007  0.1620\n",
      "     41            1.0000        \u001b[32m0.1252\u001b[0m       0.4271            0.4271        1.4054  0.0007  0.1387\n",
      "     42            1.0000        0.1336       0.4271            0.4271        1.4358  0.0007  0.1432\n",
      "     43            1.0000        0.1723       0.4062            0.4062        1.4619  0.0006  0.1586\n",
      "     44            1.0000        \u001b[32m0.0826\u001b[0m       0.4167            0.4167        1.4819  0.0006  0.1384\n",
      "     45            1.0000        \u001b[32m0.0649\u001b[0m       0.4062            0.4062        1.4987  0.0005  0.1363\n",
      "     46            1.0000        0.0717       0.4062            0.4062        1.5101  0.0005  0.1161\n",
      "     47            1.0000        \u001b[32m0.0478\u001b[0m       0.3958            0.3958        1.5172  0.0004  0.1321\n",
      "     48            1.0000        \u001b[32m0.0475\u001b[0m       0.3958            0.3958        1.5212  0.0004  0.1427\n",
      "     49            1.0000        0.0608       0.3958            0.3958        1.5222  0.0003  0.1375\n",
      "     50            1.0000        \u001b[32m0.0437\u001b[0m       0.4062            0.4062        1.5218  0.0003  0.1429\n",
      "Fine tuning model for subject 4 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4500        1.8640       0.5000            0.5000        1.4057  0.0004  0.1414\n",
      "     32            0.5000        1.6988       0.5104            0.5104        1.4214  0.0005  0.1386\n",
      "     33            0.6500        1.4343       0.4896            0.4896        1.4487  0.0005  0.1449\n",
      "     34            0.8000        0.9169       0.5000            0.5000        1.4743  0.0006  0.1660\n",
      "     35            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5305\u001b[0m       0.4792            0.4792        1.4972  0.0006  0.1360\n",
      "     36            \u001b[36m0.9500\u001b[0m        \u001b[32m0.3326\u001b[0m       0.4271            0.4271        1.5155  0.0007  0.1120\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2462\u001b[0m       0.4271            0.4271        1.5263  0.0007  0.1224\n",
      "     38            1.0000        0.2603       0.4375            0.4375        1.5345  0.0007  0.1207\n",
      "     39            1.0000        \u001b[32m0.2006\u001b[0m       0.4375            0.4375        1.5354  0.0007  0.1150\n",
      "     40            1.0000        0.2966       0.4375            0.4375        1.5341  0.0007  0.1189\n",
      "     41            1.0000        \u001b[32m0.1336\u001b[0m       0.4479            0.4479        1.5247  0.0007  0.1171\n",
      "     42            1.0000        0.1443       0.4375            0.4375        1.5074  0.0007  0.1290\n",
      "     43            1.0000        \u001b[32m0.1047\u001b[0m       0.4271            0.4271        1.4925  0.0006  0.1196\n",
      "     44            1.0000        \u001b[32m0.0879\u001b[0m       0.3958            0.3958        1.4796  0.0006  0.1145\n",
      "     45            1.0000        0.0941       0.3646            0.3646        1.4711  0.0005  0.1116\n",
      "     46            1.0000        \u001b[32m0.0833\u001b[0m       0.3854            0.3854        1.4668  0.0005  0.1159\n",
      "     47            1.0000        \u001b[32m0.0618\u001b[0m       0.3854            0.3854        1.4640  0.0004  0.1158\n",
      "     48            1.0000        0.0758       0.4167            0.4167        1.4641  0.0004  0.1119\n",
      "     49            1.0000        \u001b[32m0.0405\u001b[0m       0.4271            0.4271        1.4654  0.0003  0.1263\n",
      "     50            1.0000        0.0490       0.4271            0.4271        1.4665  0.0003  0.1125\n",
      "Fine tuning model for subject 4 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4500        1.3721       0.5000            0.5000        1.4118  0.0004  0.1160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.4380       0.5104            0.5104        1.4469  0.0005  0.1257\n",
      "     33            0.7000        1.5638       0.4792            0.4792        1.4688  0.0005  0.1082\n",
      "     34            0.8000        1.0164       0.4688            0.4688        1.4897  0.0006  0.1121\n",
      "     35            0.8000        \u001b[32m0.6290\u001b[0m       0.4792            0.4792        1.4800  0.0006  0.1234\n",
      "     36            \u001b[36m0.9500\u001b[0m        0.6544       0.4583            0.4583        1.4294  0.0007  0.1384\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3424\u001b[0m       0.4583            0.4583        1.3707  0.0007  0.1114\n",
      "     38            1.0000        \u001b[32m0.1734\u001b[0m       0.5104            0.5104        1.3367  0.0007  0.1086\n",
      "     39            1.0000        0.2003       0.4479            0.4479        1.3487  0.0007  0.1174\n",
      "     40            1.0000        0.1737       0.4375            0.4375        1.4014  0.0007  0.1098\n",
      "     41            0.9500        0.1749       0.3542            0.3542        1.4800  0.0007  0.1143\n",
      "     42            0.9500        \u001b[32m0.1483\u001b[0m       0.3333            0.3333        1.5611  0.0007  0.1096\n",
      "     43            0.9500        \u001b[32m0.0674\u001b[0m       0.3333            0.3333        1.6278  0.0006  0.1174\n",
      "     44            0.9500        0.0828       0.3438            0.3438        1.6675  0.0006  0.1086\n",
      "     45            0.9500        \u001b[32m0.0475\u001b[0m       0.3333            0.3333        1.6887  0.0005  0.1099\n",
      "     46            0.9500        0.0580       0.3333            0.3333        1.6916  0.0005  0.1119\n",
      "     47            0.9500        0.0545       0.3229            0.3229        1.6806  0.0004  0.1154\n",
      "     48            0.9500        0.0661       0.3125            0.3125        1.6599  0.0004  0.1095\n",
      "     49            0.9500        \u001b[32m0.0383\u001b[0m       0.3125            0.3125        1.6349  0.0003  0.1149\n",
      "     50            1.0000        0.0508       0.3125            0.3125        1.6084  0.0003  0.1159\n",
      "Fine tuning model for subject 4 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.6012       0.5104            0.5104        1.4154  0.0004  0.1113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4500        1.4616       0.4792            0.4792        1.4506  0.0005  0.1338\n",
      "     33            0.6500        1.1125       0.4375            0.4375        1.5159  0.0005  0.1405\n",
      "     34            0.8000        1.0082       0.4375            0.4375        1.6324  0.0006  0.1190\n",
      "     35            \u001b[36m0.8500\u001b[0m        0.7721       0.4271            0.4271        1.7450  0.0006  0.1135\n",
      "     36            0.8500        \u001b[32m0.4266\u001b[0m       0.4062            0.4062        1.7947  0.0007  0.1179\n",
      "     37            0.8500        \u001b[32m0.2401\u001b[0m       0.4375            0.4375        1.7644  0.0007  0.1169\n",
      "     38            \u001b[36m0.9000\u001b[0m        \u001b[32m0.1752\u001b[0m       0.4167            0.4167        1.6947  0.0007  0.1169\n",
      "     39            \u001b[36m0.9500\u001b[0m        \u001b[32m0.1744\u001b[0m       0.4062            0.4062        1.6193  0.0007  0.1071\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1257\u001b[0m       0.4375            0.4375        1.5584  0.0007  0.1189\n",
      "     41            1.0000        \u001b[32m0.0972\u001b[0m       0.4375            0.4375        1.5217  0.0007  0.1124\n",
      "     42            1.0000        \u001b[32m0.0657\u001b[0m       0.4479            0.4479        1.5050  0.0007  0.1118\n",
      "     43            1.0000        0.0760       0.4375            0.4375        1.5008  0.0006  0.1059\n",
      "     44            1.0000        0.0854       0.4167            0.4167        1.5036  0.0006  0.1122\n",
      "     45            1.0000        \u001b[32m0.0585\u001b[0m       0.4167            0.4167        1.5076  0.0005  0.1117\n",
      "     46            1.0000        0.0635       0.4167            0.4167        1.5074  0.0005  0.1117\n",
      "     47            1.0000        \u001b[32m0.0408\u001b[0m       0.4167            0.4167        1.5036  0.0004  0.1114\n",
      "     48            1.0000        \u001b[32m0.0393\u001b[0m       0.4062            0.4062        1.4964  0.0004  0.1135\n",
      "     49            1.0000        \u001b[32m0.0330\u001b[0m       0.4062            0.4062        1.4874  0.0003  0.1140\n",
      "     50            1.0000        0.0483       0.4062            0.4062        1.4779  0.0003  0.1126\n",
      "Fine tuning model for subject 4 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4500        1.4877       0.5000            0.5000        1.4087  0.0004  0.1061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5000        1.5255       0.4896            0.4896        1.4362  0.0005  0.1151\n",
      "     33            0.6500        1.2434       0.4792            0.4792        1.4694  0.0005  0.1084\n",
      "     34            0.8000        1.0649       0.4896            0.4896        1.5007  0.0006  0.1109\n",
      "     35            \u001b[36m0.9500\u001b[0m        0.7517       0.4792            0.4792        1.5405  0.0006  0.1161\n",
      "     36            0.9500        \u001b[32m0.3283\u001b[0m       0.3854            0.3854        1.6099  0.0007  0.1091\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3126\u001b[0m       0.3542            0.3542        1.6848  0.0007  0.1085\n",
      "     38            1.0000        \u001b[32m0.2679\u001b[0m       0.3125            0.3125        1.7382  0.0007  0.1082\n",
      "     39            1.0000        \u001b[32m0.2436\u001b[0m       0.3333            0.3333        1.7761  0.0007  0.1141\n",
      "     40            1.0000        \u001b[32m0.1897\u001b[0m       0.3438            0.3438        1.8001  0.0007  0.1157\n",
      "     41            1.0000        0.1912       0.3125            0.3125        1.8062  0.0007  0.1179\n",
      "     42            1.0000        \u001b[32m0.1368\u001b[0m       0.3333            0.3333        1.7999  0.0007  0.1148\n",
      "     43            1.0000        \u001b[32m0.1360\u001b[0m       0.3333            0.3333        1.7842  0.0006  0.1124\n",
      "     44            1.0000        0.1388       0.3333            0.3333        1.7649  0.0006  0.1098\n",
      "     45            1.0000        \u001b[32m0.0916\u001b[0m       0.3750            0.3750        1.7402  0.0005  0.1134\n",
      "     46            1.0000        0.0970       0.3854            0.3854        1.7170  0.0005  0.1122\n",
      "     47            1.0000        \u001b[32m0.0803\u001b[0m       0.3958            0.3958        1.6951  0.0004  0.1121\n",
      "     48            1.0000        0.1043       0.4167            0.4167        1.6758  0.0004  0.1171\n",
      "     49            1.0000        \u001b[32m0.0422\u001b[0m       0.4271            0.4271        1.6585  0.0003  0.1048\n",
      "     50            1.0000        0.0858       0.4167            0.4167        1.6432  0.0003  0.1174\n",
      "Fine tuning model for subject 4 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        1.9968       0.4896            0.4896        1.3931  0.0004  0.1126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5000        1.5283       0.4896            0.4896        1.3901  0.0005  0.1200\n",
      "     33            0.5500        1.0847       0.5208            0.5208        1.3859  0.0005  0.1176\n",
      "     34            \u001b[36m0.9000\u001b[0m        0.7750       0.5208            0.5208        1.4011  0.0006  0.1289\n",
      "     35            0.9000        \u001b[32m0.6057\u001b[0m       0.5000            0.5000        1.4574  0.0006  0.1117\n",
      "     36            0.9000        \u001b[32m0.5662\u001b[0m       0.4583            0.4583        1.5514  0.0007  0.1121\n",
      "     37            0.9000        \u001b[32m0.3625\u001b[0m       0.4271            0.4271        1.6640  0.0007  0.1200\n",
      "     38            0.9000        \u001b[32m0.3259\u001b[0m       0.3854            0.3854        1.7853  0.0007  0.1104\n",
      "     39            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2014\u001b[0m       0.3438            0.3438        1.9068  0.0007  0.1125\n",
      "     40            0.9000        \u001b[32m0.1457\u001b[0m       0.3438            0.3438        2.0199  0.0007  0.1184\n",
      "     41            0.9000        \u001b[32m0.1318\u001b[0m       0.3333            0.3333        2.1116  0.0007  0.1074\n",
      "     42            0.9000        \u001b[32m0.1177\u001b[0m       0.3229            0.3229        2.1770  0.0007  0.1179\n",
      "     43            0.9500        \u001b[32m0.0623\u001b[0m       0.3229            0.3229        2.2163  0.0006  0.1137\n",
      "     44            \u001b[36m1.0000\u001b[0m        0.0735       0.3229            0.3229        2.2292  0.0006  0.1103\n",
      "     45            1.0000        0.0896       0.3229            0.3229        2.2112  0.0005  0.0999\n",
      "     46            1.0000        0.0671       0.3229            0.3229        2.1710  0.0005  0.1105\n",
      "     47            1.0000        0.0688       0.3333            0.3333        2.1147  0.0004  0.1170\n",
      "     48            1.0000        \u001b[32m0.0512\u001b[0m       0.3125            0.3125        2.0507  0.0004  0.1093\n",
      "     49            1.0000        0.0784       0.3438            0.3438        1.9837  0.0003  0.1081\n",
      "     50            1.0000        0.0640       0.3438            0.3438        1.9165  0.0003  0.1135\n",
      "Fine tuning model for subject 4 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.2482       0.5312            0.5312        1.3895  0.0004  0.1344\n",
      "     32            0.8000        1.6278       0.5312            0.5312        1.3627  0.0005  0.1350\n",
      "     33            \u001b[36m0.8500\u001b[0m        0.9766       0.5312            0.5312        1.3258  0.0005  0.1276\n",
      "     34            0.8500        0.7318       0.5417            0.5417        1.2860  0.0006  0.1380\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4470\u001b[0m       0.5104            0.5104        1.2551  0.0006  0.1280\n",
      "     36            1.0000        0.4720       0.5000            0.5000        1.2394  0.0007  0.1759\n",
      "     37            1.0000        \u001b[32m0.2202\u001b[0m       0.5000            0.5000        1.2365  0.0007  0.1334\n",
      "     38            1.0000        \u001b[32m0.1514\u001b[0m       0.5000            0.5000        1.2560  0.0007  0.1388\n",
      "     39            1.0000        \u001b[32m0.1165\u001b[0m       0.5000            0.5000        1.2970  0.0007  0.1365\n",
      "     40            1.0000        0.1346       0.4583            0.4583        1.3542  0.0007  0.1564\n",
      "     41            1.0000        \u001b[32m0.0964\u001b[0m       0.4479            0.4479        1.4200  0.0007  0.1385\n",
      "     42            1.0000        0.1123       0.4271            0.4271        1.4818  0.0007  0.1324\n",
      "     43            1.0000        \u001b[32m0.0643\u001b[0m       0.4375            0.4375        1.5363  0.0006  0.1284\n",
      "     44            1.0000        \u001b[32m0.0632\u001b[0m       0.4479            0.4479        1.5814  0.0006  0.1450\n",
      "     45            1.0000        \u001b[32m0.0610\u001b[0m       0.4271            0.4271        1.6140  0.0005  0.1317\n",
      "     46            1.0000        0.0643       0.4062            0.4062        1.6325  0.0005  0.1715\n",
      "     47            1.0000        \u001b[32m0.0313\u001b[0m       0.4375            0.4375        1.6392  0.0004  0.1428\n",
      "     48            1.0000        0.0482       0.4583            0.4583        1.6381  0.0004  0.1726\n",
      "     49            1.0000        0.0388       0.4479            0.4479        1.6296  0.0003  0.1558\n",
      "     50            1.0000        0.0340       0.4479            0.4479        1.6162  0.0003  0.1171\n",
      "Fine tuning model for subject 4 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5500        1.3814       0.5208            0.5208        1.4038  0.0004  0.1095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6500        1.2953       0.5104            0.5104        1.4143  0.0005  0.1127\n",
      "     33            \u001b[36m0.9000\u001b[0m        1.0154       0.4688            0.4688        1.4217  0.0005  0.1093\n",
      "     34            0.9000        0.8237       0.4375            0.4375        1.4320  0.0006  0.1220\n",
      "     35            0.9000        \u001b[32m0.5606\u001b[0m       0.4479            0.4479        1.4463  0.0006  0.1199\n",
      "     36            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5160\u001b[0m       0.4583            0.4583        1.4691  0.0007  0.1140\n",
      "     37            0.9500        \u001b[32m0.2577\u001b[0m       0.4583            0.4583        1.4976  0.0007  0.1117\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1604\u001b[0m       0.4479            0.4479        1.5302  0.0007  0.1117\n",
      "     39            1.0000        \u001b[32m0.1444\u001b[0m       0.4375            0.4375        1.5632  0.0007  0.1035\n",
      "     40            1.0000        \u001b[32m0.1025\u001b[0m       0.4271            0.4271        1.5898  0.0007  0.1126\n",
      "     41            1.0000        \u001b[32m0.0636\u001b[0m       0.4062            0.4062        1.6112  0.0007  0.1181\n",
      "     42            1.0000        0.1006       0.4062            0.4062        1.6277  0.0007  0.1123\n",
      "     43            1.0000        \u001b[32m0.0535\u001b[0m       0.3958            0.3958        1.6395  0.0006  0.1121\n",
      "     44            1.0000        0.0678       0.3958            0.3958        1.6476  0.0006  0.1124\n",
      "     45            1.0000        \u001b[32m0.0427\u001b[0m       0.3854            0.3854        1.6527  0.0005  0.1081\n",
      "     46            1.0000        0.0690       0.3958            0.3958        1.6542  0.0005  0.1128\n",
      "     47            1.0000        0.0450       0.3958            0.3958        1.6541  0.0004  0.1100\n",
      "     48            1.0000        0.0533       0.4062            0.4062        1.6511  0.0004  0.1029\n",
      "     49            1.0000        \u001b[32m0.0338\u001b[0m       0.3958            0.3958        1.6481  0.0003  0.1150\n",
      "     50            1.0000        0.0500       0.3854            0.3854        1.6446  0.0003  0.1092\n",
      "Fine tuning model for subject 4 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4400        1.3236       0.5104            0.5104        1.3911  0.0004  0.1177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5200        1.2155       0.5000            0.5000        1.3882  0.0005  0.1227\n",
      "     33            0.6400        1.0291       0.5000            0.5000        1.3869  0.0005  0.1171\n",
      "     34            0.6400        0.8008       0.5208            0.5208        1.3884  0.0006  0.1218\n",
      "     35            \u001b[36m0.8800\u001b[0m        0.7099       0.5104            0.5104        1.3899  0.0006  0.1177\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4703\u001b[0m       0.4896            0.4896        1.3841  0.0007  0.1217\n",
      "     37            1.0000        \u001b[32m0.3099\u001b[0m       0.4896            0.4896        1.3791  0.0007  0.1172\n",
      "     38            1.0000        \u001b[32m0.2905\u001b[0m       0.4792            0.4792        1.3789  0.0007  0.1173\n",
      "     39            1.0000        \u001b[32m0.2293\u001b[0m       0.4583            0.4583        1.3887  0.0007  0.1143\n",
      "     40            1.0000        \u001b[32m0.1528\u001b[0m       0.4688            0.4688        1.4024  0.0007  0.1217\n",
      "     41            1.0000        0.1559       0.4479            0.4479        1.4165  0.0007  0.1172\n",
      "     42            1.0000        \u001b[32m0.0885\u001b[0m       0.4375            0.4375        1.4281  0.0007  0.1210\n",
      "     43            1.0000        0.1132       0.4479            0.4479        1.4343  0.0006  0.1255\n",
      "     44            1.0000        0.1063       0.4167            0.4167        1.4401  0.0006  0.1192\n",
      "     45            1.0000        \u001b[32m0.0772\u001b[0m       0.3958            0.3958        1.4453  0.0005  0.1145\n",
      "     46            1.0000        \u001b[32m0.0530\u001b[0m       0.3958            0.3958        1.4496  0.0005  0.1164\n",
      "     47            1.0000        0.0587       0.3958            0.3958        1.4531  0.0004  0.1203\n",
      "     48            1.0000        \u001b[32m0.0346\u001b[0m       0.4062            0.4062        1.4563  0.0004  0.1215\n",
      "     49            1.0000        0.0620       0.3958            0.3958        1.4584  0.0003  0.1191\n",
      "     50            1.0000        0.0461       0.3958            0.3958        1.4596  0.0003  0.1205\n",
      "Fine tuning model for subject 4 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6400        1.1843       0.5000            0.5000        1.3999  0.0004  0.1197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6800        1.1883       0.4792            0.4792        1.3985  0.0005  0.1251\n",
      "     33            \u001b[36m0.8400\u001b[0m        1.0315       0.5104            0.5104        1.3904  0.0005  0.1215\n",
      "     34            \u001b[36m0.9200\u001b[0m        0.7609       0.5000            0.5000        1.3725  0.0006  0.1125\n",
      "     35            \u001b[36m0.9600\u001b[0m        0.7255       0.4792            0.4792        1.3632  0.0006  0.1225\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4521\u001b[0m       0.4792            0.4792        1.3686  0.0007  0.1136\n",
      "     37            1.0000        \u001b[32m0.2598\u001b[0m       0.4896            0.4896        1.3934  0.0007  0.1175\n",
      "     38            1.0000        \u001b[32m0.1913\u001b[0m       0.4375            0.4375        1.4386  0.0007  0.1200\n",
      "     39            1.0000        \u001b[32m0.1400\u001b[0m       0.4062            0.4062        1.5021  0.0007  0.1173\n",
      "     40            1.0000        \u001b[32m0.1026\u001b[0m       0.4167            0.4167        1.5786  0.0007  0.1178\n",
      "     41            1.0000        \u001b[32m0.0871\u001b[0m       0.4062            0.4062        1.6501  0.0007  0.1099\n",
      "     42            1.0000        \u001b[32m0.0796\u001b[0m       0.4062            0.4062        1.7051  0.0007  0.1250\n",
      "     43            1.0000        0.0962       0.4062            0.4062        1.7455  0.0006  0.1222\n",
      "     44            1.0000        \u001b[32m0.0614\u001b[0m       0.4062            0.4062        1.7651  0.0006  0.1172\n",
      "     45            1.0000        \u001b[32m0.0573\u001b[0m       0.4062            0.4062        1.7669  0.0005  0.1143\n",
      "     46            1.0000        0.0639       0.4167            0.4167        1.7510  0.0005  0.1176\n",
      "     47            1.0000        0.0722       0.4271            0.4271        1.7242  0.0004  0.1210\n",
      "     48            1.0000        \u001b[32m0.0370\u001b[0m       0.4271            0.4271        1.6951  0.0004  0.1178\n",
      "     49            1.0000        \u001b[32m0.0284\u001b[0m       0.4062            0.4062        1.6647  0.0003  0.1974\n",
      "     50            1.0000        0.0431       0.4167            0.4167        1.6348  0.0003  0.1164\n",
      "Fine tuning model for subject 4 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.3022       0.5208            0.5208        1.4015  0.0004  0.1211\n",
      "     32            0.6800        0.9985       0.4896            0.4896        1.4066  0.0005  0.1524\n",
      "     33            \u001b[36m0.8400\u001b[0m        1.0843       0.4896            0.4896        1.4051  0.0005  0.1182\n",
      "     34            \u001b[36m0.9600\u001b[0m        0.7061       0.4896            0.4896        1.3994  0.0006  0.1160\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4850\u001b[0m       0.5000            0.5000        1.3960  0.0006  0.1176\n",
      "     36            1.0000        0.4989       0.4688            0.4688        1.4001  0.0007  0.1237\n",
      "     37            0.9600        \u001b[32m0.3322\u001b[0m       0.4375            0.4375        1.4165  0.0007  0.1268\n",
      "     38            0.9600        \u001b[32m0.3080\u001b[0m       0.4375            0.4375        1.4400  0.0007  0.1176\n",
      "     39            0.9600        \u001b[32m0.1541\u001b[0m       0.4375            0.4375        1.4747  0.0007  0.1191\n",
      "     40            0.9600        0.1616       0.4688            0.4688        1.5184  0.0007  0.1213\n",
      "     41            1.0000        0.1941       0.4375            0.4375        1.5598  0.0007  0.1161\n",
      "     42            1.0000        0.1602       0.4271            0.4271        1.5941  0.0007  0.1147\n",
      "     43            1.0000        \u001b[32m0.1303\u001b[0m       0.4167            0.4167        1.6292  0.0006  0.1229\n",
      "     44            1.0000        0.1410       0.4271            0.4271        1.6520  0.0006  0.1206\n",
      "     45            1.0000        \u001b[32m0.0956\u001b[0m       0.4167            0.4167        1.6655  0.0005  0.1172\n",
      "     46            1.0000        \u001b[32m0.0659\u001b[0m       0.3958            0.3958        1.6657  0.0005  0.1250\n",
      "     47            1.0000        \u001b[32m0.0479\u001b[0m       0.4062            0.4062        1.6598  0.0004  0.1159\n",
      "     48            1.0000        0.0690       0.3958            0.3958        1.6447  0.0004  0.1242\n",
      "     49            1.0000        0.0702       0.3958            0.3958        1.6258  0.0003  0.1187\n",
      "     50            1.0000        0.0559       0.3958            0.3958        1.6067  0.0003  0.1259\n",
      "Fine tuning model for subject 4 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.5156       0.5104            0.5104        1.4048  0.0004  0.1168\n",
      "     32            0.4400        1.6373       0.5312            0.5312        1.4209  0.0005  0.1161\n",
      "     33            0.6800        1.1673       0.5000            0.5000        1.4273  0.0005  0.1175\n",
      "     34            \u001b[36m0.8400\u001b[0m        1.0985       0.4896            0.4896        1.4090  0.0006  0.1165\n",
      "     35            \u001b[36m0.9200\u001b[0m        0.6890       0.4896            0.4896        1.3667  0.0006  0.1192\n",
      "     36            \u001b[36m0.9600\u001b[0m        \u001b[32m0.5601\u001b[0m       0.5104            0.5104        1.3171  0.0007  0.1113\n",
      "     37            0.9600        \u001b[32m0.3743\u001b[0m       0.4896            0.4896        1.2786  0.0007  0.1218\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3568\u001b[0m       0.5000            0.5000        1.2489  0.0007  0.1489\n",
      "     39            1.0000        \u001b[32m0.2407\u001b[0m       0.4896            0.4896        1.2257  0.0007  0.1428\n",
      "     40            1.0000        \u001b[32m0.2282\u001b[0m       0.5104            0.5104        1.2109  0.0007  0.1425\n",
      "     41            1.0000        \u001b[32m0.1424\u001b[0m       0.5000            0.5000        1.2037  0.0007  0.1370\n",
      "     42            1.0000        \u001b[32m0.1295\u001b[0m       0.5000            0.5000        1.2001  0.0007  0.1496\n",
      "     43            1.0000        \u001b[32m0.1079\u001b[0m       0.4896            0.4896        1.1985  0.0006  0.1677\n",
      "     44            1.0000        0.1489       0.4792            0.4792        1.1995  0.0006  0.1544\n",
      "     45            1.0000        0.1554       0.4896            0.4896        1.2022  0.0005  0.1466\n",
      "     46            1.0000        \u001b[32m0.0912\u001b[0m       0.5000            0.5000        1.2048  0.0005  0.1664\n",
      "     47            1.0000        0.1225       0.4896            0.4896        1.2079  0.0004  0.1430\n",
      "     48            1.0000        0.0951       0.4896            0.4896        1.2107  0.0004  0.1331\n",
      "     49            1.0000        \u001b[32m0.0465\u001b[0m       0.4896            0.4896        1.2127  0.0003  0.1422\n",
      "     50            1.0000        0.0715       0.5000            0.5000        1.2139  0.0003  0.1389\n",
      "Fine tuning model for subject 4 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4800        1.3671       0.4792            0.4792        1.3978  0.0004  0.1452\n",
      "     32            0.6000        1.5847       0.5000            0.5000        1.3947  0.0005  0.1793\n",
      "     33            0.7200        1.0787       0.5000            0.5000        1.3831  0.0005  0.1391\n",
      "     34            \u001b[36m0.8800\u001b[0m        1.0524       0.5208            0.5208        1.3768  0.0006  0.1910\n",
      "     35            \u001b[36m0.9200\u001b[0m        \u001b[32m0.6644\u001b[0m       0.5417            0.5417        1.3939  0.0006  0.1393\n",
      "     36            \u001b[36m0.9600\u001b[0m        \u001b[32m0.5254\u001b[0m       0.5104            0.5104        1.4324  0.0007  0.1186\n",
      "     37            0.9600        \u001b[32m0.3883\u001b[0m       0.4583            0.4583        1.4867  0.0007  0.1197\n",
      "     38            0.9600        0.4069       0.4583            0.4583        1.5491  0.0007  0.1208\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3268\u001b[0m       0.4167            0.4167        1.6036  0.0007  0.1175\n",
      "     40            1.0000        \u001b[32m0.1896\u001b[0m       0.4167            0.4167        1.6448  0.0007  0.1324\n",
      "     41            1.0000        \u001b[32m0.1819\u001b[0m       0.4062            0.4062        1.6699  0.0007  0.1184\n",
      "     42            1.0000        \u001b[32m0.1233\u001b[0m       0.4271            0.4271        1.6795  0.0007  0.1268\n",
      "     43            1.0000        0.1678       0.4271            0.4271        1.6751  0.0006  0.1181\n",
      "     44            1.0000        \u001b[32m0.0966\u001b[0m       0.4167            0.4167        1.6603  0.0006  0.1177\n",
      "     45            1.0000        \u001b[32m0.0929\u001b[0m       0.4271            0.4271        1.6393  0.0005  0.1225\n",
      "     46            1.0000        \u001b[32m0.0900\u001b[0m       0.4167            0.4167        1.6144  0.0005  0.1192\n",
      "     47            1.0000        0.0908       0.4167            0.4167        1.5861  0.0004  0.1384\n",
      "     48            1.0000        \u001b[32m0.0697\u001b[0m       0.4167            0.4167        1.5564  0.0004  0.1193\n",
      "     49            1.0000        \u001b[32m0.0573\u001b[0m       0.4583            0.4583        1.5283  0.0003  0.1270\n",
      "     50            1.0000        \u001b[32m0.0548\u001b[0m       0.4688            0.4688        1.5011  0.0003  0.1257\n",
      "Fine tuning model for subject 4 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2800        1.7274       0.5208            0.5208        1.3933  0.0004  0.1170\n",
      "     32            0.4800        1.6042       0.5104            0.5104        1.3822  0.0005  0.1222\n",
      "     33            0.6800        1.6183       0.5104            0.5104        1.3655  0.0005  0.1227\n",
      "     34            0.6800        1.1119       0.5104            0.5104        1.3565  0.0006  0.1141\n",
      "     35            0.7600        0.6914       0.5000            0.5000        1.3655  0.0006  0.1131\n",
      "     36            \u001b[36m0.8400\u001b[0m        \u001b[32m0.5962\u001b[0m       0.4375            0.4375        1.4059  0.0007  0.1159\n",
      "     37            \u001b[36m0.8800\u001b[0m        \u001b[32m0.4370\u001b[0m       0.4375            0.4375        1.4709  0.0007  0.1193\n",
      "     38            0.8800        \u001b[32m0.3411\u001b[0m       0.4167            0.4167        1.5478  0.0007  0.1176\n",
      "     39            \u001b[36m0.9600\u001b[0m        \u001b[32m0.2498\u001b[0m       0.3646            0.3646        1.6409  0.0007  0.1174\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2228\u001b[0m       0.3646            0.3646        1.7380  0.0007  0.1171\n",
      "     41            1.0000        \u001b[32m0.1544\u001b[0m       0.3438            0.3438        1.8358  0.0007  0.1197\n",
      "     42            1.0000        \u001b[32m0.1240\u001b[0m       0.3646            0.3646        1.9182  0.0007  0.1123\n",
      "     43            1.0000        \u001b[32m0.1131\u001b[0m       0.4062            0.4062        1.9806  0.0006  0.1196\n",
      "     44            1.0000        \u001b[32m0.0882\u001b[0m       0.3854            0.3854        2.0205  0.0006  0.1137\n",
      "     45            1.0000        \u001b[32m0.0815\u001b[0m       0.3750            0.3750        2.0419  0.0005  0.1225\n",
      "     46            1.0000        \u001b[32m0.0725\u001b[0m       0.3750            0.3750        2.0474  0.0005  0.1182\n",
      "     47            1.0000        0.0785       0.3750            0.3750        2.0422  0.0004  0.1236\n",
      "     48            1.0000        \u001b[32m0.0697\u001b[0m       0.3854            0.3854        2.0302  0.0004  0.1201\n",
      "     49            1.0000        \u001b[32m0.0647\u001b[0m       0.3854            0.3854        2.0117  0.0003  0.1273\n",
      "     50            1.0000        \u001b[32m0.0510\u001b[0m       0.4062            0.4062        1.9892  0.0003  0.1179\n",
      "Fine tuning model for subject 4 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.1698       0.4896            0.4896        1.4016  0.0004  0.1227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6400        1.2308       0.4792            0.4792        1.4147  0.0005  0.1175\n",
      "     33            0.7200        1.1461       0.4792            0.4792        1.4236  0.0005  0.1203\n",
      "     34            \u001b[36m0.8400\u001b[0m        0.8305       0.4583            0.4583        1.4341  0.0006  0.1229\n",
      "     35            0.8400        0.7051       0.4479            0.4479        1.4607  0.0006  0.1191\n",
      "     36            \u001b[36m0.9200\u001b[0m        \u001b[32m0.5689\u001b[0m       0.4479            0.4479        1.5054  0.0007  0.1172\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4181\u001b[0m       0.4375            0.4375        1.5764  0.0007  0.1186\n",
      "     38            1.0000        \u001b[32m0.2506\u001b[0m       0.4167            0.4167        1.6773  0.0007  0.1221\n",
      "     39            1.0000        0.2663       0.3958            0.3958        1.7970  0.0007  0.1217\n",
      "     40            1.0000        \u001b[32m0.1392\u001b[0m       0.3854            0.3854        1.9189  0.0007  0.1175\n",
      "     41            1.0000        \u001b[32m0.1163\u001b[0m       0.3646            0.3646        2.0322  0.0007  0.1174\n",
      "     42            0.9600        0.1246       0.3646            0.3646        2.1297  0.0007  0.1211\n",
      "     43            1.0000        0.1220       0.3542            0.3542        2.2011  0.0006  0.1224\n",
      "     44            1.0000        \u001b[32m0.0888\u001b[0m       0.3542            0.3542        2.2455  0.0006  0.1184\n",
      "     45            1.0000        0.0979       0.3542            0.3542        2.2622  0.0005  0.1294\n",
      "     46            1.0000        \u001b[32m0.0822\u001b[0m       0.3542            0.3542        2.2551  0.0005  0.1184\n",
      "     47            1.0000        \u001b[32m0.0763\u001b[0m       0.3438            0.3438        2.2307  0.0004  0.1231\n",
      "     48            1.0000        \u001b[32m0.0495\u001b[0m       0.3646            0.3646        2.1927  0.0004  0.1120\n",
      "     49            1.0000        0.0573       0.3750            0.3750        2.1459  0.0003  0.1173\n",
      "     50            1.0000        \u001b[32m0.0365\u001b[0m       0.3750            0.3750        2.0951  0.0003  0.1184\n",
      "Fine tuning model for subject 4 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4400        1.3885       0.5104            0.5104        1.3957  0.0004  0.1135\n",
      "     32            0.4800        1.5697       0.5104            0.5104        1.3867  0.0005  0.1218\n",
      "     33            0.6000        1.2538       0.5000            0.5000        1.3682  0.0005  0.1199\n",
      "     34            0.8000        1.0060       0.4792            0.4792        1.3609  0.0006  0.1176\n",
      "     35            \u001b[36m0.8400\u001b[0m        \u001b[32m0.6290\u001b[0m       0.4479            0.4479        1.3685  0.0006  0.1178\n",
      "     36            0.8400        \u001b[32m0.3542\u001b[0m       0.4583            0.4583        1.3899  0.0007  0.1199\n",
      "     37            0.8400        \u001b[32m0.3107\u001b[0m       0.4583            0.4583        1.4378  0.0007  0.1156\n",
      "     38            \u001b[36m0.9200\u001b[0m        \u001b[32m0.2805\u001b[0m       0.4479            0.4479        1.4945  0.0007  0.1205\n",
      "     39            \u001b[36m0.9600\u001b[0m        0.2829       0.4583            0.4583        1.5472  0.0007  0.1212\n",
      "     40            0.9600        \u001b[32m0.2091\u001b[0m       0.4271            0.4271        1.6045  0.0007  0.1240\n",
      "     41            0.9200        \u001b[32m0.1816\u001b[0m       0.4167            0.4167        1.6625  0.0007  0.1368\n",
      "     42            0.9200        \u001b[32m0.1471\u001b[0m       0.4167            0.4167        1.7070  0.0007  0.1180\n",
      "     43            0.9600        \u001b[32m0.0717\u001b[0m       0.4167            0.4167        1.7365  0.0006  0.1246\n",
      "     44            0.9600        0.0782       0.4062            0.4062        1.7483  0.0006  0.1137\n",
      "     45            0.9600        \u001b[32m0.0649\u001b[0m       0.4062            0.4062        1.7450  0.0005  0.1142\n",
      "     46            0.9600        0.0818       0.4167            0.4167        1.7249  0.0005  0.1158\n",
      "     47            0.9600        \u001b[32m0.0625\u001b[0m       0.4062            0.4062        1.6948  0.0004  0.1103\n",
      "     48            0.9600        0.0961       0.4062            0.4062        1.6601  0.0004  0.1173\n",
      "     49            0.9600        \u001b[32m0.0573\u001b[0m       0.3958            0.3958        1.6231  0.0003  0.1155\n",
      "     50            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0469\u001b[0m       0.3958            0.3958        1.5844  0.0003  0.1176\n",
      "Fine tuning model for subject 4 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2400        1.6616       0.5000            0.5000        1.4133  0.0004  0.1169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4800        1.6326       0.4792            0.4792        1.4438  0.0005  0.1279\n",
      "     33            0.5200        1.4234       0.4792            0.4792        1.5083  0.0005  0.1384\n",
      "     34            0.6800        0.9298       0.4375            0.4375        1.6248  0.0006  0.1232\n",
      "     35            0.7600        0.8070       0.4271            0.4271        1.7736  0.0006  0.1129\n",
      "     36            0.7600        \u001b[32m0.4495\u001b[0m       0.4062            0.4062        1.8924  0.0007  0.1164\n",
      "     37            0.7600        \u001b[32m0.2967\u001b[0m       0.3854            0.3854        1.9548  0.0007  0.1175\n",
      "     38            \u001b[36m0.8800\u001b[0m        \u001b[32m0.2186\u001b[0m       0.3750            0.3750        1.9776  0.0007  0.1191\n",
      "     39            0.8800        0.2310       0.4062            0.4062        1.9762  0.0007  0.1170\n",
      "     40            \u001b[36m0.9600\u001b[0m        \u001b[32m0.1569\u001b[0m       0.3542            0.3542        1.9551  0.0007  0.1172\n",
      "     41            0.9600        0.1678       0.3542            0.3542        1.9303  0.0007  0.1184\n",
      "     42            \u001b[36m1.0000\u001b[0m        0.1937       0.3438            0.3438        1.9033  0.0007  0.1387\n",
      "     43            1.0000        \u001b[32m0.1113\u001b[0m       0.3438            0.3438        1.8762  0.0006  0.1401\n",
      "     44            1.0000        0.1379       0.3333            0.3333        1.8590  0.0006  0.1402\n",
      "     45            1.0000        0.1134       0.3229            0.3229        1.8452  0.0005  0.1405\n",
      "     46            1.0000        \u001b[32m0.1093\u001b[0m       0.3229            0.3229        1.8341  0.0005  0.1494\n",
      "     47            1.0000        \u001b[32m0.1000\u001b[0m       0.3125            0.3125        1.8236  0.0004  0.1464\n",
      "     48            1.0000        \u001b[32m0.0940\u001b[0m       0.3229            0.3229        1.8122  0.0004  0.1818\n",
      "     49            1.0000        \u001b[32m0.0888\u001b[0m       0.3229            0.3229        1.8007  0.0003  0.1447\n",
      "     50            1.0000        0.0956       0.3438            0.3438        1.7877  0.0003  0.1439\n",
      "Fine tuning model for subject 4 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3600        1.5602       0.4896            0.4896        1.4118  0.0004  0.1678\n",
      "     32            0.4400        1.4402       0.5208            0.5208        1.4452  0.0005  0.1391\n",
      "     33            0.6000        1.1848       0.5000            0.5000        1.4970  0.0005  0.1391\n",
      "     34            0.7600        1.0034       0.4583            0.4583        1.5584  0.0006  0.1316\n",
      "     35            \u001b[36m0.8400\u001b[0m        0.7259       0.4375            0.4375        1.6338  0.0006  0.1366\n",
      "     36            \u001b[36m0.9200\u001b[0m        \u001b[32m0.6884\u001b[0m       0.3958            0.3958        1.7208  0.0007  0.1583\n",
      "     37            0.9200        \u001b[32m0.4168\u001b[0m       0.3854            0.3854        1.7924  0.0007  0.1466\n",
      "     38            0.9200        \u001b[32m0.2987\u001b[0m       0.3542            0.3542        1.8425  0.0007  0.1436\n",
      "     39            0.8800        \u001b[32m0.2328\u001b[0m       0.3438            0.3438        1.8628  0.0007  0.1957\n",
      "     40            0.8800        \u001b[32m0.1539\u001b[0m       0.3438            0.3438        1.8636  0.0007  0.1477\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1367\u001b[0m       0.3542            0.3542        1.8476  0.0007  0.1179\n",
      "     42            1.0000        0.1416       0.3646            0.3646        1.8172  0.0007  0.1158\n",
      "     43            1.0000        \u001b[32m0.1065\u001b[0m       0.3750            0.3750        1.7857  0.0006  0.1168\n",
      "     44            1.0000        0.1323       0.3750            0.3750        1.7517  0.0006  0.1430\n",
      "     45            1.0000        \u001b[32m0.0676\u001b[0m       0.3646            0.3646        1.7174  0.0005  0.1192\n",
      "     46            1.0000        \u001b[32m0.0656\u001b[0m       0.3542            0.3542        1.6852  0.0005  0.1206\n",
      "     47            1.0000        0.1106       0.3542            0.3542        1.6573  0.0004  0.1294\n",
      "     48            1.0000        0.0659       0.3646            0.3646        1.6353  0.0004  0.1197\n",
      "     49            1.0000        \u001b[32m0.0584\u001b[0m       0.3646            0.3646        1.6152  0.0003  0.1222\n",
      "     50            1.0000        0.0718       0.3542            0.3542        1.5957  0.0003  0.1185\n",
      "Fine tuning model for subject 4 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4667        1.6330       0.4896            0.4896        1.4027  0.0004  0.1409\n",
      "     32            0.5000        1.4444       0.4896            0.4896        1.4095  0.0005  0.1283\n",
      "     33            0.6000        1.2796       0.5000            0.5000        1.4070  0.0005  0.1275\n",
      "     34            0.7333        1.0828       0.5000            0.5000        1.4025  0.0006  0.1705\n",
      "     35            \u001b[36m0.9667\u001b[0m        0.9063       0.4688            0.4688        1.4174  0.0006  0.1296\n",
      "     36            0.9667        \u001b[32m0.5462\u001b[0m       0.4375            0.4375        1.4758  0.0007  0.1213\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3556\u001b[0m       0.3958            0.3958        1.5806  0.0007  0.1285\n",
      "     38            1.0000        \u001b[32m0.3251\u001b[0m       0.3750            0.3750        1.7124  0.0007  0.1284\n",
      "     39            0.9000        \u001b[32m0.2680\u001b[0m       0.3750            0.3750        1.8520  0.0007  0.1323\n",
      "     40            0.9000        0.2893       0.3333            0.3333        1.9678  0.0007  0.1244\n",
      "     41            0.9000        \u001b[32m0.2495\u001b[0m       0.3333            0.3333        2.0604  0.0007  0.1222\n",
      "     42            0.8667        \u001b[32m0.1809\u001b[0m       0.3229            0.3229        2.1231  0.0007  0.1274\n",
      "     43            0.8667        \u001b[32m0.1758\u001b[0m       0.3229            0.3229        2.1579  0.0006  0.1303\n",
      "     44            0.9000        \u001b[32m0.1338\u001b[0m       0.3229            0.3229        2.1698  0.0006  0.1297\n",
      "     45            0.9333        \u001b[32m0.1153\u001b[0m       0.3229            0.3229        2.1610  0.0005  0.1273\n",
      "     46            0.9333        \u001b[32m0.0916\u001b[0m       0.3229            0.3229        2.1285  0.0005  0.1228\n",
      "     47            0.9333        0.0977       0.3229            0.3229        2.0817  0.0004  0.1295\n",
      "     48            0.9667        0.0977       0.3229            0.3229        2.0246  0.0004  0.1293\n",
      "     49            0.9667        0.1016       0.3438            0.3438        1.9607  0.0003  0.1227\n",
      "     50            1.0000        \u001b[32m0.0853\u001b[0m       0.3750            0.3750        1.8969  0.0003  0.1291\n",
      "Fine tuning model for subject 4 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4333        1.5646       0.4896            0.4896        1.3869  0.0004  0.1272\n",
      "     32            0.4667        1.4560       0.4792            0.4792        1.3652  0.0005  0.1235\n",
      "     33            0.5333        1.2336       0.4688            0.4688        1.3432  0.0005  0.1231\n",
      "     34            0.7333        1.1536       0.4583            0.4583        1.3223  0.0006  0.1349\n",
      "     35            \u001b[36m0.8333\u001b[0m        0.9353       0.4167            0.4167        1.3259  0.0006  0.1299\n",
      "     36            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4888\u001b[0m       0.3958            0.3958        1.3696  0.0007  0.1389\n",
      "     37            \u001b[36m0.9667\u001b[0m        0.6023       0.3542            0.3542        1.4432  0.0007  0.1288\n",
      "     38            0.9667        \u001b[32m0.4120\u001b[0m       0.3854            0.3854        1.5378  0.0007  0.1282\n",
      "     39            0.9333        \u001b[32m0.3293\u001b[0m       0.3542            0.3542        1.6299  0.0007  0.1335\n",
      "     40            0.9000        \u001b[32m0.2102\u001b[0m       0.4062            0.4062        1.7141  0.0007  0.1258\n",
      "     41            0.8667        0.2160       0.4062            0.4062        1.7785  0.0007  0.1251\n",
      "     42            0.8667        \u001b[32m0.1673\u001b[0m       0.3958            0.3958        1.8244  0.0007  0.1310\n",
      "     43            0.8667        0.2048       0.3958            0.3958        1.8481  0.0006  0.1280\n",
      "     44            0.8667        \u001b[32m0.1377\u001b[0m       0.4167            0.4167        1.8537  0.0006  0.1289\n",
      "     45            0.8667        0.1419       0.4062            0.4062        1.8458  0.0005  0.1262\n",
      "     46            0.9000        \u001b[32m0.0990\u001b[0m       0.4062            0.4062        1.8280  0.0005  0.1221\n",
      "     47            0.9333        0.1028       0.4062            0.4062        1.8028  0.0004  0.1271\n",
      "     48            0.9333        \u001b[32m0.0714\u001b[0m       0.4062            0.4062        1.7727  0.0004  0.1348\n",
      "     49            0.9667        0.0903       0.4062            0.4062        1.7403  0.0003  0.1270\n",
      "     50            0.9667        0.0757       0.4062            0.4062        1.7065  0.0003  0.1330\n",
      "Fine tuning model for subject 4 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3667        1.8999       0.5000            0.5000        1.4051  0.0004  0.1278\n",
      "     32            0.5000        1.5247       0.5000            0.5000        1.4173  0.0005  0.1298\n",
      "     33            0.7000        1.6511       0.4583            0.4583        1.4231  0.0005  0.1281\n",
      "     34            \u001b[36m0.8333\u001b[0m        1.0772       0.4583            0.4583        1.4260  0.0006  0.1289\n",
      "     35            0.8333        0.7977       0.4375            0.4375        1.4636  0.0006  0.1287\n",
      "     36            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4860\u001b[0m       0.3750            0.3750        1.5401  0.0007  0.1273\n",
      "     37            0.9333        \u001b[32m0.3994\u001b[0m       0.3542            0.3542        1.6422  0.0007  0.1347\n",
      "     38            0.9333        \u001b[32m0.3384\u001b[0m       0.3333            0.3333        1.7605  0.0007  0.1285\n",
      "     39            0.8667        \u001b[32m0.2300\u001b[0m       0.3125            0.3125        1.8711  0.0007  0.1316\n",
      "     40            0.8667        0.2919       0.3229            0.3229        1.9729  0.0007  0.1244\n",
      "     41            0.8667        \u001b[32m0.2056\u001b[0m       0.3125            0.3125        2.0564  0.0007  0.1400\n",
      "     42            0.8667        \u001b[32m0.1975\u001b[0m       0.3333            0.3333        2.1260  0.0007  0.1235\n",
      "     43            0.8667        \u001b[32m0.1451\u001b[0m       0.3229            0.3229        2.1801  0.0006  0.1556\n",
      "     44            0.8667        \u001b[32m0.1263\u001b[0m       0.3125            0.3125        2.2113  0.0006  0.1292\n",
      "     45            0.9000        \u001b[32m0.1132\u001b[0m       0.3229            0.3229        2.2141  0.0005  0.1283\n",
      "     46            0.9000        \u001b[32m0.0753\u001b[0m       0.3229            0.3229        2.1954  0.0005  0.1255\n",
      "     47            0.9000        0.0911       0.3333            0.3333        2.1601  0.0004  0.1317\n",
      "     48            0.9333        0.1089       0.3333            0.3333        2.1134  0.0004  0.1281\n",
      "     49            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0623\u001b[0m       0.3438            0.3438        2.0659  0.0003  0.1300\n",
      "     50            1.0000        0.0766       0.3333            0.3333        2.0197  0.0003  0.1302\n",
      "Fine tuning model for subject 4 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2667        1.8539       0.4688            0.4688        1.3956  0.0004  0.1300\n",
      "     32            0.2667        1.5167       0.4896            0.4896        1.3956  0.0005  0.1261\n",
      "     33            0.4333        1.4109       0.4583            0.4583        1.3943  0.0005  0.1241\n",
      "     34            0.5333        1.1251       0.4479            0.4479        1.4029  0.0006  0.1335\n",
      "     35            0.7000        0.8875       0.4792            0.4792        1.4447  0.0006  0.1250\n",
      "     36            \u001b[36m0.8667\u001b[0m        0.7549       0.4375            0.4375        1.5182  0.0007  0.1258\n",
      "     37            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4753\u001b[0m       0.4062            0.4062        1.6002  0.0007  0.1347\n",
      "     38            0.9000        \u001b[32m0.4035\u001b[0m       0.4062            0.4062        1.6699  0.0007  0.1280\n",
      "     39            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3294\u001b[0m       0.3958            0.3958        1.7069  0.0007  0.1328\n",
      "     40            \u001b[36m0.9667\u001b[0m        \u001b[32m0.2825\u001b[0m       0.3854            0.3854        1.7200  0.0007  0.1467\n",
      "     41            0.9667        \u001b[32m0.2818\u001b[0m       0.3958            0.3958        1.7230  0.0007  0.1389\n",
      "     42            0.9667        \u001b[32m0.2385\u001b[0m       0.3958            0.3958        1.7202  0.0007  0.1569\n",
      "     43            0.9667        \u001b[32m0.1838\u001b[0m       0.3958            0.3958        1.7168  0.0006  0.1520\n",
      "     44            0.9667        \u001b[32m0.1032\u001b[0m       0.3958            0.3958        1.7155  0.0006  0.1949\n",
      "     45            0.9667        0.1706       0.4167            0.4167        1.7138  0.0005  0.1812\n",
      "     46            0.9667        0.1134       0.4062            0.4062        1.7139  0.0005  0.1730\n",
      "     47            \u001b[36m1.0000\u001b[0m        0.1037       0.3958            0.3958        1.7157  0.0004  0.1599\n",
      "     48            1.0000        0.1064       0.3958            0.3958        1.7186  0.0004  0.1495\n",
      "     49            1.0000        \u001b[32m0.0958\u001b[0m       0.3750            0.3750        1.7211  0.0003  0.1494\n",
      "     50            1.0000        0.1135       0.3750            0.3750        1.7221  0.0003  0.1501\n",
      "Fine tuning model for subject 4 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4333        1.5688       0.5104            0.5104        1.4135  0.0004  0.1547\n",
      "     32            0.5667        1.4634       0.5104            0.5104        1.4377  0.0005  0.1494\n",
      "     33            0.6333        1.1613       0.5000            0.5000        1.4640  0.0005  0.1698\n",
      "     34            0.7000        0.9267       0.4688            0.4688        1.5017  0.0006  0.1776\n",
      "     35            0.8000        0.7265       0.4375            0.4375        1.5508  0.0006  0.1671\n",
      "     36            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6082\u001b[0m       0.4375            0.4375        1.5974  0.0007  0.2000\n",
      "     37            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4889\u001b[0m       0.4479            0.4479        1.6421  0.0007  0.1278\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2861\u001b[0m       0.4271            0.4271        1.6801  0.0007  0.1282\n",
      "     39            1.0000        \u001b[32m0.2128\u001b[0m       0.4167            0.4167        1.7046  0.0007  0.1273\n",
      "     40            1.0000        \u001b[32m0.1847\u001b[0m       0.4167            0.4167        1.7359  0.0007  0.1342\n",
      "     41            1.0000        \u001b[32m0.1705\u001b[0m       0.4167            0.4167        1.7639  0.0007  0.1324\n",
      "     42            1.0000        \u001b[32m0.1642\u001b[0m       0.3958            0.3958        1.7926  0.0007  0.1306\n",
      "     43            1.0000        \u001b[32m0.1420\u001b[0m       0.4167            0.4167        1.8169  0.0006  0.1316\n",
      "     44            1.0000        \u001b[32m0.1259\u001b[0m       0.4062            0.4062        1.8363  0.0006  0.1279\n",
      "     45            1.0000        \u001b[32m0.1079\u001b[0m       0.4062            0.4062        1.8475  0.0005  0.1221\n",
      "     46            1.0000        \u001b[32m0.0756\u001b[0m       0.4062            0.4062        1.8499  0.0005  0.1277\n",
      "     47            1.0000        0.1066       0.3958            0.3958        1.8414  0.0004  0.1301\n",
      "     48            1.0000        0.0836       0.3958            0.3958        1.8297  0.0004  0.1293\n",
      "     49            1.0000        \u001b[32m0.0700\u001b[0m       0.3958            0.3958        1.8168  0.0003  0.1285\n",
      "     50            1.0000        \u001b[32m0.0594\u001b[0m       0.3958            0.3958        1.8017  0.0003  0.1286\n",
      "Fine tuning model for subject 4 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4333        1.7840       0.4896            0.4896        1.3829  0.0004  0.1277\n",
      "     32            0.5333        1.5052       0.4792            0.4792        1.3535  0.0005  0.1267\n",
      "     33            0.6000        1.3300       0.4896            0.4896        1.3094  0.0005  0.1302\n",
      "     34            0.6667        1.2184       0.4896            0.4896        1.2633  0.0006  0.1277\n",
      "     35            \u001b[36m0.9667\u001b[0m        1.0572       0.4688            0.4688        1.2307  0.0006  0.1278\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6667\u001b[0m       0.4688            0.4688        1.2204  0.0007  0.1278\n",
      "     37            1.0000        \u001b[32m0.4638\u001b[0m       0.4479            0.4479        1.2319  0.0007  0.1285\n",
      "     38            1.0000        \u001b[32m0.3639\u001b[0m       0.4583            0.4583        1.2576  0.0007  0.1220\n",
      "     39            1.0000        \u001b[32m0.3400\u001b[0m       0.4167            0.4167        1.2875  0.0007  0.1261\n",
      "     40            1.0000        \u001b[32m0.1877\u001b[0m       0.3958            0.3958        1.3185  0.0007  0.1270\n",
      "     41            1.0000        0.2027       0.3958            0.3958        1.3475  0.0007  0.1263\n",
      "     42            1.0000        0.1997       0.3646            0.3646        1.3696  0.0007  0.1224\n",
      "     43            1.0000        \u001b[32m0.1735\u001b[0m       0.3646            0.3646        1.3855  0.0006  0.1268\n",
      "     44            1.0000        0.2162       0.3854            0.3854        1.3905  0.0006  0.1243\n",
      "     45            1.0000        0.1806       0.3750            0.3750        1.3912  0.0005  0.1239\n",
      "     46            1.0000        \u001b[32m0.1357\u001b[0m       0.3750            0.3750        1.3895  0.0005  0.1299\n",
      "     47            1.0000        0.1447       0.3646            0.3646        1.3866  0.0004  0.1333\n",
      "     48            1.0000        \u001b[32m0.1263\u001b[0m       0.3750            0.3750        1.3833  0.0004  0.1339\n",
      "     49            1.0000        \u001b[32m0.1045\u001b[0m       0.3958            0.3958        1.3802  0.0003  0.1302\n",
      "     50            1.0000        0.1397       0.3958            0.3958        1.3772  0.0003  0.1250\n",
      "Fine tuning model for subject 4 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.7691       0.5000            0.5000        1.4023  0.0004  0.1279\n",
      "     32            0.4000        1.9279       0.4896            0.4896        1.4058  0.0005  0.1388\n",
      "     33            0.4667        1.7063       0.4688            0.4688        1.4006  0.0005  0.1286\n",
      "     34            0.5667        1.3345       0.4792            0.4792        1.3815  0.0006  0.1283\n",
      "     35            0.7667        1.2297       0.4479            0.4479        1.3509  0.0006  0.1275\n",
      "     36            \u001b[36m0.9333\u001b[0m        0.8851       0.4271            0.4271        1.3231  0.0007  0.1302\n",
      "     37            \u001b[36m0.9667\u001b[0m        \u001b[32m0.6076\u001b[0m       0.4062            0.4062        1.3041  0.0007  0.1256\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4555\u001b[0m       0.4167            0.4167        1.2992  0.0007  0.1297\n",
      "     39            1.0000        \u001b[32m0.3821\u001b[0m       0.4271            0.4271        1.3096  0.0007  0.1278\n",
      "     40            1.0000        \u001b[32m0.2704\u001b[0m       0.4271            0.4271        1.3323  0.0007  0.1250\n",
      "     41            1.0000        \u001b[32m0.2244\u001b[0m       0.4375            0.4375        1.3624  0.0007  0.1281\n",
      "     42            1.0000        0.2624       0.4375            0.4375        1.3903  0.0007  0.1283\n",
      "     43            1.0000        \u001b[32m0.1810\u001b[0m       0.4375            0.4375        1.4143  0.0006  0.1250\n",
      "     44            1.0000        \u001b[32m0.1795\u001b[0m       0.4375            0.4375        1.4315  0.0006  0.1279\n",
      "     45            1.0000        \u001b[32m0.1092\u001b[0m       0.4375            0.4375        1.4401  0.0005  0.1333\n",
      "     46            1.0000        0.1176       0.4479            0.4479        1.4428  0.0005  0.1235\n",
      "     47            1.0000        0.1350       0.4271            0.4271        1.4401  0.0004  0.1252\n",
      "     48            1.0000        0.1224       0.4167            0.4167        1.4345  0.0004  0.1293\n",
      "     49            1.0000        \u001b[32m0.0944\u001b[0m       0.4375            0.4375        1.4284  0.0003  0.1285\n",
      "     50            1.0000        \u001b[32m0.0751\u001b[0m       0.4271            0.4271        1.4235  0.0003  0.1238\n",
      "Fine tuning model for subject 4 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4333        1.7460       0.5104            0.5104        1.3864  0.0004  0.1299\n",
      "     32            0.4667        1.9060       0.5104            0.5104        1.3726  0.0005  0.1306\n",
      "     33            0.6000        1.5209       0.4896            0.4896        1.3535  0.0005  0.1305\n",
      "     34            0.7000        1.1031       0.5104            0.5104        1.3459  0.0006  0.1271\n",
      "     35            0.7667        0.8592       0.5000            0.5000        1.3505  0.0006  0.1255\n",
      "     36            \u001b[36m0.8667\u001b[0m        \u001b[32m0.5650\u001b[0m       0.4688            0.4688        1.3634  0.0007  0.1318\n",
      "     37            \u001b[36m0.9333\u001b[0m        0.5663       0.4792            0.4792        1.3848  0.0007  0.1265\n",
      "     38            0.9333        \u001b[32m0.4804\u001b[0m       0.4688            0.4688        1.4097  0.0007  0.1340\n",
      "     39            0.9333        \u001b[32m0.3266\u001b[0m       0.4375            0.4375        1.4322  0.0007  0.1281\n",
      "     40            0.9333        \u001b[32m0.2826\u001b[0m       0.4271            0.4271        1.4516  0.0007  0.1332\n",
      "     41            0.9333        0.3155       0.4062            0.4062        1.4629  0.0007  0.1257\n",
      "     42            0.9333        \u001b[32m0.2807\u001b[0m       0.4167            0.4167        1.4657  0.0007  0.1333\n",
      "     43            0.9333        \u001b[32m0.1706\u001b[0m       0.4167            0.4167        1.4732  0.0006  0.1260\n",
      "     44            \u001b[36m0.9667\u001b[0m        \u001b[32m0.1631\u001b[0m       0.4167            0.4167        1.4757  0.0006  0.1286\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1558\u001b[0m       0.4167            0.4167        1.4696  0.0005  0.1325\n",
      "     46            1.0000        \u001b[32m0.1159\u001b[0m       0.4167            0.4167        1.4606  0.0005  0.1334\n",
      "     47            1.0000        0.1271       0.4167            0.4167        1.4497  0.0004  0.1272\n",
      "     48            1.0000        \u001b[32m0.0892\u001b[0m       0.4271            0.4271        1.4356  0.0004  0.1315\n",
      "     49            1.0000        0.1014       0.4375            0.4375        1.4201  0.0003  0.1282\n",
      "     50            1.0000        0.0939       0.4375            0.4375        1.4054  0.0003  0.1270\n",
      "Fine tuning model for subject 4 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.7470       0.5000            0.5000        1.4062  0.0004  0.1310\n",
      "     32            0.5000        1.7373       0.5104            0.5104        1.4178  0.0005  0.1345\n",
      "     33            0.6000        1.3876       0.4583            0.4583        1.4358  0.0005  0.1262\n",
      "     34            0.7667        1.2201       0.4583            0.4583        1.4700  0.0006  0.1279\n",
      "     35            0.8000        0.9336       0.4271            0.4271        1.5234  0.0006  0.1386\n",
      "     36            \u001b[36m0.8667\u001b[0m        \u001b[32m0.4602\u001b[0m       0.3854            0.3854        1.5944  0.0007  0.1472\n",
      "     37            0.8667        0.4895       0.3542            0.3542        1.6854  0.0007  0.1665\n",
      "     38            0.8667        \u001b[32m0.3587\u001b[0m       0.3854            0.3854        1.7894  0.0007  0.1567\n",
      "     39            0.8667        \u001b[32m0.3011\u001b[0m       0.4062            0.4062        1.8860  0.0007  0.1550\n",
      "     40            0.8667        \u001b[32m0.2484\u001b[0m       0.4062            0.4062        1.9701  0.0007  0.1874\n",
      "     41            0.8667        0.2913       0.4062            0.4062        2.0380  0.0007  0.1603\n",
      "     42            0.8667        \u001b[32m0.2085\u001b[0m       0.3958            0.3958        2.0727  0.0007  0.1532\n",
      "     43            0.8667        \u001b[32m0.2016\u001b[0m       0.4167            0.4167        2.0859  0.0006  0.1494\n",
      "     44            0.8667        \u001b[32m0.1896\u001b[0m       0.3958            0.3958        2.0824  0.0006  0.1442\n",
      "     45            0.8667        \u001b[32m0.1411\u001b[0m       0.3542            0.3542        2.0654  0.0005  0.1512\n",
      "     46            \u001b[36m0.9000\u001b[0m        \u001b[32m0.1398\u001b[0m       0.3542            0.3542        2.0400  0.0005  0.1379\n",
      "     47            0.9000        \u001b[32m0.1028\u001b[0m       0.3542            0.3542        2.0069  0.0004  0.1435\n",
      "     48            0.9000        \u001b[32m0.0656\u001b[0m       0.3333            0.3333        1.9708  0.0004  0.1610\n",
      "     49            \u001b[36m0.9333\u001b[0m        0.1405       0.3333            0.3333        1.9281  0.0003  0.1512\n",
      "     50            \u001b[36m1.0000\u001b[0m        0.0781       0.3333            0.3333        1.8869  0.0003  0.1734\n",
      "Fine tuning model for subject 4 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5333        1.7306       0.4896            0.4896        1.3979  0.0004  0.1684\n",
      "     32            0.6667        1.5005       0.5208            0.5208        1.4042  0.0005  0.1917\n",
      "     33            0.7333        1.3925       0.5000            0.5000        1.4100  0.0005  0.1301\n",
      "     34            \u001b[36m0.8333\u001b[0m        0.9756       0.4792            0.4792        1.4204  0.0006  0.1349\n",
      "     35            \u001b[36m0.9000\u001b[0m        0.9009       0.4583            0.4583        1.4364  0.0006  0.1245\n",
      "     36            0.9000        \u001b[32m0.5138\u001b[0m       0.4583            0.4583        1.4557  0.0007  0.1496\n",
      "     37            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4072\u001b[0m       0.4479            0.4479        1.4676  0.0007  0.1255\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3754\u001b[0m       0.4688            0.4688        1.4691  0.0007  0.1288\n",
      "     39            1.0000        \u001b[32m0.2750\u001b[0m       0.4792            0.4792        1.4675  0.0007  0.1337\n",
      "     40            1.0000        \u001b[32m0.2092\u001b[0m       0.4896            0.4896        1.4650  0.0007  0.1244\n",
      "     41            1.0000        \u001b[32m0.1747\u001b[0m       0.4271            0.4271        1.4668  0.0007  0.1584\n",
      "     42            1.0000        \u001b[32m0.1369\u001b[0m       0.4271            0.4271        1.4704  0.0007  0.1354\n",
      "     43            1.0000        0.1650       0.4167            0.4167        1.4784  0.0006  0.1654\n",
      "     44            1.0000        \u001b[32m0.1334\u001b[0m       0.4271            0.4271        1.4878  0.0006  0.1260\n",
      "     45            1.0000        \u001b[32m0.1236\u001b[0m       0.4375            0.4375        1.4946  0.0005  0.1311\n",
      "     46            1.0000        \u001b[32m0.0946\u001b[0m       0.4375            0.4375        1.4983  0.0005  0.1316\n",
      "     47            1.0000        0.0959       0.4375            0.4375        1.4999  0.0004  0.1272\n",
      "     48            1.0000        \u001b[32m0.0888\u001b[0m       0.4375            0.4375        1.4995  0.0004  0.1275\n",
      "     49            1.0000        \u001b[32m0.0705\u001b[0m       0.4167            0.4167        1.4983  0.0003  0.1321\n",
      "     50            1.0000        \u001b[32m0.0680\u001b[0m       0.4167            0.4167        1.4966  0.0003  0.1284\n",
      "Fine tuning model for subject 4 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2571        1.6586       0.4896            0.4896        1.3961  0.0004  0.1297\n",
      "     32            0.3143        1.5981       0.4688            0.4688        1.3908  0.0005  0.1360\n",
      "     33            0.4000        1.2304       0.4375            0.4375        1.3966  0.0005  0.1312\n",
      "     34            0.6857        1.1686       0.4271            0.4271        1.4097  0.0006  0.1378\n",
      "     35            0.8000        0.9223       0.4479            0.4479        1.4292  0.0006  0.1354\n",
      "     36            \u001b[36m0.8857\u001b[0m        0.7199       0.4167            0.4167        1.4646  0.0007  0.1349\n",
      "     37            \u001b[36m0.9429\u001b[0m        \u001b[32m0.5248\u001b[0m       0.3958            0.3958        1.5102  0.0007  0.1310\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4439\u001b[0m       0.3854            0.3854        1.5582  0.0007  0.1264\n",
      "     39            1.0000        \u001b[32m0.3445\u001b[0m       0.3750            0.3750        1.6116  0.0007  0.1350\n",
      "     40            1.0000        0.3776       0.3542            0.3542        1.6554  0.0007  0.1363\n",
      "     41            1.0000        \u001b[32m0.3100\u001b[0m       0.3438            0.3438        1.6951  0.0007  0.1330\n",
      "     42            1.0000        \u001b[32m0.2756\u001b[0m       0.3438            0.3438        1.7207  0.0007  0.1339\n",
      "     43            1.0000        \u001b[32m0.1958\u001b[0m       0.3542            0.3542        1.7313  0.0006  0.1383\n",
      "     44            1.0000        0.2175       0.3542            0.3542        1.7315  0.0006  0.1326\n",
      "     45            1.0000        \u001b[32m0.1234\u001b[0m       0.3438            0.3438        1.7238  0.0005  0.1332\n",
      "     46            1.0000        \u001b[32m0.1004\u001b[0m       0.3646            0.3646        1.7080  0.0005  0.1340\n",
      "     47            1.0000        0.1091       0.3750            0.3750        1.6857  0.0004  0.1396\n",
      "     48            1.0000        0.1378       0.3750            0.3750        1.6622  0.0004  0.1327\n",
      "     49            1.0000        \u001b[32m0.0969\u001b[0m       0.3854            0.3854        1.6385  0.0003  0.1291\n",
      "     50            1.0000        0.1401       0.3854            0.3854        1.6155  0.0003  0.1346\n",
      "Fine tuning model for subject 4 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2857        1.8547       0.4896            0.4896        1.3935  0.0004  0.1325\n",
      "     32            0.3143        1.7087       0.4896            0.4896        1.3853  0.0005  0.1336\n",
      "     33            0.4857        1.6728       0.4896            0.4896        1.3684  0.0005  0.1402\n",
      "     34            0.6571        1.2377       0.4688            0.4688        1.3443  0.0006  0.1304\n",
      "     35            0.7714        1.0745       0.4896            0.4896        1.3158  0.0006  0.1327\n",
      "     36            \u001b[36m0.8286\u001b[0m        0.8145       0.4688            0.4688        1.2932  0.0007  0.1327\n",
      "     37            \u001b[36m0.9143\u001b[0m        \u001b[32m0.5849\u001b[0m       0.4271            0.4271        1.2861  0.0007  0.1350\n",
      "     38            \u001b[36m0.9429\u001b[0m        \u001b[32m0.4957\u001b[0m       0.4479            0.4479        1.2897  0.0007  0.1358\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4239\u001b[0m       0.4688            0.4688        1.3003  0.0007  0.1387\n",
      "     40            1.0000        \u001b[32m0.3297\u001b[0m       0.4688            0.4688        1.3144  0.0007  0.1357\n",
      "     41            1.0000        \u001b[32m0.3248\u001b[0m       0.4688            0.4688        1.3314  0.0007  0.1345\n",
      "     42            1.0000        \u001b[32m0.2802\u001b[0m       0.4583            0.4583        1.3451  0.0007  0.1323\n",
      "     43            1.0000        \u001b[32m0.2516\u001b[0m       0.4583            0.4583        1.3595  0.0006  0.1331\n",
      "     44            1.0000        \u001b[32m0.2276\u001b[0m       0.4271            0.4271        1.3753  0.0006  0.1423\n",
      "     45            1.0000        \u001b[32m0.1758\u001b[0m       0.4479            0.4479        1.3916  0.0005  0.1356\n",
      "     46            1.0000        \u001b[32m0.1496\u001b[0m       0.4375            0.4375        1.4068  0.0005  0.1404\n",
      "     47            1.0000        0.1575       0.4375            0.4375        1.4186  0.0004  0.1337\n",
      "     48            1.0000        \u001b[32m0.1403\u001b[0m       0.4375            0.4375        1.4297  0.0004  0.1361\n",
      "     49            1.0000        \u001b[32m0.1082\u001b[0m       0.4271            0.4271        1.4385  0.0003  0.1348\n",
      "     50            1.0000        0.1603       0.4583            0.4583        1.4459  0.0003  0.1314\n",
      "Fine tuning model for subject 4 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3714        1.8766       0.5208            0.5208        1.4199  0.0004  0.1416\n",
      "     32            0.3714        1.7654       0.5312            0.5312        1.4598  0.0005  0.1360\n",
      "     33            0.5143        1.4357       0.4896            0.4896        1.5154  0.0005  0.1330\n",
      "     34            0.5143        1.1324       0.4583            0.4583        1.5757  0.0006  0.1346\n",
      "     35            0.6000        0.9501       0.4375            0.4375        1.6131  0.0006  0.1306\n",
      "     36            0.7143        \u001b[32m0.6597\u001b[0m       0.4688            0.4688        1.6235  0.0007  0.1410\n",
      "     37            0.7714        \u001b[32m0.5496\u001b[0m       0.4583            0.4583        1.6146  0.0007  0.1329\n",
      "     38            0.7714        \u001b[32m0.5089\u001b[0m       0.4688            0.4688        1.6016  0.0007  0.1367\n",
      "     39            0.8000        \u001b[32m0.4313\u001b[0m       0.4688            0.4688        1.5941  0.0007  0.1352\n",
      "     40            \u001b[36m0.8857\u001b[0m        \u001b[32m0.4066\u001b[0m       0.4688            0.4688        1.5935  0.0007  0.1295\n",
      "     41            0.8857        \u001b[32m0.3661\u001b[0m       0.4792            0.4792        1.5992  0.0007  0.1289\n",
      "     42            \u001b[36m0.9143\u001b[0m        \u001b[32m0.2276\u001b[0m       0.4479            0.4479        1.6123  0.0007  0.1328\n",
      "     43            \u001b[36m0.9714\u001b[0m        0.2790       0.4479            0.4479        1.6254  0.0006  0.1421\n",
      "     44            0.9714        \u001b[32m0.2260\u001b[0m       0.4479            0.4479        1.6364  0.0006  0.1350\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1854\u001b[0m       0.4479            0.4479        1.6396  0.0005  0.1384\n",
      "     46            1.0000        \u001b[32m0.1668\u001b[0m       0.4375            0.4375        1.6329  0.0005  0.1323\n",
      "     47            1.0000        \u001b[32m0.1539\u001b[0m       0.4479            0.4479        1.6201  0.0004  0.1310\n",
      "     48            1.0000        \u001b[32m0.1140\u001b[0m       0.4375            0.4375        1.6022  0.0004  0.1776\n",
      "     49            1.0000        0.1158       0.4479            0.4479        1.5801  0.0003  0.1602\n",
      "     50            1.0000        \u001b[32m0.0734\u001b[0m       0.4479            0.4479        1.5570  0.0003  0.1710\n",
      "Fine tuning model for subject 4 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4286        1.6488       0.5000            0.5000        1.4005  0.0004  0.1689\n",
      "     32            0.5143        1.6954       0.4896            0.4896        1.4135  0.0005  0.1587\n",
      "     33            0.6000        1.6633       0.4792            0.4792        1.4262  0.0005  0.1800\n",
      "     34            0.6000        1.2174       0.4688            0.4688        1.4475  0.0006  0.1687\n",
      "     35            0.7143        1.0416       0.4688            0.4688        1.4697  0.0006  0.1601\n",
      "     36            0.7714        0.8721       0.4479            0.4479        1.4910  0.0007  0.1531\n",
      "     37            \u001b[36m0.8286\u001b[0m        \u001b[32m0.6252\u001b[0m       0.4583            0.4583        1.5143  0.0007  0.1590\n",
      "     38            0.8286        0.6675       0.4375            0.4375        1.5417  0.0007  0.1548\n",
      "     39            \u001b[36m0.8571\u001b[0m        \u001b[32m0.5364\u001b[0m       0.4271            0.4271        1.5755  0.0007  0.1580\n",
      "     40            \u001b[36m0.8857\u001b[0m        \u001b[32m0.4645\u001b[0m       0.4375            0.4375        1.6149  0.0007  0.1498\n",
      "     41            0.8857        \u001b[32m0.3470\u001b[0m       0.4375            0.4375        1.6598  0.0007  0.1730\n",
      "     42            0.8857        \u001b[32m0.2914\u001b[0m       0.4375            0.4375        1.6986  0.0007  0.1811\n",
      "     43            \u001b[36m0.9143\u001b[0m        \u001b[32m0.1815\u001b[0m       0.4375            0.4375        1.7244  0.0006  0.1615\n",
      "     44            0.9143        0.2489       0.4271            0.4271        1.7453  0.0006  0.2146\n",
      "     45            0.9143        \u001b[32m0.1658\u001b[0m       0.4167            0.4167        1.7539  0.0005  0.1367\n",
      "     46            0.9143        \u001b[32m0.1399\u001b[0m       0.4062            0.4062        1.7511  0.0005  0.1363\n",
      "     47            \u001b[36m0.9429\u001b[0m        0.1753       0.4062            0.4062        1.7399  0.0004  0.1320\n",
      "     48            \u001b[36m0.9714\u001b[0m        \u001b[32m0.1278\u001b[0m       0.4062            0.4062        1.7231  0.0004  0.1369\n",
      "     49            \u001b[36m1.0000\u001b[0m        0.1339       0.4271            0.4271        1.7018  0.0003  0.1327\n",
      "     50            1.0000        \u001b[32m0.1226\u001b[0m       0.4271            0.4271        1.6789  0.0003  0.1399\n",
      "Fine tuning model for subject 4 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4857        1.5834       0.5000            0.5000        1.3900  0.0004  0.1442\n",
      "     32            0.5143        1.3293       0.5000            0.5000        1.3799  0.0005  0.1342\n",
      "     33            0.6571        1.5539       0.4896            0.4896        1.3716  0.0005  0.1362\n",
      "     34            0.7429        1.0670       0.4792            0.4792        1.3732  0.0006  0.1429\n",
      "     35            0.8000        0.7711       0.4479            0.4479        1.3883  0.0006  0.1270\n",
      "     36            0.7429        0.7386       0.4375            0.4375        1.4072  0.0007  0.1328\n",
      "     37            0.7714        \u001b[32m0.6181\u001b[0m       0.3958            0.3958        1.4205  0.0007  0.1367\n",
      "     38            \u001b[36m0.8571\u001b[0m        \u001b[32m0.4905\u001b[0m       0.4062            0.4062        1.4265  0.0007  0.1340\n",
      "     39            \u001b[36m0.9429\u001b[0m        \u001b[32m0.2784\u001b[0m       0.4271            0.4271        1.4249  0.0007  0.1378\n",
      "     40            \u001b[36m1.0000\u001b[0m        0.3054       0.4271            0.4271        1.4188  0.0007  0.1351\n",
      "     41            1.0000        \u001b[32m0.2715\u001b[0m       0.4479            0.4479        1.4135  0.0007  0.1325\n",
      "     42            1.0000        \u001b[32m0.2036\u001b[0m       0.4375            0.4375        1.4102  0.0007  0.1387\n",
      "     43            1.0000        0.2485       0.4375            0.4375        1.4059  0.0006  0.1360\n",
      "     44            1.0000        \u001b[32m0.2006\u001b[0m       0.4688            0.4688        1.4015  0.0006  0.1337\n",
      "     45            1.0000        \u001b[32m0.1753\u001b[0m       0.5000            0.5000        1.3918  0.0005  0.1366\n",
      "     46            1.0000        \u001b[32m0.1159\u001b[0m       0.5104            0.5104        1.3814  0.0005  0.1414\n",
      "     47            1.0000        \u001b[32m0.1101\u001b[0m       0.5000            0.5000        1.3695  0.0004  0.1321\n",
      "     48            1.0000        \u001b[32m0.1018\u001b[0m       0.5000            0.5000        1.3561  0.0004  0.1328\n",
      "     49            1.0000        0.1384       0.5312            0.5312        1.3425  0.0003  0.1338\n",
      "     50            1.0000        \u001b[32m0.0837\u001b[0m       0.5312            0.5312        1.3293  0.0003  0.1338\n",
      "Fine tuning model for subject 4 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3714        1.9592       0.4896            0.4896        1.3961  0.0004  0.1361\n",
      "     32            0.4000        1.5009       0.4792            0.4792        1.4104  0.0005  0.1337\n",
      "     33            0.4286        1.3876       0.4688            0.4688        1.4409  0.0005  0.1356\n",
      "     34            0.5714        1.3004       0.4375            0.4375        1.4844  0.0006  0.1236\n",
      "     35            0.6571        1.0722       0.4167            0.4167        1.5455  0.0006  0.1317\n",
      "     36            0.6857        0.7133       0.4271            0.4271        1.6125  0.0007  0.1522\n",
      "     37            0.7429        \u001b[32m0.5405\u001b[0m       0.4167            0.4167        1.6690  0.0007  0.1370\n",
      "     38            0.8000        0.5642       0.4062            0.4062        1.7235  0.0007  0.1599\n",
      "     39            \u001b[36m0.8571\u001b[0m        \u001b[32m0.3897\u001b[0m       0.3750            0.3750        1.7851  0.0007  0.1349\n",
      "     40            \u001b[36m0.8857\u001b[0m        \u001b[32m0.2859\u001b[0m       0.3333            0.3333        1.8512  0.0007  0.1329\n",
      "     41            0.8857        0.3497       0.3125            0.3125        1.9103  0.0007  0.1333\n",
      "     42            0.8857        \u001b[32m0.2648\u001b[0m       0.3125            0.3125        1.9458  0.0007  0.1331\n",
      "     43            \u001b[36m0.9143\u001b[0m        \u001b[32m0.2185\u001b[0m       0.3021            0.3021        1.9555  0.0006  0.1336\n",
      "     44            \u001b[36m0.9429\u001b[0m        \u001b[32m0.1445\u001b[0m       0.3021            0.3021        1.9482  0.0006  0.1401\n",
      "     45            0.9429        0.1668       0.3021            0.3021        1.9307  0.0005  0.1370\n",
      "     46            \u001b[36m0.9714\u001b[0m        0.1697       0.3229            0.3229        1.8942  0.0005  0.1324\n",
      "     47            0.9714        \u001b[32m0.1398\u001b[0m       0.3021            0.3021        1.8528  0.0004  0.1387\n",
      "     48            0.9714        0.1537       0.3021            0.3021        1.8069  0.0004  0.1315\n",
      "     49            0.9714        0.1481       0.3021            0.3021        1.7647  0.0003  0.1364\n",
      "     50            0.9714        0.1649       0.3333            0.3333        1.7234  0.0003  0.1415\n",
      "Fine tuning model for subject 4 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3143        1.6782       0.5000            0.5000        1.4052  0.0004  0.1373\n",
      "     32            0.3714        1.6769       0.4792            0.4792        1.4211  0.0005  0.1356\n",
      "     33            0.4857        1.6055       0.5208            0.5208        1.4485  0.0005  0.1315\n",
      "     34            0.5714        1.3923       0.5104            0.5104        1.4991  0.0006  0.1366\n",
      "     35            0.6000        1.0105       0.4583            0.4583        1.5650  0.0006  0.1338\n",
      "     36            0.7143        0.8727       0.4479            0.4479        1.6516  0.0007  0.1375\n",
      "     37            0.7143        \u001b[32m0.6734\u001b[0m       0.4375            0.4375        1.7741  0.0007  0.1408\n",
      "     38            0.7143        \u001b[32m0.4957\u001b[0m       0.4062            0.4062        1.9160  0.0007  0.1431\n",
      "     39            0.7143        \u001b[32m0.4886\u001b[0m       0.4062            0.4062        2.0626  0.0007  0.1339\n",
      "     40            0.7143        \u001b[32m0.3722\u001b[0m       0.3958            0.3958        2.1953  0.0007  0.1281\n",
      "     41            0.7143        \u001b[32m0.2931\u001b[0m       0.3750            0.3750        2.2972  0.0007  0.1425\n",
      "     42            0.7143        0.3023       0.3750            0.3750        2.3674  0.0007  0.1368\n",
      "     43            0.7143        \u001b[32m0.2723\u001b[0m       0.3646            0.3646        2.4036  0.0006  0.1339\n",
      "     44            0.7143        \u001b[32m0.2049\u001b[0m       0.3542            0.3542        2.4025  0.0006  0.1327\n",
      "     45            0.7429        \u001b[32m0.1835\u001b[0m       0.3542            0.3542        2.3735  0.0005  0.1335\n",
      "     46            0.7429        \u001b[32m0.1582\u001b[0m       0.3750            0.3750        2.3212  0.0005  0.1373\n",
      "     47            0.7714        \u001b[32m0.1143\u001b[0m       0.3646            0.3646        2.2522  0.0004  0.1340\n",
      "     48            0.8000        0.1618       0.3646            0.3646        2.1703  0.0004  0.1331\n",
      "     49            \u001b[36m0.8286\u001b[0m        0.1492       0.3438            0.3438        2.0858  0.0003  0.1357\n",
      "     50            \u001b[36m0.9143\u001b[0m        0.1569       0.3438            0.3438        2.0003  0.0003  0.1345\n",
      "Fine tuning model for subject 4 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5429        1.1657       0.5000            0.5000        1.4092  0.0004  0.1345\n",
      "     32            0.5714        1.2054       0.5312            0.5312        1.4295  0.0005  0.1336\n",
      "     33            0.6286        1.1776       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.4582  0.0005  0.1292\n",
      "     34            0.7429        1.0535       0.5417            0.5417        1.5013  0.0006  0.1345\n",
      "     35            0.7714        0.7834       0.5000            0.5000        1.5659  0.0006  0.1336\n",
      "     36            0.8000        0.7106       0.4792            0.4792        1.6323  0.0007  0.1355\n",
      "     37            \u001b[36m0.8571\u001b[0m        \u001b[32m0.4841\u001b[0m       0.4688            0.4688        1.6992  0.0007  0.1373\n",
      "     38            0.8571        \u001b[32m0.4008\u001b[0m       0.4896            0.4896        1.7393  0.0007  0.1384\n",
      "     39            0.8571        \u001b[32m0.3250\u001b[0m       0.4479            0.4479        1.7662  0.0007  0.1539\n",
      "     40            \u001b[36m0.9143\u001b[0m        \u001b[32m0.2636\u001b[0m       0.4688            0.4688        1.7794  0.0007  0.1467\n",
      "     41            \u001b[36m0.9429\u001b[0m        \u001b[32m0.2246\u001b[0m       0.4688            0.4688        1.7877  0.0007  0.1734\n",
      "     42            0.9429        \u001b[32m0.2037\u001b[0m       0.4583            0.4583        1.7898  0.0007  0.2043\n",
      "     43            0.9429        \u001b[32m0.1789\u001b[0m       0.4479            0.4479        1.7821  0.0006  0.1597\n",
      "     44            0.9429        \u001b[32m0.1473\u001b[0m       0.4271            0.4271        1.7675  0.0006  0.1918\n",
      "     45            0.9429        \u001b[32m0.1304\u001b[0m       0.4167            0.4167        1.7459  0.0005  0.1566\n",
      "     46            0.9429        \u001b[32m0.1096\u001b[0m       0.4375            0.4375        1.7210  0.0005  0.1640\n",
      "     47            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1031\u001b[0m       0.4375            0.4375        1.6970  0.0004  0.1830\n",
      "     48            1.0000        \u001b[32m0.1012\u001b[0m       0.4271            0.4271        1.6718  0.0004  0.1549\n",
      "     49            1.0000        0.1319       0.4375            0.4375        1.6472  0.0003  0.1526\n",
      "     50            1.0000        \u001b[32m0.1004\u001b[0m       0.4479            0.4479        1.6237  0.0003  0.1657\n",
      "Fine tuning model for subject 4 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4286        1.5601       0.5104            0.5104        1.3949  0.0004  0.1640\n",
      "     32            0.4857        1.6032       0.5208            0.5208        1.4057  0.0005  0.1955\n",
      "     33            0.6571        1.2784       0.5312            0.5312        1.4225  0.0005  0.2039\n",
      "     34            0.7429        1.1487       0.5000            0.5000        1.4509  0.0006  0.1881\n",
      "     35            0.7714        0.8380       0.5000            0.5000        1.4800  0.0006  0.1333\n",
      "     36            0.8000        \u001b[32m0.6187\u001b[0m       0.4896            0.4896        1.4847  0.0007  0.1340\n",
      "     37            \u001b[36m0.8571\u001b[0m        \u001b[32m0.4621\u001b[0m       0.5000            0.5000        1.4779  0.0007  0.1315\n",
      "     38            \u001b[36m0.8857\u001b[0m        \u001b[32m0.4270\u001b[0m       0.5312            0.5312        1.4628  0.0007  0.1473\n",
      "     39            0.8857        \u001b[32m0.2930\u001b[0m       0.5312            0.5312        1.4502  0.0007  0.1332\n",
      "     40            \u001b[36m0.9143\u001b[0m        \u001b[32m0.2177\u001b[0m       0.5000            0.5000        1.4431  0.0007  0.1313\n",
      "     41            \u001b[36m0.9429\u001b[0m        0.2318       0.5000            0.5000        1.4483  0.0007  0.1404\n",
      "     42            0.9429        \u001b[32m0.1854\u001b[0m       0.4583            0.4583        1.4598  0.0007  0.1386\n",
      "     43            0.9429        0.1885       0.4375            0.4375        1.4720  0.0006  0.1330\n",
      "     44            0.9429        0.2097       0.4375            0.4375        1.4810  0.0006  0.1361\n",
      "     45            0.9429        \u001b[32m0.1624\u001b[0m       0.4479            0.4479        1.4789  0.0005  0.1429\n",
      "     46            0.9429        \u001b[32m0.1129\u001b[0m       0.4583            0.4583        1.4667  0.0005  0.1344\n",
      "     47            0.9429        0.1502       0.4792            0.4792        1.4510  0.0004  0.1326\n",
      "     48            \u001b[36m0.9714\u001b[0m        \u001b[32m0.0958\u001b[0m       0.4792            0.4792        1.4298  0.0004  0.1291\n",
      "     49            \u001b[36m1.0000\u001b[0m        0.0998       0.5104            0.5104        1.4069  0.0003  0.1356\n",
      "     50            1.0000        \u001b[32m0.0918\u001b[0m       0.5104            0.5104        1.3843  0.0003  0.1333\n",
      "Fine tuning model for subject 4 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4286        1.5489       0.5104            0.5104        1.3983  0.0004  0.1386\n",
      "     32            0.4857        1.3976       0.5312            0.5312        1.4059  0.0005  0.1378\n",
      "     33            0.5714        1.2379       0.5312            0.5312        1.4100  0.0005  0.1386\n",
      "     34            0.6571        1.2216       0.5000            0.5000        1.4055  0.0006  0.1545\n",
      "     35            \u001b[36m0.8286\u001b[0m        0.9695       0.4688            0.4688        1.4006  0.0006  0.1798\n",
      "     36            \u001b[36m0.9143\u001b[0m        0.7677       0.4792            0.4792        1.3995  0.0007  0.1496\n",
      "     37            \u001b[36m0.9429\u001b[0m        \u001b[32m0.6506\u001b[0m       0.4896            0.4896        1.4083  0.0007  0.1392\n",
      "     38            \u001b[36m0.9714\u001b[0m        \u001b[32m0.4065\u001b[0m       0.4583            0.4583        1.4327  0.0007  0.1364\n",
      "     39            0.9714        \u001b[32m0.3351\u001b[0m       0.4062            0.4062        1.4729  0.0007  0.1604\n",
      "     40            0.9429        \u001b[32m0.2909\u001b[0m       0.3750            0.3750        1.5245  0.0007  0.1291\n",
      "     41            0.9429        \u001b[32m0.2428\u001b[0m       0.3542            0.3542        1.5766  0.0007  0.1407\n",
      "     42            0.9429        \u001b[32m0.1952\u001b[0m       0.3125            0.3125        1.6171  0.0007  0.1364\n",
      "     43            0.9714        0.2180       0.3125            0.3125        1.6397  0.0006  0.1363\n",
      "     44            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1813\u001b[0m       0.3125            0.3125        1.6542  0.0006  0.1338\n",
      "     45            1.0000        \u001b[32m0.1342\u001b[0m       0.3021            0.3021        1.6603  0.0005  0.1364\n",
      "     46            1.0000        0.1622       0.2917            0.2917        1.6567  0.0005  0.1333\n",
      "     47            1.0000        0.1366       0.3021            0.3021        1.6485  0.0004  0.1330\n",
      "     48            1.0000        \u001b[32m0.1203\u001b[0m       0.3438            0.3438        1.6371  0.0004  0.1515\n",
      "     49            1.0000        \u001b[32m0.1150\u001b[0m       0.3542            0.3542        1.6226  0.0003  0.1782\n",
      "     50            1.0000        \u001b[32m0.0909\u001b[0m       0.3750            0.3750        1.6065  0.0003  0.1345\n",
      "Fine tuning model for subject 4 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.1838       0.4896            0.4896        1.4040  0.0004  0.1420\n",
      "     32            0.5000        1.1921       0.4896            0.4896        1.4158  0.0005  0.1478\n",
      "     33            0.6000        1.1351       0.5000            0.5000        1.4276  0.0005  0.1478\n",
      "     34            0.7500        0.9500       0.5104            0.5104        1.4478  0.0006  0.1433\n",
      "     35            0.7750        0.7235       0.5000            0.5000        1.4843  0.0006  0.1494\n",
      "     36            0.8000        0.7553       0.4896            0.4896        1.5334  0.0007  0.1434\n",
      "     37            \u001b[36m0.8250\u001b[0m        \u001b[32m0.5028\u001b[0m       0.4792            0.4792        1.6003  0.0007  0.1406\n",
      "     38            0.8250        \u001b[32m0.4152\u001b[0m       0.4583            0.4583        1.6722  0.0007  0.1441\n",
      "     39            0.8250        0.4954       0.4792            0.4792        1.7351  0.0007  0.1493\n",
      "     40            \u001b[36m0.8500\u001b[0m        \u001b[32m0.3297\u001b[0m       0.5000            0.5000        1.7876  0.0007  0.1473\n",
      "     41            0.8500        \u001b[32m0.3016\u001b[0m       0.5000            0.5000        1.8297  0.0007  0.1418\n",
      "     42            0.8500        \u001b[32m0.1947\u001b[0m       0.4896            0.4896        1.8618  0.0007  0.1440\n",
      "     43            \u001b[36m0.9000\u001b[0m        \u001b[32m0.1770\u001b[0m       0.5000            0.5000        1.8841  0.0006  0.1443\n",
      "     44            0.9000        0.1980       0.4896            0.4896        1.8974  0.0006  0.1492\n",
      "     45            \u001b[36m0.9250\u001b[0m        \u001b[32m0.1309\u001b[0m       0.4688            0.4688        1.8984  0.0005  0.1600\n",
      "     46            \u001b[36m0.9750\u001b[0m        \u001b[32m0.1298\u001b[0m       0.4479            0.4479        1.8912  0.0005  0.1448\n",
      "     47            0.9750        \u001b[32m0.1115\u001b[0m       0.4479            0.4479        1.8718  0.0004  0.1544\n",
      "     48            \u001b[36m1.0000\u001b[0m        0.1381       0.4375            0.4375        1.8447  0.0004  0.1792\n",
      "     49            1.0000        \u001b[32m0.0777\u001b[0m       0.4375            0.4375        1.8118  0.0003  0.1790\n",
      "     50            1.0000        0.1066       0.4375            0.4375        1.7766  0.0003  0.1457\n",
      "Fine tuning model for subject 4 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4750        1.3418       0.4896            0.4896        1.3969  0.0004  0.1392\n",
      "     32            0.5000        1.1891       0.4792            0.4792        1.3951  0.0005  0.1533\n",
      "     33            0.5750        1.1314       0.4479            0.4479        1.3845  0.0005  0.1493\n",
      "     34            0.7000        0.9981       0.4688            0.4688        1.3666  0.0006  0.1432\n",
      "     35            0.7250        0.9501       0.4375            0.4375        1.3414  0.0006  0.1485\n",
      "     36            \u001b[36m0.8500\u001b[0m        0.7091       0.4583            0.4583        1.3206  0.0007  0.1537\n",
      "     37            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5387\u001b[0m       0.4688            0.4688        1.3113  0.0007  0.1492\n",
      "     38            0.9500        \u001b[32m0.4444\u001b[0m       0.5000            0.5000        1.3095  0.0007  0.1456\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3522\u001b[0m       0.4375            0.4375        1.3125  0.0007  0.1571\n",
      "     40            1.0000        \u001b[32m0.3510\u001b[0m       0.4583            0.4583        1.3202  0.0007  0.1463\n",
      "     41            1.0000        \u001b[32m0.3099\u001b[0m       0.4479            0.4479        1.3289  0.0007  0.1428\n",
      "     42            1.0000        \u001b[32m0.1878\u001b[0m       0.4375            0.4375        1.3370  0.0007  0.1469\n",
      "     43            1.0000        0.2014       0.4271            0.4271        1.3464  0.0006  0.1439\n",
      "     44            1.0000        \u001b[32m0.1708\u001b[0m       0.4375            0.4375        1.3558  0.0006  0.1431\n",
      "     45            1.0000        \u001b[32m0.1703\u001b[0m       0.4583            0.4583        1.3623  0.0005  0.1489\n",
      "     46            1.0000        \u001b[32m0.1510\u001b[0m       0.4583            0.4583        1.3666  0.0005  0.1873\n",
      "     47            1.0000        \u001b[32m0.1389\u001b[0m       0.4583            0.4583        1.3673  0.0004  0.1788\n",
      "     48            1.0000        0.1397       0.4479            0.4479        1.3662  0.0004  0.1740\n",
      "     49            1.0000        \u001b[32m0.1018\u001b[0m       0.4479            0.4479        1.3643  0.0003  0.1751\n",
      "     50            1.0000        0.1394       0.4479            0.4479        1.3616  0.0003  0.1869\n",
      "Fine tuning model for subject 4 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.4068       0.5104            0.5104        1.4025  0.0004  0.1799\n",
      "     32            0.5000        1.3258       0.5104            0.5104        1.4114  0.0005  0.1706\n",
      "     33            0.5000        1.3718       0.5000            0.5000        1.4186  0.0005  0.2098\n",
      "     34            0.5750        1.1678       0.5000            0.5000        1.4355  0.0006  0.1785\n",
      "     35            0.6000        0.8354       0.4792            0.4792        1.4573  0.0006  0.1571\n",
      "     36            0.7500        0.8939       0.4792            0.4792        1.4683  0.0007  0.1656\n",
      "     37            \u001b[36m0.8500\u001b[0m        \u001b[32m0.5473\u001b[0m       0.4479            0.4479        1.4951  0.0007  0.1558\n",
      "     38            \u001b[36m0.9250\u001b[0m        \u001b[32m0.4577\u001b[0m       0.4375            0.4375        1.5497  0.0007  0.1984\n",
      "     39            \u001b[36m0.9500\u001b[0m        \u001b[32m0.3857\u001b[0m       0.4271            0.4271        1.6293  0.0007  0.1708\n",
      "     40            0.9500        0.4326       0.3854            0.3854        1.7023  0.0007  0.2243\n",
      "     41            0.9500        \u001b[32m0.2778\u001b[0m       0.3542            0.3542        1.7730  0.0007  0.1815\n",
      "     42            0.9250        0.3184       0.3333            0.3333        1.8138  0.0007  0.1542\n",
      "     43            0.9500        0.3020       0.3542            0.3542        1.8323  0.0006  0.1500\n",
      "     44            0.9500        \u001b[32m0.2191\u001b[0m       0.3750            0.3750        1.8302  0.0006  0.1518\n",
      "     45            \u001b[36m0.9750\u001b[0m        \u001b[32m0.1991\u001b[0m       0.3750            0.3750        1.8103  0.0005  0.1495\n",
      "     46            0.9750        \u001b[32m0.1940\u001b[0m       0.3750            0.3750        1.7726  0.0005  0.1474\n",
      "     47            \u001b[36m1.0000\u001b[0m        0.2271       0.3750            0.3750        1.7310  0.0004  0.1483\n",
      "     48            1.0000        \u001b[32m0.1336\u001b[0m       0.3958            0.3958        1.6923  0.0004  0.1474\n",
      "     49            1.0000        \u001b[32m0.1330\u001b[0m       0.3958            0.3958        1.6531  0.0003  0.1577\n",
      "     50            1.0000        0.1376       0.3958            0.3958        1.6179  0.0003  0.1452\n",
      "Fine tuning model for subject 4 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4500        1.5440       0.4896            0.4896        1.3870  0.0004  0.1427\n",
      "     32            0.4750        1.2637       0.5000            0.5000        1.3718  0.0005  0.1392\n",
      "     33            0.5750        1.3851       0.5104            0.5104        1.3406  0.0005  0.1429\n",
      "     34            0.6750        0.9180       0.5000            0.5000        1.3079  0.0006  0.1475\n",
      "     35            0.7750        0.7605       0.5104            0.5104        1.2769  0.0006  0.1441\n",
      "     36            \u001b[36m0.8750\u001b[0m        0.6949       0.4896            0.4896        1.2525  0.0007  0.1483\n",
      "     37            \u001b[36m0.9250\u001b[0m        \u001b[32m0.4145\u001b[0m       0.5104            0.5104        1.2454  0.0007  0.1458\n",
      "     38            \u001b[36m0.9500\u001b[0m        \u001b[32m0.3421\u001b[0m       0.5208            0.5208        1.2564  0.0007  0.1445\n",
      "     39            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2977\u001b[0m       0.4896            0.4896        1.2794  0.0007  0.1447\n",
      "     40            0.9750        \u001b[32m0.2586\u001b[0m       0.4896            0.4896        1.3133  0.0007  0.1426\n",
      "     41            0.9750        0.2792       0.4896            0.4896        1.3565  0.0007  0.1392\n",
      "     42            0.9750        \u001b[32m0.2299\u001b[0m       0.4792            0.4792        1.4007  0.0007  0.1457\n",
      "     43            0.9750        \u001b[32m0.2100\u001b[0m       0.4792            0.4792        1.4384  0.0006  0.1412\n",
      "     44            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1895\u001b[0m       0.4583            0.4583        1.4675  0.0006  0.1457\n",
      "     45            1.0000        0.2287       0.4583            0.4583        1.4904  0.0005  0.1520\n",
      "     46            1.0000        0.2045       0.4479            0.4479        1.5070  0.0005  0.1482\n",
      "     47            1.0000        \u001b[32m0.1501\u001b[0m       0.4583            0.4583        1.5124  0.0004  0.1396\n",
      "     48            1.0000        0.1550       0.4688            0.4688        1.5113  0.0004  0.1486\n",
      "     49            1.0000        \u001b[32m0.1268\u001b[0m       0.4688            0.4688        1.5027  0.0003  0.1490\n",
      "     50            1.0000        \u001b[32m0.1231\u001b[0m       0.4688            0.4688        1.4897  0.0003  0.1458\n",
      "Fine tuning model for subject 4 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3250        1.5071       0.4896            0.4896        1.3942  0.0004  0.1455\n",
      "     32            0.4750        1.4605       0.4688            0.4688        1.3836  0.0005  0.1626\n",
      "     33            0.5750        1.5366       0.4688            0.4688        1.3765  0.0005  0.1528\n",
      "     34            0.7000        1.2145       0.4792            0.4792        1.3961  0.0006  0.1445\n",
      "     35            \u001b[36m0.8250\u001b[0m        0.9452       0.4479            0.4479        1.4564  0.0006  0.1474\n",
      "     36            0.8250        0.9188       0.4167            0.4167        1.5801  0.0007  0.1418\n",
      "     37            \u001b[36m0.8500\u001b[0m        \u001b[32m0.6236\u001b[0m       0.3750            0.3750        1.7395  0.0007  0.1430\n",
      "     38            0.7750        \u001b[32m0.5001\u001b[0m       0.3438            0.3438        1.8955  0.0007  0.1417\n",
      "     39            0.8000        \u001b[32m0.4379\u001b[0m       0.3438            0.3438        2.0289  0.0007  0.1492\n",
      "     40            0.7750        \u001b[32m0.3665\u001b[0m       0.3229            0.3229        2.1286  0.0007  0.1436\n",
      "     41            0.8250        0.4031       0.3229            0.3229        2.1924  0.0007  0.1471\n",
      "     42            0.8250        \u001b[32m0.3072\u001b[0m       0.3125            0.3125        2.2227  0.0007  0.1450\n",
      "     43            0.8250        \u001b[32m0.2699\u001b[0m       0.3333            0.3333        2.2175  0.0006  0.1482\n",
      "     44            \u001b[36m0.8750\u001b[0m        \u001b[32m0.2100\u001b[0m       0.3229            0.3229        2.1886  0.0006  0.1427\n",
      "     45            \u001b[36m0.9000\u001b[0m        \u001b[32m0.1803\u001b[0m       0.3229            0.3229        2.1460  0.0005  0.1461\n",
      "     46            \u001b[36m0.9250\u001b[0m        0.2125       0.3229            0.3229        2.0932  0.0005  0.1379\n",
      "     47            \u001b[36m0.9750\u001b[0m        0.2259       0.3438            0.3438        2.0373  0.0004  0.1462\n",
      "     48            \u001b[36m1.0000\u001b[0m        0.1844       0.3333            0.3333        1.9831  0.0004  0.1433\n",
      "     49            1.0000        0.1844       0.3229            0.3229        1.9348  0.0003  0.1493\n",
      "     50            1.0000        \u001b[32m0.1281\u001b[0m       0.3125            0.3125        1.8932  0.0003  0.1443\n",
      "Fine tuning model for subject 4 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3500        1.6806       0.4896            0.4896        1.3836  0.0004  0.1582\n",
      "     32            0.3750        1.9115       0.4583            0.4583        1.3531  0.0005  0.1456\n",
      "     33            0.4750        1.3448       0.4792            0.4792        1.3098  0.0005  0.1485\n",
      "     34            0.6500        1.2153       0.4375            0.4375        1.2660  0.0006  0.1493\n",
      "     35            0.6750        1.0459       0.4479            0.4479        1.2440  0.0006  0.1455\n",
      "     36            0.7000        0.8577       0.4896            0.4896        1.2571  0.0007  0.1412\n",
      "     37            0.7000        0.7081       0.4583            0.4583        1.3005  0.0007  0.1493\n",
      "     38            0.7250        \u001b[32m0.5818\u001b[0m       0.4375            0.4375        1.3393  0.0007  0.1435\n",
      "     39            0.7250        \u001b[32m0.4079\u001b[0m       0.4792            0.4792        1.3608  0.0007  0.1436\n",
      "     40            \u001b[36m0.8250\u001b[0m        0.4093       0.4792            0.4792        1.3694  0.0007  0.1441\n",
      "     41            \u001b[36m0.8750\u001b[0m        \u001b[32m0.2563\u001b[0m       0.4792            0.4792        1.3640  0.0007  0.1444\n",
      "     42            0.8750        0.3580       0.4688            0.4688        1.3518  0.0007  0.1516\n",
      "     43            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2193\u001b[0m       0.4792            0.4792        1.3347  0.0006  0.1442\n",
      "     44            \u001b[36m0.9750\u001b[0m        0.2373       0.5208            0.5208        1.3136  0.0006  0.1450\n",
      "     45            0.9750        \u001b[32m0.2139\u001b[0m       0.5000            0.5000        1.2911  0.0005  0.1433\n",
      "     46            0.9750        \u001b[32m0.1752\u001b[0m       0.5104            0.5104        1.2738  0.0005  0.1448\n",
      "     47            \u001b[36m1.0000\u001b[0m        0.1807       0.5208            0.5208        1.2594  0.0004  0.1439\n",
      "     48            1.0000        0.1782       0.5312            0.5312        1.2493  0.0004  0.1427\n",
      "     49            1.0000        \u001b[32m0.1009\u001b[0m       0.5417            0.5417        1.2420  0.0003  0.1475\n",
      "     50            1.0000        0.1184       0.5312            0.5312        1.2372  0.0003  0.1658\n",
      "Fine tuning model for subject 4 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4750        1.5076       0.5104            0.5104        1.4002  0.0004  0.1626\n",
      "     32            0.5000        1.4648       0.5104            0.5104        1.4002  0.0005  0.2353\n",
      "     33            0.5750        1.3439       0.5208            0.5208        1.4021  0.0005  0.1651\n",
      "     34            0.7250        1.2031       0.5104            0.5104        1.4002  0.0006  0.2190\n",
      "     35            0.7750        0.9578       0.4583            0.4583        1.3916  0.0006  0.1725\n",
      "     36            0.8000        0.8665       0.4688            0.4688        1.3722  0.0007  0.1632\n",
      "     37            \u001b[36m0.8500\u001b[0m        \u001b[32m0.6427\u001b[0m       0.4792            0.4792        1.3599  0.0007  0.1762\n",
      "     38            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5903\u001b[0m       0.4896            0.4896        1.3540  0.0007  0.1656\n",
      "     39            \u001b[36m0.9750\u001b[0m        \u001b[32m0.5493\u001b[0m       0.5208            0.5208        1.3518  0.0007  0.1785\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3910\u001b[0m       0.5104            0.5104        1.3594  0.0007  0.1752\n",
      "     41            1.0000        0.4016       0.4479            0.4479        1.3751  0.0007  0.1689\n",
      "     42            1.0000        \u001b[32m0.3015\u001b[0m       0.4167            0.4167        1.3934  0.0007  0.2390\n",
      "     43            1.0000        \u001b[32m0.2527\u001b[0m       0.4375            0.4375        1.4109  0.0006  0.2001\n",
      "     44            1.0000        \u001b[32m0.2197\u001b[0m       0.4479            0.4479        1.4252  0.0006  0.2198\n",
      "     45            1.0000        \u001b[32m0.1941\u001b[0m       0.4479            0.4479        1.4340  0.0005  0.1709\n",
      "     46            1.0000        0.1984       0.4479            0.4479        1.4388  0.0005  0.1429\n",
      "     47            1.0000        \u001b[32m0.1806\u001b[0m       0.4583            0.4583        1.4380  0.0004  0.1497\n",
      "     48            1.0000        \u001b[32m0.1540\u001b[0m       0.4583            0.4583        1.4321  0.0004  0.1431\n",
      "     49            1.0000        \u001b[32m0.1374\u001b[0m       0.4583            0.4583        1.4239  0.0003  0.1432\n",
      "     50            1.0000        \u001b[32m0.1369\u001b[0m       0.4583            0.4583        1.4146  0.0003  0.1498\n",
      "Fine tuning model for subject 4 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4250        1.2928       0.4792            0.4792        1.4011  0.0004  0.1521\n",
      "     32            0.4750        1.3200       0.4688            0.4688        1.4148  0.0005  0.1491\n",
      "     33            0.5750        1.1527       0.4792            0.4792        1.4387  0.0005  0.1493\n",
      "     34            0.6500        0.9861       0.4792            0.4792        1.4710  0.0006  0.1458\n",
      "     35            0.7500        0.8575       0.4792            0.4792        1.5136  0.0006  0.1492\n",
      "     36            \u001b[36m0.8250\u001b[0m        \u001b[32m0.6776\u001b[0m       0.4479            0.4479        1.5655  0.0007  0.1460\n",
      "     37            \u001b[36m0.9250\u001b[0m        \u001b[32m0.5709\u001b[0m       0.4062            0.4062        1.6203  0.0007  0.1527\n",
      "     38            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5018\u001b[0m       0.3958            0.3958        1.6711  0.0007  0.1455\n",
      "     39            0.9000        \u001b[32m0.3687\u001b[0m       0.3646            0.3646        1.7227  0.0007  0.1470\n",
      "     40            0.9000        \u001b[32m0.3591\u001b[0m       0.3646            0.3646        1.7678  0.0007  0.1444\n",
      "     41            0.9000        \u001b[32m0.2564\u001b[0m       0.3542            0.3542        1.8011  0.0007  0.1445\n",
      "     42            0.9250        0.2944       0.3542            0.3542        1.8066  0.0007  0.1361\n",
      "     43            0.9500        \u001b[32m0.2470\u001b[0m       0.3438            0.3438        1.7847  0.0006  0.1431\n",
      "     44            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2249\u001b[0m       0.3542            0.3542        1.7559  0.0006  0.1428\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1917\u001b[0m       0.3646            0.3646        1.7198  0.0005  0.1414\n",
      "     46            1.0000        \u001b[32m0.1579\u001b[0m       0.3958            0.3958        1.6835  0.0005  0.1436\n",
      "     47            1.0000        \u001b[32m0.1403\u001b[0m       0.4062            0.4062        1.6488  0.0004  0.1380\n",
      "     48            1.0000        0.1779       0.4271            0.4271        1.6192  0.0004  0.1484\n",
      "     49            1.0000        0.1509       0.4271            0.4271        1.5946  0.0003  0.1451\n",
      "     50            1.0000        \u001b[32m0.1215\u001b[0m       0.4271            0.4271        1.5753  0.0003  0.1427\n",
      "Fine tuning model for subject 4 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4750        1.4992       0.4896            0.4896        1.3925  0.0004  0.1414\n",
      "     32            0.4500        1.4522       0.4583            0.4583        1.3862  0.0005  0.1458\n",
      "     33            0.5750        1.3917       0.4479            0.4479        1.3803  0.0005  0.1493\n",
      "     34            0.7250        1.0761       0.4479            0.4479        1.3751  0.0006  0.1487\n",
      "     35            0.7750        0.9701       0.4479            0.4479        1.3687  0.0006  0.1491\n",
      "     36            \u001b[36m0.9000\u001b[0m        0.8059       0.4375            0.4375        1.3680  0.0007  0.1516\n",
      "     37            \u001b[36m0.9250\u001b[0m        \u001b[32m0.5680\u001b[0m       0.4271            0.4271        1.3740  0.0007  0.1495\n",
      "     38            0.9250        \u001b[32m0.5005\u001b[0m       0.4375            0.4375        1.4031  0.0007  0.1490\n",
      "     39            0.9250        \u001b[32m0.4235\u001b[0m       0.4479            0.4479        1.4563  0.0007  0.1461\n",
      "     40            0.9250        0.4433       0.4271            0.4271        1.5286  0.0007  0.1401\n",
      "     41            0.9000        \u001b[32m0.2895\u001b[0m       0.4271            0.4271        1.6058  0.0007  0.1456\n",
      "     42            0.9000        \u001b[32m0.2711\u001b[0m       0.3750            0.3750        1.6760  0.0007  0.1423\n",
      "     43            0.9000        \u001b[32m0.2100\u001b[0m       0.3646            0.3646        1.7334  0.0006  0.1408\n",
      "     44            0.9000        0.2342       0.3333            0.3333        1.7635  0.0006  0.1409\n",
      "     45            0.9250        0.2310       0.3333            0.3333        1.7718  0.0005  0.1497\n",
      "     46            \u001b[36m0.9500\u001b[0m        0.2411       0.3542            0.3542        1.7657  0.0005  0.1407\n",
      "     47            \u001b[36m0.9750\u001b[0m        \u001b[32m0.1596\u001b[0m       0.3646            0.3646        1.7481  0.0004  0.1426\n",
      "     48            \u001b[36m1.0000\u001b[0m        0.1830       0.3854            0.3854        1.7231  0.0004  0.1496\n",
      "     49            1.0000        \u001b[32m0.1532\u001b[0m       0.3854            0.3854        1.6942  0.0003  0.1436\n",
      "     50            1.0000        \u001b[32m0.1412\u001b[0m       0.3750            0.3750        1.6636  0.0003  0.1487\n",
      "Fine tuning model for subject 4 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4750        1.7069       0.5208            0.5208        1.3846  0.0004  0.1369\n",
      "     32            0.5250        1.3693       0.5104            0.5104        1.3639  0.0005  0.1428\n",
      "     33            0.6500        1.2811       0.5417            0.5417        1.3464  0.0005  0.1478\n",
      "     34            0.7750        1.1682       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.3374  0.0006  0.1527\n",
      "     35            0.8000        0.7614       0.5521            0.5521        1.3358  0.0006  0.1544\n",
      "     36            \u001b[36m0.8500\u001b[0m        0.7170       0.5417            0.5417        1.3348  0.0007  0.1574\n",
      "     37            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5216\u001b[0m       0.5208            0.5208        1.3245  0.0007  0.1416\n",
      "     38            0.9500        \u001b[32m0.4060\u001b[0m       0.5417            0.5417        1.3067  0.0007  0.1467\n",
      "     39            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3662\u001b[0m       0.5104            0.5104        1.2875  0.0007  0.1409\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2560\u001b[0m       0.4583            0.4583        1.2861  0.0007  0.1477\n",
      "     41            1.0000        \u001b[32m0.2491\u001b[0m       0.4583            0.4583        1.2995  0.0007  0.1499\n",
      "     42            1.0000        \u001b[32m0.2288\u001b[0m       0.4375            0.4375        1.3194  0.0007  0.1500\n",
      "     43            1.0000        \u001b[32m0.1801\u001b[0m       0.4688            0.4688        1.3435  0.0006  0.1427\n",
      "     44            1.0000        \u001b[32m0.1713\u001b[0m       0.4688            0.4688        1.3659  0.0006  0.1380\n",
      "     45            1.0000        \u001b[32m0.1510\u001b[0m       0.4896            0.4896        1.3843  0.0005  0.1458\n",
      "     46            1.0000        \u001b[32m0.1305\u001b[0m       0.4792            0.4792        1.3967  0.0005  0.1442\n",
      "     47            1.0000        0.1916       0.4479            0.4479        1.4031  0.0004  0.1402\n",
      "     48            1.0000        0.1492       0.4688            0.4688        1.4036  0.0004  0.1461\n",
      "     49            1.0000        \u001b[32m0.1237\u001b[0m       0.4688            0.4688        1.4015  0.0003  0.1455\n",
      "     50            1.0000        \u001b[32m0.1012\u001b[0m       0.4792            0.4792        1.3971  0.0003  0.1480\n",
      "Fine tuning model for subject 4 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4444        1.6968       0.5000            0.5000        1.4064  0.0004  0.1497\n",
      "     32            0.4667        1.6134       0.4896            0.4896        1.4241  0.0005  0.1623\n",
      "     33            0.5333        1.4371       0.5104            0.5104        1.4472  0.0005  0.1625\n",
      "     34            0.6222        1.3041       0.5000            0.5000        1.4702  0.0006  0.1740\n",
      "     35            0.6222        1.1395       0.4688            0.4688        1.4813  0.0006  0.1891\n",
      "     36            0.7111        0.9793       0.4479            0.4479        1.4978  0.0007  0.1949\n",
      "     37            0.7556        0.7091       0.4375            0.4375        1.5009  0.0007  0.2146\n",
      "     38            \u001b[36m0.8222\u001b[0m        \u001b[32m0.6710\u001b[0m       0.4375            0.4375        1.4837  0.0007  0.1811\n",
      "     39            \u001b[36m0.8667\u001b[0m        \u001b[32m0.5262\u001b[0m       0.3958            0.3958        1.4628  0.0007  0.1856\n",
      "     40            \u001b[36m0.9111\u001b[0m        \u001b[32m0.4005\u001b[0m       0.4167            0.4167        1.4415  0.0007  0.1641\n",
      "     41            \u001b[36m0.9333\u001b[0m        0.4395       0.4271            0.4271        1.4169  0.0007  0.1840\n",
      "     42            \u001b[36m0.9556\u001b[0m        \u001b[32m0.3423\u001b[0m       0.4375            0.4375        1.3935  0.0007  0.1728\n",
      "     43            \u001b[36m0.9778\u001b[0m        0.3602       0.4688            0.4688        1.3775  0.0006  0.1810\n",
      "     44            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2828\u001b[0m       0.4479            0.4479        1.3657  0.0006  0.1860\n",
      "     45            1.0000        \u001b[32m0.2321\u001b[0m       0.4375            0.4375        1.3576  0.0005  0.2145\n",
      "     46            1.0000        0.2387       0.4271            0.4271        1.3535  0.0005  0.1797\n",
      "     47            1.0000        0.2809       0.4271            0.4271        1.3497  0.0004  0.2226\n",
      "     48            1.0000        0.2748       0.4271            0.4271        1.3448  0.0004  0.1514\n",
      "     49            1.0000        \u001b[32m0.2221\u001b[0m       0.4271            0.4271        1.3410  0.0003  0.1491\n",
      "     50            1.0000        \u001b[32m0.1667\u001b[0m       0.4167            0.4167        1.3372  0.0003  0.1681\n",
      "Fine tuning model for subject 4 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4667        1.5704       0.4896            0.4896        1.3937  0.0004  0.1546\n",
      "     32            0.4889        1.3292       0.5208            0.5208        1.3856  0.0005  0.1559\n",
      "     33            0.5556        1.2516       0.4896            0.4896        1.3611  0.0005  0.1681\n",
      "     34            0.7111        1.1660       0.4896            0.4896        1.3277  0.0006  0.1609\n",
      "     35            0.7778        0.9467       0.4896            0.4896        1.2863  0.0006  0.1755\n",
      "     36            0.8000        0.7483       0.4896            0.4896        1.2628  0.0007  0.1528\n",
      "     37            \u001b[36m0.8222\u001b[0m        \u001b[32m0.6213\u001b[0m       0.4583            0.4583        1.2590  0.0007  0.1495\n",
      "     38            \u001b[36m0.8444\u001b[0m        \u001b[32m0.5420\u001b[0m       0.4688            0.4688        1.2607  0.0007  0.1540\n",
      "     39            \u001b[36m0.8889\u001b[0m        \u001b[32m0.3906\u001b[0m       0.4792            0.4792        1.2612  0.0007  0.1567\n",
      "     40            \u001b[36m0.9556\u001b[0m        0.3945       0.4792            0.4792        1.2614  0.0007  0.1590\n",
      "     41            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3314\u001b[0m       0.5000            0.5000        1.2581  0.0007  0.1523\n",
      "     42            0.9778        \u001b[32m0.3120\u001b[0m       0.5208            0.5208        1.2557  0.0007  0.1490\n",
      "     43            0.9778        \u001b[32m0.2661\u001b[0m       0.5104            0.5104        1.2571  0.0006  0.1502\n",
      "     44            \u001b[36m1.0000\u001b[0m        0.2758       0.5000            0.5000        1.2602  0.0006  0.1526\n",
      "     45            1.0000        0.2989       0.5000            0.5000        1.2616  0.0005  0.1494\n",
      "     46            1.0000        \u001b[32m0.1741\u001b[0m       0.5000            0.5000        1.2624  0.0005  0.1552\n",
      "     47            1.0000        0.2445       0.4896            0.4896        1.2617  0.0004  0.1502\n",
      "     48            1.0000        0.2022       0.4688            0.4688        1.2595  0.0004  0.1554\n",
      "     49            1.0000        0.1968       0.4688            0.4688        1.2570  0.0003  0.1551\n",
      "     50            1.0000        0.1792       0.4896            0.4896        1.2540  0.0003  0.1587\n",
      "Fine tuning model for subject 4 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4222        1.6813       0.5000            0.5000        1.3935  0.0004  0.1461\n",
      "     32            0.4444        1.6916       0.5000            0.5000        1.3866  0.0005  0.1521\n",
      "     33            0.4889        1.5589       0.4896            0.4896        1.3649  0.0005  0.1631\n",
      "     34            0.5333        1.3474       0.5000            0.5000        1.3398  0.0006  0.1642\n",
      "     35            0.6222        1.1417       0.4896            0.4896        1.3093  0.0006  0.1840\n",
      "     36            0.7556        0.8900       0.5104            0.5104        1.2780  0.0007  0.1520\n",
      "     37            0.8000        0.8178       0.5104            0.5104        1.2568  0.0007  0.1550\n",
      "     38            \u001b[36m0.8222\u001b[0m        0.7177       0.5000            0.5000        1.2457  0.0007  0.1571\n",
      "     39            \u001b[36m0.8889\u001b[0m        \u001b[32m0.5375\u001b[0m       0.4792            0.4792        1.2429  0.0007  0.1484\n",
      "     40            \u001b[36m0.9111\u001b[0m        \u001b[32m0.4635\u001b[0m       0.4896            0.4896        1.2530  0.0007  0.2030\n",
      "     41            \u001b[36m0.9778\u001b[0m        \u001b[32m0.4198\u001b[0m       0.4896            0.4896        1.2734  0.0007  0.1498\n",
      "     42            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2837\u001b[0m       0.5000            0.5000        1.3035  0.0007  0.1508\n",
      "     43            1.0000        0.3162       0.4688            0.4688        1.3351  0.0006  0.1511\n",
      "     44            1.0000        \u001b[32m0.2696\u001b[0m       0.4688            0.4688        1.3643  0.0006  0.1527\n",
      "     45            1.0000        0.2873       0.4688            0.4688        1.3893  0.0005  0.1533\n",
      "     46            1.0000        \u001b[32m0.2015\u001b[0m       0.4479            0.4479        1.4067  0.0005  0.1517\n",
      "     47            1.0000        \u001b[32m0.1707\u001b[0m       0.4479            0.4479        1.4162  0.0004  0.1563\n",
      "     48            1.0000        0.2061       0.4583            0.4583        1.4199  0.0004  0.1547\n",
      "     49            1.0000        \u001b[32m0.1473\u001b[0m       0.4583            0.4583        1.4201  0.0003  0.1519\n",
      "     50            1.0000        0.1598       0.4583            0.4583        1.4166  0.0003  0.1503\n",
      "Fine tuning model for subject 4 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4889        1.3093       0.5208            0.5208        1.3955  0.0004  0.1520\n",
      "     32            0.5333        1.4531       0.5000            0.5000        1.3877  0.0005  0.1489\n",
      "     33            0.5778        1.3265       0.5208            0.5208        1.3773  0.0005  0.1530\n",
      "     34            0.6444        1.1067       0.5312            0.5312        1.3570  0.0006  0.1564\n",
      "     35            0.7111        0.9694       0.5312            0.5312        1.3296  0.0006  0.1550\n",
      "     36            0.7778        0.8266       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.2962  0.0007  0.1615\n",
      "     37            \u001b[36m0.9111\u001b[0m        \u001b[32m0.6306\u001b[0m       0.5312            0.5312        1.2668  0.0007  0.1590\n",
      "     38            \u001b[36m0.9556\u001b[0m        0.6416       0.5312            0.5312        1.2471  0.0007  0.1623\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4610\u001b[0m       0.5104            0.5104        1.2336  0.0007  0.1523\n",
      "     40            1.0000        \u001b[32m0.4246\u001b[0m       0.5312            0.5312        1.2261  0.0007  0.1492\n",
      "     41            0.9778        0.4391       0.5104            0.5104        1.2247  0.0007  0.1517\n",
      "     42            0.9778        \u001b[32m0.3375\u001b[0m       0.4792            0.4792        1.2305  0.0007  0.1492\n",
      "     43            0.9778        \u001b[32m0.2918\u001b[0m       0.4792            0.4792        1.2385  0.0006  0.1673\n",
      "     44            0.9778        \u001b[32m0.2764\u001b[0m       0.4688            0.4688        1.2486  0.0006  0.1499\n",
      "     45            0.9778        0.2791       0.4479            0.4479        1.2572  0.0005  0.1545\n",
      "     46            0.9778        \u001b[32m0.2461\u001b[0m       0.4375            0.4375        1.2644  0.0005  0.1544\n",
      "     47            0.9778        \u001b[32m0.1928\u001b[0m       0.4375            0.4375        1.2684  0.0004  0.1504\n",
      "     48            0.9778        \u001b[32m0.1673\u001b[0m       0.4479            0.4479        1.2711  0.0004  0.1504\n",
      "     49            0.9778        \u001b[32m0.1535\u001b[0m       0.4479            0.4479        1.2721  0.0003  0.1475\n",
      "     50            0.9778        \u001b[32m0.1492\u001b[0m       0.4375            0.4375        1.2721  0.0003  0.1474\n",
      "Fine tuning model for subject 4 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3556        1.7795       0.4896            0.4896        1.3968  0.0004  0.1786\n",
      "     32            0.4222        1.7030       0.4896            0.4896        1.4005  0.0005  0.1825\n",
      "     33            0.4667        1.5910       0.5000            0.5000        1.3943  0.0005  0.2146\n",
      "     34            0.5778        1.4078       0.5000            0.5000        1.3864  0.0006  0.1686\n",
      "     35            0.6889        1.0182       0.5104            0.5104        1.3800  0.0006  0.2188\n",
      "     36            0.8000        1.0161       0.5208            0.5208        1.3855  0.0007  0.1768\n",
      "     37            \u001b[36m0.8889\u001b[0m        0.7318       0.4896            0.4896        1.3900  0.0007  0.1818\n",
      "     38            0.8667        \u001b[32m0.4713\u001b[0m       0.4688            0.4688        1.3949  0.0007  0.2288\n",
      "     39            0.8667        0.4851       0.4896            0.4896        1.3924  0.0007  0.1781\n",
      "     40            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4086\u001b[0m       0.5000            0.5000        1.3753  0.0007  0.1706\n",
      "     41            0.9333        \u001b[32m0.3631\u001b[0m       0.5104            0.5104        1.3517  0.0007  0.1874\n",
      "     42            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3363\u001b[0m       0.5208            0.5208        1.3307  0.0007  0.2114\n",
      "     43            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2520\u001b[0m       0.5417            0.5417        1.3086  0.0006  0.1895\n",
      "     44            1.0000        0.2758       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.2913  0.0006  0.2116\n",
      "     45            1.0000        \u001b[32m0.2363\u001b[0m       0.5417            0.5417        1.2768  0.0005  0.1693\n",
      "     46            1.0000        0.2881       0.5417            0.5417        1.2656  0.0005  0.1507\n",
      "     47            1.0000        \u001b[32m0.2123\u001b[0m       0.5208            0.5208        1.2580  0.0004  0.1714\n",
      "     48            1.0000        \u001b[32m0.2062\u001b[0m       0.5208            0.5208        1.2514  0.0004  0.1718\n",
      "     49            1.0000        \u001b[32m0.1994\u001b[0m       0.5208            0.5208        1.2445  0.0003  0.1636\n",
      "     50            1.0000        \u001b[32m0.1862\u001b[0m       0.5208            0.5208        1.2381  0.0003  0.1509\n",
      "Fine tuning model for subject 4 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.5505       0.5208            0.5208        1.4004  0.0004  0.1560\n",
      "     32            0.4444        1.5185       0.5312            0.5312        1.4082  0.0005  0.1629\n",
      "     33            0.4667        1.4455       0.5208            0.5208        1.4238  0.0005  0.1664\n",
      "     34            0.5778        1.0767       0.5208            0.5208        1.4466  0.0006  0.1897\n",
      "     35            0.6889        0.9684       0.5208            0.5208        1.4720  0.0006  0.1550\n",
      "     36            0.7111        0.7935       0.5104            0.5104        1.4801  0.0007  0.1533\n",
      "     37            0.8000        \u001b[32m0.6564\u001b[0m       0.5104            0.5104        1.4713  0.0007  0.1573\n",
      "     38            \u001b[36m0.8444\u001b[0m        \u001b[32m0.4933\u001b[0m       0.4896            0.4896        1.4521  0.0007  0.1536\n",
      "     39            \u001b[36m0.8667\u001b[0m        \u001b[32m0.4617\u001b[0m       0.4688            0.4688        1.4166  0.0007  0.1571\n",
      "     40            \u001b[36m0.9556\u001b[0m        \u001b[32m0.3643\u001b[0m       0.4479            0.4479        1.3828  0.0007  0.1516\n",
      "     41            \u001b[36m0.9778\u001b[0m        0.4062       0.4583            0.4583        1.3580  0.0007  0.1490\n",
      "     42            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2822\u001b[0m       0.4792            0.4792        1.3425  0.0007  0.1574\n",
      "     43            1.0000        \u001b[32m0.2762\u001b[0m       0.5000            0.5000        1.3337  0.0006  0.1492\n",
      "     44            1.0000        \u001b[32m0.2189\u001b[0m       0.4896            0.4896        1.3331  0.0006  0.1492\n",
      "     45            1.0000        0.2339       0.5000            0.5000        1.3383  0.0005  0.1538\n",
      "     46            1.0000        \u001b[32m0.2027\u001b[0m       0.4896            0.4896        1.3414  0.0005  0.1600\n",
      "     47            1.0000        \u001b[32m0.1810\u001b[0m       0.4896            0.4896        1.3453  0.0004  0.1489\n",
      "     48            1.0000        \u001b[32m0.1764\u001b[0m       0.4792            0.4792        1.3496  0.0004  0.1535\n",
      "     49            1.0000        \u001b[32m0.1520\u001b[0m       0.4792            0.4792        1.3526  0.0003  0.1543\n",
      "     50            1.0000        0.1779       0.4896            0.4896        1.3536  0.0003  0.1486\n",
      "Fine tuning model for subject 4 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.5064       0.5208            0.5208        1.4044  0.0004  0.1652\n",
      "     32            0.4222        1.5733       0.5208            0.5208        1.4130  0.0005  0.1652\n",
      "     33            0.4889        1.5060       0.5208            0.5208        1.4177  0.0005  0.1721\n",
      "     34            0.5556        1.1345       0.5000            0.5000        1.4147  0.0006  0.1556\n",
      "     35            0.6889        1.0273       0.5104            0.5104        1.4148  0.0006  0.1546\n",
      "     36            0.7333        0.8949       0.5104            0.5104        1.4180  0.0007  0.1596\n",
      "     37            \u001b[36m0.8667\u001b[0m        0.7035       0.5104            0.5104        1.4211  0.0007  0.1599\n",
      "     38            \u001b[36m0.8889\u001b[0m        \u001b[32m0.6038\u001b[0m       0.5417            0.5417        1.4246  0.0007  0.1557\n",
      "     39            \u001b[36m0.9556\u001b[0m        \u001b[32m0.4342\u001b[0m       0.5417            0.5417        1.4422  0.0007  0.1528\n",
      "     40            0.9556        \u001b[32m0.3947\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.4628  0.0007  0.1549\n",
      "     41            \u001b[36m0.9778\u001b[0m        \u001b[32m0.2638\u001b[0m       0.5312            0.5312        1.4825  0.0007  0.1542\n",
      "     42            \u001b[36m1.0000\u001b[0m        0.3003       0.5312            0.5312        1.5027  0.0007  0.1572\n",
      "     43            1.0000        0.2723       0.5208            0.5208        1.5154  0.0006  0.1561\n",
      "     44            1.0000        \u001b[32m0.2557\u001b[0m       0.5208            0.5208        1.5211  0.0006  0.1485\n",
      "     45            1.0000        \u001b[32m0.2437\u001b[0m       0.5312            0.5312        1.5179  0.0005  0.1589\n",
      "     46            1.0000        \u001b[32m0.1970\u001b[0m       0.5208            0.5208        1.5107  0.0005  0.1525\n",
      "     47            1.0000        \u001b[32m0.1959\u001b[0m       0.5312            0.5312        1.5025  0.0004  0.1504\n",
      "     48            1.0000        \u001b[32m0.1796\u001b[0m       0.5312            0.5312        1.4926  0.0004  0.1599\n",
      "     49            1.0000        0.2001       0.5208            0.5208        1.4812  0.0003  0.1492\n",
      "     50            1.0000        \u001b[32m0.1711\u001b[0m       0.5208            0.5208        1.4700  0.0003  0.1601\n",
      "Fine tuning model for subject 4 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.7078       0.5000            0.5000        1.3965  0.0004  0.1509\n",
      "     32            0.4889        1.5353       0.5000            0.5000        1.3901  0.0005  0.1631\n",
      "     33            0.5333        1.5362       0.5104            0.5104        1.3739  0.0005  0.1634\n",
      "     34            0.6000        1.4558       0.5104            0.5104        1.3468  0.0006  0.1543\n",
      "     35            0.6000        1.1580       0.5312            0.5312        1.3160  0.0006  0.1607\n",
      "     36            0.6667        0.9038       0.5208            0.5208        1.2899  0.0007  0.1515\n",
      "     37            0.6889        0.7542       0.5104            0.5104        1.2635  0.0007  0.1588\n",
      "     38            0.7333        \u001b[32m0.6248\u001b[0m       0.5208            0.5208        1.2477  0.0007  0.1557\n",
      "     39            0.7556        \u001b[32m0.5387\u001b[0m       0.5104            0.5104        1.2463  0.0007  0.1501\n",
      "     40            0.7556        \u001b[32m0.4780\u001b[0m       0.5000            0.5000        1.2609  0.0007  0.1524\n",
      "     41            0.7556        \u001b[32m0.4374\u001b[0m       0.4792            0.4792        1.2835  0.0007  0.1463\n",
      "     42            0.8000        \u001b[32m0.3450\u001b[0m       0.5000            0.5000        1.3035  0.0007  0.1564\n",
      "     43            0.8000        \u001b[32m0.3443\u001b[0m       0.4792            0.4792        1.3158  0.0006  0.1577\n",
      "     44            \u001b[36m0.8667\u001b[0m        \u001b[32m0.3126\u001b[0m       0.5000            0.5000        1.3219  0.0006  0.1581\n",
      "     45            \u001b[36m0.8889\u001b[0m        0.3441       0.5000            0.5000        1.3195  0.0005  0.1541\n",
      "     46            \u001b[36m0.9111\u001b[0m        \u001b[32m0.2234\u001b[0m       0.5000            0.5000        1.3117  0.0005  0.1531\n",
      "     47            \u001b[36m0.9333\u001b[0m        0.2687       0.5208            0.5208        1.3008  0.0004  0.1514\n",
      "     48            0.9333        0.2611       0.5312            0.5312        1.2862  0.0004  0.1564\n",
      "     49            \u001b[36m0.9556\u001b[0m        0.2343       0.5104            0.5104        1.2711  0.0003  0.1893\n",
      "     50            \u001b[36m0.9778\u001b[0m        \u001b[32m0.1909\u001b[0m       0.5208            0.5208        1.2582  0.0003  0.1804\n",
      "Fine tuning model for subject 4 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.8411       0.5104            0.5104        1.3927  0.0004  0.1941\n",
      "     32            0.4444        1.7229       0.5104            0.5104        1.3890  0.0005  0.1777\n",
      "     33            0.4667        1.5664       0.4792            0.4792        1.3813  0.0005  0.2028\n",
      "     34            0.5556        1.4089       0.4688            0.4688        1.3768  0.0006  0.1745\n",
      "     35            0.5778        1.2216       0.4583            0.4583        1.3785  0.0006  0.1945\n",
      "     36            0.6000        0.9545       0.4375            0.4375        1.3778  0.0007  0.1815\n",
      "     37            0.6889        0.8194       0.4375            0.4375        1.3762  0.0007  0.1728\n",
      "     38            0.7556        \u001b[32m0.6223\u001b[0m       0.3854            0.3854        1.3738  0.0007  0.1739\n",
      "     39            \u001b[36m0.8222\u001b[0m        \u001b[32m0.5036\u001b[0m       0.3958            0.3958        1.3661  0.0007  0.1793\n",
      "     40            \u001b[36m0.8444\u001b[0m        \u001b[32m0.4447\u001b[0m       0.3958            0.3958        1.3548  0.0007  0.1847\n",
      "     41            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4059\u001b[0m       0.3958            0.3958        1.3360  0.0007  0.1975\n",
      "     42            \u001b[36m0.9778\u001b[0m        \u001b[32m0.2919\u001b[0m       0.4271            0.4271        1.3183  0.0007  0.1749\n",
      "     43            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2738\u001b[0m       0.4375            0.4375        1.3048  0.0006  0.2136\n",
      "     44            1.0000        \u001b[32m0.2375\u001b[0m       0.4479            0.4479        1.2976  0.0006  0.1839\n",
      "     45            1.0000        \u001b[32m0.2199\u001b[0m       0.4583            0.4583        1.2943  0.0005  0.1497\n",
      "     46            1.0000        \u001b[32m0.2013\u001b[0m       0.4583            0.4583        1.2923  0.0005  0.1649\n",
      "     47            1.0000        0.2118       0.4792            0.4792        1.2919  0.0004  0.1580\n",
      "     48            1.0000        \u001b[32m0.1998\u001b[0m       0.4688            0.4688        1.2924  0.0004  0.1541\n",
      "     49            1.0000        \u001b[32m0.1114\u001b[0m       0.4688            0.4688        1.2935  0.0003  0.1572\n",
      "     50            1.0000        0.1740       0.4792            0.4792        1.2949  0.0003  0.1532\n",
      "Fine tuning model for subject 4 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.7474       0.5312            0.5312        1.3987  0.0004  0.1691\n",
      "     32            0.5111        1.5236       0.5417            0.5417        1.4041  0.0005  0.1560\n",
      "     33            0.6000        1.4931       0.5312            0.5312        1.4109  0.0005  0.1520\n",
      "     34            0.6444        1.2939       0.5104            0.5104        1.4155  0.0006  0.1540\n",
      "     35            0.7556        0.9852       0.5000            0.5000        1.4175  0.0006  0.1567\n",
      "     36            \u001b[36m0.8222\u001b[0m        0.8875       0.5000            0.5000        1.4310  0.0007  0.1572\n",
      "     37            \u001b[36m0.8444\u001b[0m        0.7812       0.4896            0.4896        1.4529  0.0007  0.1517\n",
      "     38            \u001b[36m0.8667\u001b[0m        \u001b[32m0.5391\u001b[0m       0.5000            0.5000        1.4885  0.0007  0.1490\n",
      "     39            0.8667        \u001b[32m0.4219\u001b[0m       0.4896            0.4896        1.5429  0.0007  0.1526\n",
      "     40            \u001b[36m0.9111\u001b[0m        \u001b[32m0.4173\u001b[0m       0.4479            0.4479        1.6196  0.0007  0.1580\n",
      "     41            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3274\u001b[0m       0.4375            0.4375        1.7032  0.0007  0.1596\n",
      "     42            \u001b[36m0.9556\u001b[0m        \u001b[32m0.2543\u001b[0m       0.4583            0.4583        1.7833  0.0007  0.1519\n",
      "     43            0.9556        0.2971       0.4375            0.4375        1.8416  0.0006  0.1525\n",
      "     44            \u001b[36m0.9778\u001b[0m        \u001b[32m0.2366\u001b[0m       0.4167            0.4167        1.8773  0.0006  0.1590\n",
      "     45            0.9778        \u001b[32m0.2103\u001b[0m       0.3958            0.3958        1.8877  0.0005  0.1612\n",
      "     46            0.9778        \u001b[32m0.1657\u001b[0m       0.3958            0.3958        1.8803  0.0005  0.1549\n",
      "     47            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1297\u001b[0m       0.3958            0.3958        1.8595  0.0004  0.1540\n",
      "     48            1.0000        0.1782       0.4167            0.4167        1.8278  0.0004  0.1551\n",
      "     49            1.0000        \u001b[32m0.1237\u001b[0m       0.4271            0.4271        1.7914  0.0003  0.1549\n",
      "     50            1.0000        \u001b[32m0.1017\u001b[0m       0.4062            0.4062        1.7548  0.0003  0.1562\n",
      "Hold out data from subject 5\n",
      "Pre-training model with data from all subjects but subject 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.4086\u001b[0m        \u001b[32m1.5805\u001b[0m       \u001b[35m0.3568\u001b[0m            \u001b[31m0.3568\u001b[0m        \u001b[94m1.3735\u001b[0m  0.0007  7.5418\n",
      "      2            \u001b[36m0.5190\u001b[0m        \u001b[32m1.3943\u001b[0m       \u001b[35m0.4362\u001b[0m            \u001b[31m0.4362\u001b[0m        \u001b[94m1.2451\u001b[0m  0.0007  7.4631\n",
      "      3            \u001b[36m0.5359\u001b[0m        \u001b[32m1.2636\u001b[0m       \u001b[35m0.4635\u001b[0m            \u001b[31m0.4635\u001b[0m        1.2453  0.0007  8.0837\n",
      "      4            \u001b[36m0.5724\u001b[0m        \u001b[32m1.1709\u001b[0m       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        \u001b[94m1.1906\u001b[0m  0.0007  7.3391\n",
      "      5            \u001b[36m0.6117\u001b[0m        \u001b[32m1.1274\u001b[0m       \u001b[35m0.4974\u001b[0m            \u001b[31m0.4974\u001b[0m        \u001b[94m1.1637\u001b[0m  0.0007  7.8106\n",
      "      6            \u001b[36m0.6266\u001b[0m        \u001b[32m1.0711\u001b[0m       0.4505            0.4505        1.1891  0.0006  8.1648\n",
      "      7            \u001b[36m0.6401\u001b[0m        \u001b[32m1.0370\u001b[0m       0.4974            0.4974        1.1714  0.0006  7.3973\n",
      "      8            \u001b[36m0.6435\u001b[0m        \u001b[32m0.9989\u001b[0m       0.4948            0.4948        \u001b[94m1.1588\u001b[0m  0.0006  7.5748\n",
      "      9            \u001b[36m0.6753\u001b[0m        \u001b[32m0.9768\u001b[0m       \u001b[35m0.5117\u001b[0m            \u001b[31m0.5117\u001b[0m        \u001b[94m1.1344\u001b[0m  0.0006  7.9757\n",
      "     10            \u001b[36m0.6872\u001b[0m        \u001b[32m0.9326\u001b[0m       0.4909            0.4909        1.1604  0.0005  7.3222\n",
      "     11            \u001b[36m0.7023\u001b[0m        \u001b[32m0.9004\u001b[0m       \u001b[35m0.5208\u001b[0m            \u001b[31m0.5208\u001b[0m        \u001b[94m1.0891\u001b[0m  0.0005  7.8750\n",
      "     12            \u001b[36m0.7083\u001b[0m        \u001b[32m0.8827\u001b[0m       \u001b[35m0.5299\u001b[0m            \u001b[31m0.5299\u001b[0m        1.1056  0.0005  7.7655\n",
      "     13            \u001b[36m0.7167\u001b[0m        \u001b[32m0.8645\u001b[0m       \u001b[35m0.5534\u001b[0m            \u001b[31m0.5534\u001b[0m        \u001b[94m1.0681\u001b[0m  0.0004  7.4098\n",
      "     14            \u001b[36m0.7206\u001b[0m        \u001b[32m0.8403\u001b[0m       0.5508            0.5508        1.0901  0.0004  8.0733\n",
      "     15            \u001b[36m0.7388\u001b[0m        \u001b[32m0.8325\u001b[0m       0.5508            0.5508        1.0885  0.0004  7.8893\n",
      "     16            \u001b[36m0.7661\u001b[0m        \u001b[32m0.8006\u001b[0m       \u001b[35m0.5586\u001b[0m            \u001b[31m0.5586\u001b[0m        \u001b[94m1.0598\u001b[0m  0.0003  7.8400\n",
      "     17            \u001b[36m0.7716\u001b[0m        \u001b[32m0.7795\u001b[0m       \u001b[35m0.5651\u001b[0m            \u001b[31m0.5651\u001b[0m        \u001b[94m1.0469\u001b[0m  0.0003  8.0731\n",
      "     18            \u001b[36m0.7768\u001b[0m        \u001b[32m0.7589\u001b[0m       0.5638            0.5638        \u001b[94m1.0433\u001b[0m  0.0003  8.1259\n",
      "     19            \u001b[36m0.7849\u001b[0m        \u001b[32m0.7446\u001b[0m       0.5573            0.5573        1.0688  0.0002  7.5309\n",
      "     20            0.7674        \u001b[32m0.7225\u001b[0m       0.5612            0.5612        1.0631  0.0002  8.3014\n",
      "     21            \u001b[36m0.8063\u001b[0m        0.7261       \u001b[35m0.5716\u001b[0m            \u001b[31m0.5716\u001b[0m        \u001b[94m1.0330\u001b[0m  0.0002  8.0240\n",
      "     22            0.8047        \u001b[32m0.7070\u001b[0m       0.5586            0.5586        1.0357  0.0001  7.5751\n",
      "     23            \u001b[36m0.8107\u001b[0m        \u001b[32m0.6984\u001b[0m       0.5677            0.5677        \u001b[94m1.0316\u001b[0m  0.0001  8.6078\n",
      "     24            \u001b[36m0.8122\u001b[0m        \u001b[32m0.6904\u001b[0m       0.5690            0.5690        \u001b[94m1.0263\u001b[0m  0.0001  8.2624\n",
      "     25            \u001b[36m0.8174\u001b[0m        \u001b[32m0.6724\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.0279  0.0001  8.1436\n",
      "     26            \u001b[36m0.8180\u001b[0m        \u001b[32m0.6714\u001b[0m       0.5729            0.5729        \u001b[94m1.0233\u001b[0m  0.0000  7.6163\n",
      "     27            \u001b[36m0.8198\u001b[0m        0.6813       \u001b[35m0.5768\u001b[0m            \u001b[31m0.5768\u001b[0m        \u001b[94m1.0209\u001b[0m  0.0000  8.2588\n",
      "     28            0.8198        \u001b[32m0.6699\u001b[0m       0.5768            0.5768        1.0220  0.0000  7.7977\n",
      "     29            0.8198        \u001b[32m0.6686\u001b[0m       \u001b[35m0.5781\u001b[0m            \u001b[31m0.5781\u001b[0m        1.0224  0.0000  7.5130\n",
      "     30            0.8190        \u001b[32m0.6512\u001b[0m       0.5781            0.5781        \u001b[94m1.0206\u001b[0m  0.0000  8.2310\n",
      "Before finetuning for subject 5, the baseline accuracy is 0.3125\n",
      "Fine tuning model for subject 5 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2000        2.0162       0.3021            0.3021        2.0785  0.0004  0.1233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        1.3376       0.2812            0.2812        1.9717  0.0005  0.1327\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3068\u001b[0m       0.3021            0.3021        1.9687  0.0005  0.1469\n",
      "     34            1.0000        \u001b[32m0.1739\u001b[0m       0.2708            0.2708        2.0153  0.0006  0.1343\n",
      "     35            1.0000        \u001b[32m0.1510\u001b[0m       0.3021            0.3021        2.1223  0.0006  0.1330\n",
      "     36            1.0000        \u001b[32m0.0902\u001b[0m       0.2708            0.2708        2.2415  0.0007  0.1296\n",
      "     37            1.0000        \u001b[32m0.0440\u001b[0m       0.2708            0.2708        2.3269  0.0007  0.1310\n",
      "     38            1.0000        0.0610       0.2812            0.2812        2.3725  0.0007  0.1281\n",
      "     39            1.0000        \u001b[32m0.0367\u001b[0m       0.2604            0.2604        2.3894  0.0007  0.1286\n",
      "     40            1.0000        \u001b[32m0.0197\u001b[0m       0.2604            0.2604        2.3843  0.0007  0.1339\n",
      "     41            1.0000        \u001b[32m0.0131\u001b[0m       0.2708            0.2708        2.3664  0.0007  0.1295\n",
      "     42            1.0000        0.0149       0.2500            0.2500        2.3415  0.0007  0.1254\n",
      "     43            1.0000        0.0162       0.2500            0.2500        2.3148  0.0006  0.1359\n",
      "     44            1.0000        \u001b[32m0.0110\u001b[0m       0.2396            0.2396        2.2891  0.0006  0.1321\n",
      "     45            1.0000        0.0405       0.2500            0.2500        2.2701  0.0005  0.1333\n",
      "     46            1.0000        0.0150       0.2500            0.2500        2.2522  0.0005  0.1286\n",
      "     47            1.0000        \u001b[32m0.0080\u001b[0m       0.2604            0.2604        2.2348  0.0004  0.1329\n",
      "     48            1.0000        0.0084       0.2396            0.2396        2.2179  0.0004  0.1314\n",
      "     49            1.0000        0.0088       0.2396            0.2396        2.2014  0.0003  0.1375\n",
      "     50            1.0000        0.0081       0.2396            0.2396        2.1855  0.0003  0.1324\n",
      "Fine tuning model for subject 5 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.8000        1.2029       0.3021            0.3021        2.0090  0.0004  0.1316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        0.9061       0.2708            0.2708        1.8447  0.0005  0.1278\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1843\u001b[0m       0.2500            0.2500        1.8391  0.0005  0.1293\n",
      "     34            1.0000        \u001b[32m0.1083\u001b[0m       0.2604            0.2604        1.9652  0.0006  0.1818\n",
      "     35            1.0000        \u001b[32m0.0229\u001b[0m       0.2812            0.2812        2.1711  0.0006  0.1514\n",
      "     36            1.0000        0.0432       0.2917            0.2917        2.4036  0.0007  0.1251\n",
      "     37            1.0000        0.0783       0.2604            0.2604        2.6147  0.0007  0.1329\n",
      "     38            1.0000        0.0478       0.2500            0.2500        2.7666  0.0007  0.1281\n",
      "     39            1.0000        0.0282       0.2396            0.2396        2.8845  0.0007  0.1320\n",
      "     40            1.0000        \u001b[32m0.0098\u001b[0m       0.2396            0.2396        2.9682  0.0007  0.1338\n",
      "     41            1.0000        \u001b[32m0.0071\u001b[0m       0.2500            0.2500        3.0157  0.0007  0.1291\n",
      "     42            1.0000        \u001b[32m0.0059\u001b[0m       0.2292            0.2292        3.0318  0.0007  0.1371\n",
      "     43            1.0000        0.0063       0.2292            0.2292        3.0232  0.0006  0.1403\n",
      "     44            1.0000        \u001b[32m0.0046\u001b[0m       0.2083            0.2083        2.9922  0.0006  0.1598\n",
      "     45            1.0000        \u001b[32m0.0039\u001b[0m       0.2292            0.2292        2.9433  0.0005  0.1391\n",
      "     46            1.0000        \u001b[32m0.0030\u001b[0m       0.2292            0.2292        2.8830  0.0005  0.1419\n",
      "     47            1.0000        0.0068       0.2396            0.2396        2.8159  0.0004  0.1419\n",
      "     48            1.0000        \u001b[32m0.0026\u001b[0m       0.2396            0.2396        2.7460  0.0004  0.1341\n",
      "     49            1.0000        \u001b[32m0.0018\u001b[0m       0.2396            0.2396        2.6766  0.0003  0.1451\n",
      "     50            1.0000        0.0032       0.2500            0.2500        2.6102  0.0003  0.1650\n",
      "Fine tuning model for subject 5 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.1971       0.3333            0.3333        2.1011  0.0004  0.1678\n",
      "     32            0.8000        1.0869       0.3438            0.3438        1.9941  0.0005  0.1830\n",
      "     33            0.8000        \u001b[32m0.6313\u001b[0m       0.3333            0.3333        1.9253  0.0005  0.2069\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1252\u001b[0m       0.3333            0.3333        1.8559  0.0006  0.1826\n",
      "     35            1.0000        0.1780       0.3125            0.3125        1.8231  0.0006  0.1838\n",
      "     36            1.0000        \u001b[32m0.0244\u001b[0m       0.3333            0.3333        1.8170  0.0007  0.1730\n",
      "     37            1.0000        0.0435       0.3438            0.3438        1.8165  0.0007  0.1627\n",
      "     38            1.0000        \u001b[32m0.0213\u001b[0m       0.3333            0.3333        1.8048  0.0007  0.1725\n",
      "     39            1.0000        0.0407       0.3125            0.3125        1.7838  0.0007  0.1665\n",
      "     40            1.0000        0.0270       0.2917            0.2917        1.7592  0.0007  0.1711\n",
      "     41            1.0000        0.0460       0.2917            0.2917        1.7371  0.0007  0.1651\n",
      "     42            1.0000        0.0301       0.2917            0.2917        1.7198  0.0007  0.1774\n",
      "     43            1.0000        0.0237       0.2812            0.2812        1.7095  0.0006  0.1759\n",
      "     44            1.0000        \u001b[32m0.0120\u001b[0m       0.2812            0.2812        1.7047  0.0006  0.1501\n",
      "     45            1.0000        0.0235       0.2917            0.2917        1.7032  0.0005  0.1399\n",
      "     46            1.0000        0.0151       0.2812            0.2812        1.7034  0.0005  0.1302\n",
      "     47            1.0000        0.0125       0.2812            0.2812        1.7044  0.0004  0.1307\n",
      "     48            1.0000        \u001b[32m0.0077\u001b[0m       0.3125            0.3125        1.7051  0.0004  0.1254\n",
      "     49            1.0000        \u001b[32m0.0050\u001b[0m       0.3125            0.3125        1.7052  0.0003  0.1333\n",
      "     50            1.0000        0.0124       0.3021            0.3021        1.7053  0.0003  0.1285\n",
      "Fine tuning model for subject 5 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.8000        1.1340       0.3125            0.3125        2.1283  0.0004  0.1291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        1.1435       0.3750            0.3750        2.0214  0.0005  0.1370\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2668\u001b[0m       0.3125            0.3125        1.8911  0.0005  0.1290\n",
      "     34            1.0000        \u001b[32m0.2048\u001b[0m       0.2292            0.2292        1.8144  0.0006  0.1279\n",
      "     35            1.0000        \u001b[32m0.0568\u001b[0m       0.2292            0.2292        1.8307  0.0006  0.1249\n",
      "     36            1.0000        \u001b[32m0.0318\u001b[0m       0.2188            0.2188        1.9044  0.0007  0.1284\n",
      "     37            1.0000        0.0430       0.2292            0.2292        1.9955  0.0007  0.1283\n",
      "     38            1.0000        \u001b[32m0.0087\u001b[0m       0.2292            0.2292        2.0794  0.0007  0.1332\n",
      "     39            1.0000        0.0284       0.2292            0.2292        2.1501  0.0007  0.1390\n",
      "     40            1.0000        0.0184       0.2500            0.2500        2.2035  0.0007  0.1286\n",
      "     41            1.0000        0.0184       0.2396            0.2396        2.2388  0.0007  0.1278\n",
      "     42            1.0000        0.0090       0.2708            0.2708        2.2574  0.0007  0.1280\n",
      "     43            1.0000        0.0127       0.2500            0.2500        2.2619  0.0006  0.1339\n",
      "     44            1.0000        0.0162       0.2708            0.2708        2.2551  0.0006  0.1236\n",
      "     45            1.0000        \u001b[32m0.0056\u001b[0m       0.2708            0.2708        2.2414  0.0005  0.1316\n",
      "     46            1.0000        0.0061       0.2604            0.2604        2.2245  0.0005  0.1273\n",
      "     47            1.0000        0.0137       0.2604            0.2604        2.2078  0.0004  0.1269\n",
      "     48            1.0000        0.0147       0.2708            0.2708        2.1945  0.0004  0.1258\n",
      "     49            1.0000        0.0072       0.2812            0.2812        2.1858  0.0003  0.1331\n",
      "     50            1.0000        \u001b[32m0.0054\u001b[0m       0.2812            0.2812        2.1828  0.0003  0.1292\n",
      "Fine tuning model for subject 5 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.5106       0.3125            0.3125        2.3331  0.0004  0.1304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        0.7740       0.3021            0.3021        2.6098  0.0005  0.1368\n",
      "     33            0.8000        \u001b[32m0.3147\u001b[0m       0.2812            0.2812        2.9081  0.0005  0.1355\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2394\u001b[0m       0.2812            0.2812        3.0463  0.0006  0.1342\n",
      "     35            1.0000        \u001b[32m0.0509\u001b[0m       0.2708            0.2708        3.0185  0.0006  0.1285\n",
      "     36            1.0000        \u001b[32m0.0234\u001b[0m       0.2708            0.2708        2.8831  0.0007  0.1278\n",
      "     37            1.0000        0.0249       0.2500            0.2500        2.7211  0.0007  0.1332\n",
      "     38            1.0000        \u001b[32m0.0116\u001b[0m       0.2500            0.2500        2.5779  0.0007  0.1269\n",
      "     39            1.0000        0.0466       0.2292            0.2292        2.4971  0.0007  0.1371\n",
      "     40            1.0000        0.0143       0.2188            0.2188        2.4570  0.0007  0.1267\n",
      "     41            1.0000        \u001b[32m0.0106\u001b[0m       0.2188            0.2188        2.4470  0.0007  0.1251\n",
      "     42            1.0000        0.0280       0.1875            0.1875        2.4552  0.0007  0.1249\n",
      "     43            1.0000        0.0163       0.1979            0.1979        2.4682  0.0006  0.1325\n",
      "     44            1.0000        0.0331       0.1979            0.1979        2.4772  0.0006  0.1263\n",
      "     45            1.0000        0.0110       0.1979            0.1979        2.4820  0.0005  0.1263\n",
      "     46            1.0000        0.0183       0.1771            0.1771        2.4832  0.0005  0.1319\n",
      "     47            1.0000        \u001b[32m0.0034\u001b[0m       0.1667            0.1667        2.4803  0.0004  0.1308\n",
      "     48            1.0000        0.0189       0.1771            0.1771        2.4742  0.0004  0.1319\n",
      "     49            1.0000        0.0065       0.1771            0.1771        2.4661  0.0003  0.1269\n",
      "     50            1.0000        0.0042       0.1875            0.1875        2.4566  0.0003  0.1282\n",
      "Fine tuning model for subject 5 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.5889       0.3021            0.3021        2.1394  0.0004  0.1276\n",
      "     32            0.8000        0.8319       0.2917            0.2917        2.0430  0.0005  0.1330\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3308\u001b[0m       0.2500            0.2500        1.9411  0.0005  0.1297\n",
      "     34            1.0000        \u001b[32m0.1948\u001b[0m       0.2396            0.2396        1.8807  0.0006  0.1298\n",
      "     35            1.0000        \u001b[32m0.1001\u001b[0m       0.2500            0.2500        1.8403  0.0006  0.1296\n",
      "     36            1.0000        \u001b[32m0.0280\u001b[0m       0.2292            0.2292        1.8318  0.0007  0.1304\n",
      "     37            1.0000        0.0623       0.2604            0.2604        1.8619  0.0007  0.1288\n",
      "     38            1.0000        \u001b[32m0.0077\u001b[0m       0.2500            0.2500        1.9128  0.0007  0.1263\n",
      "     39            1.0000        0.0214       0.2604            0.2604        1.9665  0.0007  0.1271\n",
      "     40            1.0000        0.0231       0.2396            0.2396        2.0138  0.0007  0.1354\n",
      "     41            1.0000        0.0091       0.2500            0.2500        2.0496  0.0007  0.1360\n",
      "     42            1.0000        0.0116       0.2500            0.2500        2.0737  0.0007  0.1253\n",
      "     43            1.0000        0.0086       0.2604            0.2604        2.0880  0.0006  0.1256\n",
      "     44            1.0000        \u001b[32m0.0073\u001b[0m       0.2604            0.2604        2.0948  0.0006  0.1336\n",
      "     45            1.0000        0.0187       0.2604            0.2604        2.0957  0.0005  0.1327\n",
      "     46            1.0000        0.0076       0.2604            0.2604        2.0933  0.0005  0.1273\n",
      "     47            1.0000        \u001b[32m0.0051\u001b[0m       0.2604            0.2604        2.0886  0.0004  0.1339\n",
      "     48            1.0000        0.0085       0.2604            0.2604        2.0824  0.0004  0.1293\n",
      "     49            1.0000        0.0164       0.2500            0.2500        2.0751  0.0003  0.1314\n",
      "     50            1.0000        \u001b[32m0.0046\u001b[0m       0.2604            0.2604        2.0677  0.0003  0.1267\n",
      "Fine tuning model for subject 5 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.5753       0.3125            0.3125        2.1040  0.0004  0.1280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        0.9137       0.2812            0.2812        1.9388  0.0005  0.1332\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2436\u001b[0m       0.2396            0.2396        1.8572  0.0005  0.1302\n",
      "     34            1.0000        \u001b[32m0.1586\u001b[0m       0.2708            0.2708        1.9132  0.0006  0.1366\n",
      "     35            1.0000        \u001b[32m0.0285\u001b[0m       0.2708            0.2708        2.0447  0.0006  0.1320\n",
      "     36            1.0000        0.0322       0.2500            0.2500        2.1787  0.0007  0.1328\n",
      "     37            1.0000        \u001b[32m0.0264\u001b[0m       0.2917            0.2917        2.2898  0.0007  0.1282\n",
      "     38            1.0000        \u001b[32m0.0208\u001b[0m       0.2812            0.2812        2.3695  0.0007  0.1239\n",
      "     39            1.0000        \u001b[32m0.0081\u001b[0m       0.3021            0.3021        2.4175  0.0007  0.1259\n",
      "     40            1.0000        0.0345       0.2708            0.2708        2.4324  0.0007  0.1330\n",
      "     41            1.0000        0.0209       0.2812            0.2812        2.4217  0.0007  0.1288\n",
      "     42            1.0000        0.0189       0.2812            0.2812        2.3912  0.0007  0.1317\n",
      "     43            1.0000        0.0130       0.2812            0.2812        2.3480  0.0006  0.1273\n",
      "     44            1.0000        0.0103       0.2812            0.2812        2.2983  0.0006  0.1278\n",
      "     45            1.0000        \u001b[32m0.0063\u001b[0m       0.2812            0.2812        2.2459  0.0005  0.1253\n",
      "     46            1.0000        0.0076       0.3021            0.3021        2.1950  0.0005  0.1404\n",
      "     47            1.0000        \u001b[32m0.0054\u001b[0m       0.3021            0.3021        2.1472  0.0004  0.1384\n",
      "     48            1.0000        0.0081       0.2917            0.2917        2.1038  0.0004  0.1328\n",
      "     49            1.0000        0.0068       0.2812            0.2812        2.0655  0.0003  0.1333\n",
      "     50            1.0000        0.0057       0.2708            0.2708        2.0321  0.0003  0.1493\n",
      "Fine tuning model for subject 5 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.5988       0.3125            0.3125        2.0724  0.0004  0.1267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4000        1.0374       0.3021            0.3021        1.9431  0.0005  0.1312\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4642\u001b[0m       0.2917            0.2917        1.8212  0.0005  0.1393\n",
      "     34            1.0000        \u001b[32m0.0932\u001b[0m       0.2500            0.2500        1.7953  0.0006  0.1288\n",
      "     35            1.0000        \u001b[32m0.0672\u001b[0m       0.2083            0.2083        1.8543  0.0006  0.1617\n",
      "     36            1.0000        \u001b[32m0.0599\u001b[0m       0.1875            0.1875        1.9379  0.0007  0.1719\n",
      "     37            1.0000        \u001b[32m0.0267\u001b[0m       0.2188            0.2188        2.0062  0.0007  0.1674\n",
      "     38            1.0000        0.0691       0.2083            0.2083        2.0480  0.0007  0.1641\n",
      "     39            1.0000        \u001b[32m0.0058\u001b[0m       0.2188            0.2188        2.0733  0.0007  0.1709\n",
      "     40            1.0000        \u001b[32m0.0058\u001b[0m       0.2188            0.2188        2.0914  0.0007  0.1706\n",
      "     41            1.0000        0.0102       0.1979            0.1979        2.1078  0.0007  0.1567\n",
      "     42            1.0000        0.0063       0.1875            0.1875        2.1247  0.0007  0.1632\n",
      "     43            1.0000        0.0104       0.1875            0.1875        2.1421  0.0006  0.1759\n",
      "     44            1.0000        0.0205       0.1875            0.1875        2.1605  0.0006  0.1659\n",
      "     45            1.0000        0.0151       0.1875            0.1875        2.1794  0.0005  0.1675\n",
      "     46            1.0000        \u001b[32m0.0031\u001b[0m       0.1667            0.1667        2.1982  0.0005  0.1690\n",
      "     47            1.0000        \u001b[32m0.0030\u001b[0m       0.1771            0.1771        2.2170  0.0004  0.1703\n",
      "     48            1.0000        0.0035       0.1771            0.1771        2.2356  0.0004  0.1663\n",
      "     49            1.0000        0.0037       0.1771            0.1771        2.2540  0.0003  0.1722\n",
      "     50            1.0000        0.0046       0.1875            0.1875        2.2720  0.0003  0.1516\n",
      "Fine tuning model for subject 5 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        2.1578       0.3229            0.3229        2.0942  0.0004  0.1334\n",
      "     32            0.8000        1.2030       0.2708            0.2708        1.9767  0.0005  0.1319\n",
      "     33            0.8000        \u001b[32m0.2866\u001b[0m       0.2604            0.2604        2.1416  0.0005  0.1229\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0785\u001b[0m       0.2188            0.2188        2.5135  0.0006  0.1299\n",
      "     35            1.0000        \u001b[32m0.0547\u001b[0m       0.2292            0.2292        2.8437  0.0006  0.1280\n",
      "     36            1.0000        \u001b[32m0.0353\u001b[0m       0.2292            0.2292        3.0134  0.0007  0.1280\n",
      "     37            1.0000        0.0415       0.2396            0.2396        3.0247  0.0007  0.1306\n",
      "     38            1.0000        \u001b[32m0.0135\u001b[0m       0.2500            0.2500        2.9347  0.0007  0.1348\n",
      "     39            1.0000        0.0215       0.2604            0.2604        2.7929  0.0007  0.1277\n",
      "     40            1.0000        0.0143       0.2396            0.2396        2.6391  0.0007  0.1279\n",
      "     41            1.0000        0.0172       0.2188            0.2188        2.4955  0.0007  0.1392\n",
      "     42            1.0000        \u001b[32m0.0115\u001b[0m       0.1875            0.1875        2.3748  0.0007  0.1547\n",
      "     43            1.0000        \u001b[32m0.0033\u001b[0m       0.1562            0.1562        2.2790  0.0006  0.1575\n",
      "     44            1.0000        0.0092       0.1875            0.1875        2.2056  0.0006  0.1709\n",
      "     45            1.0000        0.0073       0.1875            0.1875        2.1507  0.0005  0.1311\n",
      "     46            1.0000        0.0046       0.1979            0.1979        2.1104  0.0005  0.1392\n",
      "     47            1.0000        0.0064       0.1979            0.1979        2.0806  0.0004  0.1366\n",
      "     48            1.0000        \u001b[32m0.0020\u001b[0m       0.2083            0.2083        2.0584  0.0004  0.1303\n",
      "     49            1.0000        0.0100       0.2188            0.2188        2.0416  0.0003  0.1331\n",
      "     50            1.0000        0.0057       0.2396            0.2396        2.0284  0.0003  0.1433\n",
      "Fine tuning model for subject 5 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2000        1.6534       0.3229            0.3229        2.1526  0.0004  0.1271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4000        1.5406       0.3229            0.3229        2.1374  0.0005  0.1290\n",
      "     33            0.8000        0.9519       0.3125            0.3125        2.1049  0.0005  0.1415\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2403\u001b[0m       0.2917            0.2917        2.0781  0.0006  0.1321\n",
      "     35            1.0000        0.2811       0.3021            0.3021        2.0995  0.0006  0.1330\n",
      "     36            1.0000        \u001b[32m0.0552\u001b[0m       0.2604            0.2604        2.1189  0.0007  0.1277\n",
      "     37            1.0000        0.0898       0.2396            0.2396        2.1342  0.0007  0.1320\n",
      "     38            1.0000        \u001b[32m0.0316\u001b[0m       0.2292            0.2292        2.1837  0.0007  0.1392\n",
      "     39            1.0000        \u001b[32m0.0108\u001b[0m       0.2708            0.2708        2.2734  0.0007  0.1376\n",
      "     40            1.0000        0.0184       0.2708            0.2708        2.3822  0.0007  0.1290\n",
      "     41            1.0000        \u001b[32m0.0103\u001b[0m       0.2604            0.2604        2.4882  0.0007  0.1324\n",
      "     42            1.0000        0.0113       0.2917            0.2917        2.5779  0.0007  0.1379\n",
      "     43            1.0000        \u001b[32m0.0050\u001b[0m       0.2917            0.2917        2.6477  0.0006  0.1279\n",
      "     44            1.0000        \u001b[32m0.0031\u001b[0m       0.3125            0.3125        2.6974  0.0006  0.1345\n",
      "     45            1.0000        0.0065       0.3125            0.3125        2.7282  0.0005  0.1310\n",
      "     46            1.0000        0.0125       0.3438            0.3438        2.7425  0.0005  0.1283\n",
      "     47            1.0000        0.0085       0.3333            0.3333        2.7453  0.0004  0.1354\n",
      "     48            1.0000        0.0039       0.3333            0.3333        2.7404  0.0004  0.1332\n",
      "     49            1.0000        0.0052       0.3438            0.3438        2.7300  0.0003  0.1277\n",
      "     50            1.0000        0.0038       0.3333            0.3333        2.7164  0.0003  0.1240\n",
      "Fine tuning model for subject 5 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.8073       0.3542            0.3542        2.0775  0.0004  0.1076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        1.6727       0.3021            0.3021        2.0847  0.0005  0.1098\n",
      "     33            0.8000        \u001b[32m0.6021\u001b[0m       0.2708            0.2708        2.3781  0.0005  0.1095\n",
      "     34            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5066\u001b[0m       0.2604            0.2604        2.7563  0.0006  0.1133\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1934\u001b[0m       0.2604            0.2604        2.9745  0.0006  0.1119\n",
      "     36            1.0000        \u001b[32m0.0846\u001b[0m       0.2292            0.2292        3.0498  0.0007  0.1130\n",
      "     37            1.0000        \u001b[32m0.0445\u001b[0m       0.2188            0.2188        3.0304  0.0007  0.1137\n",
      "     38            1.0000        0.0593       0.2396            0.2396        2.9874  0.0007  0.1175\n",
      "     39            1.0000        0.0532       0.2604            0.2604        2.9368  0.0007  0.1085\n",
      "     40            1.0000        \u001b[32m0.0400\u001b[0m       0.2708            0.2708        2.8771  0.0007  0.1105\n",
      "     41            1.0000        \u001b[32m0.0321\u001b[0m       0.2604            0.2604        2.8161  0.0007  0.1114\n",
      "     42            1.0000        0.0521       0.2604            0.2604        2.7741  0.0007  0.1099\n",
      "     43            1.0000        \u001b[32m0.0236\u001b[0m       0.2500            0.2500        2.7376  0.0006  0.1188\n",
      "     44            1.0000        \u001b[32m0.0149\u001b[0m       0.2500            0.2500        2.7057  0.0006  0.1167\n",
      "     45            1.0000        0.0164       0.2500            0.2500        2.6796  0.0005  0.1121\n",
      "     46            1.0000        \u001b[32m0.0139\u001b[0m       0.2604            0.2604        2.6602  0.0005  0.1135\n",
      "     47            1.0000        \u001b[32m0.0078\u001b[0m       0.2604            0.2604        2.6452  0.0004  0.1125\n",
      "     48            1.0000        0.0145       0.2500            0.2500        2.6342  0.0004  0.1068\n",
      "     49            1.0000        0.0151       0.2708            0.2708        2.6267  0.0003  0.1098\n",
      "     50            1.0000        0.0120       0.2604            0.2604        2.6216  0.0003  0.1139\n",
      "Fine tuning model for subject 5 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.6261       0.3125            0.3125        2.1171  0.0004  0.1132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        1.4553       0.2604            0.2604        1.9694  0.0005  0.1173\n",
      "     33            0.8000        0.8640       0.2604            0.2604        1.9206  0.0005  0.1070\n",
      "     34            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6313\u001b[0m       0.2604            0.2604        1.9190  0.0006  0.1117\n",
      "     35            0.8000        \u001b[32m0.2322\u001b[0m       0.2083            0.2083        1.9402  0.0006  0.1119\n",
      "     36            0.9000        \u001b[32m0.1756\u001b[0m       0.2188            0.2188        1.9449  0.0007  0.1104\n",
      "     37            0.9000        0.1865       0.2500            0.2500        1.9128  0.0007  0.1134\n",
      "     38            \u001b[36m1.0000\u001b[0m        0.1924       0.2604            0.2604        1.8803  0.0007  0.1126\n",
      "     39            1.0000        \u001b[32m0.1089\u001b[0m       0.2396            0.2396        1.8549  0.0007  0.1092\n",
      "     40            1.0000        0.1185       0.2292            0.2292        1.8529  0.0007  0.1055\n",
      "     41            1.0000        \u001b[32m0.1000\u001b[0m       0.2188            0.2188        1.8717  0.0007  0.1109\n",
      "     42            1.0000        \u001b[32m0.0397\u001b[0m       0.2396            0.2396        1.8993  0.0007  0.1174\n",
      "     43            1.0000        \u001b[32m0.0311\u001b[0m       0.2604            0.2604        1.9306  0.0006  0.1095\n",
      "     44            1.0000        \u001b[32m0.0301\u001b[0m       0.2396            0.2396        1.9629  0.0006  0.1095\n",
      "     45            1.0000        0.0423       0.2292            0.2292        1.9925  0.0005  0.1188\n",
      "     46            1.0000        0.0365       0.2604            0.2604        2.0193  0.0005  0.1177\n",
      "     47            1.0000        0.0520       0.2604            0.2604        2.0405  0.0004  0.1193\n",
      "     48            1.0000        \u001b[32m0.0207\u001b[0m       0.2604            0.2604        2.0573  0.0004  0.1124\n",
      "     49            1.0000        \u001b[32m0.0118\u001b[0m       0.2604            0.2604        2.0723  0.0003  0.1177\n",
      "     50            1.0000        0.0211       0.2604            0.2604        2.0856  0.0003  0.1159\n",
      "Fine tuning model for subject 5 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2000        1.6879       0.3125            0.3125        2.1130  0.0004  0.1174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.3000        1.4064       0.3021            0.3021        2.0198  0.0005  0.1114\n",
      "     33            0.5000        1.1894       0.3021            0.3021        1.9458  0.0005  0.1123\n",
      "     34            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6264\u001b[0m       0.2812            0.2812        1.9336  0.0006  0.1095\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3211\u001b[0m       0.2917            0.2917        1.9775  0.0006  0.1135\n",
      "     36            1.0000        \u001b[32m0.1462\u001b[0m       0.2500            0.2500        2.0574  0.0007  0.1169\n",
      "     37            1.0000        \u001b[32m0.0951\u001b[0m       0.2500            0.2500        2.1299  0.0007  0.1099\n",
      "     38            1.0000        \u001b[32m0.0914\u001b[0m       0.2812            0.2812        2.1995  0.0007  0.1052\n",
      "     39            1.0000        \u001b[32m0.0396\u001b[0m       0.2917            0.2917        2.2620  0.0007  0.1140\n",
      "     40            1.0000        0.0990       0.2812            0.2812        2.3224  0.0007  0.1172\n",
      "     41            1.0000        0.0696       0.2917            0.2917        2.3797  0.0007  0.1137\n",
      "     42            1.0000        0.1174       0.2917            0.2917        2.4543  0.0007  0.1110\n",
      "     43            1.0000        \u001b[32m0.0340\u001b[0m       0.2917            0.2917        2.5211  0.0006  0.1126\n",
      "     44            1.0000        \u001b[32m0.0292\u001b[0m       0.3021            0.3021        2.5834  0.0006  0.1138\n",
      "     45            1.0000        0.0385       0.3021            0.3021        2.6372  0.0005  0.1080\n",
      "     46            1.0000        \u001b[32m0.0287\u001b[0m       0.2917            0.2917        2.6811  0.0005  0.1176\n",
      "     47            1.0000        0.0333       0.2917            0.2917        2.7173  0.0004  0.1516\n",
      "     48            1.0000        \u001b[32m0.0225\u001b[0m       0.2812            0.2812        2.7444  0.0004  0.1469\n",
      "     49            1.0000        \u001b[32m0.0091\u001b[0m       0.2812            0.2812        2.7623  0.0003  0.1415\n",
      "     50            1.0000        0.0178       0.2812            0.2812        2.7735  0.0003  0.1447\n",
      "Fine tuning model for subject 5 with 10 = 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        1.6794       0.3333            0.3333        2.1197  0.0004  0.1510\n",
      "     32            0.4000        1.4126       0.3333            0.3333        1.9851  0.0005  0.1462\n",
      "     33            0.7000        1.0156       0.2812            0.2812        1.8601  0.0005  0.1388\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4479\u001b[0m       0.3125            0.3125        1.8302  0.0006  0.1423\n",
      "     35            1.0000        \u001b[32m0.2456\u001b[0m       0.2708            0.2708        1.8880  0.0006  0.1352\n",
      "     36            1.0000        \u001b[32m0.1048\u001b[0m       0.2188            0.2188        1.9933  0.0007  0.1390\n",
      "     37            1.0000        0.1694       0.2292            0.2292        2.1228  0.0007  0.1399\n",
      "     38            1.0000        \u001b[32m0.0769\u001b[0m       0.2083            0.2083        2.2549  0.0007  0.1432\n",
      "     39            1.0000        0.0782       0.2188            0.2188        2.3674  0.0007  0.1373\n",
      "     40            1.0000        \u001b[32m0.0546\u001b[0m       0.1979            0.1979        2.4637  0.0007  0.1466\n",
      "     41            1.0000        0.0779       0.1979            0.1979        2.5294  0.0007  0.1385\n",
      "     42            1.0000        0.0584       0.2083            0.2083        2.5733  0.0007  0.1359\n",
      "     43            1.0000        \u001b[32m0.0254\u001b[0m       0.2188            0.2188        2.6010  0.0006  0.1491\n",
      "     44            1.0000        0.0370       0.2188            0.2188        2.6130  0.0006  0.1752\n",
      "     45            1.0000        \u001b[32m0.0214\u001b[0m       0.2292            0.2292        2.6120  0.0005  0.1154\n",
      "     46            1.0000        \u001b[32m0.0172\u001b[0m       0.2396            0.2396        2.6010  0.0005  0.1174\n",
      "     47            1.0000        0.0175       0.2292            0.2292        2.5818  0.0004  0.1103\n",
      "     48            1.0000        0.0569       0.2292            0.2292        2.5563  0.0004  0.1155\n",
      "     49            1.0000        0.0180       0.2396            0.2396        2.5280  0.0003  0.1228\n",
      "     50            1.0000        0.0333       0.2292            0.2292        2.4962  0.0003  0.1130\n",
      "Fine tuning model for subject 5 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.8395       0.3229            0.3229        2.1837  0.0004  0.1118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4000        1.4979       0.3333            0.3333        2.1343  0.0005  0.1069\n",
      "     33            0.4000        1.0949       0.3021            0.3021        2.1479  0.0005  0.1103\n",
      "     34            0.5000        \u001b[32m0.3920\u001b[0m       0.2708            0.2708        2.2758  0.0006  0.1123\n",
      "     35            0.5000        \u001b[32m0.2108\u001b[0m       0.2500            0.2500        2.4665  0.0006  0.1127\n",
      "     36            0.5000        \u001b[32m0.1650\u001b[0m       0.2500            0.2500        2.7128  0.0007  0.1128\n",
      "     37            0.7000        0.1839       0.2500            0.2500        2.9378  0.0007  0.1125\n",
      "     38            0.8000        \u001b[32m0.0434\u001b[0m       0.2500            0.2500        3.0761  0.0007  0.1099\n",
      "     39            0.8000        0.0436       0.2500            0.2500        3.1560  0.0007  0.1114\n",
      "     40            \u001b[36m0.9000\u001b[0m        \u001b[32m0.0359\u001b[0m       0.2500            0.2500        3.1647  0.0007  0.1213\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0245\u001b[0m       0.2604            0.2604        3.1236  0.0007  0.1123\n",
      "     42            1.0000        \u001b[32m0.0150\u001b[0m       0.2604            0.2604        3.0525  0.0007  0.1527\n",
      "     43            1.0000        0.0190       0.2708            0.2708        2.9657  0.0006  0.1137\n",
      "     44            1.0000        0.0307       0.3229            0.3229        2.8775  0.0006  0.1095\n",
      "     45            1.0000        0.0267       0.2917            0.2917        2.7881  0.0005  0.1108\n",
      "     46            1.0000        0.0160       0.3021            0.3021        2.7045  0.0005  0.1075\n",
      "     47            1.0000        \u001b[32m0.0096\u001b[0m       0.2812            0.2812        2.6282  0.0004  0.1109\n",
      "     48            1.0000        0.0214       0.2812            0.2812        2.5601  0.0004  0.1125\n",
      "     49            1.0000        0.0124       0.2917            0.2917        2.4999  0.0003  0.1096\n",
      "     50            1.0000        0.0115       0.2812            0.2812        2.4477  0.0003  0.1132\n",
      "Fine tuning model for subject 5 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.3115       0.3229            0.3229        2.1038  0.0004  0.1177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5000        0.9308       0.3125            0.3125        2.0407  0.0005  0.1109\n",
      "     33            0.6000        \u001b[32m0.6324\u001b[0m       0.3229            0.3229        2.0096  0.0005  0.1075\n",
      "     34            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4158\u001b[0m       0.3750            0.3750        2.0354  0.0006  0.1141\n",
      "     35            0.9000        \u001b[32m0.2197\u001b[0m       0.3646            0.3646        2.0899  0.0006  0.1170\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1368\u001b[0m       0.3229            0.3229        2.1375  0.0007  0.1121\n",
      "     37            1.0000        0.1441       0.3125            0.3125        2.1507  0.0007  0.1106\n",
      "     38            1.0000        \u001b[32m0.1110\u001b[0m       0.3125            0.3125        2.1235  0.0007  0.1095\n",
      "     39            1.0000        \u001b[32m0.0456\u001b[0m       0.2708            0.2708        2.0923  0.0007  0.1108\n",
      "     40            1.0000        0.0737       0.2708            0.2708        2.0619  0.0007  0.1110\n",
      "     41            1.0000        0.0461       0.2708            0.2708        2.0410  0.0007  0.1082\n",
      "     42            1.0000        0.0987       0.2604            0.2604        2.0254  0.0007  0.1145\n",
      "     43            1.0000        \u001b[32m0.0381\u001b[0m       0.2500            0.2500        2.0195  0.0006  0.1083\n",
      "     44            1.0000        0.0536       0.2396            0.2396        2.0175  0.0006  0.1097\n",
      "     45            1.0000        0.0500       0.2396            0.2396        2.0167  0.0005  0.1077\n",
      "     46            1.0000        0.0454       0.2292            0.2292        2.0164  0.0005  0.1093\n",
      "     47            1.0000        \u001b[32m0.0292\u001b[0m       0.2292            0.2292        2.0153  0.0004  0.1161\n",
      "     48            1.0000        0.0384       0.2396            0.2396        2.0121  0.0004  0.1270\n",
      "     49            1.0000        \u001b[32m0.0251\u001b[0m       0.2396            0.2396        2.0077  0.0003  0.1338\n",
      "     50            1.0000        \u001b[32m0.0205\u001b[0m       0.2604            0.2604        2.0017  0.0003  0.1379\n",
      "Fine tuning model for subject 5 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.1000        1.6801       0.3333            0.3333        2.1046  0.0004  0.1077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4000        1.0353       0.3229            0.3229        2.0226  0.0005  0.1109\n",
      "     33            0.7000        \u001b[32m0.6234\u001b[0m       0.3229            0.3229        1.9866  0.0005  0.1080\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5582\u001b[0m       0.3646            0.3646        1.9979  0.0006  0.1237\n",
      "     35            1.0000        \u001b[32m0.1966\u001b[0m       0.3125            0.3125        2.0319  0.0006  0.1175\n",
      "     36            1.0000        \u001b[32m0.1272\u001b[0m       0.3021            0.3021        2.1301  0.0007  0.1115\n",
      "     37            1.0000        0.1346       0.2917            0.2917        2.2951  0.0007  0.1171\n",
      "     38            1.0000        \u001b[32m0.0658\u001b[0m       0.2917            0.2917        2.4986  0.0007  0.1135\n",
      "     39            1.0000        0.0700       0.3021            0.3021        2.7138  0.0007  0.1123\n",
      "     40            1.0000        \u001b[32m0.0321\u001b[0m       0.2812            0.2812        2.9113  0.0007  0.1114\n",
      "     41            1.0000        0.0479       0.3021            0.3021        3.0645  0.0007  0.1120\n",
      "     42            1.0000        \u001b[32m0.0123\u001b[0m       0.3021            0.3021        3.1702  0.0007  0.1174\n",
      "     43            1.0000        0.0927       0.2917            0.2917        3.2173  0.0006  0.1153\n",
      "     44            1.0000        0.0530       0.2812            0.2812        3.2130  0.0006  0.1044\n",
      "     45            1.0000        \u001b[32m0.0106\u001b[0m       0.2812            0.2812        3.1728  0.0005  0.1134\n",
      "     46            1.0000        0.0417       0.2917            0.2917        3.1013  0.0005  0.1166\n",
      "     47            1.0000        0.0203       0.2917            0.2917        3.0096  0.0004  0.1120\n",
      "     48            1.0000        0.0151       0.3125            0.3125        2.9074  0.0004  0.1070\n",
      "     49            1.0000        0.0248       0.3438            0.3438        2.7995  0.0003  0.1126\n",
      "     50            1.0000        0.0248       0.3542            0.3542        2.6930  0.0003  0.1074\n",
      "Fine tuning model for subject 5 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        1.6847       0.3125            0.3125        2.1282  0.0004  0.1162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.3000        1.8738       0.3229            0.3229        1.9869  0.0005  0.1122\n",
      "     33            0.6000        0.9586       0.3125            0.3125        1.8429  0.0005  0.1097\n",
      "     34            0.8000        \u001b[32m0.5632\u001b[0m       0.3021            0.3021        1.7597  0.0006  0.1099\n",
      "     35            \u001b[36m0.9000\u001b[0m        \u001b[32m0.2152\u001b[0m       0.2917            0.2917        1.7472  0.0006  0.1135\n",
      "     36            \u001b[36m1.0000\u001b[0m        0.2191       0.3229            0.3229        1.7438  0.0007  0.1111\n",
      "     37            1.0000        \u001b[32m0.1009\u001b[0m       0.2604            0.2604        1.7253  0.0007  0.1134\n",
      "     38            1.0000        0.1348       0.2708            0.2708        1.7040  0.0007  0.1110\n",
      "     39            1.0000        \u001b[32m0.0783\u001b[0m       0.2917            0.2917        1.6948  0.0007  0.1114\n",
      "     40            1.0000        \u001b[32m0.0552\u001b[0m       0.3542            0.3542        1.7319  0.0007  0.1143\n",
      "     41            1.0000        \u001b[32m0.0300\u001b[0m       0.3438            0.3438        1.8227  0.0007  0.1217\n",
      "     42            1.0000        \u001b[32m0.0267\u001b[0m       0.3438            0.3438        1.9475  0.0007  0.1127\n",
      "     43            1.0000        0.0316       0.3646            0.3646        2.0764  0.0006  0.1159\n",
      "     44            1.0000        \u001b[32m0.0161\u001b[0m       0.3542            0.3542        2.1843  0.0006  0.1050\n",
      "     45            1.0000        0.0363       0.3542            0.3542        2.2590  0.0005  0.1153\n",
      "     46            1.0000        0.0315       0.3542            0.3542        2.3001  0.0005  0.1120\n",
      "     47            1.0000        0.0530       0.3333            0.3333        2.3114  0.0004  0.1155\n",
      "     48            1.0000        0.0212       0.3542            0.3542        2.2983  0.0004  0.1173\n",
      "     49            1.0000        0.0237       0.3542            0.3542        2.2702  0.0003  0.1045\n",
      "     50            1.0000        0.0241       0.3542            0.3542        2.2318  0.0003  0.1139\n",
      "Fine tuning model for subject 5 with 10 = 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.1000        2.3767       0.3438            0.3438        2.0425  0.0004  0.1078\n",
      "     32            0.6000        1.9538       0.3646            0.3646        1.8361  0.0005  0.1118\n",
      "     33            0.8000        1.0961       0.3333            0.3333        1.7229  0.0005  0.1105\n",
      "     34            \u001b[36m1.0000\u001b[0m        0.7505       0.2604            0.2604        1.7873  0.0006  0.1109\n",
      "     35            1.0000        \u001b[32m0.4672\u001b[0m       0.2500            0.2500        1.9828  0.0006  0.1101\n",
      "     36            0.9000        \u001b[32m0.1622\u001b[0m       0.2396            0.2396        2.2220  0.0007  0.1096\n",
      "     37            0.8000        \u001b[32m0.0993\u001b[0m       0.2500            0.2500        2.4457  0.0007  0.1133\n",
      "     38            0.8000        \u001b[32m0.0755\u001b[0m       0.2708            0.2708        2.6301  0.0007  0.1100\n",
      "     39            0.8000        \u001b[32m0.0514\u001b[0m       0.2708            0.2708        2.7597  0.0007  0.1108\n",
      "     40            0.8000        \u001b[32m0.0449\u001b[0m       0.2708            0.2708        2.8335  0.0007  0.1126\n",
      "     41            0.8000        0.0503       0.2708            0.2708        2.8585  0.0007  0.1128\n",
      "     42            0.8000        0.0688       0.2708            0.2708        2.8147  0.0007  0.1138\n",
      "     43            0.8000        0.0518       0.2812            0.2812        2.7339  0.0006  0.1156\n",
      "     44            1.0000        \u001b[32m0.0431\u001b[0m       0.2917            0.2917        2.6292  0.0006  0.1245\n",
      "     45            1.0000        \u001b[32m0.0276\u001b[0m       0.2917            0.2917        2.5168  0.0005  0.1360\n",
      "     46            1.0000        \u001b[32m0.0193\u001b[0m       0.3021            0.3021        2.4062  0.0005  0.1506\n",
      "     47            1.0000        0.0284       0.3125            0.3125        2.3027  0.0004  0.1437\n",
      "     48            1.0000        0.0230       0.2917            0.2917        2.2114  0.0004  0.1448\n",
      "     49            1.0000        \u001b[32m0.0137\u001b[0m       0.3333            0.3333        2.1335  0.0003  0.1403\n",
      "     50            1.0000        0.0205       0.3125            0.3125        2.0693  0.0003  0.1718\n",
      "Fine tuning model for subject 5 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.4794       0.3125            0.3125        2.1215  0.0004  0.1378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.4013       0.3125            0.3125        1.9984  0.0005  0.1458\n",
      "     33            0.7000        0.7605       0.2812            0.2812        1.8861  0.0005  0.1362\n",
      "     34            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4848\u001b[0m       0.2917            0.2917        1.8192  0.0006  0.1319\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3408\u001b[0m       0.2917            0.2917        1.8175  0.0006  0.1456\n",
      "     36            1.0000        \u001b[32m0.1510\u001b[0m       0.3125            0.3125        1.8374  0.0007  0.1384\n",
      "     37            1.0000        \u001b[32m0.1446\u001b[0m       0.3021            0.3021        1.8743  0.0007  0.1382\n",
      "     38            1.0000        \u001b[32m0.0762\u001b[0m       0.3333            0.3333        1.9206  0.0007  0.1503\n",
      "     39            1.0000        0.0797       0.3229            0.3229        1.9798  0.0007  0.1416\n",
      "     40            1.0000        \u001b[32m0.0548\u001b[0m       0.3125            0.3125        2.0506  0.0007  0.1328\n",
      "     41            1.0000        \u001b[32m0.0461\u001b[0m       0.3021            0.3021        2.1289  0.0007  0.1814\n",
      "     42            1.0000        0.0467       0.3438            0.3438        2.2041  0.0007  0.1608\n",
      "     43            1.0000        \u001b[32m0.0451\u001b[0m       0.3333            0.3333        2.2759  0.0006  0.1166\n",
      "     44            1.0000        \u001b[32m0.0246\u001b[0m       0.3438            0.3438        2.3302  0.0006  0.1164\n",
      "     45            1.0000        0.0254       0.3125            0.3125        2.3666  0.0005  0.1195\n",
      "     46            1.0000        \u001b[32m0.0207\u001b[0m       0.3125            0.3125        2.3854  0.0005  0.1150\n",
      "     47            1.0000        0.0216       0.3125            0.3125        2.3891  0.0004  0.1062\n",
      "     48            1.0000        \u001b[32m0.0167\u001b[0m       0.3125            0.3125        2.3808  0.0004  0.1115\n",
      "     49            1.0000        0.0317       0.3125            0.3125        2.3622  0.0003  0.1114\n",
      "     50            1.0000        0.0198       0.3021            0.3021        2.3374  0.0003  0.1130\n",
      "Fine tuning model for subject 5 with 15 = 15 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.1333        0.9992       0.3333            0.3333        2.1132  0.0004  0.1125\n",
      "     32            0.2667        1.1985       0.3333            0.3333        2.0161  0.0005  0.1139\n",
      "     33            0.6000        1.0988       0.3021            0.3021        1.9093  0.0005  0.1111\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5914\u001b[0m       0.2812            0.2812        1.8155  0.0006  0.1147\n",
      "     35            1.0000        \u001b[32m0.4201\u001b[0m       0.2604            0.2604        1.7590  0.0006  0.1128\n",
      "     36            1.0000        \u001b[32m0.3661\u001b[0m       0.3125            0.3125        1.7645  0.0007  0.1095\n",
      "     37            1.0000        \u001b[32m0.1448\u001b[0m       0.2917            0.2917        1.8041  0.0007  0.1140\n",
      "     38            1.0000        \u001b[32m0.1338\u001b[0m       0.2812            0.2812        1.8559  0.0007  0.1177\n",
      "     39            1.0000        \u001b[32m0.1303\u001b[0m       0.2604            0.2604        1.9034  0.0007  0.1065\n",
      "     40            1.0000        \u001b[32m0.0960\u001b[0m       0.2708            0.2708        1.9274  0.0007  0.1135\n",
      "     41            1.0000        \u001b[32m0.0621\u001b[0m       0.2812            0.2812        1.9470  0.0007  0.1106\n",
      "     42            1.0000        0.0784       0.2917            0.2917        1.9667  0.0007  0.1105\n",
      "     43            1.0000        0.0669       0.3021            0.3021        1.9806  0.0006  0.1061\n",
      "     44            1.0000        \u001b[32m0.0485\u001b[0m       0.3021            0.3021        1.9893  0.0006  0.1082\n",
      "     45            1.0000        \u001b[32m0.0483\u001b[0m       0.2917            0.2917        1.9934  0.0005  0.1066\n",
      "     46            1.0000        \u001b[32m0.0263\u001b[0m       0.3021            0.3021        1.9966  0.0005  0.1111\n",
      "     47            1.0000        0.0469       0.3021            0.3021        1.9977  0.0004  0.1097\n",
      "     48            1.0000        0.0566       0.3021            0.3021        1.9971  0.0004  0.1129\n",
      "     49            1.0000        0.0279       0.3021            0.3021        1.9950  0.0003  0.1064\n",
      "     50            1.0000        0.0368       0.3021            0.3021        1.9916  0.0003  0.1057\n",
      "Fine tuning model for subject 5 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.1333        1.6395       0.3229            0.3229        2.1245  0.0004  0.1066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.1333        1.5406       0.3125            0.3125        2.0538  0.0005  0.1169\n",
      "     33            0.3333        1.2394       0.3229            0.3229        2.0619  0.0005  0.1110\n",
      "     34            0.3333        0.7122       0.2917            0.2917        2.2392  0.0006  0.1119\n",
      "     35            0.3333        \u001b[32m0.5615\u001b[0m       0.2812            0.2812        2.5002  0.0006  0.1113\n",
      "     36            0.4000        \u001b[32m0.3597\u001b[0m       0.2917            0.2917        2.6619  0.0007  0.1078\n",
      "     37            0.6667        0.4143       0.2917            0.2917        2.5173  0.0007  0.1122\n",
      "     38            0.7333        \u001b[32m0.1206\u001b[0m       0.3021            0.3021        2.3436  0.0007  0.1115\n",
      "     39            0.8000        0.1575       0.3125            0.3125        2.2322  0.0007  0.1044\n",
      "     40            \u001b[36m0.9333\u001b[0m        \u001b[32m0.0969\u001b[0m       0.3125            0.3125        2.1606  0.0007  0.1130\n",
      "     41            0.9333        \u001b[32m0.0964\u001b[0m       0.3542            0.3542        2.1255  0.0007  0.1092\n",
      "     42            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0876\u001b[0m       0.3229            0.3229        2.1103  0.0007  0.1090\n",
      "     43            1.0000        \u001b[32m0.0528\u001b[0m       0.3333            0.3333        2.1022  0.0006  0.1130\n",
      "     44            1.0000        0.0591       0.3021            0.3021        2.0950  0.0006  0.1127\n",
      "     45            1.0000        0.0586       0.2917            0.2917        2.0837  0.0005  0.1122\n",
      "     46            1.0000        0.1094       0.2917            0.2917        2.0651  0.0005  0.1127\n",
      "     47            1.0000        \u001b[32m0.0297\u001b[0m       0.3021            0.3021        2.0451  0.0004  0.1095\n",
      "     48            1.0000        0.0462       0.3021            0.3021        2.0245  0.0004  0.1037\n",
      "     49            1.0000        \u001b[32m0.0274\u001b[0m       0.3021            0.3021        2.0048  0.0003  0.1145\n",
      "     50            1.0000        0.0347       0.2917            0.2917        1.9876  0.0003  0.1091\n",
      "Fine tuning model for subject 5 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2667        1.7315       0.3125            0.3125        2.1647  0.0004  0.1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.3333        1.8839       0.3229            0.3229        2.1432  0.0005  0.1085\n",
      "     33            0.4667        1.4239       0.3229            0.3229        2.1201  0.0005  0.1126\n",
      "     34            0.6667        0.9147       0.3125            0.3125        2.0380  0.0006  0.1091\n",
      "     35            0.7333        0.7112       0.3333            0.3333        1.9738  0.0006  0.1137\n",
      "     36            \u001b[36m0.8667\u001b[0m        \u001b[32m0.5334\u001b[0m       0.3646            0.3646        1.9271  0.0007  0.1123\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3094\u001b[0m       0.3750            0.3750        1.8831  0.0007  0.1292\n",
      "     38            1.0000        \u001b[32m0.1907\u001b[0m       0.3333            0.3333        1.8515  0.0007  0.1252\n",
      "     39            1.0000        \u001b[32m0.1174\u001b[0m       0.3125            0.3125        1.8261  0.0007  0.1190\n",
      "     40            1.0000        0.1732       0.3125            0.3125        1.8194  0.0007  0.1142\n",
      "     41            1.0000        0.1410       0.2812            0.2812        1.8274  0.0007  0.1143\n",
      "     42            1.0000        \u001b[32m0.0632\u001b[0m       0.2708            0.2708        1.8462  0.0007  0.1122\n",
      "     43            1.0000        0.0851       0.2708            0.2708        1.8690  0.0006  0.1171\n",
      "     44            1.0000        0.0888       0.2708            0.2708        1.8928  0.0006  0.1123\n",
      "     45            1.0000        0.0859       0.2812            0.2812        1.9142  0.0005  0.1170\n",
      "     46            1.0000        \u001b[32m0.0500\u001b[0m       0.2917            0.2917        1.9296  0.0005  0.1156\n",
      "     47            1.0000        \u001b[32m0.0399\u001b[0m       0.2812            0.2812        1.9387  0.0004  0.1071\n",
      "     48            1.0000        0.0528       0.2604            0.2604        1.9426  0.0004  0.1112\n",
      "     49            1.0000        0.0594       0.2396            0.2396        1.9421  0.0003  0.1098\n",
      "     50            1.0000        \u001b[32m0.0283\u001b[0m       0.2396            0.2396        1.9367  0.0003  0.1088\n",
      "Fine tuning model for subject 5 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2000        1.3583       0.3333            0.3333        2.1007  0.0004  0.1084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4000        1.4229       0.3438            0.3438        1.9540  0.0005  0.1096\n",
      "     33            0.6000        1.0598       0.3438            0.3438        1.8014  0.0005  0.1126\n",
      "     34            \u001b[36m0.9333\u001b[0m        0.8180       0.3333            0.3333        1.6966  0.0006  0.1093\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3726\u001b[0m       0.3438            0.3438        1.6481  0.0006  0.1197\n",
      "     36            1.0000        \u001b[32m0.2638\u001b[0m       0.3542            0.3542        1.6385  0.0007  0.1252\n",
      "     37            1.0000        0.2774       0.3333            0.3333        1.6520  0.0007  0.1331\n",
      "     38            1.0000        \u001b[32m0.2325\u001b[0m       0.3438            0.3438        1.6668  0.0007  0.1446\n",
      "     39            0.9333        \u001b[32m0.0792\u001b[0m       0.3438            0.3438        1.6812  0.0007  0.1216\n",
      "     40            0.9333        0.0966       0.3542            0.3542        1.6886  0.0007  0.1174\n",
      "     41            1.0000        0.1053       0.3854            0.3854        1.7000  0.0007  0.1123\n",
      "     42            1.0000        \u001b[32m0.0552\u001b[0m       0.3854            0.3854        1.7088  0.0007  0.1083\n",
      "     43            1.0000        0.0817       0.3750            0.3750        1.7170  0.0006  0.1127\n",
      "     44            1.0000        0.1078       0.3854            0.3854        1.7212  0.0006  0.1172\n",
      "     45            1.0000        0.0943       0.3854            0.3854        1.7268  0.0005  0.1163\n",
      "     46            1.0000        \u001b[32m0.0375\u001b[0m       0.3646            0.3646        1.7340  0.0005  0.1152\n",
      "     47            1.0000        0.0531       0.3438            0.3438        1.7445  0.0004  0.1109\n",
      "     48            1.0000        0.0480       0.3438            0.3438        1.7560  0.0004  0.1107\n",
      "     49            1.0000        0.0623       0.3438            0.3438        1.7666  0.0003  0.1067\n",
      "     50            1.0000        \u001b[32m0.0189\u001b[0m       0.3542            0.3542        1.7779  0.0003  0.1100\n",
      "Fine tuning model for subject 5 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.0667        1.9321       0.3021            0.3021        2.0615  0.0004  0.1093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.1333        1.5762       0.3125            0.3125        1.9005  0.0005  0.1124\n",
      "     33            0.4000        1.1230       0.2812            0.2812        1.7723  0.0005  0.1067\n",
      "     34            0.6667        0.7296       0.2708            0.2708        1.7162  0.0006  0.1120\n",
      "     35            \u001b[36m0.8667\u001b[0m        \u001b[32m0.5176\u001b[0m       0.2917            0.2917        1.7300  0.0006  0.1135\n",
      "     36            \u001b[36m0.9333\u001b[0m        0.5309       0.3333            0.3333        1.7602  0.0007  0.1097\n",
      "     37            0.9333        \u001b[32m0.3041\u001b[0m       0.3125            0.3125        1.8052  0.0007  0.1103\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2388\u001b[0m       0.3229            0.3229        1.8273  0.0007  0.1136\n",
      "     39            1.0000        \u001b[32m0.1618\u001b[0m       0.3333            0.3333        1.8384  0.0007  0.1076\n",
      "     40            1.0000        \u001b[32m0.1051\u001b[0m       0.3229            0.3229        1.8435  0.0007  0.1144\n",
      "     41            1.0000        \u001b[32m0.0477\u001b[0m       0.3438            0.3438        1.8526  0.0007  0.1349\n",
      "     42            1.0000        0.0707       0.3333            0.3333        1.8685  0.0007  0.1337\n",
      "     43            1.0000        0.0511       0.3333            0.3333        1.8891  0.0006  0.1324\n",
      "     44            1.0000        0.0510       0.3229            0.3229        1.9140  0.0006  0.1261\n",
      "     45            1.0000        0.0622       0.3229            0.3229        1.9400  0.0005  0.1350\n",
      "     46            1.0000        0.0518       0.3333            0.3333        1.9652  0.0005  0.1266\n",
      "     47            1.0000        \u001b[32m0.0342\u001b[0m       0.3229            0.3229        1.9872  0.0004  0.1688\n",
      "     48            1.0000        0.0540       0.3229            0.3229        2.0077  0.0004  0.1277\n",
      "     49            1.0000        0.0463       0.3125            0.3125        2.0259  0.0003  0.1415\n",
      "     50            1.0000        0.0410       0.3125            0.3125        2.0407  0.0003  0.1319\n",
      "Fine tuning model for subject 5 with 15 = 15 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2667        1.4079       0.3125            0.3125        2.1340  0.0004  0.1384\n",
      "     32            0.4000        1.3576       0.3125            0.3125        2.0635  0.0005  0.1417\n",
      "     33            0.4667        1.1708       0.2708            0.2708        2.0228  0.0005  0.1347\n",
      "     34            0.7333        \u001b[32m0.4146\u001b[0m       0.2812            0.2812        1.9750  0.0006  0.1443\n",
      "     35            0.8000        \u001b[32m0.3337\u001b[0m       0.3125            0.3125        1.9163  0.0006  0.1377\n",
      "     36            \u001b[36m0.9333\u001b[0m        \u001b[32m0.2000\u001b[0m       0.3125            0.3125        1.8986  0.0007  0.1514\n",
      "     37            0.9333        \u001b[32m0.1707\u001b[0m       0.3021            0.3021        1.9182  0.0007  0.1349\n",
      "     38            0.9333        0.2577       0.3021            0.3021        1.9536  0.0007  0.1544\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1617\u001b[0m       0.3333            0.3333        1.9839  0.0007  0.1662\n",
      "     40            1.0000        \u001b[32m0.0995\u001b[0m       0.3125            0.3125        2.0123  0.0007  0.1233\n",
      "     41            1.0000        0.1136       0.2917            0.2917        2.0319  0.0007  0.1176\n",
      "     42            1.0000        \u001b[32m0.0808\u001b[0m       0.2708            0.2708        2.0455  0.0007  0.1102\n",
      "     43            1.0000        0.1075       0.2812            0.2812        2.0555  0.0006  0.1101\n",
      "     44            1.0000        \u001b[32m0.0603\u001b[0m       0.2604            0.2604        2.0610  0.0006  0.1611\n",
      "     45            1.0000        0.0649       0.2500            0.2500        2.0605  0.0005  0.1148\n",
      "     46            1.0000        0.0654       0.2500            0.2500        2.0523  0.0005  0.1092\n",
      "     47            1.0000        \u001b[32m0.0364\u001b[0m       0.2604            0.2604        2.0405  0.0004  0.1117\n",
      "     48            1.0000        0.0551       0.2708            0.2708        2.0269  0.0004  0.1100\n",
      "     49            1.0000        \u001b[32m0.0346\u001b[0m       0.2708            0.2708        2.0115  0.0003  0.1115\n",
      "     50            1.0000        \u001b[32m0.0284\u001b[0m       0.2708            0.2708        1.9955  0.0003  0.1173\n",
      "Fine tuning model for subject 5 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3333        2.0599       0.3229            0.3229        2.1219  0.0004  0.1087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4667        1.8412       0.3125            0.3125        1.9836  0.0005  0.1117\n",
      "     33            0.7333        0.9835       0.2708            0.2708        1.8390  0.0005  0.1114\n",
      "     34            \u001b[36m1.0000\u001b[0m        0.6934       0.2500            0.2500        1.7556  0.0006  0.1282\n",
      "     35            1.0000        \u001b[32m0.3726\u001b[0m       0.2708            0.2708        1.7584  0.0006  0.1161\n",
      "     36            1.0000        \u001b[32m0.3427\u001b[0m       0.2500            0.2500        1.7946  0.0007  0.1126\n",
      "     37            1.0000        \u001b[32m0.2764\u001b[0m       0.2396            0.2396        1.8267  0.0007  0.1144\n",
      "     38            1.0000        \u001b[32m0.1702\u001b[0m       0.2708            0.2708        1.8392  0.0007  0.1174\n",
      "     39            1.0000        0.2436       0.2604            0.2604        1.8437  0.0007  0.1091\n",
      "     40            1.0000        0.1705       0.2500            0.2500        1.8496  0.0007  0.1204\n",
      "     41            1.0000        \u001b[32m0.0746\u001b[0m       0.2500            0.2500        1.8611  0.0007  0.1056\n",
      "     42            1.0000        0.1529       0.2604            0.2604        1.8770  0.0007  0.1098\n",
      "     43            1.0000        \u001b[32m0.0586\u001b[0m       0.2500            0.2500        1.8960  0.0006  0.1108\n",
      "     44            1.0000        \u001b[32m0.0506\u001b[0m       0.2708            0.2708        1.9145  0.0006  0.1096\n",
      "     45            1.0000        0.0539       0.2708            0.2708        1.9307  0.0005  0.1119\n",
      "     46            1.0000        0.0571       0.2917            0.2917        1.9442  0.0005  0.1110\n",
      "     47            1.0000        0.0904       0.2708            0.2708        1.9541  0.0004  0.1120\n",
      "     48            1.0000        0.0516       0.2708            0.2708        1.9614  0.0004  0.1120\n",
      "     49            1.0000        0.0872       0.2604            0.2604        1.9677  0.0003  0.1136\n",
      "     50            1.0000        \u001b[32m0.0342\u001b[0m       0.2604            0.2604        1.9724  0.0003  0.1182\n",
      "Fine tuning model for subject 5 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2667        1.6220       0.3125            0.3125        2.1088  0.0004  0.1156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5333        1.3584       0.2812            0.2812        1.9932  0.0005  0.1153\n",
      "     33            0.6667        1.0016       0.3229            0.3229        1.8901  0.0005  0.1118\n",
      "     34            \u001b[36m0.9333\u001b[0m        0.7053       0.2812            0.2812        1.8775  0.0006  0.1127\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4394\u001b[0m       0.2604            0.2604        1.9446  0.0006  0.1128\n",
      "     36            1.0000        \u001b[32m0.1293\u001b[0m       0.2917            0.2917        2.0363  0.0007  0.1151\n",
      "     37            1.0000        \u001b[32m0.1070\u001b[0m       0.2604            0.2604        2.1304  0.0007  0.1109\n",
      "     38            0.9333        0.1418       0.2708            0.2708        2.2185  0.0007  0.1116\n",
      "     39            0.9333        \u001b[32m0.1064\u001b[0m       0.2500            0.2500        2.2920  0.0007  0.1099\n",
      "     40            0.9333        \u001b[32m0.0948\u001b[0m       0.2500            0.2500        2.3470  0.0007  0.1093\n",
      "     41            0.9333        0.1406       0.2812            0.2812        2.3894  0.0007  0.1478\n",
      "     42            0.9333        0.1071       0.3021            0.3021        2.4190  0.0007  0.1143\n",
      "     43            1.0000        \u001b[32m0.0876\u001b[0m       0.3021            0.3021        2.4352  0.0006  0.1134\n",
      "     44            1.0000        \u001b[32m0.0320\u001b[0m       0.2812            0.2812        2.4429  0.0006  0.1219\n",
      "     45            1.0000        0.0546       0.2708            0.2708        2.4426  0.0005  0.1129\n",
      "     46            1.0000        0.0630       0.2708            0.2708        2.4325  0.0005  0.1116\n",
      "     47            1.0000        0.0406       0.2708            0.2708        2.4168  0.0004  0.1070\n",
      "     48            1.0000        0.0464       0.2708            0.2708        2.3983  0.0004  0.1195\n",
      "     49            1.0000        0.0383       0.2500            0.2500        2.3783  0.0003  0.1098\n",
      "     50            1.0000        0.0346       0.2500            0.2500        2.3581  0.0003  0.1119\n",
      "Fine tuning model for subject 5 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4667        1.8390       0.3021            0.3021        2.1338  0.0004  0.1098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5333        1.5625       0.3125            0.3125        2.0869  0.0005  0.1108\n",
      "     33            0.4667        0.9906       0.3229            0.3229        2.0445  0.0005  0.1074\n",
      "     34            0.6667        0.7300       0.3021            0.3021        1.9883  0.0006  0.1125\n",
      "     35            \u001b[36m0.8667\u001b[0m        \u001b[32m0.4807\u001b[0m       0.2917            0.2917        1.9186  0.0006  0.1141\n",
      "     36            0.8667        \u001b[32m0.3756\u001b[0m       0.2812            0.2812        1.8812  0.0007  0.1205\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3099\u001b[0m       0.3021            0.3021        1.8642  0.0007  0.1304\n",
      "     38            1.0000        \u001b[32m0.2082\u001b[0m       0.3229            0.3229        1.8654  0.0007  0.1644\n",
      "     39            1.0000        0.2318       0.3125            0.3125        1.8895  0.0007  0.1133\n",
      "     40            1.0000        \u001b[32m0.1248\u001b[0m       0.3125            0.3125        1.9216  0.0007  0.1108\n",
      "     41            1.0000        \u001b[32m0.1218\u001b[0m       0.3229            0.3229        1.9558  0.0007  0.1173\n",
      "     42            1.0000        0.1226       0.3229            0.3229        1.9883  0.0007  0.1210\n",
      "     43            1.0000        \u001b[32m0.1050\u001b[0m       0.3333            0.3333        2.0140  0.0006  0.1169\n",
      "     44            1.0000        \u001b[32m0.0817\u001b[0m       0.3333            0.3333        2.0316  0.0006  0.1149\n",
      "     45            1.0000        \u001b[32m0.0665\u001b[0m       0.3333            0.3333        2.0399  0.0005  0.1167\n",
      "     46            1.0000        0.0792       0.3333            0.3333        2.0411  0.0005  0.1120\n",
      "     47            1.0000        0.0679       0.3333            0.3333        2.0373  0.0004  0.1114\n",
      "     48            1.0000        0.0751       0.3333            0.3333        2.0278  0.0004  0.1129\n",
      "     49            1.0000        \u001b[32m0.0589\u001b[0m       0.3229            0.3229        2.0144  0.0003  0.1102\n",
      "     50            1.0000        \u001b[32m0.0568\u001b[0m       0.3229            0.3229        1.9986  0.0003  0.1155\n",
      "Fine tuning model for subject 5 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2667        1.5884       0.3333            0.3333        2.0943  0.0004  0.1150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4000        1.3431       0.3438            0.3438        1.9715  0.0005  0.1084\n",
      "     33            0.6667        1.0970       0.2917            0.2917        1.8870  0.0005  0.1157\n",
      "     34            0.7333        0.7709       0.3438            0.3438        1.8408  0.0006  0.1348\n",
      "     35            0.7333        \u001b[32m0.4500\u001b[0m       0.3333            0.3333        1.8160  0.0006  0.1076\n",
      "     36            \u001b[36m0.8667\u001b[0m        \u001b[32m0.3882\u001b[0m       0.3125            0.3125        1.8325  0.0007  0.1105\n",
      "     37            \u001b[36m0.9333\u001b[0m        \u001b[32m0.2017\u001b[0m       0.3021            0.3021        1.9065  0.0007  0.1087\n",
      "     38            0.9333        \u001b[32m0.1855\u001b[0m       0.3021            0.3021        1.9795  0.0007  0.1147\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1557\u001b[0m       0.2917            0.2917        2.0975  0.0007  0.1121\n",
      "     40            1.0000        \u001b[32m0.1027\u001b[0m       0.2812            0.2812        2.1780  0.0007  0.1159\n",
      "     41            1.0000        \u001b[32m0.0897\u001b[0m       0.2812            0.2812        2.2257  0.0007  0.1115\n",
      "     42            1.0000        \u001b[32m0.0424\u001b[0m       0.2708            0.2708        2.2456  0.0007  0.1122\n",
      "     43            1.0000        0.1042       0.2604            0.2604        2.2591  0.0006  0.1129\n",
      "     44            1.0000        0.1162       0.2708            0.2708        2.2562  0.0006  0.1067\n",
      "     45            1.0000        0.0477       0.2708            0.2708        2.2463  0.0005  0.1116\n",
      "     46            1.0000        0.0442       0.2708            0.2708        2.2321  0.0005  0.1121\n",
      "     47            1.0000        \u001b[32m0.0213\u001b[0m       0.2708            0.2708        2.2058  0.0004  0.1073\n",
      "     48            1.0000        0.0220       0.2812            0.2812        2.1751  0.0004  0.1179\n",
      "     49            1.0000        \u001b[32m0.0198\u001b[0m       0.2917            0.2917        2.1376  0.0003  0.1171\n",
      "     50            1.0000        0.0325       0.2917            0.2917        2.0998  0.0003  0.1117\n",
      "Fine tuning model for subject 5 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.8085       0.3229            0.3229        2.1511  0.0004  0.1140\n",
      "     32            0.5500        1.3091       0.3438            0.3438        2.1055  0.0005  0.1187\n",
      "     33            0.7500        1.2373       0.3542            0.3542        2.0583  0.0005  0.1091\n",
      "     34            0.8000        0.8792       0.3333            0.3333        2.0194  0.0006  0.1120\n",
      "     35            \u001b[36m1.0000\u001b[0m        0.7647       0.3021            0.3021        1.9991  0.0006  0.1098\n",
      "     36            1.0000        \u001b[32m0.3603\u001b[0m       0.2812            0.2812        1.9770  0.0007  0.1468\n",
      "     37            1.0000        0.3657       0.2396            0.2396        1.9468  0.0007  0.1336\n",
      "     38            1.0000        \u001b[32m0.3201\u001b[0m       0.2604            0.2604        1.9124  0.0007  0.1440\n",
      "     39            1.0000        \u001b[32m0.2628\u001b[0m       0.2708            0.2708        1.8941  0.0007  0.1586\n",
      "     40            1.0000        \u001b[32m0.1863\u001b[0m       0.2604            0.2604        1.9015  0.0007  0.1547\n",
      "     41            1.0000        \u001b[32m0.0865\u001b[0m       0.2604            0.2604        1.9254  0.0007  0.1742\n",
      "     42            1.0000        \u001b[32m0.0763\u001b[0m       0.2812            0.2812        1.9576  0.0007  0.1624\n",
      "     43            1.0000        \u001b[32m0.0718\u001b[0m       0.2708            0.2708        1.9937  0.0006  0.1423\n",
      "     44            1.0000        0.0932       0.2500            0.2500        2.0268  0.0006  0.1477\n",
      "     45            1.0000        0.1141       0.2500            0.2500        2.0581  0.0005  0.1453\n",
      "     46            1.0000        \u001b[32m0.0543\u001b[0m       0.2500            0.2500        2.0871  0.0005  0.1531\n",
      "     47            1.0000        \u001b[32m0.0483\u001b[0m       0.2500            0.2500        2.1126  0.0004  0.1558\n",
      "     48            1.0000        0.0809       0.2396            0.2396        2.1358  0.0004  0.1363\n",
      "     49            1.0000        0.0565       0.2396            0.2396        2.1561  0.0003  0.1247\n",
      "     50            1.0000        0.0517       0.2500            0.2500        2.1753  0.0003  0.1544\n",
      "Fine tuning model for subject 5 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.1500        1.6207       0.3229            0.3229        2.1104  0.0004  0.1338\n",
      "     32            0.3000        1.5611       0.3646            0.3646        2.0003  0.0005  0.1555\n",
      "     33            0.4000        1.3203       0.3438            0.3438        1.8897  0.0005  0.1563\n",
      "     34            0.6000        0.8322       0.3333            0.3333        1.8302  0.0006  0.1231\n",
      "     35            0.8000        0.7467       0.3542            0.3542        1.7876  0.0006  0.1191\n",
      "     36            0.8000        \u001b[32m0.4507\u001b[0m       0.3542            0.3542        1.7756  0.0007  0.1127\n",
      "     37            \u001b[36m0.8500\u001b[0m        \u001b[32m0.3570\u001b[0m       0.3333            0.3333        1.7620  0.0007  0.1188\n",
      "     38            0.8500        \u001b[32m0.2654\u001b[0m       0.3125            0.3125        1.7445  0.0007  0.1101\n",
      "     39            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2254\u001b[0m       0.3229            0.3229        1.7449  0.0007  0.1340\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1693\u001b[0m       0.3021            0.3021        1.7496  0.0007  0.1108\n",
      "     41            1.0000        \u001b[32m0.1355\u001b[0m       0.2917            0.2917        1.7628  0.0007  0.1125\n",
      "     42            1.0000        \u001b[32m0.0822\u001b[0m       0.2812            0.2812        1.7750  0.0007  0.1118\n",
      "     43            1.0000        0.1288       0.2604            0.2604        1.7803  0.0006  0.1158\n",
      "     44            1.0000        0.1094       0.2708            0.2708        1.7822  0.0006  0.1173\n",
      "     45            1.0000        0.0976       0.2604            0.2604        1.7826  0.0005  0.1119\n",
      "     46            1.0000        \u001b[32m0.0731\u001b[0m       0.2708            0.2708        1.7813  0.0005  0.1119\n",
      "     47            1.0000        \u001b[32m0.0418\u001b[0m       0.2604            0.2604        1.7769  0.0004  0.1272\n",
      "     48            1.0000        0.0585       0.2604            0.2604        1.7720  0.0004  0.1110\n",
      "     49            1.0000        0.0593       0.2812            0.2812        1.7676  0.0003  0.1114\n",
      "     50            1.0000        0.0431       0.2917            0.2917        1.7647  0.0003  0.1140\n",
      "Fine tuning model for subject 5 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2500        1.9314       0.3125            0.3125        2.1281  0.0004  0.1518\n",
      "     32            0.4000        1.6386       0.3125            0.3125        2.0346  0.0005  0.1117\n",
      "     33            0.4500        1.3373       0.3125            0.3125        1.9405  0.0005  0.1134\n",
      "     34            0.7500        1.0539       0.2708            0.2708        1.8467  0.0006  0.1346\n",
      "     35            \u001b[36m0.8500\u001b[0m        0.6583       0.2396            0.2396        1.8239  0.0006  0.1129\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4330\u001b[0m       0.2500            0.2500        1.8470  0.0007  0.1168\n",
      "     37            1.0000        \u001b[32m0.3005\u001b[0m       0.3021            0.3021        1.8935  0.0007  0.1131\n",
      "     38            1.0000        \u001b[32m0.2574\u001b[0m       0.3021            0.3021        1.9483  0.0007  0.1158\n",
      "     39            1.0000        \u001b[32m0.2178\u001b[0m       0.3333            0.3333        1.9917  0.0007  0.1182\n",
      "     40            1.0000        0.2227       0.3333            0.3333        2.0297  0.0007  0.1176\n",
      "     41            1.0000        \u001b[32m0.1247\u001b[0m       0.3333            0.3333        2.0568  0.0007  0.1133\n",
      "     42            1.0000        \u001b[32m0.0739\u001b[0m       0.3333            0.3333        2.0782  0.0007  0.1079\n",
      "     43            1.0000        0.1214       0.3438            0.3438        2.0928  0.0006  0.1121\n",
      "     44            1.0000        0.1213       0.3333            0.3333        2.0952  0.0006  0.1225\n",
      "     45            1.0000        \u001b[32m0.0691\u001b[0m       0.3229            0.3229        2.0914  0.0005  0.1166\n",
      "     46            1.0000        \u001b[32m0.0578\u001b[0m       0.3125            0.3125        2.0867  0.0005  0.1172\n",
      "     47            1.0000        0.0876       0.3333            0.3333        2.0781  0.0004  0.1167\n",
      "     48            1.0000        0.0686       0.3438            0.3438        2.0685  0.0004  0.1105\n",
      "     49            1.0000        \u001b[32m0.0503\u001b[0m       0.3438            0.3438        2.0583  0.0003  0.1083\n",
      "     50            1.0000        \u001b[32m0.0502\u001b[0m       0.3542            0.3542        2.0487  0.0003  0.1115\n",
      "Fine tuning model for subject 5 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.9067       0.3229            0.3229        2.0504  0.0004  0.1129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4500        2.0339       0.2396            0.2396        1.9250  0.0005  0.1143\n",
      "     33            0.6000        1.2197       0.2708            0.2708        1.9129  0.0005  0.1110\n",
      "     34            0.7500        0.9141       0.2292            0.2292        1.9074  0.0006  0.1142\n",
      "     35            0.8000        0.8644       0.2396            0.2396        1.9472  0.0006  0.1245\n",
      "     36            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5883\u001b[0m       0.2292            0.2292        1.9915  0.0007  0.1141\n",
      "     37            0.9000        \u001b[32m0.4382\u001b[0m       0.2188            0.2188        2.0090  0.0007  0.1130\n",
      "     38            0.9000        \u001b[32m0.3394\u001b[0m       0.2292            0.2292        2.0026  0.0007  0.1092\n",
      "     39            0.9000        \u001b[32m0.2347\u001b[0m       0.2292            0.2292        2.0175  0.0007  0.1172\n",
      "     40            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2162\u001b[0m       0.1979            0.1979        2.0354  0.0007  0.1123\n",
      "     41            0.9500        \u001b[32m0.2089\u001b[0m       0.2292            0.2292        2.0700  0.0007  0.1060\n",
      "     42            0.9500        \u001b[32m0.1498\u001b[0m       0.2396            0.2396        2.1128  0.0007  0.1174\n",
      "     43            0.9500        0.1663       0.2396            0.2396        2.1664  0.0006  0.1121\n",
      "     44            0.9500        \u001b[32m0.0918\u001b[0m       0.2292            0.2292        2.2080  0.0006  0.1214\n",
      "     45            0.9500        0.0943       0.2708            0.2708        2.2416  0.0005  0.1148\n",
      "     46            \u001b[36m1.0000\u001b[0m        0.1279       0.2812            0.2812        2.2686  0.0005  0.1116\n",
      "     47            1.0000        \u001b[32m0.0688\u001b[0m       0.2917            0.2917        2.2737  0.0004  0.1175\n",
      "     48            1.0000        \u001b[32m0.0673\u001b[0m       0.2708            0.2708        2.2698  0.0004  0.1250\n",
      "     49            1.0000        0.0675       0.2812            0.2812        2.2545  0.0003  0.1175\n",
      "     50            1.0000        \u001b[32m0.0614\u001b[0m       0.2917            0.2917        2.2326  0.0003  0.1097\n",
      "Fine tuning model for subject 5 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2500        1.6724       0.3125            0.3125        2.1383  0.0004  0.1216\n",
      "     32            0.2500        1.4689       0.2917            0.2917        2.0992  0.0005  0.1160\n",
      "     33            0.3000        1.1366       0.2917            0.2917        2.0586  0.0005  0.1108\n",
      "     34            0.6500        0.7078       0.2812            0.2812        2.0020  0.0006  0.1215\n",
      "     35            0.7000        \u001b[32m0.5467\u001b[0m       0.2708            0.2708        2.0058  0.0006  0.1075\n",
      "     36            \u001b[36m0.8500\u001b[0m        \u001b[32m0.3604\u001b[0m       0.2604            0.2604        2.0507  0.0007  0.1229\n",
      "     37            \u001b[36m0.9000\u001b[0m        \u001b[32m0.2473\u001b[0m       0.2812            0.2812        2.0728  0.0007  0.1223\n",
      "     38            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2198\u001b[0m       0.2812            0.2812        2.0753  0.0007  0.1189\n",
      "     39            0.9500        \u001b[32m0.1865\u001b[0m       0.2708            0.2708        2.0781  0.0007  0.1107\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0879\u001b[0m       0.2812            0.2812        2.0800  0.0007  0.1107\n",
      "     41            1.0000        \u001b[32m0.0707\u001b[0m       0.3021            0.3021        2.0844  0.0007  0.1130\n",
      "     42            1.0000        0.0775       0.2812            0.2812        2.0867  0.0007  0.1160\n",
      "     43            1.0000        0.0905       0.3021            0.3021        2.0966  0.0006  0.1113\n",
      "     44            1.0000        \u001b[32m0.0547\u001b[0m       0.2708            0.2708        2.0985  0.0006  0.1225\n",
      "     45            1.0000        0.0771       0.2604            0.2604        2.0955  0.0005  0.1124\n",
      "     46            1.0000        0.0622       0.2812            0.2812        2.0921  0.0005  0.1174\n",
      "     47            1.0000        \u001b[32m0.0449\u001b[0m       0.2917            0.2917        2.0875  0.0004  0.1117\n",
      "     48            1.0000        0.0475       0.3229            0.3229        2.0823  0.0004  0.1113\n",
      "     49            1.0000        \u001b[32m0.0381\u001b[0m       0.3229            0.3229        2.0767  0.0003  0.1094\n",
      "     50            1.0000        \u001b[32m0.0360\u001b[0m       0.3438            0.3438        2.0709  0.0003  0.1122\n",
      "Fine tuning model for subject 5 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        2.1163       0.3333            0.3333        2.0808  0.0004  0.1142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.3500        1.6962       0.3542            0.3542        1.9619  0.0005  0.1148\n",
      "     33            0.3500        1.4851       0.2812            0.2812        2.0166  0.0005  0.1146\n",
      "     34            0.5000        0.9178       0.2917            0.2917        2.1656  0.0006  0.1126\n",
      "     35            0.6000        0.7351       0.2396            0.2396        2.1488  0.0006  0.1166\n",
      "     36            0.7000        \u001b[32m0.4639\u001b[0m       0.2500            0.2500        2.0092  0.0007  0.1180\n",
      "     37            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3789\u001b[0m       0.2604            0.2604        1.8959  0.0007  0.1116\n",
      "     38            0.9000        \u001b[32m0.2984\u001b[0m       0.3021            0.3021        1.8388  0.0007  0.1172\n",
      "     39            0.9000        \u001b[32m0.2925\u001b[0m       0.3333            0.3333        1.8375  0.0007  0.1152\n",
      "     40            0.9000        0.4303       0.3438            0.3438        1.8583  0.0007  0.1115\n",
      "     41            0.9000        0.2959       0.3229            0.3229        1.8925  0.0007  0.1254\n",
      "     42            0.9000        \u001b[32m0.2059\u001b[0m       0.3021            0.3021        1.9261  0.0007  0.1180\n",
      "     43            0.9000        \u001b[32m0.1820\u001b[0m       0.2708            0.2708        1.9504  0.0006  0.1070\n",
      "     44            0.9000        \u001b[32m0.1022\u001b[0m       0.2917            0.2917        1.9704  0.0006  0.1157\n",
      "     45            \u001b[36m0.9500\u001b[0m        \u001b[32m0.0870\u001b[0m       0.2708            0.2708        1.9859  0.0005  0.1099\n",
      "     46            0.9500        0.1091       0.2812            0.2812        1.9804  0.0005  0.1117\n",
      "     47            0.9500        0.1021       0.2812            0.2812        1.9667  0.0004  0.1386\n",
      "     48            0.9500        \u001b[32m0.0681\u001b[0m       0.2917            0.2917        1.9491  0.0004  0.1535\n",
      "     49            \u001b[36m1.0000\u001b[0m        0.0685       0.2917            0.2917        1.9299  0.0003  0.1378\n",
      "     50            1.0000        0.0745       0.3021            0.3021        1.9128  0.0003  0.1384\n",
      "Fine tuning model for subject 5 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.1746       0.3125            0.3125        2.1866  0.0004  0.1373\n",
      "     32            0.6000        1.3672       0.3229            0.3229        2.2358  0.0005  0.1648\n",
      "     33            0.7000        0.9019       0.2917            0.2917        2.3213  0.0005  0.1345\n",
      "     34            0.7500        0.7222       0.2917            0.2917        2.3814  0.0006  0.1340\n",
      "     35            0.8000        \u001b[32m0.3826\u001b[0m       0.3021            0.3021        2.3671  0.0006  0.1386\n",
      "     36            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3047\u001b[0m       0.2917            0.2917        2.2858  0.0007  0.1395\n",
      "     37            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2429\u001b[0m       0.2812            0.2812        2.2088  0.0007  0.1413\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1929\u001b[0m       0.2604            0.2604        2.1465  0.0007  0.1298\n",
      "     39            1.0000        0.2048       0.2708            0.2708        2.0906  0.0007  0.1453\n",
      "     40            1.0000        0.2176       0.2708            0.2708        2.0628  0.0007  0.1385\n",
      "     41            1.0000        \u001b[32m0.1113\u001b[0m       0.2708            0.2708        2.0387  0.0007  0.1561\n",
      "     42            1.0000        0.1113       0.2708            0.2708        2.0228  0.0007  0.1374\n",
      "     43            1.0000        \u001b[32m0.0956\u001b[0m       0.2604            0.2604        2.0236  0.0006  0.1338\n",
      "     44            1.0000        0.1001       0.2708            0.2708        2.0401  0.0006  0.1794\n",
      "     45            1.0000        0.1054       0.2708            0.2708        2.0526  0.0005  0.1198\n",
      "     46            1.0000        \u001b[32m0.0495\u001b[0m       0.2812            0.2812        2.0627  0.0005  0.1145\n",
      "     47            1.0000        \u001b[32m0.0468\u001b[0m       0.2708            0.2708        2.0712  0.0004  0.1195\n",
      "     48            1.0000        \u001b[32m0.0458\u001b[0m       0.2604            0.2604        2.0764  0.0004  0.1126\n",
      "     49            1.0000        \u001b[32m0.0369\u001b[0m       0.2708            0.2708        2.0789  0.0003  0.1161\n",
      "     50            1.0000        0.0460       0.2708            0.2708        2.0793  0.0003  0.1144\n",
      "Fine tuning model for subject 5 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3500        1.9294       0.3125            0.3125        2.2093  0.0004  0.1118\n",
      "     32            0.3500        1.8660       0.3021            0.3021        2.3114  0.0005  0.1162\n",
      "     33            0.3500        1.3911       0.2917            0.2917        2.4292  0.0005  0.1144\n",
      "     34            0.3500        1.1173       0.2917            0.2917        2.4877  0.0006  0.1173\n",
      "     35            0.4000        0.6773       0.2812            0.2812        2.5387  0.0006  0.1239\n",
      "     36            0.5000        \u001b[32m0.4696\u001b[0m       0.2604            0.2604        2.5797  0.0007  0.1435\n",
      "     37            0.6000        \u001b[32m0.3929\u001b[0m       0.2812            0.2812        2.6493  0.0007  0.1172\n",
      "     38            0.7500        \u001b[32m0.3467\u001b[0m       0.3021            0.3021        2.6899  0.0007  0.1304\n",
      "     39            \u001b[36m0.8500\u001b[0m        \u001b[32m0.2804\u001b[0m       0.3229            0.3229        2.7183  0.0007  0.1137\n",
      "     40            0.8500        0.3313       0.3125            0.3125        2.7190  0.0007  0.1151\n",
      "     41            0.8500        \u001b[32m0.1448\u001b[0m       0.3021            0.3021        2.6997  0.0007  0.1149\n",
      "     42            \u001b[36m0.9500\u001b[0m        \u001b[32m0.1239\u001b[0m       0.3125            0.3125        2.6791  0.0007  0.1179\n",
      "     43            0.9500        \u001b[32m0.1232\u001b[0m       0.3021            0.3021        2.6618  0.0006  0.1134\n",
      "     44            0.9500        0.1509       0.2812            0.2812        2.6511  0.0006  0.1176\n",
      "     45            0.9500        \u001b[32m0.0936\u001b[0m       0.2812            0.2812        2.6359  0.0005  0.1087\n",
      "     46            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0656\u001b[0m       0.2812            0.2812        2.6164  0.0005  0.1142\n",
      "     47            1.0000        \u001b[32m0.0599\u001b[0m       0.2812            0.2812        2.5913  0.0004  0.1127\n",
      "     48            1.0000        0.0720       0.2812            0.2812        2.5620  0.0004  0.1171\n",
      "     49            1.0000        0.1104       0.2604            0.2604        2.5308  0.0003  0.1184\n",
      "     50            1.0000        \u001b[32m0.0571\u001b[0m       0.2500            0.2500        2.5005  0.0003  0.1122\n",
      "Fine tuning model for subject 5 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        1.6959       0.3229            0.3229        2.0880  0.0004  0.1158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.3500        1.6992       0.2917            0.2917        1.9432  0.0005  0.1139\n",
      "     33            0.6000        1.6114       0.2604            0.2604        1.8188  0.0005  0.1570\n",
      "     34            \u001b[36m0.9000\u001b[0m        1.0112       0.2604            0.2604        1.7424  0.0006  0.1116\n",
      "     35            \u001b[36m1.0000\u001b[0m        0.7040       0.2604            0.2604        1.7315  0.0006  0.1169\n",
      "     36            1.0000        \u001b[32m0.4613\u001b[0m       0.2917            0.2917        1.7711  0.0007  0.1094\n",
      "     37            1.0000        \u001b[32m0.3158\u001b[0m       0.2708            0.2708        1.8224  0.0007  0.1085\n",
      "     38            1.0000        0.3238       0.2917            0.2917        1.8888  0.0007  0.1172\n",
      "     39            1.0000        \u001b[32m0.2782\u001b[0m       0.2708            0.2708        1.9772  0.0007  0.1096\n",
      "     40            1.0000        \u001b[32m0.2031\u001b[0m       0.2917            0.2917        2.0792  0.0007  0.1115\n",
      "     41            0.9500        \u001b[32m0.1558\u001b[0m       0.3021            0.3021        2.1911  0.0007  0.1127\n",
      "     42            0.9500        0.1832       0.2812            0.2812        2.3047  0.0007  0.1328\n",
      "     43            0.9500        \u001b[32m0.1305\u001b[0m       0.2708            0.2708        2.4068  0.0006  0.1172\n",
      "     44            0.9500        \u001b[32m0.0977\u001b[0m       0.2812            0.2812        2.4869  0.0006  0.1355\n",
      "     45            0.9500        0.1311       0.2917            0.2917        2.5453  0.0005  0.1340\n",
      "     46            0.9500        \u001b[32m0.0642\u001b[0m       0.2917            0.2917        2.5796  0.0005  0.1964\n",
      "     47            1.0000        0.0753       0.2604            0.2604        2.5933  0.0004  0.1897\n",
      "     48            1.0000        \u001b[32m0.0518\u001b[0m       0.2396            0.2396        2.5863  0.0004  0.1863\n",
      "     49            1.0000        \u001b[32m0.0460\u001b[0m       0.2396            0.2396        2.5645  0.0003  0.1121\n",
      "     50            1.0000        0.0528       0.2292            0.2292        2.5338  0.0003  0.1212\n",
      "Fine tuning model for subject 5 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        1.4520       0.3229            0.3229        2.1307  0.0004  0.1128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5000        1.6321       0.3333            0.3333        2.0409  0.0005  0.1177\n",
      "     33            0.7000        1.1207       0.3125            0.3125        1.9381  0.0005  0.1126\n",
      "     34            0.8000        0.8612       0.3021            0.3021        1.8619  0.0006  0.1151\n",
      "     35            0.7500        0.7088       0.2708            0.2708        1.8679  0.0006  0.1171\n",
      "     36            0.7000        \u001b[32m0.3866\u001b[0m       0.2292            0.2292        1.9613  0.0007  0.1186\n",
      "     37            0.7500        \u001b[32m0.3344\u001b[0m       0.1979            0.1979        2.1206  0.0007  0.1124\n",
      "     38            0.7500        0.3519       0.1979            0.1979        2.2715  0.0007  0.1173\n",
      "     39            0.7500        \u001b[32m0.2586\u001b[0m       0.1979            0.1979        2.3778  0.0007  0.1122\n",
      "     40            0.8000        \u001b[32m0.2164\u001b[0m       0.2083            0.2083        2.4532  0.0007  0.1116\n",
      "     41            \u001b[36m0.8500\u001b[0m        \u001b[32m0.1432\u001b[0m       0.2083            0.2083        2.5029  0.0007  0.1119\n",
      "     42            \u001b[36m0.9500\u001b[0m        0.1867       0.2500            0.2500        2.5096  0.0007  0.1154\n",
      "     43            0.9500        \u001b[32m0.1166\u001b[0m       0.2604            0.2604        2.5019  0.0006  0.1120\n",
      "     44            0.9500        \u001b[32m0.0857\u001b[0m       0.2500            0.2500        2.4769  0.0006  0.1124\n",
      "     45            0.9500        \u001b[32m0.0714\u001b[0m       0.2604            0.2604        2.4427  0.0005  0.1122\n",
      "     46            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0679\u001b[0m       0.2604            0.2604        2.4025  0.0005  0.1138\n",
      "     47            1.0000        0.0924       0.2396            0.2396        2.3579  0.0004  0.1146\n",
      "     48            1.0000        0.0816       0.2396            0.2396        2.3133  0.0004  0.1174\n",
      "     49            1.0000        \u001b[32m0.0664\u001b[0m       0.2708            0.2708        2.2720  0.0003  0.1118\n",
      "     50            1.0000        \u001b[32m0.0626\u001b[0m       0.2604            0.2604        2.2332  0.0003  0.1175\n",
      "Fine tuning model for subject 5 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4400        1.6931       0.3229            0.3229        2.1300  0.0004  0.1304\n",
      "     32            0.4800        1.3995       0.3229            0.3229        2.0257  0.0005  0.1240\n",
      "     33            0.6000        1.4710       0.3333            0.3333        1.9680  0.0005  0.1347\n",
      "     34            0.7200        1.0916       0.3333            0.3333        1.9065  0.0006  0.1236\n",
      "     35            \u001b[36m0.8400\u001b[0m        0.7085       0.3125            0.3125        1.8747  0.0006  0.1281\n",
      "     36            0.8400        0.6951       0.3125            0.3125        1.8765  0.0007  0.1246\n",
      "     37            \u001b[36m0.8800\u001b[0m        \u001b[32m0.5297\u001b[0m       0.3021            0.3021        1.9418  0.0007  0.1210\n",
      "     38            0.8800        \u001b[32m0.3795\u001b[0m       0.3021            0.3021        2.0304  0.0007  0.1228\n",
      "     39            0.8800        \u001b[32m0.3090\u001b[0m       0.2812            0.2812        2.1241  0.0007  0.1196\n",
      "     40            0.8000        0.3200       0.3125            0.3125        2.2105  0.0007  0.1174\n",
      "     41            0.8000        \u001b[32m0.2289\u001b[0m       0.3125            0.3125        2.2552  0.0007  0.1174\n",
      "     42            0.8400        \u001b[32m0.1933\u001b[0m       0.3021            0.3021        2.2615  0.0007  0.1245\n",
      "     43            0.8800        \u001b[32m0.1513\u001b[0m       0.3125            0.3125        2.2482  0.0006  0.1197\n",
      "     44            \u001b[36m0.9200\u001b[0m        \u001b[32m0.1461\u001b[0m       0.3125            0.3125        2.2219  0.0006  0.1231\n",
      "     45            \u001b[36m0.9600\u001b[0m        0.1491       0.3021            0.3021        2.1887  0.0005  0.1225\n",
      "     46            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1081\u001b[0m       0.3021            0.3021        2.1578  0.0005  0.1241\n",
      "     47            1.0000        \u001b[32m0.1045\u001b[0m       0.2917            0.2917        2.1248  0.0004  0.1166\n",
      "     48            1.0000        \u001b[32m0.0902\u001b[0m       0.3229            0.3229        2.0944  0.0004  0.1187\n",
      "     49            1.0000        \u001b[32m0.0839\u001b[0m       0.3229            0.3229        2.0642  0.0003  0.1279\n",
      "     50            1.0000        0.0970       0.3229            0.3229        2.0341  0.0003  0.1277\n",
      "Fine tuning model for subject 5 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2800        1.4502       0.3333            0.3333        2.0849  0.0004  0.1171\n",
      "     32            0.4400        1.4104       0.3229            0.3229        1.9425  0.0005  0.1229\n",
      "     33            0.4800        1.1814       0.2917            0.2917        1.8848  0.0005  0.1235\n",
      "     34            0.4800        0.9415       0.2396            0.2396        1.9170  0.0006  0.1422\n",
      "     35            0.4800        0.6624       0.2500            0.2500        1.9254  0.0006  0.1443\n",
      "     36            0.5600        \u001b[32m0.5955\u001b[0m       0.2500            0.2500        1.8999  0.0007  0.1464\n",
      "     37            0.8000        \u001b[32m0.3852\u001b[0m       0.2292            0.2292        1.8490  0.0007  0.1389\n",
      "     38            \u001b[36m0.9600\u001b[0m        0.4504       0.2396            0.2396        1.8360  0.0007  0.1493\n",
      "     39            0.9600        \u001b[32m0.2703\u001b[0m       0.2396            0.2396        1.8449  0.0007  0.1812\n",
      "     40            0.9200        \u001b[32m0.2317\u001b[0m       0.2604            0.2604        1.8691  0.0007  0.1546\n",
      "     41            0.9200        0.2634       0.2500            0.2500        1.8975  0.0007  0.1553\n",
      "     42            0.9200        \u001b[32m0.1597\u001b[0m       0.2292            0.2292        1.9139  0.0007  0.1471\n",
      "     43            0.9600        \u001b[32m0.1368\u001b[0m       0.2292            0.2292        1.9168  0.0006  0.1403\n",
      "     44            0.9600        \u001b[32m0.1253\u001b[0m       0.2500            0.2500        1.9127  0.0006  0.1597\n",
      "     45            0.9600        0.1469       0.2396            0.2396        1.9027  0.0005  0.1472\n",
      "     46            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1127\u001b[0m       0.2604            0.2604        1.8986  0.0005  0.1496\n",
      "     47            1.0000        \u001b[32m0.0907\u001b[0m       0.2500            0.2500        1.8865  0.0004  0.1521\n",
      "     48            1.0000        0.1150       0.2604            0.2604        1.8710  0.0004  0.1815\n",
      "     49            1.0000        \u001b[32m0.0906\u001b[0m       0.2708            0.2708        1.8544  0.0003  0.1551\n",
      "     50            1.0000        \u001b[32m0.0692\u001b[0m       0.2708            0.2708        1.8364  0.0003  0.1749\n",
      "Fine tuning model for subject 5 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2800        1.6368       0.3125            0.3125        2.1423  0.0004  0.1257\n",
      "     32            0.3600        1.5358       0.3333            0.3333        2.0763  0.0005  0.1229\n",
      "     33            0.4000        1.1609       0.3125            0.3125        2.0198  0.0005  0.1197\n",
      "     34            0.4800        0.9795       0.2917            0.2917        1.9650  0.0006  0.1173\n",
      "     35            0.4800        0.8574       0.2708            0.2708        1.9290  0.0006  0.1407\n",
      "     36            0.7600        0.8467       0.2812            0.2812        1.9031  0.0007  0.1171\n",
      "     37            0.7600        \u001b[32m0.4664\u001b[0m       0.2604            0.2604        1.8853  0.0007  0.1217\n",
      "     38            \u001b[36m0.8400\u001b[0m        \u001b[32m0.4533\u001b[0m       0.2708            0.2708        1.8718  0.0007  0.1226\n",
      "     39            \u001b[36m0.9200\u001b[0m        \u001b[32m0.3719\u001b[0m       0.2500            0.2500        1.8792  0.0007  0.1224\n",
      "     40            0.9200        \u001b[32m0.2493\u001b[0m       0.2812            0.2812        1.8866  0.0007  0.1226\n",
      "     41            \u001b[36m0.9600\u001b[0m        0.2704       0.2812            0.2812        1.8908  0.0007  0.1225\n",
      "     42            \u001b[36m1.0000\u001b[0m        0.2972       0.2917            0.2917        1.8921  0.0007  0.1306\n",
      "     43            1.0000        \u001b[32m0.2231\u001b[0m       0.2812            0.2812        1.8947  0.0006  0.1243\n",
      "     44            1.0000        \u001b[32m0.2098\u001b[0m       0.2812            0.2812        1.8971  0.0006  0.1231\n",
      "     45            1.0000        \u001b[32m0.1369\u001b[0m       0.2812            0.2812        1.8977  0.0005  0.1167\n",
      "     46            1.0000        \u001b[32m0.1309\u001b[0m       0.2604            0.2604        1.9008  0.0005  0.1190\n",
      "     47            1.0000        0.1354       0.2917            0.2917        1.9071  0.0004  0.1213\n",
      "     48            1.0000        \u001b[32m0.1224\u001b[0m       0.3021            0.3021        1.9118  0.0004  0.1218\n",
      "     49            1.0000        \u001b[32m0.1119\u001b[0m       0.3125            0.3125        1.9157  0.0003  0.1222\n",
      "     50            1.0000        0.1160       0.3021            0.3021        1.9198  0.0003  0.1222\n",
      "Fine tuning model for subject 5 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.6616       0.3125            0.3125        2.1456  0.0004  0.1203\n",
      "     32            0.5600        1.2926       0.3229            0.3229        2.1029  0.0005  0.1212\n",
      "     33            0.6800        1.1331       0.3021            0.3021        2.0504  0.0005  0.1271\n",
      "     34            0.8000        0.9835       0.2708            0.2708        1.9713  0.0006  0.1178\n",
      "     35            0.8000        0.7674       0.2917            0.2917        1.9206  0.0006  0.1212\n",
      "     36            0.7200        \u001b[32m0.5809\u001b[0m       0.3438            0.3438        1.9413  0.0007  0.1230\n",
      "     37            0.7200        \u001b[32m0.5570\u001b[0m       0.2812            0.2812        1.9937  0.0007  0.1220\n",
      "     38            0.7600        \u001b[32m0.4763\u001b[0m       0.2812            0.2812        2.0489  0.0007  0.1201\n",
      "     39            0.7600        \u001b[32m0.4225\u001b[0m       0.3021            0.3021        2.0899  0.0007  0.1277\n",
      "     40            0.8000        \u001b[32m0.3366\u001b[0m       0.2917            0.2917        2.1018  0.0007  0.1202\n",
      "     41            \u001b[36m0.8400\u001b[0m        \u001b[32m0.3110\u001b[0m       0.2917            0.2917        2.0899  0.0007  0.1232\n",
      "     42            \u001b[36m0.9200\u001b[0m        \u001b[32m0.3090\u001b[0m       0.2917            0.2917        2.0667  0.0007  0.1186\n",
      "     43            0.9200        \u001b[32m0.2161\u001b[0m       0.3021            0.3021        2.0512  0.0006  0.1216\n",
      "     44            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1695\u001b[0m       0.3125            0.3125        2.0354  0.0006  0.1196\n",
      "     45            1.0000        0.1723       0.3229            0.3229        2.0166  0.0005  0.1253\n",
      "     46            1.0000        \u001b[32m0.1264\u001b[0m       0.3229            0.3229        1.9984  0.0005  0.1282\n",
      "     47            1.0000        \u001b[32m0.1243\u001b[0m       0.3229            0.3229        1.9793  0.0004  0.1406\n",
      "     48            1.0000        \u001b[32m0.1101\u001b[0m       0.3125            0.3125        1.9633  0.0004  0.1325\n",
      "     49            1.0000        0.1181       0.3438            0.3438        1.9490  0.0003  0.1273\n",
      "     50            1.0000        0.1261       0.3438            0.3438        1.9395  0.0003  0.1214\n",
      "Fine tuning model for subject 5 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2400        1.0997       0.3229            0.3229        2.0850  0.0004  0.1195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.3600        1.3345       0.2812            0.2812        1.9380  0.0005  0.1237\n",
      "     33            0.6000        1.0179       0.2292            0.2292        1.8072  0.0005  0.1213\n",
      "     34            0.8000        0.8203       0.2083            0.2083        1.7426  0.0006  0.1232\n",
      "     35            \u001b[36m0.8800\u001b[0m        0.6829       0.2188            0.2188        1.7492  0.0006  0.1199\n",
      "     36            0.8800        \u001b[32m0.5107\u001b[0m       0.2083            0.2083        1.7860  0.0007  0.1249\n",
      "     37            \u001b[36m0.9200\u001b[0m        \u001b[32m0.4160\u001b[0m       0.2500            0.2500        1.8301  0.0007  0.1239\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2971\u001b[0m       0.2396            0.2396        1.8678  0.0007  0.1212\n",
      "     39            1.0000        0.3257       0.2396            0.2396        1.8942  0.0007  0.1239\n",
      "     40            1.0000        \u001b[32m0.2461\u001b[0m       0.2292            0.2292        1.9314  0.0007  0.1192\n",
      "     41            1.0000        \u001b[32m0.1857\u001b[0m       0.2396            0.2396        1.9380  0.0007  0.1190\n",
      "     42            1.0000        \u001b[32m0.1273\u001b[0m       0.2292            0.2292        1.9252  0.0007  0.1188\n",
      "     43            1.0000        \u001b[32m0.1188\u001b[0m       0.2396            0.2396        1.9097  0.0006  0.1295\n",
      "     44            1.0000        0.1268       0.2500            0.2500        1.8944  0.0006  0.1251\n",
      "     45            1.0000        \u001b[32m0.1015\u001b[0m       0.2604            0.2604        1.8872  0.0005  0.1283\n",
      "     46            1.0000        \u001b[32m0.0870\u001b[0m       0.2604            0.2604        1.8876  0.0005  0.1263\n",
      "     47            1.0000        \u001b[32m0.0846\u001b[0m       0.2500            0.2500        1.8896  0.0004  0.1218\n",
      "     48            1.0000        \u001b[32m0.0746\u001b[0m       0.2396            0.2396        1.8945  0.0004  0.1230\n",
      "     49            1.0000        \u001b[32m0.0586\u001b[0m       0.2500            0.2500        1.9000  0.0003  0.1211\n",
      "     50            1.0000        0.0671       0.2396            0.2396        1.9051  0.0003  0.1183\n",
      "Fine tuning model for subject 5 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3200        1.5223       0.3125            0.3125        2.1048  0.0004  0.1208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.3600        1.4880       0.3333            0.3333        1.9576  0.0005  0.1234\n",
      "     33            0.4400        1.3043       0.3542            0.3542        1.8292  0.0005  0.1192\n",
      "     34            0.6000        1.1142       0.3438            0.3438        1.7693  0.0006  0.1195\n",
      "     35            0.6800        0.7081       0.3125            0.3125        1.7859  0.0006  0.1225\n",
      "     36            0.7200        \u001b[32m0.4425\u001b[0m       0.3125            0.3125        1.8331  0.0007  0.1387\n",
      "     37            \u001b[36m0.8400\u001b[0m        \u001b[32m0.4106\u001b[0m       0.3438            0.3438        1.8830  0.0007  0.1204\n",
      "     38            \u001b[36m0.8800\u001b[0m        \u001b[32m0.3690\u001b[0m       0.3542            0.3542        1.9139  0.0007  0.1163\n",
      "     39            \u001b[36m0.9200\u001b[0m        \u001b[32m0.2598\u001b[0m       0.3646            0.3646        1.9287  0.0007  0.1168\n",
      "     40            0.9200        0.3385       0.3646            0.3646        1.9352  0.0007  0.1180\n",
      "     41            0.9200        \u001b[32m0.2312\u001b[0m       0.3750            0.3750        1.9387  0.0007  0.1224\n",
      "     42            \u001b[36m0.9600\u001b[0m        \u001b[32m0.2005\u001b[0m       0.3750            0.3750        1.9427  0.0007  0.1231\n",
      "     43            0.9600        \u001b[32m0.1805\u001b[0m       0.3750            0.3750        1.9525  0.0006  0.1170\n",
      "     44            0.9600        \u001b[32m0.1388\u001b[0m       0.3750            0.3750        1.9607  0.0006  0.1252\n",
      "     45            0.9200        \u001b[32m0.1303\u001b[0m       0.3542            0.3542        1.9607  0.0005  0.1186\n",
      "     46            0.9600        \u001b[32m0.1123\u001b[0m       0.3542            0.3542        1.9539  0.0005  0.1177\n",
      "     47            0.9600        \u001b[32m0.0887\u001b[0m       0.3542            0.3542        1.9427  0.0004  0.1231\n",
      "     48            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0806\u001b[0m       0.3646            0.3646        1.9316  0.0004  0.1188\n",
      "     49            1.0000        \u001b[32m0.0804\u001b[0m       0.3854            0.3854        1.9237  0.0003  0.1187\n",
      "     50            1.0000        \u001b[32m0.0716\u001b[0m       0.3854            0.3854        1.9151  0.0003  0.1209\n",
      "Fine tuning model for subject 5 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4800        1.9449       0.3229            0.3229        2.1481  0.0004  0.1218\n",
      "     32            0.5600        1.6899       0.3229            0.3229        2.1011  0.0005  0.1275\n",
      "     33            0.6000        1.2618       0.3438            0.3438        1.9827  0.0005  0.1280\n",
      "     34            0.7600        0.8704       0.3438            0.3438        1.7952  0.0006  0.1187\n",
      "     35            \u001b[36m0.9200\u001b[0m        0.6698       0.3333            0.3333        1.7055  0.0006  0.1275\n",
      "     36            0.9200        \u001b[32m0.5526\u001b[0m       0.3125            0.3125        1.7316  0.0007  0.1462\n",
      "     37            0.9200        \u001b[32m0.4135\u001b[0m       0.3021            0.3021        1.8179  0.0007  0.1441\n",
      "     38            0.8800        \u001b[32m0.2759\u001b[0m       0.3021            0.3021        1.9009  0.0007  0.1442\n",
      "     39            0.8800        \u001b[32m0.1646\u001b[0m       0.2812            0.2812        1.9736  0.0007  0.1729\n",
      "     40            0.9200        \u001b[32m0.1546\u001b[0m       0.2917            0.2917        2.0162  0.0007  0.1412\n",
      "     41            \u001b[36m1.0000\u001b[0m        0.1782       0.3229            0.3229        2.0350  0.0007  0.1542\n",
      "     42            1.0000        0.1830       0.3229            0.3229        2.0382  0.0007  0.1421\n",
      "     43            1.0000        0.2099       0.3125            0.3125        2.0361  0.0006  0.1391\n",
      "     44            1.0000        0.1555       0.3021            0.3021        2.0368  0.0006  0.1590\n",
      "     45            1.0000        \u001b[32m0.1489\u001b[0m       0.3125            0.3125        2.0417  0.0005  0.1601\n",
      "     46            1.0000        0.1587       0.2917            0.2917        2.0470  0.0005  0.1495\n",
      "     47            1.0000        \u001b[32m0.0803\u001b[0m       0.3021            0.3021        2.0504  0.0004  0.1278\n",
      "     48            1.0000        0.0973       0.2917            0.2917        2.0494  0.0004  0.1508\n",
      "     49            1.0000        0.1152       0.2917            0.2917        2.0404  0.0003  0.1432\n",
      "     50            1.0000        0.1145       0.3021            0.3021        2.0279  0.0003  0.1530\n",
      "Fine tuning model for subject 5 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2800        2.2353       0.3125            0.3125        2.1188  0.0004  0.1427\n",
      "     32            0.3600        1.9353       0.3021            0.3021        2.0195  0.0005  0.1865\n",
      "     33            0.4800        1.6075       0.2708            0.2708        1.9575  0.0005  0.1515\n",
      "     34            0.5600        1.2604       0.2708            0.2708        1.9860  0.0006  0.1185\n",
      "     35            0.5600        0.9008       0.2708            0.2708        2.1224  0.0006  0.1221\n",
      "     36            0.5200        \u001b[32m0.6342\u001b[0m       0.2812            0.2812        2.2869  0.0007  0.1200\n",
      "     37            0.5600        \u001b[32m0.4207\u001b[0m       0.2812            0.2812        2.3907  0.0007  0.1218\n",
      "     38            0.6000        \u001b[32m0.4046\u001b[0m       0.2812            0.2812        2.4066  0.0007  0.1464\n",
      "     39            0.7200        \u001b[32m0.3222\u001b[0m       0.2812            0.2812        2.3921  0.0007  0.1216\n",
      "     40            0.7600        \u001b[32m0.2455\u001b[0m       0.2500            0.2500        2.3469  0.0007  0.1178\n",
      "     41            \u001b[36m0.9200\u001b[0m        \u001b[32m0.2176\u001b[0m       0.2604            0.2604        2.3135  0.0007  0.1232\n",
      "     42            0.9200        \u001b[32m0.1778\u001b[0m       0.2292            0.2292        2.3021  0.0007  0.1194\n",
      "     43            0.9200        0.1998       0.2188            0.2188        2.2991  0.0006  0.1170\n",
      "     44            0.9200        \u001b[32m0.1293\u001b[0m       0.2292            0.2292        2.2976  0.0006  0.1222\n",
      "     45            \u001b[36m0.9600\u001b[0m        0.1471       0.2396            0.2396        2.2940  0.0005  0.1331\n",
      "     46            \u001b[36m1.0000\u001b[0m        0.1452       0.2396            0.2396        2.2871  0.0005  0.1242\n",
      "     47            1.0000        \u001b[32m0.0982\u001b[0m       0.2292            0.2292        2.2767  0.0004  0.1210\n",
      "     48            1.0000        0.1225       0.2292            0.2292        2.2630  0.0004  0.1174\n",
      "     49            1.0000        \u001b[32m0.0912\u001b[0m       0.2292            0.2292        2.2481  0.0003  0.1175\n",
      "     50            1.0000        0.1054       0.2292            0.2292        2.2331  0.0003  0.1224\n",
      "Fine tuning model for subject 5 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3600        1.6258       0.3125            0.3125        2.1126  0.0004  0.1179\n",
      "     32            0.4000        1.5527       0.3333            0.3333        2.0234  0.0005  0.1253\n",
      "     33            0.4000        1.4456       0.3229            0.3229        1.9309  0.0005  0.1164\n",
      "     34            0.5600        1.2719       0.3542            0.3542        1.7866  0.0006  0.1269\n",
      "     35            0.6400        0.6678       0.3646            0.3646        1.6514  0.0006  0.1181\n",
      "     36            0.7200        0.6988       0.3542            0.3542        1.6167  0.0007  0.1209\n",
      "     37            0.8000        \u001b[32m0.5384\u001b[0m       0.3125            0.3125        1.6404  0.0007  0.1223\n",
      "     38            0.8000        \u001b[32m0.4202\u001b[0m       0.2708            0.2708        1.6637  0.0007  0.1173\n",
      "     39            \u001b[36m0.8400\u001b[0m        \u001b[32m0.3070\u001b[0m       0.2708            0.2708        1.6575  0.0007  0.1200\n",
      "     40            \u001b[36m0.8800\u001b[0m        0.3795       0.2708            0.2708        1.6388  0.0007  0.1254\n",
      "     41            0.8800        \u001b[32m0.3047\u001b[0m       0.2708            0.2708        1.6409  0.0007  0.1218\n",
      "     42            \u001b[36m0.9600\u001b[0m        \u001b[32m0.2699\u001b[0m       0.3229            0.3229        1.6795  0.0007  0.1135\n",
      "     43            0.9600        \u001b[32m0.1800\u001b[0m       0.3542            0.3542        1.7306  0.0006  0.1201\n",
      "     44            0.9600        \u001b[32m0.1701\u001b[0m       0.3542            0.3542        1.7688  0.0006  0.1268\n",
      "     45            0.9600        \u001b[32m0.1597\u001b[0m       0.3333            0.3333        1.7917  0.0005  0.1181\n",
      "     46            0.9600        \u001b[32m0.1425\u001b[0m       0.3438            0.3438        1.7958  0.0005  0.1170\n",
      "     47            \u001b[36m1.0000\u001b[0m        0.1827       0.3438            0.3438        1.7925  0.0004  0.1185\n",
      "     48            1.0000        0.1449       0.3333            0.3333        1.7793  0.0004  0.1267\n",
      "     49            1.0000        \u001b[32m0.1111\u001b[0m       0.3333            0.3333        1.7603  0.0003  0.1182\n",
      "     50            1.0000        \u001b[32m0.0994\u001b[0m       0.3229            0.3229        1.7366  0.0003  0.1221\n",
      "Fine tuning model for subject 5 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3600        2.0039       0.3125            0.3125        2.1300  0.0004  0.1244\n",
      "     32            0.3600        1.5625       0.3125            0.3125        2.0774  0.0005  0.1224\n",
      "     33            0.4000        1.2429       0.2917            0.2917        2.0369  0.0005  0.1156\n",
      "     34            0.4800        0.9878       0.2604            0.2604        2.1156  0.0006  0.1214\n",
      "     35            0.4800        0.7166       0.2708            0.2708        2.3771  0.0006  0.1219\n",
      "     36            0.4400        0.6778       0.2604            0.2604        2.7382  0.0007  0.1206\n",
      "     37            0.5200        \u001b[32m0.5412\u001b[0m       0.2708            0.2708        3.1177  0.0007  0.1170\n",
      "     38            0.5200        \u001b[32m0.3385\u001b[0m       0.2604            0.2604        3.3980  0.0007  0.1160\n",
      "     39            0.5200        \u001b[32m0.3004\u001b[0m       0.2604            0.2604        3.5530  0.0007  0.1249\n",
      "     40            0.5200        \u001b[32m0.2883\u001b[0m       0.2708            0.2708        3.6162  0.0007  0.1206\n",
      "     41            0.6400        \u001b[32m0.2279\u001b[0m       0.2708            0.2708        3.5858  0.0007  0.1186\n",
      "     42            0.8000        \u001b[32m0.1616\u001b[0m       0.2917            0.2917        3.4944  0.0007  0.1222\n",
      "     43            \u001b[36m0.8400\u001b[0m        0.1907       0.2812            0.2812        3.3834  0.0006  0.1288\n",
      "     44            0.8400        \u001b[32m0.1166\u001b[0m       0.2812            0.2812        3.2591  0.0006  0.1209\n",
      "     45            \u001b[36m0.8800\u001b[0m        0.1386       0.2812            0.2812        3.1432  0.0005  0.1244\n",
      "     46            \u001b[36m1.0000\u001b[0m        0.1827       0.2917            0.2917        3.0463  0.0005  0.1244\n",
      "     47            1.0000        0.1622       0.2708            0.2708        2.9525  0.0004  0.1195\n",
      "     48            1.0000        0.1344       0.2708            0.2708        2.8672  0.0004  0.1143\n",
      "     49            1.0000        \u001b[32m0.0995\u001b[0m       0.2917            0.2917        2.7949  0.0003  0.1239\n",
      "     50            1.0000        \u001b[32m0.0560\u001b[0m       0.2812            0.2812        2.7290  0.0003  0.1233\n",
      "Fine tuning model for subject 5 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3333        1.8557       0.3229            0.3229        2.1399  0.0004  0.1415\n",
      "     32            0.4333        1.7110       0.3438            0.3438        2.0979  0.0005  0.1270\n",
      "     33            0.5000        1.7078       0.3125            0.3125        2.1008  0.0005  0.1291\n",
      "     34            0.6000        1.1394       0.3125            0.3125        2.1384  0.0006  0.1273\n",
      "     35            0.5667        0.7261       0.3125            0.3125        2.1851  0.0006  0.1291\n",
      "     36            0.6667        0.7653       0.3125            0.3125        2.2046  0.0007  0.1435\n",
      "     37            0.7000        \u001b[32m0.4948\u001b[0m       0.2917            0.2917        2.2002  0.0007  0.1618\n",
      "     38            0.7667        \u001b[32m0.4269\u001b[0m       0.2812            0.2812        2.1871  0.0007  0.1552\n",
      "     39            0.8000        \u001b[32m0.4044\u001b[0m       0.2812            0.2812        2.1672  0.0007  0.1285\n",
      "     40            \u001b[36m0.8333\u001b[0m        \u001b[32m0.3190\u001b[0m       0.2812            0.2812        2.1360  0.0007  0.1279\n",
      "     41            \u001b[36m0.9000\u001b[0m        \u001b[32m0.2981\u001b[0m       0.2708            0.2708        2.0889  0.0007  0.1334\n",
      "     42            0.9000        0.3283       0.2708            0.2708        2.0331  0.0007  0.1338\n",
      "     43            \u001b[36m0.9333\u001b[0m        \u001b[32m0.2614\u001b[0m       0.2604            0.2604        1.9782  0.0006  0.1335\n",
      "     44            \u001b[36m0.9667\u001b[0m        0.2973       0.2396            0.2396        1.9294  0.0006  0.1368\n",
      "     45            \u001b[36m1.0000\u001b[0m        0.2705       0.2500            0.2500        1.8863  0.0005  0.1279\n",
      "     46            1.0000        \u001b[32m0.1976\u001b[0m       0.2500            0.2500        1.8507  0.0005  0.1249\n",
      "     47            1.0000        \u001b[32m0.1741\u001b[0m       0.2292            0.2292        1.8175  0.0004  0.1281\n",
      "     48            1.0000        0.1777       0.2188            0.2188        1.7890  0.0004  0.1283\n",
      "     49            1.0000        0.1747       0.2708            0.2708        1.7662  0.0003  0.1269\n",
      "     50            1.0000        \u001b[32m0.1640\u001b[0m       0.2708            0.2708        1.7488  0.0003  0.2071\n",
      "Fine tuning model for subject 5 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        1.5964       0.3125            0.3125        2.1558  0.0004  0.1323\n",
      "     32            0.3333        1.4959       0.3125            0.3125        2.1479  0.0005  0.1288\n",
      "     33            0.4000        1.3516       0.3125            0.3125        2.1733  0.0005  0.1321\n",
      "     34            0.4000        0.9001       0.3125            0.3125        2.2245  0.0006  0.1329\n",
      "     35            0.4333        0.6567       0.3333            0.3333        2.2192  0.0006  0.1299\n",
      "     36            0.5667        \u001b[32m0.5306\u001b[0m       0.3333            0.3333        2.1531  0.0007  0.1449\n",
      "     37            0.6667        \u001b[32m0.3476\u001b[0m       0.3542            0.3542        2.0575  0.0007  0.1524\n",
      "     38            \u001b[36m0.8333\u001b[0m        0.4204       0.3333            0.3333        1.9491  0.0007  0.1604\n",
      "     39            \u001b[36m0.9000\u001b[0m        0.4051       0.3542            0.3542        1.8840  0.0007  0.1502\n",
      "     40            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3001\u001b[0m       0.3646            0.3646        1.8278  0.0007  0.1472\n",
      "     41            \u001b[36m0.9667\u001b[0m        \u001b[32m0.2320\u001b[0m       0.3542            0.3542        1.7865  0.0007  0.1695\n",
      "     42            0.9667        \u001b[32m0.1913\u001b[0m       0.3542            0.3542        1.7638  0.0007  0.1439\n",
      "     43            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1797\u001b[0m       0.3958            0.3958        1.7547  0.0006  0.1481\n",
      "     44            1.0000        \u001b[32m0.1745\u001b[0m       0.3646            0.3646        1.7555  0.0006  0.2291\n",
      "     45            1.0000        \u001b[32m0.1559\u001b[0m       0.3333            0.3333        1.7630  0.0005  0.1556\n",
      "     46            1.0000        0.1616       0.3333            0.3333        1.7703  0.0005  0.1440\n",
      "     47            1.0000        \u001b[32m0.1083\u001b[0m       0.3333            0.3333        1.7789  0.0004  0.1529\n",
      "     48            1.0000        \u001b[32m0.0979\u001b[0m       0.3438            0.3438        1.7881  0.0004  0.1537\n",
      "     49            1.0000        \u001b[32m0.0846\u001b[0m       0.3438            0.3438        1.7973  0.0003  0.1586\n",
      "     50            1.0000        0.1036       0.3438            0.3438        1.8054  0.0003  0.1548\n",
      "Fine tuning model for subject 5 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2333        1.6623       0.3333            0.3333        2.1078  0.0004  0.2060\n",
      "     32            0.3333        1.6522       0.3229            0.3229        1.9909  0.0005  0.1411\n",
      "     33            0.4667        1.6094       0.3021            0.3021        1.9165  0.0005  0.1306\n",
      "     34            0.5333        1.0290       0.3021            0.3021        1.9106  0.0006  0.1343\n",
      "     35            0.5667        0.9311       0.2812            0.2812        1.9353  0.0006  0.1378\n",
      "     36            0.5667        0.7759       0.2500            0.2500        1.9544  0.0007  0.1401\n",
      "     37            0.7667        \u001b[32m0.4487\u001b[0m       0.2604            0.2604        1.9321  0.0007  0.1270\n",
      "     38            \u001b[36m0.8333\u001b[0m        0.4515       0.2812            0.2812        1.8911  0.0007  0.1273\n",
      "     39            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3740\u001b[0m       0.2812            0.2812        1.8681  0.0007  0.1315\n",
      "     40            0.9000        0.3766       0.2917            0.2917        1.8442  0.0007  0.1327\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3445\u001b[0m       0.2917            0.2917        1.8295  0.0007  0.1310\n",
      "     42            1.0000        \u001b[32m0.2712\u001b[0m       0.2708            0.2708        1.8194  0.0007  0.1251\n",
      "     43            1.0000        \u001b[32m0.2317\u001b[0m       0.3021            0.3021        1.8140  0.0006  0.1386\n",
      "     44            1.0000        \u001b[32m0.1939\u001b[0m       0.3125            0.3125        1.8154  0.0006  0.1317\n",
      "     45            1.0000        0.2071       0.3125            0.3125        1.8170  0.0005  0.1274\n",
      "     46            1.0000        \u001b[32m0.1788\u001b[0m       0.3125            0.3125        1.8201  0.0005  0.1266\n",
      "     47            1.0000        \u001b[32m0.1451\u001b[0m       0.3229            0.3229        1.8223  0.0004  0.1333\n",
      "     48            1.0000        0.1603       0.3229            0.3229        1.8224  0.0004  0.1239\n",
      "     49            1.0000        \u001b[32m0.1068\u001b[0m       0.3333            0.3333        1.8218  0.0003  0.1282\n",
      "     50            1.0000        \u001b[32m0.1033\u001b[0m       0.3333            0.3333        1.8210  0.0003  0.1279\n",
      "Fine tuning model for subject 5 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3333        1.9705       0.3229            0.3229        2.1518  0.0004  0.1328\n",
      "     32            0.5000        1.9404       0.3333            0.3333        2.0951  0.0005  0.1259\n",
      "     33            0.5333        1.5753       0.3333            0.3333        2.0566  0.0005  0.1369\n",
      "     34            0.6000        1.3551       0.3229            0.3229        1.9995  0.0006  0.1318\n",
      "     35            0.7667        1.0536       0.3125            0.3125        1.8986  0.0006  0.1338\n",
      "     36            0.8000        0.6805       0.2812            0.2812        1.8056  0.0007  0.1285\n",
      "     37            \u001b[36m0.8667\u001b[0m        \u001b[32m0.4614\u001b[0m       0.2917            0.2917        1.7680  0.0007  0.1273\n",
      "     38            0.8667        \u001b[32m0.3998\u001b[0m       0.3125            0.3125        1.7647  0.0007  0.1277\n",
      "     39            \u001b[36m0.9000\u001b[0m        0.4363       0.2917            0.2917        1.7797  0.0007  0.1317\n",
      "     40            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3662\u001b[0m       0.2812            0.2812        1.8077  0.0007  0.1345\n",
      "     41            0.9667        \u001b[32m0.3044\u001b[0m       0.2708            0.2708        1.8364  0.0007  0.1241\n",
      "     42            0.9667        \u001b[32m0.2073\u001b[0m       0.2708            0.2708        1.8634  0.0007  0.1277\n",
      "     43            0.9667        0.2961       0.3125            0.3125        1.8902  0.0006  0.1312\n",
      "     44            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1234\u001b[0m       0.2917            0.2917        1.9133  0.0006  0.1289\n",
      "     45            1.0000        0.1776       0.3125            0.3125        1.9302  0.0005  0.1327\n",
      "     46            1.0000        0.1445       0.3021            0.3021        1.9462  0.0005  0.1286\n",
      "     47            1.0000        0.1687       0.3125            0.3125        1.9609  0.0004  0.1273\n",
      "     48            1.0000        \u001b[32m0.1085\u001b[0m       0.3125            0.3125        1.9717  0.0004  0.1366\n",
      "     49            1.0000        0.1288       0.3021            0.3021        1.9793  0.0003  0.1280\n",
      "     50            1.0000        \u001b[32m0.1017\u001b[0m       0.3021            0.3021        1.9840  0.0003  0.1259\n",
      "Fine tuning model for subject 5 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2667        1.6621       0.3333            0.3333        2.1059  0.0004  0.1354\n",
      "     32            0.4333        1.4391       0.3229            0.3229        1.9847  0.0005  0.1293\n",
      "     33            0.5333        1.1222       0.3021            0.3021        1.8791  0.0005  0.1329\n",
      "     34            0.7667        0.8929       0.3021            0.3021        1.8072  0.0006  0.1289\n",
      "     35            0.7667        0.7109       0.2917            0.2917        1.7952  0.0006  0.1265\n",
      "     36            \u001b[36m0.8333\u001b[0m        \u001b[32m0.6012\u001b[0m       0.2917            0.2917        1.8199  0.0007  0.1307\n",
      "     37            \u001b[36m0.8667\u001b[0m        \u001b[32m0.5149\u001b[0m       0.2917            0.2917        1.8382  0.0007  0.1346\n",
      "     38            0.8667        \u001b[32m0.4128\u001b[0m       0.2917            0.2917        1.8182  0.0007  0.1317\n",
      "     39            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3596\u001b[0m       0.3021            0.3021        1.7814  0.0007  0.1312\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2947\u001b[0m       0.2917            0.2917        1.7497  0.0007  0.1282\n",
      "     41            1.0000        \u001b[32m0.1970\u001b[0m       0.2917            0.2917        1.7294  0.0007  0.1283\n",
      "     42            1.0000        0.2206       0.3021            0.3021        1.7237  0.0007  0.1239\n",
      "     43            1.0000        0.2064       0.2812            0.2812        1.7217  0.0006  0.1312\n",
      "     44            1.0000        \u001b[32m0.1688\u001b[0m       0.2708            0.2708        1.7230  0.0006  0.1288\n",
      "     45            1.0000        \u001b[32m0.1471\u001b[0m       0.2604            0.2604        1.7274  0.0005  0.1299\n",
      "     46            1.0000        \u001b[32m0.1322\u001b[0m       0.2604            0.2604        1.7335  0.0005  0.1333\n",
      "     47            1.0000        0.1408       0.2604            0.2604        1.7356  0.0004  0.1239\n",
      "     48            1.0000        \u001b[32m0.1108\u001b[0m       0.2500            0.2500        1.7370  0.0004  0.1200\n",
      "     49            1.0000        \u001b[32m0.1098\u001b[0m       0.2500            0.2500        1.7351  0.0003  0.1249\n",
      "     50            1.0000        \u001b[32m0.1023\u001b[0m       0.2500            0.2500        1.7326  0.0003  0.1219\n",
      "Fine tuning model for subject 5 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3667        1.5073       0.3229            0.3229        2.1376  0.0004  0.1328\n",
      "     32            0.4000        1.3831       0.3125            0.3125        2.0640  0.0005  0.1272\n",
      "     33            0.4667        1.3587       0.3229            0.3229        1.9863  0.0005  0.1328\n",
      "     34            0.5667        1.0027       0.2917            0.2917        1.9191  0.0006  0.1226\n",
      "     35            0.7333        0.8367       0.2708            0.2708        1.8542  0.0006  0.1302\n",
      "     36            0.8000        0.8541       0.2812            0.2812        1.8162  0.0007  0.1343\n",
      "     37            0.8000        \u001b[32m0.5481\u001b[0m       0.2812            0.2812        1.8245  0.0007  0.1276\n",
      "     38            0.7667        \u001b[32m0.4713\u001b[0m       0.2917            0.2917        1.8932  0.0007  0.1292\n",
      "     39            0.7000        \u001b[32m0.4361\u001b[0m       0.3021            0.3021        2.0063  0.0007  0.1276\n",
      "     40            0.7000        \u001b[32m0.3861\u001b[0m       0.3125            0.3125        2.1392  0.0007  0.1302\n",
      "     41            0.7000        0.4096       0.3021            0.3021        2.2647  0.0007  0.1244\n",
      "     42            0.7667        \u001b[32m0.2230\u001b[0m       0.2917            0.2917        2.3540  0.0007  0.1285\n",
      "     43            0.7667        0.2546       0.2812            0.2812        2.4016  0.0006  0.1286\n",
      "     44            0.8000        \u001b[32m0.1874\u001b[0m       0.2812            0.2812        2.4158  0.0006  0.1258\n",
      "     45            \u001b[36m0.8333\u001b[0m        \u001b[32m0.1759\u001b[0m       0.2812            0.2812        2.3897  0.0005  0.1323\n",
      "     46            \u001b[36m0.9333\u001b[0m        \u001b[32m0.1229\u001b[0m       0.2812            0.2812        2.3465  0.0005  0.1287\n",
      "     47            0.9333        0.1240       0.2917            0.2917        2.2900  0.0004  0.1334\n",
      "     48            \u001b[36m0.9667\u001b[0m        0.1325       0.2812            0.2812        2.2293  0.0004  0.1268\n",
      "     49            \u001b[36m1.0000\u001b[0m        0.1388       0.2708            0.2708        2.1704  0.0003  0.1291\n",
      "     50            1.0000        0.1531       0.2708            0.2708        2.1127  0.0003  0.1392\n",
      "Fine tuning model for subject 5 with 30 = 30 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3667        1.7356       0.3125            0.3125        2.1221  0.0004  0.1304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4333        1.3151       0.3021            0.3021        2.0422  0.0005  0.1420\n",
      "     33            0.4667        1.2237       0.3021            0.3021        1.9392  0.0005  0.1387\n",
      "     34            0.6667        0.8771       0.3229            0.3229        1.8241  0.0006  0.1627\n",
      "     35            0.8000        0.7293       0.3229            0.3229        1.7771  0.0006  0.1521\n",
      "     36            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5602\u001b[0m       0.3646            0.3646        1.8328  0.0007  0.1542\n",
      "     37            0.9000        \u001b[32m0.5285\u001b[0m       0.3854            0.3854        1.9313  0.0007  0.1492\n",
      "     38            0.9000        \u001b[32m0.3966\u001b[0m       0.3542            0.3542        2.0392  0.0007  0.1633\n",
      "     39            0.9000        \u001b[32m0.3086\u001b[0m       0.3438            0.3438        2.1327  0.0007  0.1555\n",
      "     40            0.9000        \u001b[32m0.2881\u001b[0m       0.3542            0.3542        2.2253  0.0007  0.1600\n",
      "     41            0.8000        \u001b[32m0.2637\u001b[0m       0.3854            0.3854        2.3216  0.0007  0.1369\n",
      "     42            0.8000        \u001b[32m0.2180\u001b[0m       0.3646            0.3646        2.4130  0.0007  0.1561\n",
      "     43            0.8000        \u001b[32m0.2135\u001b[0m       0.3333            0.3333        2.4833  0.0006  0.1504\n",
      "     44            0.8333        \u001b[32m0.2101\u001b[0m       0.3333            0.3333        2.5393  0.0006  0.1562\n",
      "     45            0.8333        \u001b[32m0.1541\u001b[0m       0.3229            0.3229        2.5733  0.0005  0.1570\n",
      "     46            0.8333        \u001b[32m0.1348\u001b[0m       0.3021            0.3021        2.5823  0.0005  0.1519\n",
      "     47            0.9000        0.1485       0.3021            0.3021        2.5708  0.0004  0.1538\n",
      "     48            \u001b[36m0.9333\u001b[0m        \u001b[32m0.1346\u001b[0m       0.2812            0.2812        2.5448  0.0004  0.1600\n",
      "     49            0.9333        0.1547       0.2708            0.2708        2.5023  0.0003  0.1852\n",
      "     50            0.9333        \u001b[32m0.1144\u001b[0m       0.2812            0.2812        2.4471  0.0003  0.1448\n",
      "Fine tuning model for subject 5 with 30 = 30 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2000        1.8286       0.3333            0.3333        2.1086  0.0004  0.1308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.2667        1.8876       0.3542            0.3542        1.9780  0.0005  0.1326\n",
      "     33            0.4000        1.5511       0.3646            0.3646        1.8319  0.0005  0.1475\n",
      "     34            0.5667        1.3073       0.3438            0.3438        1.7148  0.0006  0.1303\n",
      "     35            0.6667        1.0324       0.3229            0.3229        1.6379  0.0006  0.1286\n",
      "     36            0.8000        0.6594       0.3646            0.3646        1.6140  0.0007  0.1254\n",
      "     37            \u001b[36m0.8667\u001b[0m        0.6839       0.3750            0.3750        1.6515  0.0007  0.1259\n",
      "     38            0.8333        0.6691       0.3542            0.3542        1.7386  0.0007  0.1266\n",
      "     39            0.7667        \u001b[32m0.6063\u001b[0m       0.3542            0.3542        1.8607  0.0007  0.1282\n",
      "     40            0.7333        \u001b[32m0.3079\u001b[0m       0.3229            0.3229        1.9948  0.0007  0.1387\n",
      "     41            0.7667        0.3176       0.3021            0.3021        2.0886  0.0007  0.1338\n",
      "     42            0.8333        \u001b[32m0.2956\u001b[0m       0.3125            0.3125        2.1545  0.0007  0.1321\n",
      "     43            0.8333        \u001b[32m0.2441\u001b[0m       0.3229            0.3229        2.1965  0.0006  0.1355\n",
      "     44            0.8333        0.2654       0.2917            0.2917        2.2196  0.0006  0.1275\n",
      "     45            0.8333        0.2790       0.2812            0.2812        2.2172  0.0005  0.1365\n",
      "     46            0.8667        0.2828       0.3021            0.3021        2.2071  0.0005  0.1261\n",
      "     47            0.8667        \u001b[32m0.1757\u001b[0m       0.3021            0.3021        2.1830  0.0004  0.1241\n",
      "     48            \u001b[36m0.9000\u001b[0m        0.2338       0.2917            0.2917        2.1417  0.0004  0.1270\n",
      "     49            \u001b[36m0.9333\u001b[0m        0.2105       0.2708            0.2708        2.0977  0.0003  0.1297\n",
      "     50            \u001b[36m0.9667\u001b[0m        \u001b[32m0.1230\u001b[0m       0.2708            0.2708        2.0596  0.0003  0.1266\n",
      "Fine tuning model for subject 5 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4333        1.3021       0.3125            0.3125        2.1514  0.0004  0.1364\n",
      "     32            0.4000        1.1687       0.3229            0.3229        2.1132  0.0005  0.1273\n",
      "     33            0.4333        1.0016       0.3021            0.3021        2.0568  0.0005  0.1261\n",
      "     34            0.5667        0.8694       0.3333            0.3333        1.9736  0.0006  0.1220\n",
      "     35            0.7333        \u001b[32m0.6355\u001b[0m       0.2917            0.2917        1.8886  0.0006  0.1313\n",
      "     36            0.7000        \u001b[32m0.6188\u001b[0m       0.3021            0.3021        1.8693  0.0007  0.1362\n",
      "     37            0.7333        \u001b[32m0.4532\u001b[0m       0.3333            0.3333        1.8928  0.0007  0.1252\n",
      "     38            0.7667        \u001b[32m0.3619\u001b[0m       0.3438            0.3438        1.9340  0.0007  0.1308\n",
      "     39            0.8000        0.4173       0.3542            0.3542        1.9602  0.0007  0.1246\n",
      "     40            0.7667        \u001b[32m0.2651\u001b[0m       0.3333            0.3333        1.9966  0.0007  0.1293\n",
      "     41            \u001b[36m0.8333\u001b[0m        \u001b[32m0.2325\u001b[0m       0.3229            0.3229        2.0292  0.0007  0.1273\n",
      "     42            \u001b[36m0.8667\u001b[0m        0.2361       0.3021            0.3021        2.0567  0.0007  0.1278\n",
      "     43            \u001b[36m0.9333\u001b[0m        \u001b[32m0.2108\u001b[0m       0.3229            0.3229        2.0659  0.0006  0.1263\n",
      "     44            \u001b[36m0.9667\u001b[0m        \u001b[32m0.2102\u001b[0m       0.3333            0.3333        2.0570  0.0006  0.1287\n",
      "     45            0.9667        \u001b[32m0.1307\u001b[0m       0.3229            0.3229        2.0396  0.0005  0.1380\n",
      "     46            0.9667        0.1429       0.3229            0.3229        2.0153  0.0005  0.1342\n",
      "     47            0.9667        0.1378       0.3542            0.3542        1.9864  0.0004  0.1340\n",
      "     48            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1000\u001b[0m       0.3438            0.3438        1.9509  0.0004  0.1269\n",
      "     49            1.0000        0.1105       0.3438            0.3438        1.9105  0.0003  0.1978\n",
      "     50            1.0000        0.1021       0.3438            0.3438        1.8711  0.0003  0.2074\n",
      "Fine tuning model for subject 5 with 30 = 30 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2000        1.7041       0.3333            0.3333        2.0926  0.0004  0.1260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.2667        1.7698       0.3125            0.3125        1.9714  0.0005  0.1298\n",
      "     33            0.4333        1.3146       0.2604            0.2604        1.8741  0.0005  0.1270\n",
      "     34            0.6000        1.1828       0.2812            0.2812        1.8365  0.0006  0.1287\n",
      "     35            0.6000        0.8004       0.2812            0.2812        1.8403  0.0006  0.1340\n",
      "     36            0.7333        0.8434       0.2812            0.2812        1.8738  0.0007  0.1300\n",
      "     37            0.7667        \u001b[32m0.6069\u001b[0m       0.3125            0.3125        1.9128  0.0007  0.1290\n",
      "     38            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3596\u001b[0m       0.2604            0.2604        1.9524  0.0007  0.1309\n",
      "     39            0.8333        0.4335       0.2812            0.2812        1.9946  0.0007  0.1312\n",
      "     40            0.8333        0.4391       0.2604            0.2604        2.0411  0.0007  0.1283\n",
      "     41            0.8333        \u001b[32m0.3123\u001b[0m       0.2708            0.2708        2.0855  0.0007  0.1331\n",
      "     42            0.8667        \u001b[32m0.2389\u001b[0m       0.2604            0.2604        2.1106  0.0007  0.1288\n",
      "     43            0.9000        \u001b[32m0.2028\u001b[0m       0.2604            0.2604        2.1207  0.0006  0.1200\n",
      "     44            0.9000        \u001b[32m0.2023\u001b[0m       0.2604            0.2604        2.1265  0.0006  0.1226\n",
      "     45            0.9000        \u001b[32m0.1451\u001b[0m       0.2604            0.2604        2.1227  0.0005  0.1302\n",
      "     46            0.9000        0.1719       0.2604            0.2604        2.1058  0.0005  0.1314\n",
      "     47            \u001b[36m0.9667\u001b[0m        \u001b[32m0.1321\u001b[0m       0.2396            0.2396        2.0861  0.0004  0.1286\n",
      "     48            \u001b[36m1.0000\u001b[0m        0.1694       0.2396            0.2396        2.0655  0.0004  0.1278\n",
      "     49            1.0000        \u001b[32m0.1316\u001b[0m       0.2188            0.2188        2.0450  0.0003  0.1279\n",
      "     50            1.0000        \u001b[32m0.1171\u001b[0m       0.2292            0.2292        2.0245  0.0003  0.1268\n",
      "Fine tuning model for subject 5 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3429        1.5963       0.3125            0.3125        2.1396  0.0004  0.1380\n",
      "     32            0.4571        1.4070       0.2917            0.2917        2.0842  0.0005  0.1323\n",
      "     33            0.4857        1.2869       0.3021            0.3021        2.0558  0.0005  0.1336\n",
      "     34            0.5714        0.9709       0.3229            0.3229        2.0265  0.0006  0.1353\n",
      "     35            0.6571        0.8253       0.3229            0.3229        1.9915  0.0006  0.1323\n",
      "     36            0.6857        0.6994       0.3333            0.3333        1.9406  0.0007  0.1315\n",
      "     37            0.8000        \u001b[32m0.4761\u001b[0m       0.3125            0.3125        1.9024  0.0007  0.1353\n",
      "     38            \u001b[36m0.8571\u001b[0m        0.4925       0.3125            0.3125        1.8674  0.0007  0.1310\n",
      "     39            0.8571        \u001b[32m0.3947\u001b[0m       0.3125            0.3125        1.8350  0.0007  0.1342\n",
      "     40            \u001b[36m0.8857\u001b[0m        \u001b[32m0.3253\u001b[0m       0.2812            0.2812        1.8209  0.0007  0.1260\n",
      "     41            \u001b[36m0.9143\u001b[0m        0.4051       0.2604            0.2604        1.8119  0.0007  0.1331\n",
      "     42            \u001b[36m0.9714\u001b[0m        \u001b[32m0.3191\u001b[0m       0.2500            0.2500        1.8090  0.0007  0.1254\n",
      "     43            0.9714        \u001b[32m0.2662\u001b[0m       0.2604            0.2604        1.8081  0.0006  0.1386\n",
      "     44            0.9714        \u001b[32m0.2613\u001b[0m       0.2708            0.2708        1.8082  0.0006  0.1336\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2184\u001b[0m       0.2812            0.2812        1.8089  0.0005  0.1308\n",
      "     46            1.0000        \u001b[32m0.2049\u001b[0m       0.2812            0.2812        1.8132  0.0005  0.1335\n",
      "     47            1.0000        0.2149       0.2917            0.2917        1.8210  0.0004  0.1343\n",
      "     48            1.0000        \u001b[32m0.1842\u001b[0m       0.2917            0.2917        1.8281  0.0004  0.1657\n",
      "     49            1.0000        0.1952       0.2917            0.2917        1.8357  0.0003  0.1549\n",
      "     50            1.0000        \u001b[32m0.1534\u001b[0m       0.2812            0.2812        1.8396  0.0003  0.1610\n",
      "Fine tuning model for subject 5 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2286        1.5898       0.3125            0.3125        2.1247  0.0004  0.1497\n",
      "     32            0.2000        1.4793       0.3125            0.3125        2.0790  0.0005  0.2001\n",
      "     33            0.2000        1.4557       0.2396            0.2396        2.0800  0.0005  0.1551\n",
      "     34            0.4286        1.0581       0.2396            0.2396        2.1231  0.0006  0.1486\n",
      "     35            0.5143        0.8360       0.2708            0.2708        2.1950  0.0006  0.1677\n",
      "     36            0.6571        0.6696       0.2604            0.2604        2.2613  0.0007  0.1622\n",
      "     37            0.7143        \u001b[32m0.5837\u001b[0m       0.2812            0.2812        2.3081  0.0007  0.1793\n",
      "     38            0.7143        0.5849       0.3021            0.3021        2.3279  0.0007  0.1630\n",
      "     39            0.7143        \u001b[32m0.4685\u001b[0m       0.3229            0.3229        2.3234  0.0007  0.1680\n",
      "     40            0.7429        \u001b[32m0.4168\u001b[0m       0.3333            0.3333        2.2961  0.0007  0.1904\n",
      "     41            0.8000        0.4191       0.3333            0.3333        2.2579  0.0007  0.1464\n",
      "     42            \u001b[36m0.8286\u001b[0m        \u001b[32m0.3022\u001b[0m       0.3333            0.3333        2.2188  0.0007  0.2174\n",
      "     43            \u001b[36m0.8571\u001b[0m        \u001b[32m0.2593\u001b[0m       0.3333            0.3333        2.1784  0.0006  0.1583\n",
      "     44            0.8571        0.2624       0.3229            0.3229        2.1485  0.0006  0.1342\n",
      "     45            \u001b[36m0.8857\u001b[0m        0.3240       0.3229            0.3229        2.1201  0.0005  0.1404\n",
      "     46            \u001b[36m0.9429\u001b[0m        \u001b[32m0.2178\u001b[0m       0.3125            0.3125        2.0932  0.0005  0.1397\n",
      "     47            \u001b[36m0.9714\u001b[0m        0.3232       0.3125            0.3125        2.0670  0.0004  0.1425\n",
      "     48            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1781\u001b[0m       0.3125            0.3125        2.0423  0.0004  0.1282\n",
      "     49            1.0000        \u001b[32m0.1677\u001b[0m       0.3333            0.3333        2.0200  0.0003  0.1342\n",
      "     50            1.0000        \u001b[32m0.1553\u001b[0m       0.3333            0.3333        2.0003  0.0003  0.1367\n",
      "Fine tuning model for subject 5 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3714        1.2726       0.3125            0.3125        2.1474  0.0004  0.1357\n",
      "     32            0.4286        1.3130       0.3333            0.3333        2.0860  0.0005  0.1359\n",
      "     33            0.4857        1.2439       0.3333            0.3333        2.0615  0.0005  0.1416\n",
      "     34            0.5714        1.0997       0.2708            0.2708        2.0955  0.0006  0.1327\n",
      "     35            0.6286        0.8999       0.2500            0.2500        2.1159  0.0006  0.1366\n",
      "     36            0.6286        0.6698       0.2604            0.2604        2.0744  0.0007  0.1370\n",
      "     37            0.7143        \u001b[32m0.5859\u001b[0m       0.3125            0.3125        2.0533  0.0007  0.1362\n",
      "     38            0.6571        \u001b[32m0.5337\u001b[0m       0.3021            0.3021        2.1262  0.0007  0.1369\n",
      "     39            0.5429        \u001b[32m0.3303\u001b[0m       0.2812            0.2812        2.2784  0.0007  0.1311\n",
      "     40            0.4286        \u001b[32m0.3033\u001b[0m       0.2604            0.2604        2.4594  0.0007  0.1362\n",
      "     41            0.4571        0.3042       0.2708            0.2708        2.5809  0.0007  0.1411\n",
      "     42            0.4857        \u001b[32m0.2190\u001b[0m       0.2604            0.2604        2.6511  0.0007  0.1312\n",
      "     43            0.4857        \u001b[32m0.2172\u001b[0m       0.2708            0.2708        2.6331  0.0006  0.1320\n",
      "     44            0.5429        \u001b[32m0.2010\u001b[0m       0.2708            0.2708        2.5394  0.0006  0.1340\n",
      "     45            0.6571        0.2061       0.2812            0.2812        2.4124  0.0005  0.1330\n",
      "     46            0.7429        0.2395       0.2917            0.2917        2.2726  0.0005  0.1385\n",
      "     47            \u001b[36m0.8857\u001b[0m        \u001b[32m0.1720\u001b[0m       0.2708            0.2708        2.1409  0.0004  0.1345\n",
      "     48            \u001b[36m0.9143\u001b[0m        \u001b[32m0.1347\u001b[0m       0.2604            0.2604        2.0282  0.0004  0.1353\n",
      "     49            \u001b[36m0.9714\u001b[0m        \u001b[32m0.0871\u001b[0m       0.3021            0.3021        1.9354  0.0003  0.1437\n",
      "     50            \u001b[36m1.0000\u001b[0m        0.1178       0.3021            0.3021        1.8643  0.0003  0.1340\n",
      "Fine tuning model for subject 5 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2857        1.4892       0.3021            0.3021        2.0976  0.0004  0.1330\n",
      "     32            0.2857        1.4977       0.3333            0.3333        1.9781  0.0005  0.1324\n",
      "     33            0.3429        1.3121       0.3125            0.3125        1.8752  0.0005  0.1334\n",
      "     34            0.5714        0.9714       0.2708            0.2708        1.8108  0.0006  0.1389\n",
      "     35            0.6857        0.8173       0.2500            0.2500        1.8466  0.0006  0.1270\n",
      "     36            0.7429        0.7061       0.2604            0.2604        1.9809  0.0007  0.1309\n",
      "     37            0.7714        \u001b[32m0.5923\u001b[0m       0.2604            0.2604        2.1284  0.0007  0.1547\n",
      "     38            0.8000        0.6221       0.2396            0.2396        2.2552  0.0007  0.1246\n",
      "     39            0.8000        \u001b[32m0.4519\u001b[0m       0.2396            0.2396        2.3424  0.0007  0.1327\n",
      "     40            \u001b[36m0.8286\u001b[0m        \u001b[32m0.4161\u001b[0m       0.2396            0.2396        2.3999  0.0007  0.1363\n",
      "     41            0.8286        \u001b[32m0.3433\u001b[0m       0.2604            0.2604        2.4575  0.0007  0.1286\n",
      "     42            \u001b[36m0.8571\u001b[0m        0.3843       0.2708            0.2708        2.4940  0.0007  0.1360\n",
      "     43            0.8571        \u001b[32m0.2164\u001b[0m       0.2812            0.2812        2.5100  0.0006  0.1346\n",
      "     44            \u001b[36m0.9143\u001b[0m        0.2383       0.2812            0.2812        2.5031  0.0006  0.1379\n",
      "     45            \u001b[36m0.9714\u001b[0m        0.2223       0.2812            0.2812        2.4818  0.0005  0.1335\n",
      "     46            0.9714        \u001b[32m0.1578\u001b[0m       0.2917            0.2917        2.4535  0.0005  0.1325\n",
      "     47            0.9714        \u001b[32m0.1290\u001b[0m       0.2917            0.2917        2.4220  0.0004  0.1396\n",
      "     48            0.9714        0.1957       0.2917            0.2917        2.3903  0.0004  0.1374\n",
      "     49            0.9714        0.1439       0.2917            0.2917        2.3538  0.0003  0.1347\n",
      "     50            0.9714        \u001b[32m0.1226\u001b[0m       0.2917            0.2917        2.3160  0.0003  0.1321\n",
      "Fine tuning model for subject 5 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3429        1.5639       0.3021            0.3021        2.1241  0.0004  0.1400\n",
      "     32            0.3714        1.4252       0.3125            0.3125        2.0237  0.0005  0.1325\n",
      "     33            0.4000        1.1301       0.3125            0.3125        1.9346  0.0005  0.1387\n",
      "     34            0.5714        0.9138       0.2812            0.2812        1.8556  0.0006  0.1377\n",
      "     35            0.6571        0.8187       0.2917            0.2917        1.8233  0.0006  0.1359\n",
      "     36            0.7714        \u001b[32m0.6156\u001b[0m       0.2708            0.2708        1.8537  0.0007  0.1329\n",
      "     37            \u001b[36m0.8286\u001b[0m        0.6162       0.2812            0.2812        1.9170  0.0007  0.1284\n",
      "     38            \u001b[36m0.9143\u001b[0m        \u001b[32m0.5918\u001b[0m       0.3229            0.3229        1.9948  0.0007  0.1344\n",
      "     39            0.9143        \u001b[32m0.3791\u001b[0m       0.2917            0.2917        2.0722  0.0007  0.1330\n",
      "     40            \u001b[36m0.9429\u001b[0m        \u001b[32m0.3388\u001b[0m       0.2604            0.2604        2.1536  0.0007  0.1351\n",
      "     41            \u001b[36m0.9714\u001b[0m        \u001b[32m0.3143\u001b[0m       0.2604            0.2604        2.2449  0.0007  0.1437\n",
      "     42            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2708\u001b[0m       0.2708            0.2708        2.3226  0.0007  0.1341\n",
      "     43            1.0000        \u001b[32m0.2652\u001b[0m       0.2812            0.2812        2.3687  0.0006  0.1332\n",
      "     44            1.0000        \u001b[32m0.2362\u001b[0m       0.2708            0.2708        2.3942  0.0006  0.1369\n",
      "     45            1.0000        \u001b[32m0.1849\u001b[0m       0.2708            0.2708        2.3961  0.0005  0.1391\n",
      "     46            1.0000        0.2151       0.2604            0.2604        2.3790  0.0005  0.1333\n",
      "     47            1.0000        \u001b[32m0.1542\u001b[0m       0.2500            0.2500        2.3604  0.0004  0.1287\n",
      "     48            1.0000        \u001b[32m0.1522\u001b[0m       0.2500            0.2500        2.3379  0.0004  0.1239\n",
      "     49            1.0000        0.1771       0.2604            0.2604        2.3142  0.0003  0.1338\n",
      "     50            1.0000        0.1538       0.2708            0.2708        2.2932  0.0003  0.1370\n",
      "Fine tuning model for subject 5 with 35 = 35 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2857        1.6511       0.3333            0.3333        2.1635  0.0004  0.1334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.3714        1.5837       0.3438            0.3438        2.1481  0.0005  0.1363\n",
      "     33            0.4571        1.4855       0.3333            0.3333        2.1449  0.0005  0.1343\n",
      "     34            0.5143        1.2775       0.2917            0.2917        2.1274  0.0006  0.1367\n",
      "     35            0.6571        1.1559       0.3021            0.3021        2.1194  0.0006  0.1317\n",
      "     36            0.7429        0.8152       0.2812            0.2812        2.1280  0.0007  0.1304\n",
      "     37            0.7714        \u001b[32m0.5908\u001b[0m       0.2812            0.2812        2.1159  0.0007  0.1368\n",
      "     38            0.8000        \u001b[32m0.5186\u001b[0m       0.3125            0.3125        2.1035  0.0007  0.1330\n",
      "     39            0.7714        \u001b[32m0.4323\u001b[0m       0.3229            0.3229        2.0787  0.0007  0.1547\n",
      "     40            0.8000        \u001b[32m0.3351\u001b[0m       0.2917            0.2917        2.0471  0.0007  0.1553\n",
      "     41            \u001b[36m0.8571\u001b[0m        0.3637       0.3021            0.3021        2.0162  0.0007  0.1587\n",
      "     42            \u001b[36m0.8857\u001b[0m        \u001b[32m0.2797\u001b[0m       0.2812            0.2812        1.9906  0.0007  0.1549\n",
      "     43            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2752\u001b[0m       0.2812            0.2812        1.9600  0.0006  0.1613\n",
      "     44            1.0000        0.2888       0.2500            0.2500        1.9401  0.0006  0.1635\n",
      "     45            1.0000        \u001b[32m0.1872\u001b[0m       0.2812            0.2812        1.9290  0.0005  0.1623\n",
      "     46            1.0000        \u001b[32m0.1592\u001b[0m       0.2708            0.2708        1.9232  0.0005  0.1536\n",
      "     47            1.0000        \u001b[32m0.1525\u001b[0m       0.2500            0.2500        1.9177  0.0004  0.2282\n",
      "     48            1.0000        \u001b[32m0.1520\u001b[0m       0.2604            0.2604        1.9133  0.0004  0.1640\n",
      "     49            1.0000        \u001b[32m0.1177\u001b[0m       0.2292            0.2292        1.9110  0.0003  0.1592\n",
      "     50            1.0000        0.1189       0.2292            0.2292        1.9083  0.0003  0.1517\n",
      "Fine tuning model for subject 5 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3429        1.8326       0.3229            0.3229        2.1447  0.0004  0.2021\n",
      "     32            0.4286        1.6097       0.3125            0.3125        2.0684  0.0005  0.1802\n",
      "     33            0.4857        1.4278       0.2917            0.2917        1.9897  0.0005  0.1667\n",
      "     34            0.5429        1.1604       0.2917            0.2917        1.9466  0.0006  0.1906\n",
      "     35            0.5714        0.9333       0.2917            0.2917        1.9124  0.0006  0.1496\n",
      "     36            0.7429        0.7756       0.3229            0.3229        1.8569  0.0007  0.1345\n",
      "     37            \u001b[36m0.8286\u001b[0m        0.6762       0.3125            0.3125        1.7737  0.0007  0.1567\n",
      "     38            \u001b[36m0.9429\u001b[0m        \u001b[32m0.4748\u001b[0m       0.3125            0.3125        1.7027  0.0007  0.1838\n",
      "     39            \u001b[36m0.9714\u001b[0m        0.4983       0.3021            0.3021        1.6627  0.0007  0.1430\n",
      "     40            0.9714        0.4907       0.3021            0.3021        1.6530  0.0007  0.1319\n",
      "     41            0.9714        \u001b[32m0.3754\u001b[0m       0.3333            0.3333        1.6637  0.0007  0.1521\n",
      "     42            0.9429        \u001b[32m0.3054\u001b[0m       0.3542            0.3542        1.6817  0.0007  0.1376\n",
      "     43            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3012\u001b[0m       0.3438            0.3438        1.6978  0.0006  0.1385\n",
      "     44            1.0000        \u001b[32m0.2470\u001b[0m       0.3438            0.3438        1.7114  0.0006  0.1366\n",
      "     45            1.0000        \u001b[32m0.2287\u001b[0m       0.3438            0.3438        1.7225  0.0005  0.1279\n",
      "     46            1.0000        \u001b[32m0.2202\u001b[0m       0.3333            0.3333        1.7335  0.0005  0.1442\n",
      "     47            1.0000        0.2282       0.3333            0.3333        1.7422  0.0004  0.1514\n",
      "     48            1.0000        0.2219       0.3333            0.3333        1.7519  0.0004  0.1322\n",
      "     49            1.0000        \u001b[32m0.1712\u001b[0m       0.3438            0.3438        1.7574  0.0003  0.1393\n",
      "     50            1.0000        \u001b[32m0.1646\u001b[0m       0.3438            0.3438        1.7585  0.0003  0.1337\n",
      "Fine tuning model for subject 5 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3714        1.7272       0.3125            0.3125        2.1043  0.0004  0.1387\n",
      "     32            0.4000        1.6523       0.3229            0.3229        1.9926  0.0005  0.1338\n",
      "     33            0.3714        1.4312       0.2917            0.2917        1.8799  0.0005  0.1325\n",
      "     34            0.6571        1.3321       0.3021            0.3021        1.7841  0.0006  0.1302\n",
      "     35            0.7429        1.0868       0.3333            0.3333        1.7579  0.0006  0.1416\n",
      "     36            0.7714        0.7317       0.2604            0.2604        1.8257  0.0007  0.1328\n",
      "     37            0.7429        \u001b[32m0.6461\u001b[0m       0.2917            0.2917        1.9328  0.0007  0.1337\n",
      "     38            0.7429        0.7466       0.2917            0.2917        2.0186  0.0007  0.1374\n",
      "     39            0.7429        \u001b[32m0.5222\u001b[0m       0.2500            0.2500        2.0787  0.0007  0.1393\n",
      "     40            0.7714        \u001b[32m0.3944\u001b[0m       0.2396            0.2396        2.1209  0.0007  0.1405\n",
      "     41            0.7714        0.4480       0.2500            0.2500        2.1482  0.0007  0.1485\n",
      "     42            0.7429        \u001b[32m0.3708\u001b[0m       0.2500            0.2500        2.1654  0.0007  0.1325\n",
      "     43            0.7714        \u001b[32m0.2609\u001b[0m       0.2500            0.2500        2.1797  0.0006  0.1354\n",
      "     44            0.8000        \u001b[32m0.2487\u001b[0m       0.2292            0.2292        2.1811  0.0006  0.1346\n",
      "     45            \u001b[36m0.8571\u001b[0m        0.2910       0.2396            0.2396        2.1691  0.0005  0.1361\n",
      "     46            \u001b[36m0.8857\u001b[0m        0.2534       0.2500            0.2500        2.1526  0.0005  0.1320\n",
      "     47            \u001b[36m0.9143\u001b[0m        \u001b[32m0.1689\u001b[0m       0.2500            0.2500        2.1353  0.0004  0.1388\n",
      "     48            0.9143        0.1761       0.2292            0.2292        2.1140  0.0004  0.1547\n",
      "     49            0.9143        \u001b[32m0.1523\u001b[0m       0.2500            0.2500        2.0896  0.0003  0.1415\n",
      "     50            \u001b[36m0.9429\u001b[0m        0.1936       0.2500            0.2500        2.0674  0.0003  0.1407\n",
      "Fine tuning model for subject 5 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2571        1.7496       0.3229            0.3229        2.1112  0.0004  0.1510\n",
      "     32            0.2571        1.7284       0.3438            0.3438        1.9810  0.0005  0.1386\n",
      "     33            0.3429        1.5861       0.3646            0.3646        1.8444  0.0005  0.1413\n",
      "     34            0.6286        1.4513       0.3542            0.3542        1.7296  0.0006  0.1407\n",
      "     35            0.6857        1.0568       0.3333            0.3333        1.6721  0.0006  0.1367\n",
      "     36            \u001b[36m0.8286\u001b[0m        0.8422       0.3229            0.3229        1.6940  0.0007  0.1396\n",
      "     37            0.7714        \u001b[32m0.6016\u001b[0m       0.3333            0.3333        1.7785  0.0007  0.1291\n",
      "     38            0.7714        \u001b[32m0.5936\u001b[0m       0.3021            0.3021        1.8640  0.0007  0.1368\n",
      "     39            0.8000        \u001b[32m0.5218\u001b[0m       0.3333            0.3333        1.9237  0.0007  0.1357\n",
      "     40            \u001b[36m0.8857\u001b[0m        \u001b[32m0.4796\u001b[0m       0.3229            0.3229        1.9502  0.0007  0.1377\n",
      "     41            0.8857        \u001b[32m0.3724\u001b[0m       0.3229            0.3229        1.9539  0.0007  0.1322\n",
      "     42            \u001b[36m0.9143\u001b[0m        \u001b[32m0.2872\u001b[0m       0.3229            0.3229        1.9417  0.0007  0.1361\n",
      "     43            \u001b[36m0.9714\u001b[0m        \u001b[32m0.2198\u001b[0m       0.3229            0.3229        1.9206  0.0006  0.1379\n",
      "     44            0.9714        0.2529       0.3229            0.3229        1.8997  0.0006  0.1521\n",
      "     45            0.9714        0.2493       0.3438            0.3438        1.8761  0.0005  0.1343\n",
      "     46            0.9714        \u001b[32m0.1969\u001b[0m       0.3438            0.3438        1.8521  0.0005  0.1343\n",
      "     47            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1675\u001b[0m       0.3333            0.3333        1.8314  0.0004  0.1720\n",
      "     48            1.0000        0.2176       0.3125            0.3125        1.8193  0.0004  0.1329\n",
      "     49            1.0000        \u001b[32m0.1504\u001b[0m       0.3125            0.3125        1.8113  0.0003  0.1340\n",
      "     50            1.0000        \u001b[32m0.1154\u001b[0m       0.3333            0.3333        1.8073  0.0003  0.1331\n",
      "Fine tuning model for subject 5 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5143        1.7398       0.3125            0.3125        2.0890  0.0004  0.1345\n",
      "     32            0.4857        1.4998       0.3021            0.3021        1.9457  0.0005  0.1357\n",
      "     33            0.5714        1.3428       0.2917            0.2917        1.8263  0.0005  0.1301\n",
      "     34            0.6286        1.2117       0.2708            0.2708        1.7295  0.0006  0.1325\n",
      "     35            0.7143        0.9975       0.3021            0.3021        1.6713  0.0006  0.1387\n",
      "     36            0.7714        0.7709       0.3438            0.3438        1.6549  0.0007  0.1319\n",
      "     37            \u001b[36m0.8857\u001b[0m        \u001b[32m0.6243\u001b[0m       0.4062            0.4062        1.6573  0.0007  0.1381\n",
      "     38            \u001b[36m0.9429\u001b[0m        \u001b[32m0.5395\u001b[0m       0.4271            0.4271        1.6594  0.0007  0.1357\n",
      "     39            0.9429        \u001b[32m0.4246\u001b[0m       0.4167            0.4167        1.6600  0.0007  0.1368\n",
      "     40            \u001b[36m0.9714\u001b[0m        \u001b[32m0.3898\u001b[0m       0.4062            0.4062        1.6592  0.0007  0.1330\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3824\u001b[0m       0.3646            0.3646        1.6550  0.0007  0.1321\n",
      "     42            1.0000        \u001b[32m0.3065\u001b[0m       0.3542            0.3542        1.6544  0.0007  0.1311\n",
      "     43            1.0000        0.3295       0.3333            0.3333        1.6578  0.0006  0.1400\n",
      "     44            1.0000        \u001b[32m0.1836\u001b[0m       0.3125            0.3125        1.6609  0.0006  0.1373\n",
      "     45            1.0000        0.2987       0.3125            0.3125        1.6656  0.0005  0.1331\n",
      "     46            1.0000        0.2278       0.3229            0.3229        1.6698  0.0005  0.1339\n",
      "     47            1.0000        0.2752       0.3125            0.3125        1.6755  0.0004  0.1333\n",
      "     48            1.0000        \u001b[32m0.1478\u001b[0m       0.3021            0.3021        1.6809  0.0004  0.1418\n",
      "     49            1.0000        0.1970       0.3021            0.3021        1.6879  0.0003  0.1666\n",
      "     50            1.0000        0.2138       0.2917            0.2917        1.6946  0.0003  0.2026\n",
      "Fine tuning model for subject 5 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        1.4452       0.3229            0.3229        2.0935  0.0004  0.1599\n",
      "     32            0.3000        1.4579       0.3333            0.3333        1.9494  0.0005  0.1597\n",
      "     33            0.4250        1.3705       0.3021            0.3021        1.8140  0.0005  0.1874\n",
      "     34            0.5750        1.1958       0.2708            0.2708        1.6985  0.0006  0.1762\n",
      "     35            0.7750        1.0242       0.2500            0.2500        1.6404  0.0006  0.1828\n",
      "     36            \u001b[36m0.8750\u001b[0m        0.8811       0.2708            0.2708        1.6786  0.0007  0.1658\n",
      "     37            0.8750        0.7896       0.2917            0.2917        1.7659  0.0007  0.1688\n",
      "     38            0.8500        \u001b[32m0.5810\u001b[0m       0.2500            0.2500        1.8525  0.0007  0.1631\n",
      "     39            0.8250        0.5820       0.2708            0.2708        1.9178  0.0007  0.1603\n",
      "     40            0.7750        \u001b[32m0.4844\u001b[0m       0.2812            0.2812        1.9729  0.0007  0.1709\n",
      "     41            0.8000        \u001b[32m0.4828\u001b[0m       0.2500            0.2500        2.0056  0.0007  0.1697\n",
      "     42            0.8500        \u001b[32m0.3555\u001b[0m       0.2500            0.2500        2.0136  0.0007  0.1810\n",
      "     43            \u001b[36m0.9000\u001b[0m        0.3865       0.2500            0.2500        2.0074  0.0006  0.1656\n",
      "     44            \u001b[36m0.9250\u001b[0m        \u001b[32m0.2801\u001b[0m       0.2500            0.2500        1.9756  0.0006  0.2136\n",
      "     45            0.9250        0.2829       0.2500            0.2500        1.9466  0.0005  0.1496\n",
      "     46            0.9250        \u001b[32m0.2388\u001b[0m       0.2188            0.2188        1.9166  0.0005  0.1594\n",
      "     47            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2212\u001b[0m       0.2292            0.2292        1.8926  0.0004  0.1706\n",
      "     48            0.9500        0.2357       0.2500            0.2500        1.8730  0.0004  0.1546\n",
      "     49            \u001b[36m1.0000\u001b[0m        0.2235       0.2604            0.2604        1.8515  0.0003  0.1436\n",
      "     50            1.0000        0.2230       0.2812            0.2812        1.8362  0.0003  0.1439\n",
      "Fine tuning model for subject 5 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3750        1.3674       0.3229            0.3229        2.1375  0.0004  0.1439\n",
      "     32            0.4250        1.2920       0.3125            0.3125        2.0586  0.0005  0.1469\n",
      "     33            0.4750        1.3807       0.3125            0.3125        2.0040  0.0005  0.1426\n",
      "     34            0.5250        1.1330       0.3021            0.3021        1.9826  0.0006  0.1480\n",
      "     35            0.6250        0.9361       0.2917            0.2917        1.9707  0.0006  0.1419\n",
      "     36            0.7250        0.6872       0.2917            0.2917        1.9914  0.0007  0.1446\n",
      "     37            0.7250        \u001b[32m0.5311\u001b[0m       0.2708            0.2708        2.0462  0.0007  0.1440\n",
      "     38            0.7250        \u001b[32m0.4864\u001b[0m       0.2708            0.2708        2.1227  0.0007  0.1484\n",
      "     39            0.7250        \u001b[32m0.4767\u001b[0m       0.2708            0.2708        2.1850  0.0007  0.1434\n",
      "     40            0.7250        \u001b[32m0.4548\u001b[0m       0.2812            0.2812        2.2188  0.0007  0.1407\n",
      "     41            0.7000        \u001b[32m0.3381\u001b[0m       0.2917            0.2917        2.2243  0.0007  0.1524\n",
      "     42            0.7250        0.3546       0.3125            0.3125        2.2246  0.0007  0.1446\n",
      "     43            0.8000        \u001b[32m0.2934\u001b[0m       0.3125            0.3125        2.2225  0.0006  0.1430\n",
      "     44            \u001b[36m0.8250\u001b[0m        \u001b[32m0.2682\u001b[0m       0.3229            0.3229        2.2384  0.0006  0.1445\n",
      "     45            \u001b[36m0.8500\u001b[0m        \u001b[32m0.1643\u001b[0m       0.3125            0.3125        2.2486  0.0005  0.1583\n",
      "     46            \u001b[36m0.9000\u001b[0m        0.2359       0.3125            0.3125        2.2509  0.0005  0.1496\n",
      "     47            \u001b[36m0.9500\u001b[0m        \u001b[32m0.1600\u001b[0m       0.3125            0.3125        2.2478  0.0004  0.1787\n",
      "     48            0.9500        0.1788       0.3125            0.3125        2.2336  0.0004  0.1514\n",
      "     49            0.9500        0.1824       0.3021            0.3021        2.2112  0.0003  0.1523\n",
      "     50            \u001b[36m0.9750\u001b[0m        0.2031       0.3021            0.3021        2.1851  0.0003  0.1470\n",
      "Fine tuning model for subject 5 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3250        1.6010       0.3229            0.3229        2.1096  0.0004  0.1476\n",
      "     32            0.3250        1.4620       0.3021            0.3021        1.9844  0.0005  0.1392\n",
      "     33            0.3500        1.3108       0.2917            0.2917        1.8502  0.0005  0.1519\n",
      "     34            0.4750        1.1772       0.3021            0.3021        1.7457  0.0006  0.1677\n",
      "     35            0.5250        1.0904       0.2500            0.2500        1.6837  0.0006  0.1429\n",
      "     36            0.8000        1.0436       0.2604            0.2604        1.6697  0.0007  0.1441\n",
      "     37            \u001b[36m0.8250\u001b[0m        0.8474       0.2500            0.2500        1.6984  0.0007  0.1454\n",
      "     38            \u001b[36m0.8500\u001b[0m        \u001b[32m0.6022\u001b[0m       0.2500            0.2500        1.7256  0.0007  0.1418\n",
      "     39            \u001b[36m0.8750\u001b[0m        \u001b[32m0.5644\u001b[0m       0.2188            0.2188        1.7439  0.0007  0.1462\n",
      "     40            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4992\u001b[0m       0.2292            0.2292        1.7659  0.0007  0.1426\n",
      "     41            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2926\u001b[0m       0.2396            0.2396        1.7782  0.0007  0.1460\n",
      "     42            \u001b[36m0.9750\u001b[0m        0.3430       0.2292            0.2292        1.7877  0.0007  0.1406\n",
      "     43            0.9750        0.3337       0.1875            0.1875        1.7962  0.0006  0.1473\n",
      "     44            0.9750        0.3213       0.1875            0.1875        1.8093  0.0006  0.1405\n",
      "     45            0.9750        \u001b[32m0.2430\u001b[0m       0.1875            0.1875        1.8245  0.0005  0.1485\n",
      "     46            0.9750        0.2516       0.1875            0.1875        1.8338  0.0005  0.1474\n",
      "     47            0.9750        \u001b[32m0.1912\u001b[0m       0.1875            0.1875        1.8388  0.0004  0.1441\n",
      "     48            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1738\u001b[0m       0.1875            0.1875        1.8414  0.0004  0.1445\n",
      "     49            1.0000        \u001b[32m0.1648\u001b[0m       0.1875            0.1875        1.8405  0.0003  0.1454\n",
      "     50            1.0000        \u001b[32m0.1481\u001b[0m       0.1875            0.1875        1.8371  0.0003  0.1483\n",
      "Fine tuning model for subject 5 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3250        1.8591       0.3333            0.3333        2.1332  0.0004  0.1457\n",
      "     32            0.3750        1.7253       0.3333            0.3333        2.0181  0.0005  0.1426\n",
      "     33            0.4500        1.6664       0.3125            0.3125        1.8637  0.0005  0.1406\n",
      "     34            0.5000        1.2859       0.3229            0.3229        1.7384  0.0006  0.1442\n",
      "     35            0.5750        1.1270       0.3333            0.3333        1.6971  0.0006  0.1427\n",
      "     36            0.6250        1.0614       0.3333            0.3333        1.7507  0.0007  0.1508\n",
      "     37            0.7000        0.8504       0.3333            0.3333        1.8282  0.0007  0.1457\n",
      "     38            0.7250        0.8232       0.3542            0.3542        1.8748  0.0007  0.1443\n",
      "     39            0.7750        0.6566       0.3542            0.3542        1.8928  0.0007  0.1497\n",
      "     40            0.8000        \u001b[32m0.5779\u001b[0m       0.3542            0.3542        1.8885  0.0007  0.1419\n",
      "     41            \u001b[36m0.8250\u001b[0m        \u001b[32m0.5125\u001b[0m       0.3229            0.3229        1.8839  0.0007  0.1445\n",
      "     42            0.8250        \u001b[32m0.4203\u001b[0m       0.3229            0.3229        1.8823  0.0007  0.1493\n",
      "     43            \u001b[36m0.8500\u001b[0m        \u001b[32m0.3690\u001b[0m       0.3125            0.3125        1.8859  0.0006  0.1427\n",
      "     44            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3653\u001b[0m       0.3229            0.3229        1.8955  0.0006  0.1440\n",
      "     45            \u001b[36m0.9250\u001b[0m        0.4043       0.2917            0.2917        1.9079  0.0005  0.1400\n",
      "     46            0.9250        \u001b[32m0.2991\u001b[0m       0.3333            0.3333        1.9196  0.0005  0.1509\n",
      "     47            \u001b[36m0.9500\u001b[0m        0.3336       0.3333            0.3333        1.9286  0.0004  0.1441\n",
      "     48            0.9500        0.3064       0.3438            0.3438        1.9357  0.0004  0.1402\n",
      "     49            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2567\u001b[0m       0.3542            0.3542        1.9401  0.0003  0.1410\n",
      "     50            1.0000        \u001b[32m0.2517\u001b[0m       0.3333            0.3333        1.9420  0.0003  0.1440\n",
      "Fine tuning model for subject 5 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3250        1.7708       0.3229            0.3229        2.1252  0.0004  0.1443\n",
      "     32            0.3250        1.4572       0.3438            0.3438        2.0343  0.0005  0.1421\n",
      "     33            0.3750        1.5096       0.3229            0.3229        1.9309  0.0005  0.1733\n",
      "     34            0.5000        1.0184       0.3229            0.3229        1.8055  0.0006  0.1671\n",
      "     35            0.6250        1.0141       0.3229            0.3229        1.6724  0.0006  0.1755\n",
      "     36            \u001b[36m0.8250\u001b[0m        0.9713       0.3125            0.3125        1.5999  0.0007  0.1696\n",
      "     37            \u001b[36m0.8750\u001b[0m        0.8070       0.2604            0.2604        1.5855  0.0007  0.1626\n",
      "     38            0.8750        \u001b[32m0.5728\u001b[0m       0.2708            0.2708        1.5991  0.0007  0.2040\n",
      "     39            \u001b[36m0.9500\u001b[0m        0.6170       0.2708            0.2708        1.6109  0.0007  0.1586\n",
      "     40            \u001b[36m0.9750\u001b[0m        0.6173       0.2917            0.2917        1.6173  0.0007  0.1737\n",
      "     41            0.9500        \u001b[32m0.4680\u001b[0m       0.3125            0.3125        1.6276  0.0007  0.1500\n",
      "     42            0.9750        0.4899       0.2917            0.2917        1.6460  0.0007  0.1747\n",
      "     43            0.9750        \u001b[32m0.3192\u001b[0m       0.2917            0.2917        1.6725  0.0006  0.1555\n",
      "     44            0.9750        0.3964       0.2708            0.2708        1.6988  0.0006  0.1728\n",
      "     45            0.9750        \u001b[32m0.2796\u001b[0m       0.2708            0.2708        1.7181  0.0005  0.1732\n",
      "     46            0.9750        0.3246       0.2708            0.2708        1.7258  0.0005  0.1603\n",
      "     47            0.9750        0.2979       0.2396            0.2396        1.7238  0.0004  0.1737\n",
      "     48            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2333\u001b[0m       0.2396            0.2396        1.7174  0.0004  0.1540\n",
      "     49            1.0000        \u001b[32m0.2040\u001b[0m       0.2500            0.2500        1.7085  0.0003  0.2135\n",
      "     50            1.0000        \u001b[32m0.2009\u001b[0m       0.2708            0.2708        1.6987  0.0003  0.1703\n",
      "Fine tuning model for subject 5 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4500        1.7407       0.3125            0.3125        2.1437  0.0004  0.1546\n",
      "     32            0.4250        1.6037       0.3125            0.3125        2.0988  0.0005  0.1472\n",
      "     33            0.4000        1.3350       0.3438            0.3438        2.0666  0.0005  0.1452\n",
      "     34            0.4500        1.2314       0.3125            0.3125        2.0380  0.0006  0.1423\n",
      "     35            0.4750        0.8710       0.3021            0.3021        2.0113  0.0006  0.1483\n",
      "     36            0.6000        0.8002       0.3125            0.3125        1.9830  0.0007  0.1437\n",
      "     37            0.6750        0.7205       0.3125            0.3125        1.9528  0.0007  0.1518\n",
      "     38            0.7500        \u001b[32m0.5901\u001b[0m       0.3021            0.3021        1.9369  0.0007  0.1426\n",
      "     39            0.8000        \u001b[32m0.4638\u001b[0m       0.3021            0.3021        1.9379  0.0007  0.1434\n",
      "     40            \u001b[36m0.8750\u001b[0m        0.5226       0.3125            0.3125        1.9371  0.0007  0.1473\n",
      "     41            0.8750        0.4831       0.3333            0.3333        1.9471  0.0007  0.1481\n",
      "     42            0.8750        \u001b[32m0.3138\u001b[0m       0.3229            0.3229        1.9549  0.0007  0.1514\n",
      "     43            0.8750        \u001b[32m0.2972\u001b[0m       0.3125            0.3125        1.9676  0.0006  0.1468\n",
      "     44            \u001b[36m0.9000\u001b[0m        \u001b[32m0.2527\u001b[0m       0.3125            0.3125        1.9702  0.0006  0.1443\n",
      "     45            \u001b[36m0.9250\u001b[0m        0.3020       0.3125            0.3125        1.9695  0.0005  0.1424\n",
      "     46            0.9250        \u001b[32m0.2150\u001b[0m       0.3021            0.3021        1.9625  0.0005  0.1444\n",
      "     47            0.9250        0.2405       0.3021            0.3021        1.9458  0.0004  0.1371\n",
      "     48            \u001b[36m0.9500\u001b[0m        \u001b[32m0.1771\u001b[0m       0.2917            0.2917        1.9262  0.0004  0.1444\n",
      "     49            0.9500        0.2098       0.3021            0.3021        1.9078  0.0003  0.1388\n",
      "     50            \u001b[36m0.9750\u001b[0m        \u001b[32m0.1379\u001b[0m       0.2917            0.2917        1.8844  0.0003  0.1419\n",
      "Fine tuning model for subject 5 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3250        1.6457       0.3125            0.3125        2.1215  0.0004  0.1494\n",
      "     32            0.3250        1.6022       0.3125            0.3125        2.0483  0.0005  0.1393\n",
      "     33            0.4250        1.3800       0.2812            0.2812        1.9751  0.0005  0.1478\n",
      "     34            0.5250        1.2169       0.2604            0.2604        1.9133  0.0006  0.1493\n",
      "     35            0.6500        0.9056       0.2500            0.2500        1.8481  0.0006  0.1380\n",
      "     36            \u001b[36m0.8250\u001b[0m        0.8101       0.2708            0.2708        1.7880  0.0007  0.1426\n",
      "     37            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5804\u001b[0m       0.2917            0.2917        1.7651  0.0007  0.1457\n",
      "     38            \u001b[36m0.9750\u001b[0m        \u001b[32m0.4751\u001b[0m       0.2917            0.2917        1.7960  0.0007  0.1480\n",
      "     39            0.9500        \u001b[32m0.4707\u001b[0m       0.3125            0.3125        1.8439  0.0007  0.1540\n",
      "     40            0.9750        \u001b[32m0.4220\u001b[0m       0.3229            0.3229        1.9012  0.0007  0.1384\n",
      "     41            0.9500        \u001b[32m0.3035\u001b[0m       0.3021            0.3021        1.9680  0.0007  0.1440\n",
      "     42            0.9500        0.3319       0.3333            0.3333        2.0342  0.0007  0.1422\n",
      "     43            0.9500        \u001b[32m0.2389\u001b[0m       0.3021            0.3021        2.0770  0.0006  0.1419\n",
      "     44            0.9500        \u001b[32m0.2134\u001b[0m       0.3021            0.3021        2.1024  0.0006  0.1456\n",
      "     45            0.9750        0.2164       0.3021            0.3021        2.1137  0.0005  0.1413\n",
      "     46            0.9750        \u001b[32m0.1799\u001b[0m       0.2917            0.2917        2.1122  0.0005  0.1492\n",
      "     47            0.9750        0.2107       0.2917            0.2917        2.1025  0.0004  0.1455\n",
      "     48            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1413\u001b[0m       0.2917            0.2917        2.0910  0.0004  0.1419\n",
      "     49            1.0000        0.1474       0.3021            0.3021        2.0790  0.0003  0.1445\n",
      "     50            1.0000        0.1524       0.2917            0.2917        2.0677  0.0003  0.1448\n",
      "Fine tuning model for subject 5 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2750        1.9979       0.3229            0.3229        2.1306  0.0004  0.1389\n",
      "     32            0.4000        1.8208       0.3438            0.3438        2.0584  0.0005  0.1411\n",
      "     33            0.4250        1.5275       0.3438            0.3438        1.9981  0.0005  0.1419\n",
      "     34            0.4750        1.3518       0.2708            0.2708        1.9696  0.0006  0.1462\n",
      "     35            0.5250        1.1348       0.2917            0.2917        2.0020  0.0006  0.1479\n",
      "     36            0.6250        0.9502       0.2708            0.2708        2.0068  0.0007  0.1434\n",
      "     37            0.7500        0.7330       0.2604            0.2604        1.9944  0.0007  0.1460\n",
      "     38            \u001b[36m0.8250\u001b[0m        0.6739       0.2604            0.2604        1.9613  0.0007  0.1496\n",
      "     39            \u001b[36m0.8750\u001b[0m        \u001b[32m0.4880\u001b[0m       0.2708            0.2708        1.9166  0.0007  0.1432\n",
      "     40            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4504\u001b[0m       0.2604            0.2604        1.8894  0.0007  0.1472\n",
      "     41            0.9500        \u001b[32m0.3213\u001b[0m       0.2812            0.2812        1.8856  0.0007  0.1466\n",
      "     42            \u001b[36m1.0000\u001b[0m        0.3691       0.2708            0.2708        1.8958  0.0007  0.1421\n",
      "     43            1.0000        0.3662       0.2604            0.2604        1.9156  0.0006  0.1435\n",
      "     44            1.0000        \u001b[32m0.2706\u001b[0m       0.2708            0.2708        1.9399  0.0006  0.1437\n",
      "     45            1.0000        \u001b[32m0.2150\u001b[0m       0.2708            0.2708        1.9640  0.0005  0.1411\n",
      "     46            1.0000        \u001b[32m0.1838\u001b[0m       0.2812            0.2812        1.9850  0.0005  0.1451\n",
      "     47            1.0000        0.2116       0.2812            0.2812        1.9996  0.0004  0.1511\n",
      "     48            1.0000        \u001b[32m0.1740\u001b[0m       0.2708            0.2708        2.0092  0.0004  0.1432\n",
      "     49            1.0000        0.2077       0.2708            0.2708        2.0128  0.0003  0.1443\n",
      "     50            1.0000        \u001b[32m0.1722\u001b[0m       0.2604            0.2604        2.0109  0.0003  0.1396\n",
      "Fine tuning model for subject 5 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2750        1.7186       0.3229            0.3229        2.1141  0.0004  0.1438\n",
      "     32            0.3000        1.6463       0.3438            0.3438        1.9930  0.0005  0.1443\n",
      "     33            0.4750        1.3265       0.2917            0.2917        1.8876  0.0005  0.1416\n",
      "     34            0.5250        1.1930       0.2500            0.2500        1.8426  0.0006  0.1426\n",
      "     35            0.6000        1.0008       0.2708            0.2708        1.8721  0.0006  0.1457\n",
      "     36            0.6500        0.7833       0.2604            0.2604        1.8963  0.0007  0.1475\n",
      "     37            0.6500        \u001b[32m0.6217\u001b[0m       0.2604            0.2604        1.8581  0.0007  0.1441\n",
      "     38            0.7250        \u001b[32m0.5238\u001b[0m       0.2083            0.2083        1.7769  0.0007  0.1779\n",
      "     39            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4747\u001b[0m       0.2292            0.2292        1.7577  0.0007  0.1753\n",
      "     40            \u001b[36m0.9250\u001b[0m        \u001b[32m0.4327\u001b[0m       0.2500            0.2500        1.7985  0.0007  0.1777\n",
      "     41            0.9250        \u001b[32m0.2805\u001b[0m       0.2708            0.2708        1.8523  0.0007  0.1716\n",
      "     42            0.8750        0.3022       0.2708            0.2708        1.8992  0.0007  0.2025\n",
      "     43            0.9000        0.3676       0.2812            0.2812        1.9261  0.0006  0.1614\n",
      "     44            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2785\u001b[0m       0.3021            0.3021        1.9329  0.0006  0.1730\n",
      "     45            0.9750        \u001b[32m0.2246\u001b[0m       0.2917            0.2917        1.9264  0.0005  0.1620\n",
      "     46            0.9750        \u001b[32m0.2093\u001b[0m       0.2708            0.2708        1.9162  0.0005  0.1657\n",
      "     47            0.9750        \u001b[32m0.1907\u001b[0m       0.2708            0.2708        1.9050  0.0004  0.1666\n",
      "     48            0.9750        \u001b[32m0.1517\u001b[0m       0.2812            0.2812        1.8962  0.0004  0.1663\n",
      "     49            0.9750        \u001b[32m0.1423\u001b[0m       0.3021            0.3021        1.8875  0.0003  0.1671\n",
      "     50            0.9750        \u001b[32m0.1396\u001b[0m       0.3021            0.3021        1.8802  0.0003  0.1839\n",
      "Fine tuning model for subject 5 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4500        1.5436       0.3125            0.3125        2.1573  0.0004  0.1759\n",
      "     32            0.4750        1.5568       0.3021            0.3021        2.1206  0.0005  0.2188\n",
      "     33            0.4500        1.1685       0.3229            0.3229        2.1176  0.0005  0.1587\n",
      "     34            0.5500        1.0208       0.3542            0.3542        2.1228  0.0006  0.1429\n",
      "     35            0.6000        0.7450       0.3125            0.3125        2.1494  0.0006  0.1431\n",
      "     36            0.6750        0.8326       0.3125            0.3125        2.1921  0.0007  0.1575\n",
      "     37            0.7000        \u001b[32m0.5617\u001b[0m       0.3333            0.3333        2.2579  0.0007  0.1501\n",
      "     38            0.7000        \u001b[32m0.4978\u001b[0m       0.3438            0.3438        2.2924  0.0007  0.1419\n",
      "     39            0.7000        \u001b[32m0.4157\u001b[0m       0.3333            0.3333        2.3133  0.0007  0.1453\n",
      "     40            0.7000        \u001b[32m0.3824\u001b[0m       0.3333            0.3333        2.3331  0.0007  0.1429\n",
      "     41            0.7750        \u001b[32m0.3332\u001b[0m       0.3333            0.3333        2.3320  0.0007  0.1387\n",
      "     42            \u001b[36m0.8250\u001b[0m        \u001b[32m0.2931\u001b[0m       0.3125            0.3125        2.3083  0.0007  0.1557\n",
      "     43            \u001b[36m0.9000\u001b[0m        \u001b[32m0.2295\u001b[0m       0.3229            0.3229        2.2771  0.0006  0.1433\n",
      "     44            \u001b[36m0.9250\u001b[0m        \u001b[32m0.2006\u001b[0m       0.3021            0.3021        2.2443  0.0006  0.1438\n",
      "     45            \u001b[36m1.0000\u001b[0m        0.2346       0.2917            0.2917        2.2167  0.0005  0.1437\n",
      "     46            1.0000        \u001b[32m0.1945\u001b[0m       0.3229            0.3229        2.1949  0.0005  0.1441\n",
      "     47            1.0000        \u001b[32m0.1749\u001b[0m       0.3125            0.3125        2.1779  0.0004  0.1440\n",
      "     48            1.0000        0.2062       0.3125            0.3125        2.1686  0.0004  0.1514\n",
      "     49            1.0000        \u001b[32m0.1409\u001b[0m       0.3021            0.3021        2.1627  0.0003  0.1473\n",
      "     50            1.0000        \u001b[32m0.1380\u001b[0m       0.3021            0.3021        2.1566  0.0003  0.1401\n",
      "Fine tuning model for subject 5 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3333        1.8893       0.3229            0.3229        2.1040  0.0004  0.1772\n",
      "     32            0.3556        1.7650       0.3333            0.3333        1.9714  0.0005  0.1526\n",
      "     33            0.4889        1.6415       0.3021            0.3021        1.8195  0.0005  0.1486\n",
      "     34            0.5556        1.3816       0.2604            0.2604        1.6592  0.0006  0.1537\n",
      "     35            0.6667        1.1418       0.2812            0.2812        1.6008  0.0006  0.1563\n",
      "     36            0.6889        1.0799       0.2604            0.2604        1.6425  0.0007  0.1511\n",
      "     37            0.7556        0.9241       0.2604            0.2604        1.6986  0.0007  0.1520\n",
      "     38            0.7556        0.7978       0.2812            0.2812        1.7200  0.0007  0.1497\n",
      "     39            \u001b[36m0.8444\u001b[0m        \u001b[32m0.6368\u001b[0m       0.2917            0.2917        1.7237  0.0007  0.1553\n",
      "     40            \u001b[36m0.8667\u001b[0m        \u001b[32m0.5537\u001b[0m       0.3125            0.3125        1.7178  0.0007  0.1437\n",
      "     41            \u001b[36m0.9111\u001b[0m        \u001b[32m0.4460\u001b[0m       0.3438            0.3438        1.7246  0.0007  0.1493\n",
      "     42            \u001b[36m0.9333\u001b[0m        0.4815       0.3333            0.3333        1.7363  0.0007  0.1467\n",
      "     43            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3904\u001b[0m       0.3125            0.3125        1.7526  0.0006  0.1513\n",
      "     44            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3372\u001b[0m       0.2917            0.2917        1.7662  0.0006  0.1546\n",
      "     45            1.0000        \u001b[32m0.3156\u001b[0m       0.3021            0.3021        1.7795  0.0005  0.1548\n",
      "     46            1.0000        \u001b[32m0.3118\u001b[0m       0.3333            0.3333        1.7882  0.0005  0.1495\n",
      "     47            1.0000        \u001b[32m0.2937\u001b[0m       0.3333            0.3333        1.7928  0.0004  0.1510\n",
      "     48            1.0000        0.3432       0.3333            0.3333        1.7927  0.0004  0.1513\n",
      "     49            1.0000        \u001b[32m0.2056\u001b[0m       0.3438            0.3438        1.7891  0.0003  0.1576\n",
      "     50            1.0000        0.2629       0.3229            0.3229        1.7844  0.0003  0.1492\n",
      "Fine tuning model for subject 5 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3111        1.4329       0.3229            0.3229        2.1341  0.0004  0.1492\n",
      "     32            0.3556        1.4802       0.3125            0.3125        2.0582  0.0005  0.1573\n",
      "     33            0.4222        1.1158       0.3125            0.3125        1.9677  0.0005  0.1559\n",
      "     34            0.4889        1.2601       0.2917            0.2917        1.8774  0.0006  0.1534\n",
      "     35            0.5778        1.0101       0.3229            0.3229        1.7979  0.0006  0.1575\n",
      "     36            0.6889        0.7701       0.3021            0.3021        1.7836  0.0007  0.1544\n",
      "     37            0.7778        0.6799       0.3125            0.3125        1.8332  0.0007  0.1483\n",
      "     38            0.8000        \u001b[32m0.6389\u001b[0m       0.3125            0.3125        1.8902  0.0007  0.1683\n",
      "     39            \u001b[36m0.8444\u001b[0m        \u001b[32m0.5361\u001b[0m       0.3438            0.3438        1.8996  0.0007  0.1582\n",
      "     40            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4818\u001b[0m       0.3646            0.3646        1.8714  0.0007  0.1583\n",
      "     41            \u001b[36m0.9556\u001b[0m        \u001b[32m0.3975\u001b[0m       0.3438            0.3438        1.8271  0.0007  0.1536\n",
      "     42            0.9556        0.4438       0.3542            0.3542        1.7837  0.0007  0.1497\n",
      "     43            0.9556        \u001b[32m0.3183\u001b[0m       0.3333            0.3333        1.7571  0.0006  0.1542\n",
      "     44            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2646\u001b[0m       0.3333            0.3333        1.7403  0.0006  0.1742\n",
      "     45            1.0000        0.2699       0.3333            0.3333        1.7336  0.0005  0.1523\n",
      "     46            1.0000        0.3272       0.3229            0.3229        1.7304  0.0005  0.1494\n",
      "     47            1.0000        0.2745       0.3229            0.3229        1.7295  0.0004  0.1579\n",
      "     48            1.0000        \u001b[32m0.2356\u001b[0m       0.3438            0.3438        1.7309  0.0004  0.1505\n",
      "     49            1.0000        \u001b[32m0.1824\u001b[0m       0.3229            0.3229        1.7322  0.0003  0.1471\n",
      "     50            1.0000        \u001b[32m0.1726\u001b[0m       0.3125            0.3125        1.7302  0.0003  0.1544\n",
      "Fine tuning model for subject 5 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3111        1.6963       0.3125            0.3125        2.1222  0.0004  0.1492\n",
      "     32            0.3111        1.5150       0.3333            0.3333        2.0549  0.0005  0.1480\n",
      "     33            0.3556        1.3668       0.3229            0.3229        2.0506  0.0005  0.1544\n",
      "     34            0.3778        1.2810       0.3021            0.3021        2.0980  0.0006  0.1508\n",
      "     35            0.4444        0.9493       0.3021            0.3021        2.1681  0.0006  0.1530\n",
      "     36            0.4667        0.9599       0.3125            0.3125        2.2561  0.0007  0.1510\n",
      "     37            0.5111        0.7497       0.3333            0.3333        2.2837  0.0007  0.1504\n",
      "     38            0.6000        \u001b[32m0.6462\u001b[0m       0.3333            0.3333        2.2825  0.0007  0.1542\n",
      "     39            0.6667        \u001b[32m0.5886\u001b[0m       0.3542            0.3542        2.2482  0.0007  0.1546\n",
      "     40            0.7333        \u001b[32m0.5269\u001b[0m       0.3333            0.3333        2.2354  0.0007  0.1814\n",
      "     41            0.7556        \u001b[32m0.4730\u001b[0m       0.3438            0.3438        2.2132  0.0007  0.1916\n",
      "     42            0.7778        \u001b[32m0.3771\u001b[0m       0.3333            0.3333        2.1686  0.0007  0.1723\n",
      "     43            \u001b[36m0.8222\u001b[0m        0.4127       0.3229            0.3229        2.1166  0.0006  0.1723\n",
      "     44            0.8222        \u001b[32m0.3716\u001b[0m       0.3021            0.3021        2.0629  0.0006  0.2153\n",
      "     45            \u001b[36m0.8667\u001b[0m        \u001b[32m0.2845\u001b[0m       0.3021            0.3021        2.0161  0.0005  0.1826\n",
      "     46            \u001b[36m0.9556\u001b[0m        0.2972       0.3021            0.3021        1.9648  0.0005  0.1953\n",
      "     47            \u001b[36m0.9778\u001b[0m        \u001b[32m0.2634\u001b[0m       0.3021            0.3021        1.9248  0.0004  0.1707\n",
      "     48            0.9778        \u001b[32m0.2416\u001b[0m       0.3021            0.3021        1.8873  0.0004  0.1720\n",
      "     49            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2358\u001b[0m       0.2917            0.2917        1.8578  0.0003  0.1705\n",
      "     50            1.0000        \u001b[32m0.2278\u001b[0m       0.2917            0.2917        1.8349  0.0003  0.1819\n",
      "Fine tuning model for subject 5 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5111        1.3984       0.3125            0.3125        2.1360  0.0004  0.1707\n",
      "     32            0.6000        1.3673       0.2812            0.2812        2.0736  0.0005  0.1995\n",
      "     33            0.6889        1.2886       0.2812            0.2812        2.0283  0.0005  0.1742\n",
      "     34            0.6667        0.9990       0.3021            0.3021        2.0079  0.0006  0.2035\n",
      "     35            0.6444        0.8104       0.3229            0.3229        2.0326  0.0006  0.1516\n",
      "     36            0.6667        \u001b[32m0.6235\u001b[0m       0.2812            0.2812        2.0432  0.0007  0.1522\n",
      "     37            0.7333        0.6366       0.2917            0.2917        2.0269  0.0007  0.1554\n",
      "     38            0.7556        \u001b[32m0.4835\u001b[0m       0.3021            0.3021        1.9888  0.0007  0.1582\n",
      "     39            0.8000        \u001b[32m0.4572\u001b[0m       0.2917            0.2917        1.9564  0.0007  0.1570\n",
      "     40            \u001b[36m0.8667\u001b[0m        \u001b[32m0.3526\u001b[0m       0.3125            0.3125        1.9221  0.0007  0.1544\n",
      "     41            \u001b[36m0.8889\u001b[0m        0.4484       0.3021            0.3021        1.9064  0.0007  0.1489\n",
      "     42            \u001b[36m0.9111\u001b[0m        0.3672       0.2917            0.2917        1.9079  0.0007  0.1652\n",
      "     43            \u001b[36m0.9333\u001b[0m        \u001b[32m0.2863\u001b[0m       0.2917            0.2917        1.9125  0.0006  0.1487\n",
      "     44            \u001b[36m0.9556\u001b[0m        \u001b[32m0.2703\u001b[0m       0.2917            0.2917        1.9218  0.0006  0.1542\n",
      "     45            \u001b[36m0.9778\u001b[0m        \u001b[32m0.2265\u001b[0m       0.3021            0.3021        1.9243  0.0005  0.1563\n",
      "     46            0.9778        0.2310       0.3229            0.3229        1.9216  0.0005  0.1527\n",
      "     47            0.9778        \u001b[32m0.1886\u001b[0m       0.3229            0.3229        1.9121  0.0004  0.1539\n",
      "     48            0.9778        \u001b[32m0.1321\u001b[0m       0.3229            0.3229        1.9009  0.0004  0.1552\n",
      "     49            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1277\u001b[0m       0.3229            0.3229        1.8888  0.0003  0.1552\n",
      "     50            1.0000        0.1433       0.3229            0.3229        1.8787  0.0003  0.1518\n",
      "Fine tuning model for subject 5 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3556        1.7038       0.3125            0.3125        2.1077  0.0004  0.1496\n",
      "     32            0.4000        1.6671       0.3438            0.3438        2.0454  0.0005  0.1578\n",
      "     33            0.3778        1.5254       0.3021            0.3021        2.0907  0.0005  0.1539\n",
      "     34            0.4000        1.1660       0.3125            0.3125        2.3654  0.0006  0.1521\n",
      "     35            0.3556        1.0977       0.2812            0.2812        2.7689  0.0006  0.1494\n",
      "     36            0.3111        0.8665       0.2708            0.2708        3.1174  0.0007  0.1525\n",
      "     37            0.3333        0.6791       0.2604            0.2604        3.3167  0.0007  0.1496\n",
      "     38            0.3333        \u001b[32m0.5990\u001b[0m       0.2604            0.2604        3.3602  0.0007  0.1568\n",
      "     39            0.3333        \u001b[32m0.5653\u001b[0m       0.2604            0.2604        3.3644  0.0007  0.1542\n",
      "     40            0.3556        0.5663       0.2604            0.2604        3.2701  0.0007  0.1582\n",
      "     41            0.3556        \u001b[32m0.4083\u001b[0m       0.2604            0.2604        3.1460  0.0007  0.1540\n",
      "     42            0.4000        0.4215       0.2604            0.2604        2.9722  0.0007  0.1509\n",
      "     43            0.5111        \u001b[32m0.3183\u001b[0m       0.2604            0.2604        2.7734  0.0006  0.1618\n",
      "     44            0.5556        0.3275       0.2396            0.2396        2.6209  0.0006  0.1500\n",
      "     45            0.6444        0.3868       0.2396            0.2396        2.4599  0.0005  0.1562\n",
      "     46            0.7556        0.3763       0.2500            0.2500        2.3244  0.0005  0.1510\n",
      "     47            \u001b[36m0.8444\u001b[0m        \u001b[32m0.2934\u001b[0m       0.2396            0.2396        2.2213  0.0004  0.1561\n",
      "     48            \u001b[36m0.9111\u001b[0m        \u001b[32m0.2802\u001b[0m       0.2396            0.2396        2.1397  0.0004  0.1493\n",
      "     49            \u001b[36m0.9556\u001b[0m        \u001b[32m0.2365\u001b[0m       0.2708            0.2708        2.0767  0.0003  0.1526\n",
      "     50            0.9556        \u001b[32m0.2036\u001b[0m       0.2500            0.2500        2.0219  0.0003  0.1497\n",
      "Fine tuning model for subject 5 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3111        1.6494       0.3125            0.3125        2.1166  0.0004  0.1544\n",
      "     32            0.4222        1.6164       0.3438            0.3438        2.0462  0.0005  0.1490\n",
      "     33            0.5111        1.6045       0.3229            0.3229        2.0105  0.0005  0.1535\n",
      "     34            0.5778        1.2787       0.3438            0.3438        1.9915  0.0006  0.1492\n",
      "     35            0.5778        0.9869       0.3646            0.3646        1.9213  0.0006  0.1534\n",
      "     36            0.6667        0.8212       0.3438            0.3438        1.7896  0.0007  0.1492\n",
      "     37            0.8000        0.8952       0.3125            0.3125        1.6946  0.0007  0.1492\n",
      "     38            \u001b[36m0.8667\u001b[0m        0.6944       0.3333            0.3333        1.6690  0.0007  0.1593\n",
      "     39            0.8444        \u001b[32m0.6360\u001b[0m       0.2917            0.2917        1.6826  0.0007  0.1496\n",
      "     40            0.8667        \u001b[32m0.5430\u001b[0m       0.2917            0.2917        1.6998  0.0007  0.1513\n",
      "     41            \u001b[36m0.8889\u001b[0m        \u001b[32m0.5127\u001b[0m       0.3021            0.3021        1.7017  0.0007  0.1494\n",
      "     42            \u001b[36m0.9111\u001b[0m        \u001b[32m0.4543\u001b[0m       0.3125            0.3125        1.7005  0.0007  0.1559\n",
      "     43            \u001b[36m0.9333\u001b[0m        0.4670       0.3125            0.3125        1.7020  0.0006  0.1463\n",
      "     44            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3632\u001b[0m       0.3125            0.3125        1.7137  0.0006  0.1681\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3368\u001b[0m       0.3125            0.3125        1.7296  0.0005  0.1581\n",
      "     46            1.0000        \u001b[32m0.2767\u001b[0m       0.2917            0.2917        1.7511  0.0005  0.1535\n",
      "     47            1.0000        \u001b[32m0.2620\u001b[0m       0.2917            0.2917        1.7753  0.0004  0.1512\n",
      "     48            1.0000        0.3180       0.2917            0.2917        1.8003  0.0004  0.1550\n",
      "     49            1.0000        \u001b[32m0.2459\u001b[0m       0.2812            0.2812        1.8219  0.0003  0.1514\n",
      "     50            1.0000        \u001b[32m0.2271\u001b[0m       0.2708            0.2708        1.8371  0.0003  0.1480\n",
      "Fine tuning model for subject 5 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2889        1.7091       0.3229            0.3229        2.1376  0.0004  0.1493\n",
      "     32            0.3333        1.5377       0.3125            0.3125        2.0766  0.0005  0.1516\n",
      "     33            0.4000        1.4180       0.3229            0.3229        2.0445  0.0005  0.1599\n",
      "     34            0.4667        1.3829       0.3333            0.3333        2.0015  0.0006  0.1585\n",
      "     35            0.5556        1.1686       0.3333            0.3333        1.9206  0.0006  0.1517\n",
      "     36            0.7333        0.9228       0.3333            0.3333        1.8315  0.0007  0.1523\n",
      "     37            \u001b[36m0.8222\u001b[0m        0.7534       0.3021            0.3021        1.7644  0.0007  0.1492\n",
      "     38            \u001b[36m0.8667\u001b[0m        0.7347       0.3021            0.3021        1.7252  0.0007  0.1471\n",
      "     39            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5863\u001b[0m       0.2604            0.2604        1.7132  0.0007  0.1625\n",
      "     40            0.9333        \u001b[32m0.5429\u001b[0m       0.3125            0.3125        1.7127  0.0007  0.1708\n",
      "     41            \u001b[36m0.9556\u001b[0m        \u001b[32m0.4496\u001b[0m       0.3125            0.3125        1.7098  0.0007  0.1884\n",
      "     42            \u001b[36m0.9778\u001b[0m        \u001b[32m0.4447\u001b[0m       0.2917            0.2917        1.7109  0.0007  0.2102\n",
      "     43            0.9778        \u001b[32m0.4095\u001b[0m       0.2917            0.2917        1.7141  0.0006  0.1598\n",
      "     44            0.9778        \u001b[32m0.3846\u001b[0m       0.2500            0.2500        1.7269  0.0006  0.1865\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2795\u001b[0m       0.2500            0.2500        1.7405  0.0005  0.1685\n",
      "     46            1.0000        0.3390       0.2500            0.2500        1.7571  0.0005  0.2518\n",
      "     47            1.0000        0.3074       0.2604            0.2604        1.7696  0.0004  0.1916\n",
      "     48            1.0000        \u001b[32m0.2592\u001b[0m       0.2604            0.2604        1.7799  0.0004  0.1597\n",
      "     49            1.0000        \u001b[32m0.2202\u001b[0m       0.2604            0.2604        1.7850  0.0003  0.1833\n",
      "     50            1.0000        0.2556       0.2604            0.2604        1.7880  0.0003  0.1785\n",
      "Fine tuning model for subject 5 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2667        1.7818       0.3333            0.3333        2.0999  0.0004  0.1916\n",
      "     32            0.3556        1.6797       0.3229            0.3229        1.9806  0.0005  0.1695\n",
      "     33            0.4222        1.5622       0.2708            0.2708        1.9121  0.0005  0.2061\n",
      "     34            0.4000        1.4197       0.2292            0.2292        1.9642  0.0006  0.1657\n",
      "     35            0.3556        1.1649       0.2292            0.2292        2.0677  0.0006  0.1542\n",
      "     36            0.4444        0.9793       0.2188            0.2188        2.1110  0.0007  0.1564\n",
      "     37            0.4667        0.8283       0.2083            0.2083        2.1233  0.0007  0.1487\n",
      "     38            0.5333        0.7503       0.2083            0.2083        2.1058  0.0007  0.1560\n",
      "     39            0.5556        \u001b[32m0.5642\u001b[0m       0.2083            0.2083        2.1033  0.0007  0.1536\n",
      "     40            0.5778        0.5930       0.2396            0.2396        2.1636  0.0007  0.1551\n",
      "     41            0.5556        \u001b[32m0.4127\u001b[0m       0.2500            0.2500        2.2437  0.0007  0.1522\n",
      "     42            0.5333        0.4725       0.2708            0.2708        2.3161  0.0007  0.1619\n",
      "     43            0.5111        \u001b[32m0.3431\u001b[0m       0.2708            0.2708        2.3964  0.0006  0.1489\n",
      "     44            0.5778        0.3447       0.2917            0.2917        2.3902  0.0006  0.1507\n",
      "     45            0.6000        \u001b[32m0.2827\u001b[0m       0.2917            0.2917        2.3318  0.0005  0.1487\n",
      "     46            0.6889        \u001b[32m0.2494\u001b[0m       0.2604            0.2604        2.2284  0.0005  0.1562\n",
      "     47            0.7333        \u001b[32m0.2262\u001b[0m       0.2396            0.2396        2.1236  0.0004  0.1547\n",
      "     48            0.7556        \u001b[32m0.2004\u001b[0m       0.2396            0.2396        2.0314  0.0004  0.1484\n",
      "     49            0.8000        0.2654       0.2292            0.2292        1.9638  0.0003  0.1499\n",
      "     50            \u001b[36m0.8444\u001b[0m        0.2670       0.2396            0.2396        1.9035  0.0003  0.1497\n",
      "Fine tuning model for subject 5 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3556        1.4440       0.3125            0.3125        2.0890  0.0004  0.1531\n",
      "     32            0.4222        1.6464       0.3125            0.3125        1.9359  0.0005  0.1545\n",
      "     33            0.5333        1.5664       0.3125            0.3125        1.8011  0.0005  0.1573\n",
      "     34            0.6000        1.2833       0.3021            0.3021        1.7137  0.0006  0.1539\n",
      "     35            0.6000        1.3014       0.3021            0.3021        1.7053  0.0006  0.1494\n",
      "     36            0.6222        0.9902       0.3021            0.3021        1.7501  0.0007  0.1428\n",
      "     37            0.6667        0.8263       0.2812            0.2812        1.7919  0.0007  0.1551\n",
      "     38            0.7111        0.6877       0.2917            0.2917        1.7955  0.0007  0.1508\n",
      "     39            0.7778        0.6970       0.2812            0.2812        1.7949  0.0007  0.1586\n",
      "     40            \u001b[36m0.8444\u001b[0m        \u001b[32m0.6266\u001b[0m       0.3229            0.3229        1.7997  0.0007  0.1490\n",
      "     41            \u001b[36m0.8667\u001b[0m        \u001b[32m0.5407\u001b[0m       0.3229            0.3229        1.8015  0.0007  0.1532\n",
      "     42            \u001b[36m0.8889\u001b[0m        \u001b[32m0.4866\u001b[0m       0.3229            0.3229        1.8070  0.0007  0.1490\n",
      "     43            \u001b[36m0.9556\u001b[0m        \u001b[32m0.4327\u001b[0m       0.3438            0.3438        1.8154  0.0006  0.1604\n",
      "     44            0.9556        0.4467       0.3438            0.3438        1.8061  0.0006  0.1496\n",
      "     45            0.9556        \u001b[32m0.3821\u001b[0m       0.3438            0.3438        1.7912  0.0005  0.1490\n",
      "     46            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3293\u001b[0m       0.3229            0.3229        1.7768  0.0005  0.1539\n",
      "     47            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2706\u001b[0m       0.3125            0.3125        1.7628  0.0004  0.1548\n",
      "     48            1.0000        \u001b[32m0.2705\u001b[0m       0.3125            0.3125        1.7516  0.0004  0.1496\n",
      "     49            1.0000        \u001b[32m0.2239\u001b[0m       0.3125            0.3125        1.7418  0.0003  0.1536\n",
      "     50            1.0000        0.3016       0.3333            0.3333        1.7334  0.0003  0.1539\n",
      "Fine tuning model for subject 5 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3111        1.7228       0.3125            0.3125        2.0994  0.0004  0.1493\n",
      "     32            0.4444        1.6527       0.2812            0.2812        1.9439  0.0005  0.1493\n",
      "     33            0.4667        1.2694       0.3021            0.3021        1.8449  0.0005  0.1515\n",
      "     34            0.5333        1.1542       0.2708            0.2708        1.8879  0.0006  0.1485\n",
      "     35            0.6222        0.9790       0.2500            0.2500        2.0235  0.0006  0.1581\n",
      "     36            0.6667        0.7349       0.2604            0.2604        2.1884  0.0007  0.1549\n",
      "     37            0.7111        0.7672       0.2396            0.2396        2.3851  0.0007  0.1563\n",
      "     38            0.6667        \u001b[32m0.6251\u001b[0m       0.2500            0.2500        2.5529  0.0007  0.1503\n",
      "     39            0.6889        \u001b[32m0.5141\u001b[0m       0.2500            0.2500        2.6800  0.0007  0.1538\n",
      "     40            0.6667        \u001b[32m0.4810\u001b[0m       0.2396            0.2396        2.7759  0.0007  0.1552\n",
      "     41            0.6667        \u001b[32m0.4267\u001b[0m       0.2292            0.2292        2.8048  0.0007  0.1451\n",
      "     42            0.6667        \u001b[32m0.4163\u001b[0m       0.2292            0.2292        2.7821  0.0007  0.1508\n",
      "     43            0.6889        \u001b[32m0.3445\u001b[0m       0.2292            0.2292        2.7548  0.0006  0.1658\n",
      "     44            0.7556        \u001b[32m0.2895\u001b[0m       0.2396            0.2396        2.7020  0.0006  0.1511\n",
      "     45            0.7778        0.3351       0.2396            0.2396        2.6458  0.0005  0.1510\n",
      "     46            0.8000        \u001b[32m0.2641\u001b[0m       0.2396            0.2396        2.5902  0.0005  0.1485\n",
      "     47            \u001b[36m0.8444\u001b[0m        \u001b[32m0.2620\u001b[0m       0.2396            0.2396        2.5424  0.0004  0.1502\n",
      "     48            0.8444        \u001b[32m0.2568\u001b[0m       0.2604            0.2604        2.4870  0.0004  0.1600\n",
      "     49            \u001b[36m0.9111\u001b[0m        \u001b[32m0.1959\u001b[0m       0.2500            0.2500        2.4291  0.0003  0.1506\n",
      "     50            \u001b[36m0.9778\u001b[0m        0.1978       0.2708            0.2708        2.3787  0.0003  0.1454\n",
      "Hold out data from subject 6\n",
      "Pre-training model with data from all subjects but subject 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.4073\u001b[0m        \u001b[32m1.5551\u001b[0m       \u001b[35m0.3581\u001b[0m            \u001b[31m0.3581\u001b[0m        \u001b[94m1.4000\u001b[0m  0.0007  8.0303\n",
      "      2            \u001b[36m0.5010\u001b[0m        \u001b[32m1.3978\u001b[0m       \u001b[35m0.4089\u001b[0m            \u001b[31m0.4089\u001b[0m        \u001b[94m1.2918\u001b[0m  0.0007  7.3487\n",
      "      3            \u001b[36m0.5255\u001b[0m        \u001b[32m1.2911\u001b[0m       \u001b[35m0.4375\u001b[0m            \u001b[31m0.4375\u001b[0m        \u001b[94m1.2555\u001b[0m  0.0007  7.3126\n",
      "      4            \u001b[36m0.5646\u001b[0m        \u001b[32m1.2157\u001b[0m       \u001b[35m0.4674\u001b[0m            \u001b[31m0.4674\u001b[0m        \u001b[94m1.2349\u001b[0m  0.0007  8.2539\n",
      "      5            \u001b[36m0.6096\u001b[0m        \u001b[32m1.1661\u001b[0m       0.4609            0.4609        \u001b[94m1.1968\u001b[0m  0.0007  7.3471\n",
      "      6            \u001b[36m0.6172\u001b[0m        \u001b[32m1.1190\u001b[0m       \u001b[35m0.4961\u001b[0m            \u001b[31m0.4961\u001b[0m        \u001b[94m1.1861\u001b[0m  0.0006  8.5841\n",
      "      7            0.6167        \u001b[32m1.0624\u001b[0m       \u001b[35m0.5078\u001b[0m            \u001b[31m0.5078\u001b[0m        1.2076  0.0006  9.9796\n",
      "      8            \u001b[36m0.6576\u001b[0m        \u001b[32m1.0260\u001b[0m       0.4844            0.4844        \u001b[94m1.1840\u001b[0m  0.0006  8.3738\n",
      "      9            0.6417        \u001b[32m0.9997\u001b[0m       0.4935            0.4935        \u001b[94m1.1614\u001b[0m  0.0006  7.2799\n",
      "     10            \u001b[36m0.6807\u001b[0m        \u001b[32m0.9642\u001b[0m       \u001b[35m0.5091\u001b[0m            \u001b[31m0.5091\u001b[0m        \u001b[94m1.1582\u001b[0m  0.0005  8.1527\n",
      "     11            0.6734        \u001b[32m0.9341\u001b[0m       \u001b[35m0.5195\u001b[0m            \u001b[31m0.5195\u001b[0m        1.1634  0.0005  7.8975\n",
      "     12            \u001b[36m0.7083\u001b[0m        \u001b[32m0.9172\u001b[0m       0.4948            0.4948        1.1632  0.0005  7.4614\n",
      "     13            \u001b[36m0.7206\u001b[0m        \u001b[32m0.8978\u001b[0m       \u001b[35m0.5221\u001b[0m            \u001b[31m0.5221\u001b[0m        \u001b[94m1.1104\u001b[0m  0.0004  8.1309\n",
      "     14            \u001b[36m0.7281\u001b[0m        \u001b[32m0.8662\u001b[0m       \u001b[35m0.5286\u001b[0m            \u001b[31m0.5286\u001b[0m        1.1254  0.0004  7.8308\n",
      "     15            \u001b[36m0.7583\u001b[0m        \u001b[32m0.8545\u001b[0m       0.5273            0.5273        \u001b[94m1.0992\u001b[0m  0.0004  7.7407\n",
      "     16            0.7357        \u001b[32m0.8153\u001b[0m       0.5130            0.5130        1.1385  0.0003  8.3348\n",
      "     17            \u001b[36m0.7669\u001b[0m        \u001b[32m0.8116\u001b[0m       \u001b[35m0.5391\u001b[0m            \u001b[31m0.5391\u001b[0m        1.1140  0.0003  8.0039\n",
      "     18            \u001b[36m0.7680\u001b[0m        \u001b[32m0.7780\u001b[0m       \u001b[35m0.5456\u001b[0m            \u001b[31m0.5456\u001b[0m        \u001b[94m1.0929\u001b[0m  0.0003  7.5803\n",
      "     19            \u001b[36m0.7714\u001b[0m        \u001b[32m0.7698\u001b[0m       0.5339            0.5339        1.1264  0.0002  8.4397\n",
      "     20            0.7677        \u001b[32m0.7343\u001b[0m       0.5417            0.5417        1.0997  0.0002  8.0094\n",
      "     21            \u001b[36m0.7865\u001b[0m        0.7376       \u001b[35m0.5534\u001b[0m            \u001b[31m0.5534\u001b[0m        1.0996  0.0002  7.5737\n",
      "     22            \u001b[36m0.7888\u001b[0m        \u001b[32m0.7185\u001b[0m       0.5469            0.5469        \u001b[94m1.0785\u001b[0m  0.0001  8.2170\n",
      "     23            \u001b[36m0.8013\u001b[0m        0.7226       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        \u001b[94m1.0739\u001b[0m  0.0001  8.2524\n",
      "     24            0.7982        \u001b[32m0.7015\u001b[0m       0.5599            0.5599        1.0824  0.0001  7.5105\n",
      "     25            \u001b[36m0.8117\u001b[0m        \u001b[32m0.6953\u001b[0m       0.5495            0.5495        \u001b[94m1.0688\u001b[0m  0.0001  8.2089\n",
      "     26            0.8086        \u001b[32m0.6865\u001b[0m       0.5625            0.5625        1.0761  0.0000  8.5475\n",
      "     27            \u001b[36m0.8154\u001b[0m        0.6984       \u001b[35m0.5677\u001b[0m            \u001b[31m0.5677\u001b[0m        1.0698  0.0000  7.5884\n",
      "     28            0.8151        \u001b[32m0.6786\u001b[0m       0.5664            0.5664        1.0702  0.0000  7.8402\n",
      "     29            \u001b[36m0.8156\u001b[0m        0.6787       0.5664            0.5664        1.0704  0.0000  8.2751\n",
      "     30            0.8141        \u001b[32m0.6711\u001b[0m       0.5638            0.5638        1.0692  0.0000  7.8073\n",
      "Before finetuning for subject 6, the baseline accuracy is 0.34375\n",
      "Fine tuning model for subject 6 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.0000        2.5530       0.3333            0.3333        1.6702  0.0004  0.1707\n",
      "     32            0.6000        1.7410       0.3542            0.3542        1.6061  0.0005  0.1545\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4984\u001b[0m       0.3438            0.3438        1.5616  0.0005  0.1538\n",
      "     34            1.0000        \u001b[32m0.1447\u001b[0m       0.3438            0.3438        1.5611  0.0006  0.1305\n",
      "     35            1.0000        \u001b[32m0.0957\u001b[0m       0.3438            0.3438        1.6035  0.0006  0.1410\n",
      "     36            1.0000        \u001b[32m0.0168\u001b[0m       0.3646            0.3646        1.6910  0.0007  0.1297\n",
      "     37            1.0000        \u001b[32m0.0141\u001b[0m       0.3229            0.3229        1.8252  0.0007  0.1242\n",
      "     38            1.0000        \u001b[32m0.0080\u001b[0m       0.3229            0.3229        1.9935  0.0007  0.1259\n",
      "     39            1.0000        0.0089       0.2917            0.2917        2.1714  0.0007  0.1263\n",
      "     40            1.0000        \u001b[32m0.0047\u001b[0m       0.2708            0.2708        2.3334  0.0007  0.1341\n",
      "     41            1.0000        0.0098       0.2708            0.2708        2.4607  0.0007  0.1341\n",
      "     42            1.0000        \u001b[32m0.0039\u001b[0m       0.2708            0.2708        2.5449  0.0007  0.1313\n",
      "     43            1.0000        \u001b[32m0.0018\u001b[0m       0.2604            0.2604        2.5857  0.0006  0.1303\n",
      "     44            1.0000        0.0032       0.2812            0.2812        2.5883  0.0006  0.1283\n",
      "     45            1.0000        \u001b[32m0.0017\u001b[0m       0.2708            0.2708        2.5618  0.0005  0.1322\n",
      "     46            1.0000        0.0020       0.2917            0.2917        2.5162  0.0005  0.1300\n",
      "     47            1.0000        \u001b[32m0.0015\u001b[0m       0.3333            0.3333        2.4616  0.0004  0.1342\n",
      "     48            1.0000        \u001b[32m0.0008\u001b[0m       0.3542            0.3542        2.4062  0.0004  0.1378\n",
      "     49            1.0000        0.0016       0.3438            0.3438        2.3569  0.0003  0.1357\n",
      "     50            1.0000        0.0014       0.3333            0.3333        2.3179  0.0003  0.1248\n",
      "Fine tuning model for subject 6 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        2.0094       0.3438            0.3438        1.7072  0.0004  0.1361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        0.8547       0.3646            0.3646        1.6997  0.0005  0.1327\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2710\u001b[0m       0.3438            0.3438        1.7025  0.0005  0.1328\n",
      "     34            1.0000        \u001b[32m0.0715\u001b[0m       0.3542            0.3542        1.7072  0.0006  0.1279\n",
      "     35            1.0000        \u001b[32m0.0080\u001b[0m       0.3333            0.3333        1.7045  0.0006  0.1368\n",
      "     36            1.0000        0.0267       0.3229            0.3229        1.6902  0.0007  0.1292\n",
      "     37            1.0000        \u001b[32m0.0057\u001b[0m       0.3021            0.3021        1.6689  0.0007  0.1303\n",
      "     38            1.0000        0.0070       0.2917            0.2917        1.6472  0.0007  0.1243\n",
      "     39            1.0000        \u001b[32m0.0011\u001b[0m       0.3021            0.3021        1.6317  0.0007  0.1701\n",
      "     40            1.0000        0.0020       0.3021            0.3021        1.6254  0.0007  0.1328\n",
      "     41            1.0000        0.0075       0.3333            0.3333        1.6285  0.0007  0.1327\n",
      "     42            1.0000        0.0033       0.3333            0.3333        1.6382  0.0007  0.1321\n",
      "     43            1.0000        0.0043       0.3542            0.3542        1.6514  0.0006  0.1503\n",
      "     44            1.0000        0.0057       0.3854            0.3854        1.6656  0.0006  0.1454\n",
      "     45            1.0000        0.0048       0.3542            0.3542        1.6789  0.0005  0.1389\n",
      "     46            1.0000        0.0038       0.3542            0.3542        1.6900  0.0005  0.1339\n",
      "     47            1.0000        0.0018       0.3438            0.3438        1.6987  0.0004  0.1333\n",
      "     48            1.0000        \u001b[32m0.0011\u001b[0m       0.3646            0.3646        1.7049  0.0004  0.1321\n",
      "     49            1.0000        0.0082       0.3646            0.3646        1.7088  0.0003  0.1304\n",
      "     50            1.0000        0.0023       0.3542            0.3542        1.7110  0.0003  0.1280\n",
      "Fine tuning model for subject 6 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        2.2951       0.3646            0.3646        1.7737  0.0004  0.1329\n",
      "     32            0.8000        2.1775       0.3542            0.3542        1.8785  0.0005  0.1357\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5198\u001b[0m       0.3542            0.3542        1.9915  0.0005  0.1400\n",
      "     34            1.0000        \u001b[32m0.2208\u001b[0m       0.3125            0.3125        2.0686  0.0006  0.1393\n",
      "     35            1.0000        \u001b[32m0.0317\u001b[0m       0.3229            0.3229        2.1110  0.0006  0.1361\n",
      "     36            1.0000        \u001b[32m0.0302\u001b[0m       0.3229            0.3229        2.1370  0.0007  0.1313\n",
      "     37            1.0000        \u001b[32m0.0052\u001b[0m       0.2604            0.2604        2.1658  0.0007  0.1434\n",
      "     38            1.0000        \u001b[32m0.0043\u001b[0m       0.2604            0.2604        2.2078  0.0007  0.1291\n",
      "     39            1.0000        \u001b[32m0.0034\u001b[0m       0.2708            0.2708        2.2637  0.0007  0.1375\n",
      "     40            1.0000        0.0038       0.2604            0.2604        2.3282  0.0007  0.1265\n",
      "     41            1.0000        0.0066       0.2604            0.2604        2.3950  0.0007  0.1325\n",
      "     42            1.0000        0.0163       0.2396            0.2396        2.4598  0.0007  0.1352\n",
      "     43            1.0000        0.0121       0.2500            0.2500        2.5177  0.0006  0.1287\n",
      "     44            1.0000        0.0046       0.2500            0.2500        2.5666  0.0006  0.1281\n",
      "     45            1.0000        0.0133       0.2500            0.2500        2.6064  0.0005  0.1291\n",
      "     46            1.0000        0.0132       0.2500            0.2500        2.6377  0.0005  0.1281\n",
      "     47            1.0000        0.0051       0.2604            0.2604        2.6616  0.0004  0.1332\n",
      "     48            1.0000        0.0095       0.2604            0.2604        2.6800  0.0004  0.1307\n",
      "     49            1.0000        \u001b[32m0.0027\u001b[0m       0.2708            0.2708        2.6933  0.0003  0.1305\n",
      "     50            1.0000        0.0036       0.2604            0.2604        2.7025  0.0003  0.1283\n",
      "Fine tuning model for subject 6 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.1929       0.3542            0.3542        1.7023  0.0004  0.1366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        0.9104       0.3646            0.3646        1.6688  0.0005  0.1316\n",
      "     33            1.0000        \u001b[32m0.3590\u001b[0m       0.3958            0.3958        1.6378  0.0005  0.1247\n",
      "     34            1.0000        \u001b[32m0.2541\u001b[0m       0.4375            0.4375        1.6152  0.0006  0.1294\n",
      "     35            1.0000        \u001b[32m0.0580\u001b[0m       0.4167            0.4167        1.6107  0.0006  0.1341\n",
      "     36            1.0000        \u001b[32m0.0544\u001b[0m       0.3958            0.3958        1.6183  0.0007  0.1316\n",
      "     37            1.0000        \u001b[32m0.0321\u001b[0m       0.3854            0.3854        1.6269  0.0007  0.1325\n",
      "     38            1.0000        \u001b[32m0.0149\u001b[0m       0.3750            0.3750        1.6312  0.0007  0.1327\n",
      "     39            1.0000        0.0216       0.3542            0.3542        1.6280  0.0007  0.1389\n",
      "     40            1.0000        \u001b[32m0.0043\u001b[0m       0.3542            0.3542        1.6175  0.0007  0.1357\n",
      "     41            1.0000        0.0078       0.3750            0.3750        1.6016  0.0007  0.1447\n",
      "     42            1.0000        0.0060       0.3646            0.3646        1.5831  0.0007  0.1405\n",
      "     43            1.0000        0.0062       0.3750            0.3750        1.5647  0.0006  0.1312\n",
      "     44            1.0000        0.0051       0.3542            0.3542        1.5485  0.0006  0.1277\n",
      "     45            1.0000        \u001b[32m0.0022\u001b[0m       0.3542            0.3542        1.5357  0.0005  0.1426\n",
      "     46            1.0000        0.0029       0.3542            0.3542        1.5266  0.0005  0.1311\n",
      "     47            1.0000        0.0062       0.3750            0.3750        1.5212  0.0004  0.1395\n",
      "     48            1.0000        0.0035       0.3854            0.3854        1.5193  0.0004  0.1308\n",
      "     49            1.0000        \u001b[32m0.0016\u001b[0m       0.3750            0.3750        1.5201  0.0003  0.1330\n",
      "     50            1.0000        0.0030       0.3646            0.3646        1.5232  0.0003  0.1322\n",
      "Fine tuning model for subject 6 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.2622       0.3438            0.3438        1.7401  0.0004  0.1325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.0679       0.3333            0.3333        1.7787  0.0005  0.1335\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3642\u001b[0m       0.3438            0.3438        1.8474  0.0005  0.1309\n",
      "     34            1.0000        \u001b[32m0.1253\u001b[0m       0.2917            0.2917        1.9109  0.0006  0.1294\n",
      "     35            1.0000        \u001b[32m0.0354\u001b[0m       0.2812            0.2812        1.9626  0.0006  0.1310\n",
      "     36            1.0000        \u001b[32m0.0171\u001b[0m       0.2917            0.2917        1.9932  0.0007  0.1306\n",
      "     37            1.0000        0.0227       0.3229            0.3229        2.0025  0.0007  0.1281\n",
      "     38            1.0000        0.0629       0.2812            0.2812        1.9927  0.0007  0.1686\n",
      "     39            1.0000        \u001b[32m0.0083\u001b[0m       0.3021            0.3021        1.9699  0.0007  0.1658\n",
      "     40            1.0000        0.0097       0.2708            0.2708        1.9399  0.0007  0.1626\n",
      "     41            1.0000        0.0145       0.2604            0.2604        1.9063  0.0007  0.1562\n",
      "     42            1.0000        0.0182       0.2604            0.2604        1.8708  0.0007  0.1707\n",
      "     43            1.0000        \u001b[32m0.0066\u001b[0m       0.2708            0.2708        1.8376  0.0006  0.1641\n",
      "     44            1.0000        0.0075       0.2812            0.2812        1.8073  0.0006  0.1692\n",
      "     45            1.0000        \u001b[32m0.0032\u001b[0m       0.2812            0.2812        1.7810  0.0005  0.1628\n",
      "     46            1.0000        0.0103       0.3021            0.3021        1.7589  0.0005  0.1623\n",
      "     47            1.0000        0.0055       0.3021            0.3021        1.7409  0.0004  0.1747\n",
      "     48            1.0000        0.0068       0.3021            0.3021        1.7272  0.0004  0.1668\n",
      "     49            1.0000        0.0039       0.3333            0.3333        1.7173  0.0003  0.1680\n",
      "     50            1.0000        0.0056       0.3438            0.3438        1.7111  0.0003  0.1718\n",
      "Fine tuning model for subject 6 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.8000        1.0368       0.3438            0.3438        1.7219  0.0004  0.1879\n",
      "     32            \u001b[36m1.0000\u001b[0m        0.7022       0.3542            0.3542        1.7348  0.0005  0.1831\n",
      "     33            1.0000        \u001b[32m0.2921\u001b[0m       0.3125            0.3125        1.7626  0.0005  0.1492\n",
      "     34            1.0000        \u001b[32m0.0745\u001b[0m       0.3333            0.3333        1.8004  0.0006  0.1387\n",
      "     35            1.0000        \u001b[32m0.0729\u001b[0m       0.3125            0.3125        1.8517  0.0006  0.1332\n",
      "     36            1.0000        \u001b[32m0.0126\u001b[0m       0.3125            0.3125        1.9020  0.0007  0.1542\n",
      "     37            1.0000        0.0484       0.2812            0.2812        1.9472  0.0007  0.1314\n",
      "     38            1.0000        \u001b[32m0.0116\u001b[0m       0.2708            0.2708        1.9798  0.0007  0.1225\n",
      "     39            1.0000        \u001b[32m0.0059\u001b[0m       0.2812            0.2812        2.0005  0.0007  0.1338\n",
      "     40            1.0000        \u001b[32m0.0053\u001b[0m       0.2917            0.2917        2.0136  0.0007  0.1291\n",
      "     41            1.0000        0.0111       0.3125            0.3125        2.0244  0.0007  0.1323\n",
      "     42            1.0000        0.0077       0.2812            0.2812        2.0353  0.0007  0.1300\n",
      "     43            1.0000        \u001b[32m0.0032\u001b[0m       0.3021            0.3021        2.0464  0.0006  0.1335\n",
      "     44            1.0000        \u001b[32m0.0030\u001b[0m       0.3021            0.3021        2.0580  0.0006  0.1291\n",
      "     45            1.0000        \u001b[32m0.0020\u001b[0m       0.3125            0.3125        2.0697  0.0005  0.1291\n",
      "     46            1.0000        0.0066       0.3125            0.3125        2.0823  0.0005  0.1355\n",
      "     47            1.0000        \u001b[32m0.0016\u001b[0m       0.3125            0.3125        2.0940  0.0004  0.1351\n",
      "     48            1.0000        0.0042       0.3125            0.3125        2.1047  0.0004  0.1252\n",
      "     49            1.0000        0.0025       0.3229            0.3229        2.1143  0.0003  0.1303\n",
      "     50            1.0000        0.0017       0.3333            0.3333        2.1226  0.0003  0.1262\n",
      "Fine tuning model for subject 6 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.8000        1.9628       0.3750            0.3750        1.7281  0.0004  0.1294\n",
      "     32            \u001b[36m1.0000\u001b[0m        0.6915       0.3542            0.3542        1.7757  0.0005  0.1431\n",
      "     33            1.0000        \u001b[32m0.4966\u001b[0m       0.3333            0.3333        1.8176  0.0005  0.1290\n",
      "     34            1.0000        \u001b[32m0.0546\u001b[0m       0.3229            0.3229        1.8646  0.0006  0.1303\n",
      "     35            1.0000        \u001b[32m0.0332\u001b[0m       0.2812            0.2812        1.9239  0.0006  0.1274\n",
      "     36            1.0000        0.0466       0.2708            0.2708        1.9934  0.0007  0.1347\n",
      "     37            1.0000        \u001b[32m0.0273\u001b[0m       0.2396            0.2396        2.0629  0.0007  0.1291\n",
      "     38            1.0000        \u001b[32m0.0098\u001b[0m       0.2604            0.2604        2.1247  0.0007  0.1321\n",
      "     39            1.0000        \u001b[32m0.0087\u001b[0m       0.2292            0.2292        2.1739  0.0007  0.1317\n",
      "     40            1.0000        0.0126       0.2396            0.2396        2.2081  0.0007  0.1322\n",
      "     41            1.0000        0.0139       0.2500            0.2500        2.2292  0.0007  0.1384\n",
      "     42            1.0000        \u001b[32m0.0080\u001b[0m       0.2500            0.2500        2.2418  0.0007  0.1265\n",
      "     43            1.0000        \u001b[32m0.0026\u001b[0m       0.2500            0.2500        2.2482  0.0006  0.1287\n",
      "     44            1.0000        0.0048       0.2604            0.2604        2.2500  0.0006  0.1281\n",
      "     45            1.0000        \u001b[32m0.0021\u001b[0m       0.2604            0.2604        2.2494  0.0005  0.1486\n",
      "     46            1.0000        0.0026       0.2708            0.2708        2.2472  0.0005  0.1322\n",
      "     47            1.0000        0.0052       0.2812            0.2812        2.2441  0.0004  0.1364\n",
      "     48            1.0000        0.0040       0.2917            0.2917        2.2407  0.0004  0.1318\n",
      "     49            1.0000        0.0090       0.3021            0.3021        2.2369  0.0003  0.1350\n",
      "     50            1.0000        0.0024       0.3125            0.3125        2.2337  0.0003  0.1316\n",
      "Fine tuning model for subject 6 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.3673       0.3646            0.3646        1.7142  0.0004  0.1349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        1.0203       0.3646            0.3646        1.7064  0.0005  0.1278\n",
      "     33            1.0000        \u001b[32m0.5059\u001b[0m       0.3750            0.3750        1.6959  0.0005  0.1253\n",
      "     34            1.0000        \u001b[32m0.2329\u001b[0m       0.3854            0.3854        1.6865  0.0006  0.1341\n",
      "     35            1.0000        \u001b[32m0.0367\u001b[0m       0.3646            0.3646        1.6883  0.0006  0.1269\n",
      "     36            1.0000        \u001b[32m0.0230\u001b[0m       0.3333            0.3333        1.7052  0.0007  0.1331\n",
      "     37            1.0000        0.0509       0.3229            0.3229        1.7389  0.0007  0.1308\n",
      "     38            1.0000        \u001b[32m0.0080\u001b[0m       0.2917            0.2917        1.7902  0.0007  0.1259\n",
      "     39            1.0000        0.0126       0.2917            0.2917        1.8556  0.0007  0.1514\n",
      "     40            1.0000        \u001b[32m0.0070\u001b[0m       0.3021            0.3021        1.9302  0.0007  0.1364\n",
      "     41            1.0000        0.0107       0.3021            0.3021        2.0073  0.0007  0.1314\n",
      "     42            1.0000        0.0101       0.3021            0.3021        2.0802  0.0007  0.1279\n",
      "     43            1.0000        \u001b[32m0.0034\u001b[0m       0.3021            0.3021        2.1447  0.0006  0.1405\n",
      "     44            1.0000        \u001b[32m0.0029\u001b[0m       0.2917            0.2917        2.1980  0.0006  0.1352\n",
      "     45            1.0000        0.0033       0.2812            0.2812        2.2389  0.0005  0.1370\n",
      "     46            1.0000        0.0039       0.2708            0.2708        2.2679  0.0005  0.1357\n",
      "     47            1.0000        0.0062       0.2604            0.2604        2.2858  0.0004  0.1326\n",
      "     48            1.0000        0.0030       0.2708            0.2708        2.2948  0.0004  0.1340\n",
      "     49            1.0000        0.0090       0.2708            0.2708        2.2967  0.0003  0.1323\n",
      "     50            1.0000        \u001b[32m0.0015\u001b[0m       0.2812            0.2812        2.2933  0.0003  0.1362\n",
      "Fine tuning model for subject 6 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m1.0000\u001b[0m        1.2463       0.3438            0.3438        1.6929  0.0004  0.1331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            1.0000        0.7909       0.3438            0.3438        1.6517  0.0005  0.1377\n",
      "     33            1.0000        \u001b[32m0.1569\u001b[0m       0.3542            0.3542        1.6322  0.0005  0.1303\n",
      "     34            1.0000        \u001b[32m0.1471\u001b[0m       0.3542            0.3542        1.6363  0.0006  0.1353\n",
      "     35            1.0000        \u001b[32m0.0672\u001b[0m       0.3958            0.3958        1.6625  0.0006  0.1279\n",
      "     36            1.0000        \u001b[32m0.0185\u001b[0m       0.3542            0.3542        1.7091  0.0007  0.1325\n",
      "     37            1.0000        0.0320       0.3854            0.3854        1.7746  0.0007  0.1331\n",
      "     38            1.0000        0.0269       0.3958            0.3958        1.8563  0.0007  0.1308\n",
      "     39            1.0000        0.0384       0.3854            0.3854        1.9429  0.0007  0.1313\n",
      "     40            1.0000        \u001b[32m0.0132\u001b[0m       0.3958            0.3958        2.0302  0.0007  0.1359\n",
      "     41            1.0000        \u001b[32m0.0068\u001b[0m       0.3750            0.3750        2.1107  0.0007  0.1271\n",
      "     42            1.0000        0.0175       0.3646            0.3646        2.1779  0.0007  0.1288\n",
      "     43            1.0000        0.0069       0.3646            0.3646        2.2296  0.0006  0.1303\n",
      "     44            1.0000        \u001b[32m0.0030\u001b[0m       0.3646            0.3646        2.2653  0.0006  0.1322\n",
      "     45            1.0000        \u001b[32m0.0019\u001b[0m       0.3542            0.3542        2.2858  0.0005  0.1351\n",
      "     46            1.0000        \u001b[32m0.0013\u001b[0m       0.3542            0.3542        2.2935  0.0005  0.1262\n",
      "     47            1.0000        0.0016       0.3542            0.3542        2.2910  0.0004  0.1341\n",
      "     48            1.0000        0.0016       0.3438            0.3438        2.2807  0.0004  0.1280\n",
      "     49            1.0000        \u001b[32m0.0010\u001b[0m       0.3438            0.3438        2.2655  0.0003  0.1365\n",
      "     50            1.0000        0.0011       0.3438            0.3438        2.2475  0.0003  0.1273\n",
      "Fine tuning model for subject 6 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.0626       0.3438            0.3438        1.6887  0.0004  0.1342\n",
      "     32            0.8000        \u001b[32m0.5778\u001b[0m       0.3542            0.3542        1.6431  0.0005  0.1280\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2735\u001b[0m       0.3646            0.3646        1.6018  0.0005  0.1322\n",
      "     34            1.0000        \u001b[32m0.1413\u001b[0m       0.3542            0.3542        1.5794  0.0006  0.1347\n",
      "     35            1.0000        \u001b[32m0.0464\u001b[0m       0.3958            0.3958        1.5774  0.0006  0.1307\n",
      "     36            1.0000        \u001b[32m0.0105\u001b[0m       0.4271            0.4271        1.5908  0.0007  0.1263\n",
      "     37            1.0000        \u001b[32m0.0040\u001b[0m       0.4167            0.4167        1.6153  0.0007  0.1306\n",
      "     38            1.0000        \u001b[32m0.0030\u001b[0m       0.4375            0.4375        1.6488  0.0007  0.1333\n",
      "     39            1.0000        \u001b[32m0.0024\u001b[0m       0.4271            0.4271        1.6902  0.0007  0.1276\n",
      "     40            1.0000        \u001b[32m0.0009\u001b[0m       0.4271            0.4271        1.7373  0.0007  0.1315\n",
      "     41            1.0000        0.0016       0.3958            0.3958        1.7869  0.0007  0.1289\n",
      "     42            1.0000        \u001b[32m0.0008\u001b[0m       0.3958            0.3958        1.8349  0.0007  0.1283\n",
      "     43            1.0000        0.0016       0.3854            0.3854        1.8775  0.0006  0.1646\n",
      "     44            1.0000        \u001b[32m0.0007\u001b[0m       0.3750            0.3750        1.9120  0.0006  0.1707\n",
      "     45            1.0000        0.0008       0.3750            0.3750        1.9371  0.0005  0.1708\n",
      "     46            1.0000        0.0009       0.3750            0.3750        1.9531  0.0005  0.1704\n",
      "     47            1.0000        \u001b[32m0.0007\u001b[0m       0.3646            0.3646        1.9606  0.0004  0.1683\n",
      "     48            1.0000        0.0010       0.3646            0.3646        1.9614  0.0004  0.1621\n",
      "     49            1.0000        0.0011       0.3854            0.3854        1.9572  0.0003  0.1662\n",
      "     50            1.0000        \u001b[32m0.0007\u001b[0m       0.3854            0.3854        1.9499  0.0003  0.1583\n",
      "Fine tuning model for subject 6 with 10 = 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        1.5430       0.3438            0.3438        1.7092  0.0004  0.1453\n",
      "     32            0.4000        1.4108       0.3542            0.3542        1.6930  0.0005  0.1477\n",
      "     33            \u001b[36m0.9000\u001b[0m        1.0905       0.3333            0.3333        1.6756  0.0005  0.1334\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4580\u001b[0m       0.3750            0.3750        1.6955  0.0006  0.1498\n",
      "     35            1.0000        \u001b[32m0.1714\u001b[0m       0.3646            0.3646        1.7348  0.0006  0.1532\n",
      "     36            1.0000        0.1948       0.3438            0.3438        1.7925  0.0007  0.1460\n",
      "     37            1.0000        \u001b[32m0.1338\u001b[0m       0.3438            0.3438        1.8854  0.0007  0.1841\n",
      "     38            1.0000        \u001b[32m0.0381\u001b[0m       0.3438            0.3438        2.0102  0.0007  0.1213\n",
      "     39            0.9000        \u001b[32m0.0248\u001b[0m       0.3229            0.3229        2.1554  0.0007  0.1284\n",
      "     40            0.9000        \u001b[32m0.0248\u001b[0m       0.3229            0.3229        2.2955  0.0007  0.1087\n",
      "     41            0.9000        0.0397       0.3125            0.3125        2.4133  0.0007  0.1052\n",
      "     42            0.9000        0.0314       0.3125            0.3125        2.4973  0.0007  0.1096\n",
      "     43            0.9000        0.0334       0.3125            0.3125        2.5434  0.0006  0.1091\n",
      "     44            0.9000        0.0305       0.3229            0.3229        2.5516  0.0006  0.1115\n",
      "     45            1.0000        \u001b[32m0.0177\u001b[0m       0.3438            0.3438        2.5304  0.0005  0.1409\n",
      "     46            1.0000        \u001b[32m0.0123\u001b[0m       0.3333            0.3333        2.4895  0.0005  0.1228\n",
      "     47            1.0000        \u001b[32m0.0106\u001b[0m       0.3333            0.3333        2.4370  0.0004  0.1130\n",
      "     48            1.0000        0.0132       0.3438            0.3438        2.3801  0.0004  0.1069\n",
      "     49            1.0000        0.0228       0.3438            0.3438        2.3224  0.0003  0.1112\n",
      "     50            1.0000        0.0193       0.3542            0.3542        2.2698  0.0003  0.1078\n",
      "Fine tuning model for subject 6 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        1.6554       0.3750            0.3750        1.6999  0.0004  0.1172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.4251       0.3750            0.3750        1.6663  0.0005  0.1127\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.7967       0.3854            0.3854        1.6245  0.0005  0.1080\n",
      "     34            1.0000        \u001b[32m0.3199\u001b[0m       0.3958            0.3958        1.6086  0.0006  0.1171\n",
      "     35            1.0000        \u001b[32m0.1525\u001b[0m       0.4167            0.4167        1.6167  0.0006  0.1081\n",
      "     36            1.0000        0.1959       0.4062            0.4062        1.6486  0.0007  0.1076\n",
      "     37            1.0000        \u001b[32m0.0761\u001b[0m       0.3854            0.3854        1.6911  0.0007  0.1153\n",
      "     38            1.0000        \u001b[32m0.0358\u001b[0m       0.3646            0.3646        1.7447  0.0007  0.1113\n",
      "     39            1.0000        \u001b[32m0.0283\u001b[0m       0.3646            0.3646        1.8061  0.0007  0.1133\n",
      "     40            1.0000        0.0493       0.3438            0.3438        1.8672  0.0007  0.1177\n",
      "     41            1.0000        0.0314       0.3333            0.3333        1.9263  0.0007  0.1124\n",
      "     42            1.0000        0.0381       0.3333            0.3333        1.9801  0.0007  0.1192\n",
      "     43            1.0000        \u001b[32m0.0105\u001b[0m       0.3229            0.3229        2.0236  0.0006  0.1158\n",
      "     44            1.0000        0.0441       0.3229            0.3229        2.0544  0.0006  0.1155\n",
      "     45            1.0000        0.0267       0.3229            0.3229        2.0753  0.0005  0.1248\n",
      "     46            1.0000        0.0164       0.3229            0.3229        2.0853  0.0005  0.1390\n",
      "     47            1.0000        0.0829       0.3229            0.3229        2.0804  0.0004  0.1335\n",
      "     48            1.0000        0.0122       0.3229            0.3229        2.0699  0.0004  0.1294\n",
      "     49            1.0000        \u001b[32m0.0075\u001b[0m       0.3229            0.3229        2.0559  0.0003  0.1174\n",
      "     50            1.0000        0.0141       0.3229            0.3229        2.0393  0.0003  0.1136\n",
      "Fine tuning model for subject 6 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        1.8041       0.3542            0.3542        1.6957  0.0004  0.1119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        1.6875       0.3542            0.3542        1.6809  0.0005  0.1490\n",
      "     33            \u001b[36m0.9000\u001b[0m        1.0661       0.3646            0.3646        1.6778  0.0005  0.1412\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2524\u001b[0m       0.3438            0.3438        1.7068  0.0006  0.1155\n",
      "     35            1.0000        \u001b[32m0.2154\u001b[0m       0.3646            0.3646        1.7565  0.0006  0.1119\n",
      "     36            1.0000        \u001b[32m0.0898\u001b[0m       0.3646            0.3646        1.8118  0.0007  0.1250\n",
      "     37            1.0000        \u001b[32m0.0381\u001b[0m       0.3229            0.3229        1.8628  0.0007  0.1213\n",
      "     38            1.0000        \u001b[32m0.0229\u001b[0m       0.2812            0.2812        1.9060  0.0007  0.1095\n",
      "     39            1.0000        0.0381       0.2604            0.2604        1.9421  0.0007  0.1058\n",
      "     40            1.0000        \u001b[32m0.0209\u001b[0m       0.2708            0.2708        1.9708  0.0007  0.1116\n",
      "     41            1.0000        0.0303       0.2812            0.2812        1.9910  0.0007  0.1091\n",
      "     42            1.0000        \u001b[32m0.0166\u001b[0m       0.2812            0.2812        2.0077  0.0007  0.1116\n",
      "     43            1.0000        \u001b[32m0.0153\u001b[0m       0.2812            0.2812        2.0220  0.0006  0.1157\n",
      "     44            1.0000        0.0562       0.2604            0.2604        2.0280  0.0006  0.1105\n",
      "     45            1.0000        0.0248       0.2708            0.2708        2.0333  0.0005  0.1388\n",
      "     46            1.0000        0.0178       0.2708            0.2708        2.0391  0.0005  0.1368\n",
      "     47            1.0000        \u001b[32m0.0145\u001b[0m       0.2604            0.2604        2.0448  0.0004  0.1525\n",
      "     48            1.0000        0.0294       0.2604            0.2604        2.0493  0.0004  0.1523\n",
      "     49            1.0000        0.0217       0.2708            0.2708        2.0526  0.0003  0.1296\n",
      "     50            1.0000        \u001b[32m0.0125\u001b[0m       0.2812            0.2812        2.0556  0.0003  0.1504\n",
      "Fine tuning model for subject 6 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        2.1101       0.3646            0.3646        1.6926  0.0004  0.1054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5000        2.0857       0.3854            0.3854        1.6784  0.0005  0.1146\n",
      "     33            0.6000        1.1314       0.3854            0.3854        1.6715  0.0005  0.1089\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6274\u001b[0m       0.3125            0.3125        1.6897  0.0006  0.1180\n",
      "     35            1.0000        \u001b[32m0.3139\u001b[0m       0.3021            0.3021        1.7079  0.0006  0.1142\n",
      "     36            1.0000        \u001b[32m0.0876\u001b[0m       0.3021            0.3021        1.7129  0.0007  0.1126\n",
      "     37            1.0000        \u001b[32m0.0658\u001b[0m       0.3021            0.3021        1.7037  0.0007  0.1161\n",
      "     38            1.0000        0.0806       0.3229            0.3229        1.6913  0.0007  0.1164\n",
      "     39            1.0000        \u001b[32m0.0445\u001b[0m       0.3750            0.3750        1.6890  0.0007  0.1126\n",
      "     40            1.0000        \u001b[32m0.0168\u001b[0m       0.3542            0.3542        1.7028  0.0007  0.1127\n",
      "     41            1.0000        \u001b[32m0.0168\u001b[0m       0.3750            0.3750        1.7308  0.0007  0.1118\n",
      "     42            1.0000        \u001b[32m0.0132\u001b[0m       0.3646            0.3646        1.7661  0.0007  0.1130\n",
      "     43            1.0000        0.0141       0.3750            0.3750        1.8012  0.0006  0.1156\n",
      "     44            1.0000        0.0149       0.3646            0.3646        1.8317  0.0006  0.1126\n",
      "     45            1.0000        \u001b[32m0.0112\u001b[0m       0.3646            0.3646        1.8556  0.0005  0.1103\n",
      "     46            1.0000        \u001b[32m0.0100\u001b[0m       0.3646            0.3646        1.8729  0.0005  0.1106\n",
      "     47            1.0000        0.0119       0.3750            0.3750        1.8850  0.0004  0.1091\n",
      "     48            1.0000        0.0138       0.3750            0.3750        1.8940  0.0004  0.1122\n",
      "     49            1.0000        0.0117       0.3750            0.3750        1.9013  0.0003  0.1098\n",
      "     50            1.0000        0.0131       0.3750            0.3750        1.9083  0.0003  0.1129\n",
      "Fine tuning model for subject 6 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.1716       0.3542            0.3542        1.6954  0.0004  0.1118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        1.1650       0.3333            0.3333        1.6607  0.0005  0.1171\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.9655       0.3958            0.3958        1.6331  0.0005  0.1543\n",
      "     34            1.0000        \u001b[32m0.3146\u001b[0m       0.3750            0.3750        1.6526  0.0006  0.1077\n",
      "     35            1.0000        \u001b[32m0.2162\u001b[0m       0.3542            0.3542        1.7337  0.0006  0.1110\n",
      "     36            1.0000        \u001b[32m0.0542\u001b[0m       0.3542            0.3542        1.8392  0.0007  0.1125\n",
      "     37            1.0000        0.0720       0.3438            0.3438        1.9468  0.0007  0.1137\n",
      "     38            1.0000        \u001b[32m0.0291\u001b[0m       0.3438            0.3438        2.0390  0.0007  0.1123\n",
      "     39            1.0000        \u001b[32m0.0243\u001b[0m       0.3438            0.3438        2.1088  0.0007  0.1067\n",
      "     40            1.0000        0.0480       0.3333            0.3333        2.1556  0.0007  0.1206\n",
      "     41            1.0000        0.0380       0.3333            0.3333        2.1828  0.0007  0.1150\n",
      "     42            1.0000        0.0340       0.3333            0.3333        2.1946  0.0007  0.1135\n",
      "     43            1.0000        0.0282       0.3333            0.3333        2.1915  0.0006  0.1131\n",
      "     44            1.0000        0.0347       0.3125            0.3125        2.1776  0.0006  0.1132\n",
      "     45            1.0000        \u001b[32m0.0233\u001b[0m       0.3229            0.3229        2.1570  0.0005  0.1211\n",
      "     46            1.0000        \u001b[32m0.0086\u001b[0m       0.3229            0.3229        2.1313  0.0005  0.1104\n",
      "     47            1.0000        \u001b[32m0.0082\u001b[0m       0.3125            0.3125        2.1026  0.0004  0.1063\n",
      "     48            1.0000        0.0143       0.3125            0.3125        2.0721  0.0004  0.1135\n",
      "     49            1.0000        0.0097       0.3021            0.3021        2.0415  0.0003  0.1184\n",
      "     50            1.0000        0.0131       0.3229            0.3229        2.0117  0.0003  0.1147\n",
      "Fine tuning model for subject 6 with 10 = 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.9135       0.3646            0.3646        1.6920  0.0004  0.1069\n",
      "     32            \u001b[36m1.0000\u001b[0m        1.2892       0.3854            0.3854        1.6599  0.0005  0.1168\n",
      "     33            1.0000        0.7387       0.3854            0.3854        1.6216  0.0005  0.1124\n",
      "     34            1.0000        \u001b[32m0.2483\u001b[0m       0.3854            0.3854        1.6008  0.0006  0.1124\n",
      "     35            1.0000        \u001b[32m0.1035\u001b[0m       0.3750            0.3750        1.5998  0.0006  0.1108\n",
      "     36            1.0000        \u001b[32m0.0688\u001b[0m       0.3854            0.3854        1.6176  0.0007  0.1135\n",
      "     37            1.0000        \u001b[32m0.0509\u001b[0m       0.4062            0.4062        1.6568  0.0007  0.1237\n",
      "     38            1.0000        \u001b[32m0.0338\u001b[0m       0.4062            0.4062        1.7102  0.0007  0.1417\n",
      "     39            1.0000        \u001b[32m0.0181\u001b[0m       0.4271            0.4271        1.7721  0.0007  0.1402\n",
      "     40            1.0000        \u001b[32m0.0117\u001b[0m       0.4375            0.4375        1.8357  0.0007  0.1339\n",
      "     41            1.0000        0.0146       0.4271            0.4271        1.8947  0.0007  0.1416\n",
      "     42            1.0000        0.0129       0.4167            0.4167        1.9445  0.0007  0.1490\n",
      "     43            1.0000        0.0153       0.4062            0.4062        1.9840  0.0006  0.1471\n",
      "     44            1.0000        0.0149       0.3854            0.3854        2.0131  0.0006  0.1382\n",
      "     45            1.0000        0.0221       0.3854            0.3854        2.0310  0.0005  0.1398\n",
      "     46            1.0000        \u001b[32m0.0116\u001b[0m       0.3854            0.3854        2.0410  0.0005  0.1618\n",
      "     47            1.0000        \u001b[32m0.0087\u001b[0m       0.3854            0.3854        2.0452  0.0004  0.1419\n",
      "     48            1.0000        0.0122       0.3646            0.3646        2.0448  0.0004  0.1592\n",
      "     49            1.0000        0.0128       0.3438            0.3438        2.0416  0.0003  0.1432\n",
      "     50            1.0000        0.0276       0.3333            0.3333        2.0359  0.0003  0.1427\n",
      "Fine tuning model for subject 6 with 10 = 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.8078       0.3438            0.3438        1.7100  0.0004  0.1447\n",
      "     32            0.4000        1.6096       0.3542            0.3542        1.6832  0.0005  0.1422\n",
      "     33            0.7000        0.9697       0.3438            0.3438        1.6697  0.0005  0.1471\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6105\u001b[0m       0.3750            0.3750        1.6570  0.0006  0.1639\n",
      "     35            1.0000        \u001b[32m0.1657\u001b[0m       0.3854            0.3854        1.6718  0.0006  0.1213\n",
      "     36            1.0000        \u001b[32m0.0720\u001b[0m       0.3958            0.3958        1.7007  0.0007  0.1125\n",
      "     37            1.0000        0.0774       0.3646            0.3646        1.7207  0.0007  0.1158\n",
      "     38            1.0000        0.1000       0.3854            0.3854        1.7259  0.0007  0.1069\n",
      "     39            1.0000        \u001b[32m0.0474\u001b[0m       0.3958            0.3958        1.7255  0.0007  0.1101\n",
      "     40            1.0000        \u001b[32m0.0445\u001b[0m       0.3438            0.3438        1.7257  0.0007  0.1112\n",
      "     41            1.0000        0.0724       0.3333            0.3333        1.7262  0.0007  0.1068\n",
      "     42            1.0000        0.0591       0.3125            0.3125        1.7300  0.0007  0.1149\n",
      "     43            1.0000        0.0539       0.3125            0.3125        1.7356  0.0006  0.1115\n",
      "     44            1.0000        \u001b[32m0.0121\u001b[0m       0.3125            0.3125        1.7451  0.0006  0.1129\n",
      "     45            1.0000        0.0318       0.3125            0.3125        1.7558  0.0005  0.1120\n",
      "     46            1.0000        0.0175       0.3125            0.3125        1.7687  0.0005  0.1130\n",
      "     47            1.0000        0.0252       0.3229            0.3229        1.7824  0.0004  0.1115\n",
      "     48            1.0000        \u001b[32m0.0115\u001b[0m       0.3229            0.3229        1.7970  0.0004  0.1101\n",
      "     49            1.0000        \u001b[32m0.0105\u001b[0m       0.3229            0.3229        1.8119  0.0003  0.1108\n",
      "     50            1.0000        \u001b[32m0.0090\u001b[0m       0.3125            0.3125        1.8269  0.0003  0.1197\n",
      "Fine tuning model for subject 6 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.8096       0.3542            0.3542        1.7049  0.0004  0.1190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.5559       0.3646            0.3646        1.6818  0.0005  0.1145\n",
      "     33            \u001b[36m0.9000\u001b[0m        1.0744       0.3438            0.3438        1.6965  0.0005  0.1115\n",
      "     34            0.9000        \u001b[32m0.5148\u001b[0m       0.2917            0.2917        1.7344  0.0006  0.1105\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3875\u001b[0m       0.2917            0.2917        1.8072  0.0006  0.1107\n",
      "     36            1.0000        \u001b[32m0.0827\u001b[0m       0.2708            0.2708        1.9077  0.0007  0.1160\n",
      "     37            1.0000        \u001b[32m0.0771\u001b[0m       0.2812            0.2812        2.0282  0.0007  0.1107\n",
      "     38            1.0000        \u001b[32m0.0427\u001b[0m       0.2812            0.2812        2.1610  0.0007  0.1060\n",
      "     39            1.0000        \u001b[32m0.0332\u001b[0m       0.2604            0.2604        2.2974  0.0007  0.1113\n",
      "     40            1.0000        0.0483       0.2708            0.2708        2.4225  0.0007  0.1131\n",
      "     41            1.0000        \u001b[32m0.0108\u001b[0m       0.2812            0.2812        2.5302  0.0007  0.1124\n",
      "     42            1.0000        0.0162       0.2812            0.2812        2.6148  0.0007  0.1199\n",
      "     43            1.0000        0.0200       0.2917            0.2917        2.6743  0.0006  0.1127\n",
      "     44            1.0000        0.0164       0.2812            0.2812        2.7073  0.0006  0.1104\n",
      "     45            1.0000        0.0201       0.2917            0.2917        2.7182  0.0005  0.1153\n",
      "     46            1.0000        0.0154       0.2917            0.2917        2.7124  0.0005  0.1115\n",
      "     47            1.0000        0.0273       0.2812            0.2812        2.6925  0.0004  0.1146\n",
      "     48            1.0000        0.0142       0.2812            0.2812        2.6654  0.0004  0.1092\n",
      "     49            1.0000        \u001b[32m0.0078\u001b[0m       0.2708            0.2708        2.6359  0.0003  0.1200\n",
      "     50            1.0000        0.0135       0.2500            0.2500        2.6060  0.0003  0.1231\n",
      "Fine tuning model for subject 6 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.8934       0.3542            0.3542        1.6914  0.0004  0.1169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        1.1889       0.3333            0.3333        1.6521  0.0005  0.1115\n",
      "     33            \u001b[36m0.9000\u001b[0m        0.8856       0.3438            0.3438        1.6213  0.0005  0.1148\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4636\u001b[0m       0.3542            0.3542        1.6029  0.0006  0.1164\n",
      "     35            1.0000        \u001b[32m0.1424\u001b[0m       0.3438            0.3438        1.5964  0.0006  0.1087\n",
      "     36            1.0000        0.1609       0.3542            0.3542        1.6005  0.0007  0.1096\n",
      "     37            1.0000        \u001b[32m0.0634\u001b[0m       0.3333            0.3333        1.6188  0.0007  0.1129\n",
      "     38            1.0000        \u001b[32m0.0168\u001b[0m       0.3333            0.3333        1.6451  0.0007  0.1074\n",
      "     39            1.0000        0.0396       0.2917            0.2917        1.6750  0.0007  0.1128\n",
      "     40            1.0000        0.0218       0.2708            0.2708        1.7021  0.0007  0.1110\n",
      "     41            1.0000        \u001b[32m0.0156\u001b[0m       0.2708            0.2708        1.7239  0.0007  0.1172\n",
      "     42            1.0000        \u001b[32m0.0153\u001b[0m       0.2812            0.2812        1.7405  0.0007  0.1122\n",
      "     43            1.0000        0.0289       0.2604            0.2604        1.7507  0.0006  0.1113\n",
      "     44            1.0000        \u001b[32m0.0061\u001b[0m       0.2500            0.2500        1.7576  0.0006  0.1051\n",
      "     45            1.0000        0.0069       0.2500            0.2500        1.7624  0.0005  0.1096\n",
      "     46            1.0000        0.0104       0.2500            0.2500        1.7665  0.0005  0.1118\n",
      "     47            1.0000        \u001b[32m0.0059\u001b[0m       0.2500            0.2500        1.7695  0.0004  0.1161\n",
      "     48            1.0000        0.0126       0.2500            0.2500        1.7719  0.0004  0.1313\n",
      "     49            1.0000        0.0145       0.2604            0.2604        1.7741  0.0003  0.1093\n",
      "     50            1.0000        0.0105       0.2708            0.2708        1.7762  0.0003  0.1070\n",
      "Fine tuning model for subject 6 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        2.6996       0.3542            0.3542        1.7282  0.0004  0.1251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5000        2.4574       0.3646            0.3646        1.7487  0.0005  0.1242\n",
      "     33            \u001b[36m0.9000\u001b[0m        1.5528       0.3542            0.3542        1.8188  0.0005  0.1335\n",
      "     34            0.9000        0.8959       0.3125            0.3125        1.9351  0.0006  0.1314\n",
      "     35            0.9000        \u001b[32m0.5588\u001b[0m       0.3021            0.3021        2.0642  0.0006  0.1311\n",
      "     36            0.9000        \u001b[32m0.1616\u001b[0m       0.2604            0.2604        2.2058  0.0007  0.1153\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0490\u001b[0m       0.2708            0.2708        2.3514  0.0007  0.1115\n",
      "     38            1.0000        0.0664       0.2396            0.2396        2.5024  0.0007  0.1275\n",
      "     39            1.0000        \u001b[32m0.0315\u001b[0m       0.2396            0.2396        2.6364  0.0007  0.1065\n",
      "     40            1.0000        0.0422       0.2604            0.2604        2.7588  0.0007  0.1109\n",
      "     41            1.0000        0.0371       0.2708            0.2708        2.8582  0.0007  0.1124\n",
      "     42            1.0000        \u001b[32m0.0139\u001b[0m       0.2708            0.2708        2.9217  0.0007  0.1128\n",
      "     43            1.0000        0.0250       0.2708            0.2708        2.9509  0.0006  0.1152\n",
      "     44            1.0000        \u001b[32m0.0094\u001b[0m       0.2812            0.2812        2.9538  0.0006  0.1157\n",
      "     45            1.0000        0.0270       0.2812            0.2812        2.9408  0.0005  0.1084\n",
      "     46            1.0000        0.0162       0.3125            0.3125        2.9119  0.0005  0.1091\n",
      "     47            1.0000        \u001b[32m0.0092\u001b[0m       0.3333            0.3333        2.8730  0.0004  0.1125\n",
      "     48            1.0000        0.0117       0.3438            0.3438        2.8297  0.0004  0.1117\n",
      "     49            1.0000        \u001b[32m0.0089\u001b[0m       0.3333            0.3333        2.7858  0.0003  0.1131\n",
      "     50            1.0000        \u001b[32m0.0064\u001b[0m       0.3333            0.3333        2.7443  0.0003  0.1129\n",
      "Fine tuning model for subject 6 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5333        1.1737       0.3542            0.3542        1.7003  0.0004  0.1133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5333        1.0754       0.3542            0.3542        1.6725  0.0005  0.1133\n",
      "     33            0.7333        0.9209       0.3542            0.3542        1.6465  0.0005  0.1068\n",
      "     34            \u001b[36m0.8667\u001b[0m        \u001b[32m0.5700\u001b[0m       0.3542            0.3542        1.6242  0.0006  0.1154\n",
      "     35            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4312\u001b[0m       0.3542            0.3542        1.6183  0.0006  0.1122\n",
      "     36            0.9333        \u001b[32m0.1901\u001b[0m       0.3854            0.3854        1.6304  0.0007  0.1118\n",
      "     37            0.9333        \u001b[32m0.1771\u001b[0m       0.3542            0.3542        1.6555  0.0007  0.1099\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1427\u001b[0m       0.3542            0.3542        1.6929  0.0007  0.1224\n",
      "     39            1.0000        \u001b[32m0.0695\u001b[0m       0.3438            0.3438        1.7459  0.0007  0.1074\n",
      "     40            1.0000        \u001b[32m0.0588\u001b[0m       0.3438            0.3438        1.8000  0.0007  0.1123\n",
      "     41            1.0000        \u001b[32m0.0490\u001b[0m       0.3542            0.3542        1.8558  0.0007  0.1119\n",
      "     42            1.0000        \u001b[32m0.0267\u001b[0m       0.3333            0.3333        1.9039  0.0007  0.1095\n",
      "     43            1.0000        \u001b[32m0.0229\u001b[0m       0.3125            0.3125        1.9405  0.0006  0.1098\n",
      "     44            1.0000        0.0230       0.3229            0.3229        1.9639  0.0006  0.1172\n",
      "     45            1.0000        \u001b[32m0.0161\u001b[0m       0.3229            0.3229        1.9743  0.0005  0.1115\n",
      "     46            1.0000        0.0198       0.3333            0.3333        1.9710  0.0005  0.1127\n",
      "     47            1.0000        0.0254       0.3333            0.3333        1.9587  0.0004  0.1091\n",
      "     48            1.0000        0.0163       0.3438            0.3438        1.9405  0.0004  0.1120\n",
      "     49            1.0000        0.0334       0.3333            0.3333        1.9192  0.0003  0.1140\n",
      "     50            1.0000        0.0195       0.3333            0.3333        1.8973  0.0003  0.1085\n",
      "Fine tuning model for subject 6 with 15 = 15 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.1423       0.3333            0.3333        1.7076  0.0004  0.1068\n",
      "     32            0.6667        1.2122       0.3438            0.3438        1.6914  0.0005  0.1120\n",
      "     33            0.8000        0.7095       0.3438            0.3438        1.6928  0.0005  0.1115\n",
      "     34            \u001b[36m0.8667\u001b[0m        \u001b[32m0.4403\u001b[0m       0.3438            0.3438        1.7051  0.0006  0.1118\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4267\u001b[0m       0.3646            0.3646        1.7162  0.0006  0.1379\n",
      "     36            1.0000        \u001b[32m0.2322\u001b[0m       0.4167            0.4167        1.7164  0.0007  0.1315\n",
      "     37            1.0000        \u001b[32m0.1125\u001b[0m       0.3750            0.3750        1.7157  0.0007  0.1382\n",
      "     38            1.0000        0.1303       0.3958            0.3958        1.7236  0.0007  0.1336\n",
      "     39            1.0000        \u001b[32m0.0617\u001b[0m       0.3958            0.3958        1.7444  0.0007  0.1353\n",
      "     40            1.0000        \u001b[32m0.0581\u001b[0m       0.3646            0.3646        1.7809  0.0007  0.1353\n",
      "     41            1.0000        \u001b[32m0.0387\u001b[0m       0.3646            0.3646        1.8254  0.0007  0.1699\n",
      "     42            1.0000        \u001b[32m0.0256\u001b[0m       0.3646            0.3646        1.8710  0.0007  0.1349\n",
      "     43            1.0000        0.0290       0.3646            0.3646        1.9108  0.0006  0.1410\n",
      "     44            1.0000        0.0262       0.3750            0.3750        1.9405  0.0006  0.1331\n",
      "     45            1.0000        \u001b[32m0.0213\u001b[0m       0.3646            0.3646        1.9591  0.0005  0.1485\n",
      "     46            1.0000        0.0251       0.3646            0.3646        1.9673  0.0005  0.1442\n",
      "     47            1.0000        0.0236       0.3542            0.3542        1.9664  0.0004  0.1374\n",
      "     48            1.0000        \u001b[32m0.0176\u001b[0m       0.3646            0.3646        1.9598  0.0004  0.1395\n",
      "     49            1.0000        \u001b[32m0.0174\u001b[0m       0.3646            0.3646        1.9494  0.0003  0.1374\n",
      "     50            1.0000        0.0196       0.3542            0.3542        1.9371  0.0003  0.1539\n",
      "Fine tuning model for subject 6 with 15 = 15 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4667        1.5284       0.3646            0.3646        1.7163  0.0004  0.1299\n",
      "     32            0.6667        1.7014       0.3750            0.3750        1.7134  0.0005  0.1557\n",
      "     33            0.7333        0.9741       0.3750            0.3750        1.7149  0.0005  0.1738\n",
      "     34            \u001b[36m0.8667\u001b[0m        \u001b[32m0.5996\u001b[0m       0.3750            0.3750        1.7004  0.0006  0.1168\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2483\u001b[0m       0.3646            0.3646        1.6792  0.0006  0.1114\n",
      "     36            1.0000        \u001b[32m0.1818\u001b[0m       0.3750            0.3750        1.6619  0.0007  0.1110\n",
      "     37            1.0000        \u001b[32m0.1055\u001b[0m       0.3854            0.3854        1.6507  0.0007  0.1094\n",
      "     38            1.0000        \u001b[32m0.0726\u001b[0m       0.3854            0.3854        1.6533  0.0007  0.1102\n",
      "     39            1.0000        \u001b[32m0.0376\u001b[0m       0.4062            0.4062        1.6717  0.0007  0.1252\n",
      "     40            1.0000        0.0405       0.4062            0.4062        1.7015  0.0007  0.1111\n",
      "     41            1.0000        0.1003       0.4062            0.4062        1.7383  0.0007  0.1181\n",
      "     42            1.0000        \u001b[32m0.0212\u001b[0m       0.4167            0.4167        1.7771  0.0007  0.1133\n",
      "     43            1.0000        0.0241       0.4167            0.4167        1.8110  0.0006  0.1140\n",
      "     44            1.0000        \u001b[32m0.0170\u001b[0m       0.4062            0.4062        1.8380  0.0006  0.1091\n",
      "     45            1.0000        \u001b[32m0.0091\u001b[0m       0.4167            0.4167        1.8569  0.0005  0.1123\n",
      "     46            1.0000        0.0111       0.4062            0.4062        1.8676  0.0005  0.1105\n",
      "     47            1.0000        0.0146       0.4062            0.4062        1.8715  0.0004  0.1109\n",
      "     48            1.0000        0.0250       0.3958            0.3958        1.8697  0.0004  0.1104\n",
      "     49            1.0000        \u001b[32m0.0073\u001b[0m       0.4062            0.4062        1.8645  0.0003  0.1128\n",
      "     50            1.0000        0.0115       0.4062            0.4062        1.8572  0.0003  0.1094\n",
      "Fine tuning model for subject 6 with 15 = 15 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3333        1.9007       0.3542            0.3542        1.7113  0.0004  0.1048\n",
      "     32            0.5333        1.7722       0.3542            0.3542        1.7096  0.0005  0.1175\n",
      "     33            0.6000        0.9989       0.3542            0.3542        1.7017  0.0005  0.1257\n",
      "     34            \u001b[36m0.8667\u001b[0m        0.9261       0.3229            0.3229        1.6852  0.0006  0.1083\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6518\u001b[0m       0.3229            0.3229        1.6836  0.0006  0.1170\n",
      "     36            1.0000        \u001b[32m0.2273\u001b[0m       0.2917            0.2917        1.7138  0.0007  0.1128\n",
      "     37            1.0000        \u001b[32m0.2268\u001b[0m       0.2917            0.2917        1.7751  0.0007  0.1129\n",
      "     38            1.0000        \u001b[32m0.0947\u001b[0m       0.3021            0.3021        1.8623  0.0007  0.1090\n",
      "     39            1.0000        0.1192       0.3229            0.3229        1.9514  0.0007  0.1181\n",
      "     40            1.0000        0.1269       0.3333            0.3333        2.0264  0.0007  0.1085\n",
      "     41            0.9333        \u001b[32m0.0538\u001b[0m       0.3438            0.3438        2.0901  0.0007  0.1105\n",
      "     42            0.9333        \u001b[32m0.0415\u001b[0m       0.3333            0.3333        2.1360  0.0007  0.1147\n",
      "     43            0.9333        0.0537       0.3125            0.3125        2.1623  0.0006  0.1159\n",
      "     44            0.9333        0.0691       0.3229            0.3229        2.1699  0.0006  0.1125\n",
      "     45            1.0000        \u001b[32m0.0251\u001b[0m       0.3229            0.3229        2.1610  0.0005  0.1103\n",
      "     46            1.0000        0.0509       0.3125            0.3125        2.1396  0.0005  0.1108\n",
      "     47            1.0000        \u001b[32m0.0224\u001b[0m       0.2917            0.2917        2.1131  0.0004  0.1150\n",
      "     48            1.0000        0.0414       0.3021            0.3021        2.0845  0.0004  0.1358\n",
      "     49            1.0000        0.0382       0.3021            0.3021        2.0560  0.0003  0.1578\n",
      "     50            1.0000        0.0372       0.2917            0.2917        2.0300  0.0003  0.1370\n",
      "Fine tuning model for subject 6 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5333        1.4966       0.3542            0.3542        1.7259  0.0004  0.1057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5333        0.7658       0.3438            0.3438        1.7293  0.0005  0.1114\n",
      "     33            \u001b[36m0.8667\u001b[0m        0.8481       0.3229            0.3229        1.7404  0.0005  0.1273\n",
      "     34            \u001b[36m0.9333\u001b[0m        \u001b[32m0.6239\u001b[0m       0.3542            0.3542        1.7499  0.0006  0.1621\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2630\u001b[0m       0.3229            0.3229        1.7485  0.0006  0.1036\n",
      "     36            1.0000        \u001b[32m0.2070\u001b[0m       0.3125            0.3125        1.7408  0.0007  0.1174\n",
      "     37            1.0000        0.2285       0.3229            0.3229        1.7339  0.0007  0.1554\n",
      "     38            1.0000        \u001b[32m0.1555\u001b[0m       0.3021            0.3021        1.7416  0.0007  0.1172\n",
      "     39            1.0000        \u001b[32m0.1149\u001b[0m       0.3021            0.3021        1.7646  0.0007  0.1127\n",
      "     40            1.0000        \u001b[32m0.0425\u001b[0m       0.2917            0.2917        1.7934  0.0007  0.1130\n",
      "     41            1.0000        0.0807       0.3125            0.3125        1.8245  0.0007  0.1152\n",
      "     42            1.0000        \u001b[32m0.0321\u001b[0m       0.3229            0.3229        1.8541  0.0007  0.1146\n",
      "     43            1.0000        0.0463       0.3125            0.3125        1.8812  0.0006  0.1164\n",
      "     44            1.0000        0.0372       0.3125            0.3125        1.9034  0.0006  0.1124\n",
      "     45            1.0000        0.0616       0.3021            0.3021        1.9206  0.0005  0.1092\n",
      "     46            1.0000        \u001b[32m0.0277\u001b[0m       0.2917            0.2917        1.9346  0.0005  0.1016\n",
      "     47            1.0000        0.0290       0.2812            0.2812        1.9457  0.0004  0.1088\n",
      "     48            1.0000        \u001b[32m0.0200\u001b[0m       0.2917            0.2917        1.9550  0.0004  0.1162\n",
      "     49            1.0000        0.0934       0.2917            0.2917        1.9624  0.0003  0.1144\n",
      "     50            1.0000        0.0200       0.2917            0.2917        1.9682  0.0003  0.1123\n",
      "Fine tuning model for subject 6 with 15 = 15 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5333        1.9810       0.3542            0.3542        1.7159  0.0004  0.1067\n",
      "     32            0.5333        1.7380       0.3542            0.3542        1.6997  0.0005  0.1139\n",
      "     33            0.7333        1.4400       0.3333            0.3333        1.6880  0.0005  0.1126\n",
      "     34            \u001b[36m0.8667\u001b[0m        0.8274       0.3333            0.3333        1.6875  0.0006  0.1100\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4561\u001b[0m       0.3229            0.3229        1.6905  0.0006  0.1049\n",
      "     36            1.0000        \u001b[32m0.3494\u001b[0m       0.3542            0.3542        1.6990  0.0007  0.1103\n",
      "     37            1.0000        \u001b[32m0.1670\u001b[0m       0.3125            0.3125        1.7190  0.0007  0.1115\n",
      "     38            1.0000        \u001b[32m0.1317\u001b[0m       0.2917            0.2917        1.7428  0.0007  0.1172\n",
      "     39            1.0000        0.1338       0.2500            0.2500        1.7727  0.0007  0.1134\n",
      "     40            1.0000        \u001b[32m0.0621\u001b[0m       0.2708            0.2708        1.8039  0.0007  0.1097\n",
      "     41            1.0000        0.1021       0.2708            0.2708        1.8369  0.0007  0.1124\n",
      "     42            1.0000        \u001b[32m0.0595\u001b[0m       0.2812            0.2812        1.8667  0.0007  0.1159\n",
      "     43            1.0000        \u001b[32m0.0457\u001b[0m       0.2812            0.2812        1.8946  0.0006  0.1186\n",
      "     44            1.0000        \u001b[32m0.0371\u001b[0m       0.2812            0.2812        1.9201  0.0006  0.1076\n",
      "     45            1.0000        0.0537       0.2708            0.2708        1.9431  0.0005  0.1194\n",
      "     46            1.0000        \u001b[32m0.0259\u001b[0m       0.2812            0.2812        1.9637  0.0005  0.1152\n",
      "     47            1.0000        0.0941       0.2812            0.2812        1.9819  0.0004  0.1230\n",
      "     48            1.0000        \u001b[32m0.0241\u001b[0m       0.2604            0.2604        1.9975  0.0004  0.1518\n",
      "     49            1.0000        \u001b[32m0.0182\u001b[0m       0.2604            0.2604        2.0110  0.0003  0.1137\n",
      "     50            1.0000        0.0228       0.2500            0.2500        2.0224  0.0003  0.1174\n",
      "Fine tuning model for subject 6 with 15 = 15 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.0667        1.9311       0.3750            0.3750        1.7221  0.0004  0.1373\n",
      "     32            0.3333        1.8671       0.3750            0.3750        1.7302  0.0005  0.1184\n",
      "     33            0.6667        1.4276       0.3854            0.3854        1.7752  0.0005  0.1115\n",
      "     34            \u001b[36m0.9333\u001b[0m        0.8970       0.3229            0.3229        1.8564  0.0006  0.1170\n",
      "     35            0.9333        \u001b[32m0.4901\u001b[0m       0.3229            0.3229        1.9623  0.0006  0.1219\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3848\u001b[0m       0.3021            0.3021        2.0596  0.0007  0.1286\n",
      "     37            1.0000        \u001b[32m0.1646\u001b[0m       0.2812            0.2812        2.1559  0.0007  0.1364\n",
      "     38            1.0000        \u001b[32m0.1425\u001b[0m       0.2812            0.2812        2.2554  0.0007  0.1376\n",
      "     39            1.0000        \u001b[32m0.1404\u001b[0m       0.2812            0.2812        2.3428  0.0007  0.1134\n",
      "     40            1.0000        \u001b[32m0.0790\u001b[0m       0.2708            0.2708        2.4125  0.0007  0.1167\n",
      "     41            1.0000        \u001b[32m0.0563\u001b[0m       0.2708            0.2708        2.4631  0.0007  0.1308\n",
      "     42            1.0000        \u001b[32m0.0468\u001b[0m       0.2812            0.2812        2.4944  0.0007  0.1142\n",
      "     43            1.0000        \u001b[32m0.0263\u001b[0m       0.3021            0.3021        2.5075  0.0006  0.1118\n",
      "     44            1.0000        0.0420       0.3021            0.3021        2.5034  0.0006  0.1099\n",
      "     45            1.0000        0.0432       0.3021            0.3021        2.4818  0.0005  0.1087\n",
      "     46            1.0000        0.0460       0.3021            0.3021        2.4490  0.0005  0.1082\n",
      "     47            1.0000        0.0519       0.3125            0.3125        2.4102  0.0004  0.1087\n",
      "     48            1.0000        \u001b[32m0.0224\u001b[0m       0.3125            0.3125        2.3688  0.0004  0.1349\n",
      "     49            1.0000        0.0282       0.3333            0.3333        2.3268  0.0003  0.1370\n",
      "     50            1.0000        \u001b[32m0.0126\u001b[0m       0.3438            0.3438        2.2864  0.0003  0.1516\n",
      "Fine tuning model for subject 6 with 15 = 15 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.2359       0.3438            0.3438        1.7099  0.0004  0.1331\n",
      "     32            0.6000        1.1897       0.3438            0.3438        1.7055  0.0005  0.1338\n",
      "     33            0.8000        0.9975       0.3542            0.3542        1.7244  0.0005  0.1699\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5207\u001b[0m       0.3333            0.3333        1.7637  0.0006  0.1398\n",
      "     35            1.0000        \u001b[32m0.3850\u001b[0m       0.3125            0.3125        1.8126  0.0006  0.1294\n",
      "     36            1.0000        \u001b[32m0.1618\u001b[0m       0.2812            0.2812        1.8515  0.0007  0.1369\n",
      "     37            1.0000        \u001b[32m0.0861\u001b[0m       0.2604            0.2604        1.8863  0.0007  0.1358\n",
      "     38            1.0000        0.0905       0.2917            0.2917        1.9175  0.0007  0.1460\n",
      "     39            1.0000        \u001b[32m0.0762\u001b[0m       0.3021            0.3021        1.9493  0.0007  0.1344\n",
      "     40            1.0000        \u001b[32m0.0547\u001b[0m       0.3021            0.3021        1.9829  0.0007  0.1387\n",
      "     41            1.0000        0.0693       0.3021            0.3021        2.0173  0.0007  0.1348\n",
      "     42            1.0000        \u001b[32m0.0325\u001b[0m       0.2812            0.2812        2.0501  0.0007  0.1351\n",
      "     43            1.0000        \u001b[32m0.0190\u001b[0m       0.2917            0.2917        2.0784  0.0006  0.1584\n",
      "     44            1.0000        0.0213       0.2812            0.2812        2.1005  0.0006  0.1338\n",
      "     45            1.0000        0.0235       0.2812            0.2812        2.1161  0.0005  0.1880\n",
      "     46            1.0000        \u001b[32m0.0174\u001b[0m       0.2812            0.2812        2.1259  0.0005  0.1394\n",
      "     47            1.0000        0.0221       0.2812            0.2812        2.1317  0.0004  0.1175\n",
      "     48            1.0000        0.0200       0.2917            0.2917        2.1344  0.0004  0.1149\n",
      "     49            1.0000        0.0386       0.2917            0.2917        2.1355  0.0003  0.1112\n",
      "     50            1.0000        \u001b[32m0.0109\u001b[0m       0.2812            0.2812        2.1363  0.0003  0.1104\n",
      "Fine tuning model for subject 6 with 15 = 15 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5333        1.3417       0.3438            0.3438        1.7001  0.0004  0.1100\n",
      "     32            0.6000        1.0709       0.3542            0.3542        1.6682  0.0005  0.1230\n",
      "     33            \u001b[36m0.8667\u001b[0m        0.8022       0.3854            0.3854        1.6373  0.0005  0.1079\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6119\u001b[0m       0.3854            0.3854        1.6174  0.0006  0.1092\n",
      "     35            1.0000        \u001b[32m0.3998\u001b[0m       0.3646            0.3646        1.6176  0.0006  0.1117\n",
      "     36            1.0000        \u001b[32m0.1632\u001b[0m       0.3854            0.3854        1.6252  0.0007  0.1134\n",
      "     37            1.0000        0.1948       0.3542            0.3542        1.6319  0.0007  0.1107\n",
      "     38            1.0000        0.2086       0.3438            0.3438        1.6406  0.0007  0.1136\n",
      "     39            1.0000        \u001b[32m0.1474\u001b[0m       0.3542            0.3542        1.6578  0.0007  0.1110\n",
      "     40            1.0000        \u001b[32m0.0569\u001b[0m       0.3646            0.3646        1.6819  0.0007  0.1109\n",
      "     41            1.0000        0.0655       0.3646            0.3646        1.7127  0.0007  0.1163\n",
      "     42            1.0000        \u001b[32m0.0370\u001b[0m       0.3333            0.3333        1.7466  0.0007  0.1104\n",
      "     43            1.0000        \u001b[32m0.0276\u001b[0m       0.3229            0.3229        1.7787  0.0006  0.1123\n",
      "     44            1.0000        \u001b[32m0.0222\u001b[0m       0.3333            0.3333        1.8058  0.0006  0.1068\n",
      "     45            1.0000        0.0245       0.3229            0.3229        1.8264  0.0005  0.1173\n",
      "     46            1.0000        \u001b[32m0.0171\u001b[0m       0.3125            0.3125        1.8397  0.0005  0.1109\n",
      "     47            1.0000        \u001b[32m0.0147\u001b[0m       0.3125            0.3125        1.8465  0.0004  0.1152\n",
      "     48            1.0000        0.0156       0.3333            0.3333        1.8479  0.0004  0.1158\n",
      "     49            1.0000        0.0201       0.3229            0.3229        1.8451  0.0003  0.1107\n",
      "     50            1.0000        0.0234       0.3438            0.3438        1.8400  0.0003  0.1102\n",
      "Fine tuning model for subject 6 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3333        1.5336       0.3438            0.3438        1.7166  0.0004  0.1101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4000        1.2901       0.3438            0.3438        1.7143  0.0005  0.1183\n",
      "     33            0.7333        1.0903       0.3333            0.3333        1.7187  0.0005  0.1101\n",
      "     34            \u001b[36m1.0000\u001b[0m        0.8375       0.3333            0.3333        1.7303  0.0006  0.1205\n",
      "     35            1.0000        \u001b[32m0.4995\u001b[0m       0.3438            0.3438        1.7519  0.0006  0.1109\n",
      "     36            1.0000        \u001b[32m0.3080\u001b[0m       0.3542            0.3542        1.7801  0.0007  0.1128\n",
      "     37            1.0000        \u001b[32m0.1485\u001b[0m       0.3542            0.3542        1.8021  0.0007  0.1186\n",
      "     38            1.0000        \u001b[32m0.1162\u001b[0m       0.3854            0.3854        1.8120  0.0007  0.1137\n",
      "     39            1.0000        \u001b[32m0.0646\u001b[0m       0.3438            0.3438        1.8173  0.0007  0.1109\n",
      "     40            1.0000        \u001b[32m0.0411\u001b[0m       0.3333            0.3333        1.8265  0.0007  0.1133\n",
      "     41            1.0000        0.0912       0.3542            0.3542        1.8355  0.0007  0.1172\n",
      "     42            1.0000        0.0550       0.3646            0.3646        1.8513  0.0007  0.1083\n",
      "     43            1.0000        \u001b[32m0.0306\u001b[0m       0.3438            0.3438        1.8710  0.0006  0.1085\n",
      "     44            1.0000        0.0374       0.3438            0.3438        1.8917  0.0006  0.1094\n",
      "     45            1.0000        \u001b[32m0.0237\u001b[0m       0.3542            0.3542        1.9122  0.0005  0.1127\n",
      "     46            1.0000        0.0255       0.3438            0.3438        1.9304  0.0005  0.1121\n",
      "     47            1.0000        \u001b[32m0.0169\u001b[0m       0.3333            0.3333        1.9467  0.0004  0.1111\n",
      "     48            1.0000        0.0279       0.3333            0.3333        1.9615  0.0004  0.1115\n",
      "     49            1.0000        0.0196       0.3229            0.3229        1.9752  0.0003  0.1102\n",
      "     50            1.0000        0.0270       0.3229            0.3229        1.9889  0.0003  0.1114\n",
      "Fine tuning model for subject 6 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4500        1.4576       0.3438            0.3438        1.7070  0.0004  0.1128\n",
      "     32            0.5000        1.3612       0.3542            0.3542        1.6874  0.0005  0.1160\n",
      "     33            0.7000        1.0238       0.3229            0.3229        1.6691  0.0005  0.1181\n",
      "     34            \u001b[36m0.8500\u001b[0m        0.8492       0.3542            0.3542        1.6492  0.0006  0.1165\n",
      "     35            \u001b[36m0.9500\u001b[0m        \u001b[32m0.6187\u001b[0m       0.3438            0.3438        1.6212  0.0006  0.1217\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3245\u001b[0m       0.3542            0.3542        1.5976  0.0007  0.1116\n",
      "     37            1.0000        \u001b[32m0.2883\u001b[0m       0.3542            0.3542        1.5869  0.0007  0.1171\n",
      "     38            1.0000        \u001b[32m0.1607\u001b[0m       0.3750            0.3750        1.5800  0.0007  0.1104\n",
      "     39            1.0000        0.1974       0.3854            0.3854        1.5767  0.0007  0.1169\n",
      "     40            1.0000        \u001b[32m0.1371\u001b[0m       0.3854            0.3854        1.5760  0.0007  0.1177\n",
      "     41            1.0000        \u001b[32m0.0975\u001b[0m       0.3854            0.3854        1.5797  0.0007  0.1192\n",
      "     42            1.0000        \u001b[32m0.0567\u001b[0m       0.3854            0.3854        1.5863  0.0007  0.1148\n",
      "     43            1.0000        0.0610       0.3542            0.3542        1.5937  0.0006  0.1163\n",
      "     44            1.0000        0.0824       0.3542            0.3542        1.5989  0.0006  0.1157\n",
      "     45            1.0000        \u001b[32m0.0561\u001b[0m       0.3229            0.3229        1.6038  0.0005  0.1131\n",
      "     46            1.0000        \u001b[32m0.0293\u001b[0m       0.3333            0.3333        1.6078  0.0005  0.1174\n",
      "     47            1.0000        0.0487       0.3333            0.3333        1.6118  0.0004  0.1111\n",
      "     48            1.0000        0.0611       0.3229            0.3229        1.6152  0.0004  0.1120\n",
      "     49            1.0000        0.0461       0.3333            0.3333        1.6164  0.0003  0.1148\n",
      "     50            1.0000        0.0350       0.3438            0.3438        1.6174  0.0003  0.1155\n",
      "Fine tuning model for subject 6 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3500        1.4280       0.3438            0.3438        1.7141  0.0004  0.1164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5000        1.3772       0.3438            0.3438        1.7040  0.0005  0.1112\n",
      "     33            0.8000        0.9676       0.3438            0.3438        1.7026  0.0005  0.1102\n",
      "     34            \u001b[36m0.9500\u001b[0m        0.7242       0.3542            0.3542        1.7099  0.0006  0.1115\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4506\u001b[0m       0.3333            0.3333        1.7311  0.0006  0.1173\n",
      "     36            1.0000        \u001b[32m0.3150\u001b[0m       0.3438            0.3438        1.7507  0.0007  0.1128\n",
      "     37            1.0000        \u001b[32m0.1816\u001b[0m       0.3542            0.3542        1.7717  0.0007  0.1126\n",
      "     38            1.0000        0.1941       0.3750            0.3750        1.8014  0.0007  0.1176\n",
      "     39            1.0000        \u001b[32m0.0947\u001b[0m       0.3646            0.3646        1.8299  0.0007  0.1127\n",
      "     40            1.0000        0.1132       0.3646            0.3646        1.8594  0.0007  0.1117\n",
      "     41            1.0000        0.1068       0.3750            0.3750        1.8834  0.0007  0.1133\n",
      "     42            1.0000        \u001b[32m0.0923\u001b[0m       0.3854            0.3854        1.9010  0.0007  0.1127\n",
      "     43            1.0000        \u001b[32m0.0609\u001b[0m       0.3854            0.3854        1.9103  0.0006  0.1127\n",
      "     44            1.0000        \u001b[32m0.0521\u001b[0m       0.4062            0.4062        1.9150  0.0006  0.1177\n",
      "     45            1.0000        \u001b[32m0.0435\u001b[0m       0.4062            0.4062        1.9143  0.0005  0.1261\n",
      "     46            1.0000        \u001b[32m0.0428\u001b[0m       0.4062            0.4062        1.9125  0.0005  0.1131\n",
      "     47            1.0000        \u001b[32m0.0355\u001b[0m       0.4271            0.4271        1.9089  0.0004  0.1122\n",
      "     48            1.0000        0.0458       0.4271            0.4271        1.9036  0.0004  0.1112\n",
      "     49            1.0000        0.0400       0.4479            0.4479        1.8986  0.0003  0.1159\n",
      "     50            1.0000        \u001b[32m0.0216\u001b[0m       0.4583            0.4583        1.8935  0.0003  0.1096\n",
      "Fine tuning model for subject 6 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4500        1.5262       0.3438            0.3438        1.7134  0.0004  0.1223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4500        1.6556       0.3646            0.3646        1.7201  0.0005  0.1200\n",
      "     33            0.6500        1.0112       0.3542            0.3542        1.7342  0.0005  0.1127\n",
      "     34            0.8000        0.8964       0.3646            0.3646        1.7313  0.0006  0.1184\n",
      "     35            \u001b[36m0.9000\u001b[0m        0.7676       0.3854            0.3854        1.7285  0.0006  0.1118\n",
      "     36            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4988\u001b[0m       0.3438            0.3438        1.7301  0.0007  0.1105\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2672\u001b[0m       0.3542            0.3542        1.7424  0.0007  0.1164\n",
      "     38            1.0000        \u001b[32m0.2443\u001b[0m       0.3438            0.3438        1.7691  0.0007  0.1178\n",
      "     39            1.0000        \u001b[32m0.1775\u001b[0m       0.3646            0.3646        1.7978  0.0007  0.1123\n",
      "     40            1.0000        \u001b[32m0.1456\u001b[0m       0.3542            0.3542        1.8300  0.0007  0.1122\n",
      "     41            1.0000        \u001b[32m0.1389\u001b[0m       0.3542            0.3542        1.8699  0.0007  0.1136\n",
      "     42            1.0000        \u001b[32m0.0926\u001b[0m       0.3542            0.3542        1.9115  0.0007  0.1477\n",
      "     43            1.0000        0.1438       0.3542            0.3542        1.9451  0.0006  0.1395\n",
      "     44            1.0000        \u001b[32m0.0834\u001b[0m       0.3542            0.3542        1.9703  0.0006  0.1359\n",
      "     45            1.0000        \u001b[32m0.0740\u001b[0m       0.3646            0.3646        1.9933  0.0005  0.1340\n",
      "     46            1.0000        0.0753       0.3646            0.3646        2.0065  0.0005  0.1354\n",
      "     47            1.0000        \u001b[32m0.0586\u001b[0m       0.3646            0.3646        2.0159  0.0004  0.1372\n",
      "     48            1.0000        0.0645       0.3542            0.3542        2.0233  0.0004  0.1558\n",
      "     49            1.0000        0.0675       0.3333            0.3333        2.0270  0.0003  0.1374\n",
      "     50            1.0000        \u001b[32m0.0502\u001b[0m       0.3333            0.3333        2.0295  0.0003  0.1340\n",
      "Fine tuning model for subject 6 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6500        1.2303       0.3438            0.3438        1.6963  0.0004  0.1517\n",
      "     32            0.6500        1.1221       0.3542            0.3542        1.6577  0.0005  0.1362\n",
      "     33            0.8000        0.8371       0.3542            0.3542        1.6117  0.0005  0.1403\n",
      "     34            \u001b[36m0.8500\u001b[0m        0.6788       0.3542            0.3542        1.5749  0.0006  0.1390\n",
      "     35            \u001b[36m0.9500\u001b[0m        \u001b[32m0.3934\u001b[0m       0.3542            0.3542        1.5557  0.0006  0.1482\n",
      "     36            \u001b[36m1.0000\u001b[0m        0.4280       0.3542            0.3542        1.5590  0.0007  0.1742\n",
      "     37            1.0000        \u001b[32m0.2164\u001b[0m       0.4167            0.4167        1.5925  0.0007  0.1611\n",
      "     38            1.0000        \u001b[32m0.1743\u001b[0m       0.3854            0.3854        1.6530  0.0007  0.1388\n",
      "     39            1.0000        \u001b[32m0.1314\u001b[0m       0.3854            0.3854        1.7327  0.0007  0.1513\n",
      "     40            1.0000        0.1381       0.3854            0.3854        1.8048  0.0007  0.1579\n",
      "     41            1.0000        \u001b[32m0.0608\u001b[0m       0.3750            0.3750        1.8628  0.0007  0.1177\n",
      "     42            1.0000        \u001b[32m0.0538\u001b[0m       0.3750            0.3750        1.9063  0.0007  0.1222\n",
      "     43            1.0000        \u001b[32m0.0478\u001b[0m       0.3750            0.3750        1.9331  0.0006  0.1122\n",
      "     44            1.0000        \u001b[32m0.0464\u001b[0m       0.3750            0.3750        1.9435  0.0006  0.1373\n",
      "     45            1.0000        \u001b[32m0.0417\u001b[0m       0.3646            0.3646        1.9400  0.0005  0.1116\n",
      "     46            1.0000        0.0466       0.3750            0.3750        1.9241  0.0005  0.1110\n",
      "     47            1.0000        \u001b[32m0.0358\u001b[0m       0.3750            0.3750        1.9017  0.0004  0.1133\n",
      "     48            1.0000        \u001b[32m0.0206\u001b[0m       0.3750            0.3750        1.8772  0.0004  0.1114\n",
      "     49            1.0000        \u001b[32m0.0164\u001b[0m       0.3854            0.3854        1.8536  0.0003  0.1177\n",
      "     50            1.0000        0.0190       0.3750            0.3750        1.8325  0.0003  0.1082\n",
      "Fine tuning model for subject 6 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3500        1.6296       0.3646            0.3646        1.7110  0.0004  0.1213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4000        1.8091       0.3958            0.3958        1.6948  0.0005  0.1224\n",
      "     33            0.5500        1.5055       0.3854            0.3854        1.6803  0.0005  0.1153\n",
      "     34            0.7500        0.8479       0.3854            0.3854        1.6939  0.0006  0.1191\n",
      "     35            \u001b[36m0.8500\u001b[0m        \u001b[32m0.5740\u001b[0m       0.3333            0.3333        1.7607  0.0006  0.1138\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2868\u001b[0m       0.3229            0.3229        1.8754  0.0007  0.1269\n",
      "     37            1.0000        \u001b[32m0.2168\u001b[0m       0.3125            0.3125        2.0115  0.0007  0.1258\n",
      "     38            0.9500        \u001b[32m0.1513\u001b[0m       0.3229            0.3229        2.1425  0.0007  0.1141\n",
      "     39            0.9500        \u001b[32m0.1111\u001b[0m       0.3229            0.3229        2.2663  0.0007  0.1183\n",
      "     40            0.8500        \u001b[32m0.1081\u001b[0m       0.3542            0.3542        2.3654  0.0007  0.1147\n",
      "     41            0.8500        \u001b[32m0.0824\u001b[0m       0.3333            0.3333        2.4437  0.0007  0.1168\n",
      "     42            0.8500        0.1209       0.3333            0.3333        2.5038  0.0007  0.1161\n",
      "     43            0.9000        \u001b[32m0.0741\u001b[0m       0.3333            0.3333        2.5451  0.0006  0.1211\n",
      "     44            0.9500        \u001b[32m0.0428\u001b[0m       0.3333            0.3333        2.5663  0.0006  0.1173\n",
      "     45            0.9500        0.0509       0.3333            0.3333        2.5706  0.0005  0.1161\n",
      "     46            0.9500        \u001b[32m0.0394\u001b[0m       0.3438            0.3438        2.5597  0.0005  0.1124\n",
      "     47            1.0000        0.0820       0.3438            0.3438        2.5418  0.0004  0.1125\n",
      "     48            1.0000        0.0409       0.3438            0.3438        2.5162  0.0004  0.1173\n",
      "     49            1.0000        \u001b[32m0.0322\u001b[0m       0.3542            0.3542        2.4883  0.0003  0.1210\n",
      "     50            1.0000        \u001b[32m0.0231\u001b[0m       0.3542            0.3542        2.4590  0.0003  0.1194\n",
      "Fine tuning model for subject 6 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.1771       0.3542            0.3542        1.7103  0.0004  0.1174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.1983       0.3646            0.3646        1.7047  0.0005  0.1156\n",
      "     33            0.8000        0.9540       0.3958            0.3958        1.7084  0.0005  0.1175\n",
      "     34            \u001b[36m0.8500\u001b[0m        \u001b[32m0.6382\u001b[0m       0.3646            0.3646        1.7185  0.0006  0.1141\n",
      "     35            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5453\u001b[0m       0.3021            0.3021        1.7528  0.0006  0.1105\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2829\u001b[0m       0.2812            0.2812        1.8187  0.0007  0.1111\n",
      "     37            1.0000        \u001b[32m0.1451\u001b[0m       0.2917            0.2917        1.8950  0.0007  0.1120\n",
      "     38            1.0000        \u001b[32m0.1410\u001b[0m       0.3229            0.3229        1.9852  0.0007  0.1136\n",
      "     39            1.0000        \u001b[32m0.0872\u001b[0m       0.3021            0.3021        2.0772  0.0007  0.1125\n",
      "     40            1.0000        \u001b[32m0.0574\u001b[0m       0.2917            0.2917        2.1656  0.0007  0.1116\n",
      "     41            1.0000        0.0731       0.2812            0.2812        2.2444  0.0007  0.1162\n",
      "     42            1.0000        \u001b[32m0.0473\u001b[0m       0.2917            0.2917        2.3072  0.0007  0.1076\n",
      "     43            1.0000        \u001b[32m0.0416\u001b[0m       0.2917            0.2917        2.3506  0.0006  0.1180\n",
      "     44            1.0000        \u001b[32m0.0203\u001b[0m       0.2917            0.2917        2.3744  0.0006  0.1133\n",
      "     45            1.0000        0.0295       0.3021            0.3021        2.3829  0.0005  0.1124\n",
      "     46            1.0000        \u001b[32m0.0165\u001b[0m       0.2917            0.2917        2.3788  0.0005  0.1220\n",
      "     47            1.0000        0.0197       0.3125            0.3125        2.3660  0.0004  0.1114\n",
      "     48            1.0000        0.0172       0.3333            0.3333        2.3482  0.0004  0.1174\n",
      "     49            1.0000        0.0215       0.3333            0.3333        2.3282  0.0003  0.1121\n",
      "     50            1.0000        0.0382       0.3438            0.3438        2.3083  0.0003  0.1072\n",
      "Fine tuning model for subject 6 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.8741       0.3542            0.3542        1.7257  0.0004  0.1114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5500        1.5249       0.3542            0.3542        1.7341  0.0005  0.1142\n",
      "     33            0.7000        1.3747       0.3750            0.3750        1.7578  0.0005  0.1187\n",
      "     34            0.8000        0.8036       0.3438            0.3438        1.8025  0.0006  0.1133\n",
      "     35            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5728\u001b[0m       0.3229            0.3229        1.8615  0.0006  0.1165\n",
      "     36            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4550\u001b[0m       0.2812            0.2812        1.9224  0.0007  0.1174\n",
      "     37            0.9500        \u001b[32m0.3269\u001b[0m       0.2812            0.2812        1.9922  0.0007  0.1141\n",
      "     38            0.9500        \u001b[32m0.1216\u001b[0m       0.2604            0.2604        2.0535  0.0007  0.1173\n",
      "     39            \u001b[36m1.0000\u001b[0m        0.1679       0.2812            0.2812        2.1032  0.0007  0.1165\n",
      "     40            1.0000        0.1747       0.2604            0.2604        2.1388  0.0007  0.1126\n",
      "     41            1.0000        0.1429       0.2708            0.2708        2.1654  0.0007  0.1183\n",
      "     42            1.0000        \u001b[32m0.1214\u001b[0m       0.2812            0.2812        2.1825  0.0007  0.1188\n",
      "     43            1.0000        0.1252       0.2917            0.2917        2.1872  0.0006  0.1080\n",
      "     44            1.0000        0.1450       0.3021            0.3021        2.1781  0.0006  0.1122\n",
      "     45            1.0000        \u001b[32m0.0720\u001b[0m       0.3125            0.3125        2.1666  0.0005  0.1165\n",
      "     46            1.0000        \u001b[32m0.0568\u001b[0m       0.3125            0.3125        2.1554  0.0005  0.1138\n",
      "     47            1.0000        0.0649       0.2917            0.2917        2.1448  0.0004  0.1170\n",
      "     48            1.0000        \u001b[32m0.0557\u001b[0m       0.2917            0.2917        2.1343  0.0004  0.1123\n",
      "     49            1.0000        \u001b[32m0.0422\u001b[0m       0.3125            0.3125        2.1246  0.0003  0.1088\n",
      "     50            1.0000        0.0743       0.3125            0.3125        2.1158  0.0003  0.1133\n",
      "Fine tuning model for subject 6 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4500        2.2091       0.3438            0.3438        1.7197  0.0004  0.1133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5000        2.0502       0.3333            0.3333        1.7153  0.0005  0.1171\n",
      "     33            0.5000        1.7538       0.3229            0.3229        1.7136  0.0005  0.1124\n",
      "     34            0.8000        1.0973       0.3229            0.3229        1.7170  0.0006  0.1123\n",
      "     35            0.8000        0.9031       0.3021            0.3021        1.7463  0.0006  0.1091\n",
      "     36            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4660\u001b[0m       0.3125            0.3125        1.7973  0.0007  0.1216\n",
      "     37            0.9000        \u001b[32m0.2984\u001b[0m       0.2917            0.2917        1.8867  0.0007  0.1120\n",
      "     38            0.9000        \u001b[32m0.2313\u001b[0m       0.2812            0.2812        1.9972  0.0007  0.1129\n",
      "     39            0.9000        \u001b[32m0.1544\u001b[0m       0.2708            0.2708        2.1059  0.0007  0.1124\n",
      "     40            0.9000        0.2247       0.2812            0.2812        2.1975  0.0007  0.1123\n",
      "     41            \u001b[36m0.9500\u001b[0m        0.1806       0.2708            0.2708        2.2543  0.0007  0.1175\n",
      "     42            \u001b[36m1.0000\u001b[0m        0.1717       0.2708            0.2708        2.2803  0.0007  0.1121\n",
      "     43            1.0000        \u001b[32m0.0990\u001b[0m       0.2708            0.2708        2.2880  0.0006  0.1640\n",
      "     44            1.0000        0.0997       0.2917            0.2917        2.2767  0.0006  0.1388\n",
      "     45            1.0000        0.1086       0.2917            0.2917        2.2537  0.0005  0.1160\n",
      "     46            1.0000        \u001b[32m0.0830\u001b[0m       0.2917            0.2917        2.2236  0.0005  0.1131\n",
      "     47            1.0000        0.0892       0.2917            0.2917        2.1917  0.0004  0.1049\n",
      "     48            1.0000        \u001b[32m0.0826\u001b[0m       0.2812            0.2812        2.1605  0.0004  0.1123\n",
      "     49            1.0000        \u001b[32m0.0628\u001b[0m       0.2812            0.2812        2.1329  0.0003  0.1143\n",
      "     50            1.0000        \u001b[32m0.0506\u001b[0m       0.2604            0.2604        2.1104  0.0003  0.1093\n",
      "Fine tuning model for subject 6 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5500        1.2428       0.3646            0.3646        1.7017  0.0004  0.1132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6500        0.9417       0.3646            0.3646        1.6679  0.0005  0.1347\n",
      "     33            \u001b[36m0.9000\u001b[0m        0.9215       0.3750            0.3750        1.6271  0.0005  0.1308\n",
      "     34            0.9000        \u001b[32m0.6513\u001b[0m       0.3542            0.3542        1.5973  0.0006  0.1433\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4263\u001b[0m       0.3021            0.3021        1.5933  0.0006  0.1414\n",
      "     36            1.0000        \u001b[32m0.2991\u001b[0m       0.3125            0.3125        1.6279  0.0007  0.1410\n",
      "     37            1.0000        \u001b[32m0.1785\u001b[0m       0.3229            0.3229        1.6959  0.0007  0.1438\n",
      "     38            1.0000        \u001b[32m0.1134\u001b[0m       0.3646            0.3646        1.7824  0.0007  0.1501\n",
      "     39            1.0000        \u001b[32m0.0737\u001b[0m       0.3646            0.3646        1.8704  0.0007  0.1329\n",
      "     40            1.0000        0.0818       0.3646            0.3646        1.9529  0.0007  0.1422\n",
      "     41            1.0000        \u001b[32m0.0690\u001b[0m       0.3646            0.3646        2.0191  0.0007  0.1394\n",
      "     42            1.0000        0.0846       0.3542            0.3542        2.0751  0.0007  0.1397\n",
      "     43            1.0000        \u001b[32m0.0397\u001b[0m       0.3542            0.3542        2.1104  0.0006  0.1338\n",
      "     44            1.0000        \u001b[32m0.0394\u001b[0m       0.3438            0.3438        2.1256  0.0006  0.1353\n",
      "     45            1.0000        \u001b[32m0.0374\u001b[0m       0.3646            0.3646        2.1277  0.0005  0.1304\n",
      "     46            1.0000        \u001b[32m0.0319\u001b[0m       0.3750            0.3750        2.1196  0.0005  0.1266\n",
      "     47            1.0000        0.0541       0.3542            0.3542        2.1021  0.0004  0.1496\n",
      "     48            1.0000        \u001b[32m0.0176\u001b[0m       0.3542            0.3542        2.0812  0.0004  0.1610\n",
      "     49            1.0000        0.0434       0.3333            0.3333        2.0584  0.0003  0.1336\n",
      "     50            1.0000        0.0223       0.3438            0.3438        2.0357  0.0003  0.1541\n",
      "Fine tuning model for subject 6 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.7555       0.3333            0.3333        1.7237  0.0004  0.1125\n",
      "     32            0.6500        1.7166       0.3542            0.3542        1.7396  0.0005  0.1172\n",
      "     33            0.7500        1.2303       0.3646            0.3646        1.7592  0.0005  0.1083\n",
      "     34            \u001b[36m0.8500\u001b[0m        0.8744       0.3750            0.3750        1.7900  0.0006  0.1089\n",
      "     35            \u001b[36m0.9000\u001b[0m        0.7645       0.3229            0.3229        1.8325  0.0006  0.1150\n",
      "     36            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4986\u001b[0m       0.2708            0.2708        1.8604  0.0007  0.1264\n",
      "     37            0.9500        \u001b[32m0.3705\u001b[0m       0.2812            0.2812        1.8597  0.0007  0.1119\n",
      "     38            0.9500        \u001b[32m0.2530\u001b[0m       0.3229            0.3229        1.8505  0.0007  0.1073\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1549\u001b[0m       0.3438            0.3438        1.8402  0.0007  0.1163\n",
      "     40            1.0000        0.1564       0.3333            0.3333        1.8277  0.0007  0.1061\n",
      "     41            1.0000        \u001b[32m0.1099\u001b[0m       0.3333            0.3333        1.8196  0.0007  0.1165\n",
      "     42            1.0000        0.1310       0.3438            0.3438        1.8106  0.0007  0.1130\n",
      "     43            1.0000        \u001b[32m0.0761\u001b[0m       0.3542            0.3542        1.8011  0.0006  0.1133\n",
      "     44            1.0000        \u001b[32m0.0701\u001b[0m       0.3438            0.3438        1.7950  0.0006  0.1088\n",
      "     45            1.0000        \u001b[32m0.0689\u001b[0m       0.3438            0.3438        1.7891  0.0005  0.1128\n",
      "     46            1.0000        \u001b[32m0.0673\u001b[0m       0.3646            0.3646        1.7846  0.0005  0.1147\n",
      "     47            1.0000        \u001b[32m0.0544\u001b[0m       0.3542            0.3542        1.7821  0.0004  0.1153\n",
      "     48            1.0000        \u001b[32m0.0319\u001b[0m       0.3542            0.3542        1.7798  0.0004  0.1112\n",
      "     49            1.0000        0.0364       0.3542            0.3542        1.7778  0.0003  0.1228\n",
      "     50            1.0000        0.0437       0.3542            0.3542        1.7758  0.0003  0.1135\n",
      "Fine tuning model for subject 6 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4400        1.9304       0.3438            0.3438        1.6976  0.0004  0.1222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4800        1.8057       0.3542            0.3542        1.6634  0.0005  0.1289\n",
      "     33            0.5600        1.3462       0.3542            0.3542        1.6303  0.0005  0.1292\n",
      "     34            0.7600        1.0515       0.3542            0.3542        1.6056  0.0006  0.1385\n",
      "     35            \u001b[36m0.8800\u001b[0m        \u001b[32m0.6627\u001b[0m       0.3229            0.3229        1.5959  0.0006  0.1448\n",
      "     36            \u001b[36m0.9600\u001b[0m        \u001b[32m0.4437\u001b[0m       0.3125            0.3125        1.6106  0.0007  0.1194\n",
      "     37            0.9600        \u001b[32m0.3408\u001b[0m       0.2812            0.2812        1.6544  0.0007  0.1332\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3159\u001b[0m       0.2812            0.2812        1.7188  0.0007  0.1404\n",
      "     39            1.0000        \u001b[32m0.3102\u001b[0m       0.2917            0.2917        1.7867  0.0007  0.1237\n",
      "     40            1.0000        \u001b[32m0.3092\u001b[0m       0.2812            0.2812        1.8539  0.0007  0.1167\n",
      "     41            1.0000        \u001b[32m0.1637\u001b[0m       0.3021            0.3021        1.9231  0.0007  0.1217\n",
      "     42            1.0000        \u001b[32m0.1125\u001b[0m       0.2917            0.2917        1.9908  0.0007  0.1179\n",
      "     43            1.0000        \u001b[32m0.0779\u001b[0m       0.2917            0.2917        2.0543  0.0006  0.1226\n",
      "     44            1.0000        \u001b[32m0.0690\u001b[0m       0.3021            0.3021        2.1135  0.0006  0.1218\n",
      "     45            1.0000        0.0811       0.2917            0.2917        2.1675  0.0005  0.1268\n",
      "     46            1.0000        \u001b[32m0.0580\u001b[0m       0.3021            0.3021        2.2127  0.0005  0.1218\n",
      "     47            1.0000        0.0739       0.3438            0.3438        2.2535  0.0004  0.1238\n",
      "     48            1.0000        \u001b[32m0.0385\u001b[0m       0.3438            0.3438        2.2894  0.0004  0.1254\n",
      "     49            1.0000        \u001b[32m0.0381\u001b[0m       0.3542            0.3542        2.3203  0.0003  0.1183\n",
      "     50            1.0000        0.0637       0.3646            0.3646        2.3487  0.0003  0.1251\n",
      "Fine tuning model for subject 6 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4800        1.6390       0.3542            0.3542        1.7205  0.0004  0.1218\n",
      "     32            0.5600        1.3673       0.3646            0.3646        1.7218  0.0005  0.1225\n",
      "     33            0.7200        1.2288       0.3542            0.3542        1.7281  0.0005  0.1193\n",
      "     34            0.8000        0.7815       0.3750            0.3750        1.7528  0.0006  0.1222\n",
      "     35            0.7200        0.7515       0.3542            0.3542        1.7715  0.0006  0.1300\n",
      "     36            \u001b[36m0.8400\u001b[0m        \u001b[32m0.4188\u001b[0m       0.3646            0.3646        1.7791  0.0007  0.1221\n",
      "     37            \u001b[36m0.9200\u001b[0m        \u001b[32m0.3789\u001b[0m       0.3438            0.3438        1.7697  0.0007  0.1262\n",
      "     38            \u001b[36m0.9600\u001b[0m        \u001b[32m0.2839\u001b[0m       0.3333            0.3333        1.7658  0.0007  0.1211\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2361\u001b[0m       0.3333            0.3333        1.7713  0.0007  0.1170\n",
      "     40            1.0000        0.2989       0.3229            0.3229        1.7852  0.0007  0.1170\n",
      "     41            1.0000        \u001b[32m0.1534\u001b[0m       0.3125            0.3125        1.8107  0.0007  0.1234\n",
      "     42            1.0000        0.1901       0.3333            0.3333        1.8388  0.0007  0.1222\n",
      "     43            1.0000        \u001b[32m0.1506\u001b[0m       0.3021            0.3021        1.8675  0.0006  0.1281\n",
      "     44            1.0000        \u001b[32m0.0730\u001b[0m       0.2917            0.2917        1.8937  0.0006  0.1211\n",
      "     45            1.0000        0.0800       0.2812            0.2812        1.9158  0.0005  0.1206\n",
      "     46            1.0000        0.0938       0.2708            0.2708        1.9320  0.0005  0.1223\n",
      "     47            1.0000        \u001b[32m0.0681\u001b[0m       0.2917            0.2917        1.9434  0.0004  0.1228\n",
      "     48            1.0000        0.0965       0.2708            0.2708        1.9508  0.0004  0.1174\n",
      "     49            1.0000        0.0796       0.2917            0.2917        1.9550  0.0003  0.1262\n",
      "     50            1.0000        0.0875       0.2812            0.2812        1.9578  0.0003  0.1209\n",
      "Fine tuning model for subject 6 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4800        1.1470       0.3542            0.3542        1.7194  0.0004  0.1251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4800        1.2530       0.3646            0.3646        1.7111  0.0005  0.1185\n",
      "     33            0.6400        0.9857       0.3750            0.3750        1.6952  0.0005  0.1171\n",
      "     34            \u001b[36m0.8800\u001b[0m        0.7679       0.3542            0.3542        1.6785  0.0006  0.1208\n",
      "     35            \u001b[36m0.9600\u001b[0m        \u001b[32m0.5102\u001b[0m       0.3438            0.3438        1.6541  0.0006  0.1223\n",
      "     36            0.9600        \u001b[32m0.4675\u001b[0m       0.3438            0.3438        1.6168  0.0007  0.1209\n",
      "     37            0.9600        \u001b[32m0.3641\u001b[0m       0.3854            0.3854        1.5940  0.0007  0.1181\n",
      "     38            0.9600        \u001b[32m0.2322\u001b[0m       0.3750            0.3750        1.5840  0.0007  0.1174\n",
      "     39            0.9600        \u001b[32m0.2084\u001b[0m       0.3646            0.3646        1.5862  0.0007  0.1140\n",
      "     40            0.9600        \u001b[32m0.1247\u001b[0m       0.3854            0.3854        1.5923  0.0007  0.1214\n",
      "     41            0.9600        \u001b[32m0.1159\u001b[0m       0.4062            0.4062        1.6000  0.0007  0.1222\n",
      "     42            0.9600        \u001b[32m0.0890\u001b[0m       0.4062            0.4062        1.6091  0.0007  0.1171\n",
      "     43            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0804\u001b[0m       0.3750            0.3750        1.6189  0.0006  0.1240\n",
      "     44            1.0000        0.0918       0.3646            0.3646        1.6308  0.0006  0.1237\n",
      "     45            1.0000        \u001b[32m0.0753\u001b[0m       0.3646            0.3646        1.6423  0.0005  0.1206\n",
      "     46            1.0000        \u001b[32m0.0474\u001b[0m       0.3750            0.3750        1.6527  0.0005  0.1173\n",
      "     47            1.0000        0.0640       0.3646            0.3646        1.6632  0.0004  0.1183\n",
      "     48            1.0000        \u001b[32m0.0303\u001b[0m       0.3646            0.3646        1.6720  0.0004  0.1240\n",
      "     49            1.0000        0.0425       0.3750            0.3750        1.6794  0.0003  0.1250\n",
      "     50            1.0000        0.0322       0.3750            0.3750        1.6854  0.0003  0.1234\n",
      "Fine tuning model for subject 6 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5600        1.2387       0.3542            0.3542        1.7278  0.0004  0.1221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5600        0.9390       0.3854            0.3854        1.7567  0.0005  0.1165\n",
      "     33            \u001b[36m0.8800\u001b[0m        0.9132       0.3854            0.3854        1.7983  0.0005  0.1177\n",
      "     34            \u001b[36m0.9600\u001b[0m        0.7137       0.3542            0.3542        1.8614  0.0006  0.1161\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4885\u001b[0m       0.3125            0.3125        1.9519  0.0006  0.1214\n",
      "     36            1.0000        \u001b[32m0.3073\u001b[0m       0.3125            0.3125        2.0485  0.0007  0.1217\n",
      "     37            1.0000        \u001b[32m0.2661\u001b[0m       0.2917            0.2917        2.1434  0.0007  0.1220\n",
      "     38            1.0000        \u001b[32m0.1643\u001b[0m       0.2708            0.2708        2.2178  0.0007  0.1211\n",
      "     39            1.0000        0.1647       0.2812            0.2812        2.2725  0.0007  0.1601\n",
      "     40            1.0000        \u001b[32m0.0781\u001b[0m       0.2812            0.2812        2.3070  0.0007  0.1456\n",
      "     41            1.0000        0.0956       0.3021            0.3021        2.3171  0.0007  0.1497\n",
      "     42            1.0000        \u001b[32m0.0568\u001b[0m       0.3021            0.3021        2.3170  0.0007  0.1364\n",
      "     43            1.0000        0.0697       0.3125            0.3125        2.3066  0.0006  0.1402\n",
      "     44            1.0000        0.0729       0.3229            0.3229        2.2938  0.0006  0.1371\n",
      "     45            1.0000        \u001b[32m0.0412\u001b[0m       0.3333            0.3333        2.2789  0.0005  0.1529\n",
      "     46            1.0000        0.0603       0.3333            0.3333        2.2596  0.0005  0.1528\n",
      "     47            1.0000        \u001b[32m0.0389\u001b[0m       0.3229            0.3229        2.2419  0.0004  0.1570\n",
      "     48            1.0000        \u001b[32m0.0288\u001b[0m       0.3229            0.3229        2.2283  0.0004  0.1419\n",
      "     49            1.0000        0.0361       0.3021            0.3021        2.2165  0.0003  0.1442\n",
      "     50            1.0000        0.0301       0.2917            0.2917        2.2072  0.0003  0.1387\n",
      "Fine tuning model for subject 6 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4800        1.7401       0.3542            0.3542        1.7109  0.0004  0.1633\n",
      "     32            0.4800        1.6630       0.3750            0.3750        1.6984  0.0005  0.1401\n",
      "     33            0.5200        1.0159       0.3646            0.3646        1.6868  0.0005  0.1497\n",
      "     34            0.7200        1.0072       0.3333            0.3333        1.6785  0.0006  0.1708\n",
      "     35            \u001b[36m0.8800\u001b[0m        0.7100       0.3229            0.3229        1.6669  0.0006  0.1466\n",
      "     36            \u001b[36m0.9600\u001b[0m        \u001b[32m0.4027\u001b[0m       0.3125            0.3125        1.6641  0.0007  0.1604\n",
      "     37            0.9600        \u001b[32m0.3874\u001b[0m       0.3229            0.3229        1.6796  0.0007  0.1259\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2791\u001b[0m       0.3125            0.3125        1.6987  0.0007  0.1235\n",
      "     39            1.0000        \u001b[32m0.2761\u001b[0m       0.3333            0.3333        1.7191  0.0007  0.1215\n",
      "     40            1.0000        0.3206       0.3333            0.3333        1.7338  0.0007  0.1289\n",
      "     41            1.0000        \u001b[32m0.2145\u001b[0m       0.3229            0.3229        1.7436  0.0007  0.1279\n",
      "     42            1.0000        \u001b[32m0.1781\u001b[0m       0.3646            0.3646        1.7513  0.0007  0.1210\n",
      "     43            1.0000        \u001b[32m0.1319\u001b[0m       0.3542            0.3542        1.7565  0.0006  0.1224\n",
      "     44            1.0000        \u001b[32m0.1161\u001b[0m       0.3646            0.3646        1.7598  0.0006  0.1207\n",
      "     45            1.0000        \u001b[32m0.1031\u001b[0m       0.3750            0.3750        1.7625  0.0005  0.1227\n",
      "     46            1.0000        \u001b[32m0.0701\u001b[0m       0.3750            0.3750        1.7644  0.0005  0.1165\n",
      "     47            1.0000        \u001b[32m0.0564\u001b[0m       0.3750            0.3750        1.7670  0.0004  0.1206\n",
      "     48            1.0000        0.0666       0.3750            0.3750        1.7690  0.0004  0.1229\n",
      "     49            1.0000        0.0571       0.3750            0.3750        1.7708  0.0003  0.1257\n",
      "     50            1.0000        0.0663       0.3750            0.3750        1.7727  0.0003  0.1210\n",
      "Fine tuning model for subject 6 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.2180       0.3438            0.3438        1.7058  0.0004  0.1212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.0742       0.3646            0.3646        1.6829  0.0005  0.1266\n",
      "     33            0.6800        0.9115       0.3750            0.3750        1.6548  0.0005  0.1237\n",
      "     34            \u001b[36m0.8400\u001b[0m        0.8228       0.3750            0.3750        1.6330  0.0006  0.1237\n",
      "     35            \u001b[36m0.9600\u001b[0m        \u001b[32m0.5863\u001b[0m       0.3750            0.3750        1.6197  0.0006  0.1158\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3894\u001b[0m       0.3854            0.3854        1.6169  0.0007  0.1163\n",
      "     37            1.0000        \u001b[32m0.2338\u001b[0m       0.3958            0.3958        1.6239  0.0007  0.1247\n",
      "     38            1.0000        \u001b[32m0.2112\u001b[0m       0.4167            0.4167        1.6357  0.0007  0.1186\n",
      "     39            1.0000        \u001b[32m0.1662\u001b[0m       0.4271            0.4271        1.6435  0.0007  0.1170\n",
      "     40            1.0000        \u001b[32m0.1307\u001b[0m       0.4271            0.4271        1.6441  0.0007  0.1227\n",
      "     41            1.0000        \u001b[32m0.1194\u001b[0m       0.4271            0.4271        1.6357  0.0007  0.1232\n",
      "     42            1.0000        \u001b[32m0.1165\u001b[0m       0.4271            0.4271        1.6166  0.0007  0.1207\n",
      "     43            1.0000        \u001b[32m0.0775\u001b[0m       0.4479            0.4479        1.5923  0.0006  0.1229\n",
      "     44            1.0000        0.0870       0.4583            0.4583        1.5692  0.0006  0.1259\n",
      "     45            1.0000        \u001b[32m0.0701\u001b[0m       0.4583            0.4583        1.5463  0.0005  0.1187\n",
      "     46            1.0000        \u001b[32m0.0551\u001b[0m       0.4479            0.4479        1.5277  0.0005  0.1176\n",
      "     47            1.0000        0.0685       0.4375            0.4375        1.5136  0.0004  0.1225\n",
      "     48            1.0000        0.0889       0.4271            0.4271        1.5031  0.0004  0.1250\n",
      "     49            1.0000        \u001b[32m0.0356\u001b[0m       0.4167            0.4167        1.4958  0.0003  0.1203\n",
      "     50            1.0000        0.0475       0.4375            0.4375        1.4912  0.0003  0.1223\n",
      "Fine tuning model for subject 6 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4800        1.6171       0.3542            0.3542        1.7157  0.0004  0.1190\n",
      "     32            0.5200        1.5943       0.3646            0.3646        1.7013  0.0005  0.1251\n",
      "     33            0.6400        1.2688       0.3542            0.3542        1.6865  0.0005  0.1180\n",
      "     34            0.8000        1.0175       0.3542            0.3542        1.6881  0.0006  0.1263\n",
      "     35            \u001b[36m0.9600\u001b[0m        0.8108       0.3125            0.3125        1.7093  0.0006  0.1233\n",
      "     36            0.9600        \u001b[32m0.5584\u001b[0m       0.3333            0.3333        1.7579  0.0007  0.1206\n",
      "     37            0.9600        \u001b[32m0.2565\u001b[0m       0.3333            0.3333        1.8317  0.0007  0.1201\n",
      "     38            0.9600        0.2567       0.3333            0.3333        1.9311  0.0007  0.1190\n",
      "     39            0.9600        \u001b[32m0.2563\u001b[0m       0.3229            0.3229        2.0426  0.0007  0.1220\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1356\u001b[0m       0.3438            0.3438        2.1483  0.0007  0.1244\n",
      "     41            0.9600        \u001b[32m0.1077\u001b[0m       0.3542            0.3542        2.2377  0.0007  0.1226\n",
      "     42            0.9600        \u001b[32m0.0998\u001b[0m       0.3542            0.3542        2.2956  0.0007  0.1197\n",
      "     43            0.9600        0.1585       0.3438            0.3438        2.3172  0.0006  0.1245\n",
      "     44            0.9600        \u001b[32m0.0796\u001b[0m       0.3438            0.3438        2.3117  0.0006  0.1344\n",
      "     45            1.0000        0.0815       0.3333            0.3333        2.2821  0.0005  0.1383\n",
      "     46            1.0000        0.0852       0.3333            0.3333        2.2388  0.0005  0.1450\n",
      "     47            1.0000        \u001b[32m0.0506\u001b[0m       0.3333            0.3333        2.1944  0.0004  0.1479\n",
      "     48            1.0000        0.0663       0.3333            0.3333        2.1478  0.0004  0.1200\n",
      "     49            1.0000        0.1111       0.3438            0.3438        2.0984  0.0003  0.1219\n",
      "     50            1.0000        \u001b[32m0.0342\u001b[0m       0.3542            0.3542        2.0572  0.0003  0.1174\n",
      "Fine tuning model for subject 6 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.5830       0.3542            0.3542        1.7021  0.0004  0.1185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4800        1.5113       0.3333            0.3333        1.6684  0.0005  0.1215\n",
      "     33            0.7200        1.1329       0.3333            0.3333        1.6449  0.0005  0.1206\n",
      "     34            \u001b[36m0.8400\u001b[0m        0.9916       0.3438            0.3438        1.6418  0.0006  0.1240\n",
      "     35            \u001b[36m0.8800\u001b[0m        \u001b[32m0.6552\u001b[0m       0.3542            0.3542        1.6623  0.0006  0.1193\n",
      "     36            \u001b[36m0.9200\u001b[0m        \u001b[32m0.5050\u001b[0m       0.3854            0.3854        1.6975  0.0007  0.1215\n",
      "     37            \u001b[36m0.9600\u001b[0m        \u001b[32m0.3206\u001b[0m       0.3438            0.3438        1.7390  0.0007  0.1201\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2478\u001b[0m       0.3542            0.3542        1.7661  0.0007  0.1236\n",
      "     39            1.0000        \u001b[32m0.2032\u001b[0m       0.3229            0.3229        1.7814  0.0007  0.1180\n",
      "     40            1.0000        \u001b[32m0.1412\u001b[0m       0.3229            0.3229        1.7942  0.0007  0.1246\n",
      "     41            1.0000        0.2182       0.3333            0.3333        1.8128  0.0007  0.1209\n",
      "     42            1.0000        0.1699       0.3542            0.3542        1.8351  0.0007  0.1218\n",
      "     43            1.0000        \u001b[32m0.1269\u001b[0m       0.3542            0.3542        1.8553  0.0006  0.1221\n",
      "     44            1.0000        \u001b[32m0.1025\u001b[0m       0.3646            0.3646        1.8755  0.0006  0.1230\n",
      "     45            1.0000        0.1055       0.3542            0.3542        1.8955  0.0005  0.1252\n",
      "     46            1.0000        \u001b[32m0.0711\u001b[0m       0.3542            0.3542        1.9114  0.0005  0.1302\n",
      "     47            1.0000        \u001b[32m0.0555\u001b[0m       0.3750            0.3750        1.9228  0.0004  0.1174\n",
      "     48            1.0000        \u001b[32m0.0539\u001b[0m       0.3854            0.3854        1.9307  0.0004  0.1177\n",
      "     49            1.0000        0.0838       0.3854            0.3854        1.9369  0.0003  0.1157\n",
      "     50            1.0000        0.0700       0.3854            0.3854        1.9415  0.0003  0.1244\n",
      "Fine tuning model for subject 6 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5600        1.5288       0.3542            0.3542        1.7103  0.0004  0.1276\n",
      "     32            0.6800        1.4179       0.3750            0.3750        1.6955  0.0005  0.1198\n",
      "     33            0.7200        1.0870       0.3646            0.3646        1.6837  0.0005  0.1265\n",
      "     34            \u001b[36m0.9600\u001b[0m        0.8056       0.3750            0.3750        1.6768  0.0006  0.1170\n",
      "     35            0.9600        \u001b[32m0.6530\u001b[0m       0.3646            0.3646        1.6899  0.0006  0.1247\n",
      "     36            0.9600        \u001b[32m0.4572\u001b[0m       0.3229            0.3229        1.7227  0.0007  0.1189\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3243\u001b[0m       0.3021            0.3021        1.7627  0.0007  0.1169\n",
      "     38            1.0000        \u001b[32m0.2135\u001b[0m       0.3021            0.3021        1.8041  0.0007  0.1216\n",
      "     39            1.0000        \u001b[32m0.1830\u001b[0m       0.3125            0.3125        1.8397  0.0007  0.1216\n",
      "     40            1.0000        0.2689       0.3125            0.3125        1.8648  0.0007  0.1193\n",
      "     41            1.0000        \u001b[32m0.1149\u001b[0m       0.2917            0.2917        1.8822  0.0007  0.1241\n",
      "     42            1.0000        \u001b[32m0.0942\u001b[0m       0.2917            0.2917        1.8955  0.0007  0.1466\n",
      "     43            1.0000        0.1156       0.2708            0.2708        1.9068  0.0006  0.1601\n",
      "     44            1.0000        0.0971       0.2812            0.2812        1.9124  0.0006  0.1346\n",
      "     45            1.0000        \u001b[32m0.0690\u001b[0m       0.2812            0.2812        1.9154  0.0005  0.1384\n",
      "     46            1.0000        0.0697       0.2812            0.2812        1.9175  0.0005  0.1501\n",
      "     47            1.0000        0.0781       0.3021            0.3021        1.9184  0.0004  0.1491\n",
      "     48            1.0000        \u001b[32m0.0421\u001b[0m       0.2812            0.2812        1.9189  0.0004  0.1413\n",
      "     49            1.0000        0.0811       0.2812            0.2812        1.9192  0.0003  0.1385\n",
      "     50            1.0000        \u001b[32m0.0371\u001b[0m       0.2812            0.2812        1.9197  0.0003  0.1459\n",
      "Fine tuning model for subject 6 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3600        1.5224       0.3438            0.3438        1.7032  0.0004  0.1866\n",
      "     32            0.4800        1.4561       0.3333            0.3333        1.6763  0.0005  0.1403\n",
      "     33            0.5600        1.2760       0.3333            0.3333        1.6509  0.0005  0.1368\n",
      "     34            0.6800        1.0595       0.3333            0.3333        1.6353  0.0006  0.1453\n",
      "     35            \u001b[36m0.8800\u001b[0m        0.7663       0.3229            0.3229        1.6304  0.0006  0.1426\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4744\u001b[0m       0.3438            0.3438        1.6381  0.0007  0.1560\n",
      "     37            1.0000        0.4987       0.3125            0.3125        1.6692  0.0007  0.1481\n",
      "     38            1.0000        \u001b[32m0.2786\u001b[0m       0.3229            0.3229        1.7141  0.0007  0.1536\n",
      "     39            1.0000        \u001b[32m0.2089\u001b[0m       0.3021            0.3021        1.7655  0.0007  0.1722\n",
      "     40            1.0000        \u001b[32m0.1414\u001b[0m       0.2917            0.2917        1.8139  0.0007  0.1655\n",
      "     41            1.0000        0.1797       0.3125            0.3125        1.8476  0.0007  0.1264\n",
      "     42            1.0000        \u001b[32m0.0755\u001b[0m       0.3125            0.3125        1.8745  0.0007  0.1197\n",
      "     43            1.0000        0.1740       0.3229            0.3229        1.8926  0.0006  0.1211\n",
      "     44            1.0000        \u001b[32m0.0596\u001b[0m       0.3125            0.3125        1.9061  0.0006  0.1314\n",
      "     45            1.0000        0.1069       0.3125            0.3125        1.9151  0.0005  0.1211\n",
      "     46            1.0000        0.0965       0.3125            0.3125        1.9183  0.0005  0.1208\n",
      "     47            1.0000        \u001b[32m0.0505\u001b[0m       0.3333            0.3333        1.9193  0.0004  0.1175\n",
      "     48            1.0000        0.1136       0.3333            0.3333        1.9164  0.0004  0.1223\n",
      "     49            1.0000        0.0632       0.3229            0.3229        1.9124  0.0003  0.1181\n",
      "     50            1.0000        0.0609       0.3333            0.3333        1.9080  0.0003  0.1300\n",
      "Fine tuning model for subject 6 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.4832       0.3438            0.3438        1.7103  0.0004  0.1269\n",
      "     32            0.4333        1.6814       0.3438            0.3438        1.6897  0.0005  0.1320\n",
      "     33            0.5667        1.2672       0.3646            0.3646        1.6642  0.0005  0.1339\n",
      "     34            0.6667        0.8179       0.3333            0.3333        1.6488  0.0006  0.2103\n",
      "     35            0.8000        0.8433       0.3750            0.3750        1.6536  0.0006  0.1327\n",
      "     36            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5555\u001b[0m       0.3542            0.3542        1.6720  0.0007  0.1284\n",
      "     37            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3753\u001b[0m       0.3646            0.3646        1.7113  0.0007  0.1336\n",
      "     38            0.9667        0.4179       0.3750            0.3750        1.7833  0.0007  0.1322\n",
      "     39            0.9000        \u001b[32m0.3003\u001b[0m       0.3854            0.3854        1.8595  0.0007  0.1281\n",
      "     40            0.9000        \u001b[32m0.2403\u001b[0m       0.4167            0.4167        1.9247  0.0007  0.1256\n",
      "     41            0.9333        \u001b[32m0.2028\u001b[0m       0.4062            0.4062        1.9779  0.0007  0.1295\n",
      "     42            0.9667        \u001b[32m0.1669\u001b[0m       0.4062            0.4062        2.0082  0.0007  0.1389\n",
      "     43            0.9667        0.1877       0.3646            0.3646        2.0183  0.0006  0.1276\n",
      "     44            0.9667        \u001b[32m0.1414\u001b[0m       0.3542            0.3542        2.0137  0.0006  0.1314\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1318\u001b[0m       0.3542            0.3542        1.9974  0.0005  0.1301\n",
      "     46            1.0000        \u001b[32m0.1165\u001b[0m       0.3646            0.3646        1.9722  0.0005  0.1279\n",
      "     47            1.0000        \u001b[32m0.1098\u001b[0m       0.3438            0.3438        1.9451  0.0004  0.1280\n",
      "     48            1.0000        \u001b[32m0.0793\u001b[0m       0.3333            0.3333        1.9211  0.0004  0.1339\n",
      "     49            1.0000        0.1141       0.3750            0.3750        1.8987  0.0003  0.1234\n",
      "     50            1.0000        0.0955       0.3646            0.3646        1.8805  0.0003  0.1295\n",
      "Fine tuning model for subject 6 with 30 = 30 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4333        1.5293       0.3438            0.3438        1.6986  0.0004  0.1276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4667        1.1788       0.3646            0.3646        1.6666  0.0005  0.1268\n",
      "     33            0.5333        1.1819       0.3542            0.3542        1.6307  0.0005  0.1329\n",
      "     34            0.6333        1.2134       0.3750            0.3750        1.6039  0.0006  0.1288\n",
      "     35            \u001b[36m0.8333\u001b[0m        0.7756       0.3854            0.3854        1.6021  0.0006  0.1309\n",
      "     36            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5644\u001b[0m       0.3333            0.3333        1.6255  0.0007  0.1277\n",
      "     37            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4334\u001b[0m       0.3438            0.3438        1.6658  0.0007  0.1285\n",
      "     38            0.9333        \u001b[32m0.2480\u001b[0m       0.3646            0.3646        1.7164  0.0007  0.1282\n",
      "     39            0.9333        \u001b[32m0.1705\u001b[0m       0.3750            0.3750        1.7720  0.0007  0.1255\n",
      "     40            0.9333        0.2355       0.3750            0.3750        1.8171  0.0007  0.1369\n",
      "     41            0.9333        0.1987       0.3646            0.3646        1.8496  0.0007  0.1265\n",
      "     42            \u001b[36m0.9667\u001b[0m        \u001b[32m0.1684\u001b[0m       0.3542            0.3542        1.8648  0.0007  0.1287\n",
      "     43            0.9667        \u001b[32m0.1187\u001b[0m       0.3646            0.3646        1.8703  0.0006  0.1332\n",
      "     44            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0844\u001b[0m       0.3646            0.3646        1.8705  0.0006  0.1234\n",
      "     45            1.0000        0.1205       0.3646            0.3646        1.8623  0.0005  0.1269\n",
      "     46            1.0000        0.1126       0.3750            0.3750        1.8530  0.0005  0.1250\n",
      "     47            1.0000        \u001b[32m0.0601\u001b[0m       0.3646            0.3646        1.8461  0.0004  0.1333\n",
      "     48            1.0000        0.0968       0.3646            0.3646        1.8369  0.0004  0.1282\n",
      "     49            1.0000        0.0772       0.3542            0.3542        1.8284  0.0003  0.1289\n",
      "     50            1.0000        0.0847       0.3542            0.3542        1.8203  0.0003  0.1334\n",
      "Fine tuning model for subject 6 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3333        1.6118       0.3438            0.3438        1.6955  0.0004  0.1407\n",
      "     32            0.4333        1.6060       0.3438            0.3438        1.6550  0.0005  0.1339\n",
      "     33            0.5333        1.2013       0.3750            0.3750        1.6080  0.0005  0.1280\n",
      "     34            0.7000        1.0875       0.3750            0.3750        1.5759  0.0006  0.1269\n",
      "     35            0.8000        0.8361       0.3750            0.3750        1.5795  0.0006  0.1280\n",
      "     36            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6481\u001b[0m       0.4062            0.4062        1.6147  0.0007  0.1359\n",
      "     37            \u001b[36m0.9000\u001b[0m        \u001b[32m0.3683\u001b[0m       0.3854            0.3854        1.6900  0.0007  0.1216\n",
      "     38            \u001b[36m0.9333\u001b[0m        \u001b[32m0.2792\u001b[0m       0.3646            0.3646        1.7973  0.0007  0.1232\n",
      "     39            0.9000        \u001b[32m0.1760\u001b[0m       0.3646            0.3646        1.9178  0.0007  0.1281\n",
      "     40            0.8667        0.1872       0.3438            0.3438        2.0227  0.0007  0.1263\n",
      "     41            0.8667        0.1801       0.3438            0.3438        2.1020  0.0007  0.1231\n",
      "     42            0.8667        \u001b[32m0.1673\u001b[0m       0.3646            0.3646        2.1369  0.0007  0.1289\n",
      "     43            0.9000        0.1707       0.3646            0.3646        2.1395  0.0006  0.1280\n",
      "     44            0.9333        \u001b[32m0.1530\u001b[0m       0.3438            0.3438        2.1123  0.0006  0.2003\n",
      "     45            \u001b[36m0.9667\u001b[0m        \u001b[32m0.1213\u001b[0m       0.3438            0.3438        2.0645  0.0005  0.1326\n",
      "     46            0.9667        \u001b[32m0.1011\u001b[0m       0.3542            0.3542        2.0095  0.0005  0.1278\n",
      "     47            \u001b[36m1.0000\u001b[0m        0.1038       0.3438            0.3438        1.9515  0.0004  0.1411\n",
      "     48            1.0000        \u001b[32m0.0801\u001b[0m       0.3646            0.3646        1.8968  0.0004  0.1334\n",
      "     49            1.0000        \u001b[32m0.0442\u001b[0m       0.3750            0.3750        1.8502  0.0003  0.1473\n",
      "     50            1.0000        0.0867       0.3646            0.3646        1.8097  0.0003  0.1535\n",
      "Fine tuning model for subject 6 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        1.1671       0.3542            0.3542        1.7099  0.0004  0.1522\n",
      "     32            0.7000        1.3488       0.3646            0.3646        1.7047  0.0005  0.1285\n",
      "     33            0.7000        0.8005       0.3542            0.3542        1.6957  0.0005  0.1290\n",
      "     34            0.7333        0.8138       0.3229            0.3229        1.6916  0.0006  0.1270\n",
      "     35            0.8000        \u001b[32m0.6567\u001b[0m       0.3333            0.3333        1.6882  0.0006  0.1284\n",
      "     36            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4132\u001b[0m       0.3333            0.3333        1.6806  0.0007  0.1297\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3488\u001b[0m       0.3333            0.3333        1.6747  0.0007  0.1337\n",
      "     38            1.0000        \u001b[32m0.2255\u001b[0m       0.3438            0.3438        1.6650  0.0007  0.1347\n",
      "     39            1.0000        0.2432       0.3333            0.3333        1.6594  0.0007  0.1495\n",
      "     40            0.9667        \u001b[32m0.1716\u001b[0m       0.3438            0.3438        1.6569  0.0007  0.1606\n",
      "     41            1.0000        \u001b[32m0.1320\u001b[0m       0.3333            0.3333        1.6550  0.0007  0.1584\n",
      "     42            1.0000        \u001b[32m0.0916\u001b[0m       0.3542            0.3542        1.6560  0.0007  0.1560\n",
      "     43            1.0000        \u001b[32m0.0866\u001b[0m       0.3646            0.3646        1.6588  0.0006  0.1644\n",
      "     44            1.0000        0.1323       0.3750            0.3750        1.6623  0.0006  0.1632\n",
      "     45            1.0000        \u001b[32m0.0765\u001b[0m       0.3854            0.3854        1.6668  0.0005  0.1560\n",
      "     46            1.0000        0.0873       0.4062            0.4062        1.6728  0.0005  0.1423\n",
      "     47            1.0000        \u001b[32m0.0752\u001b[0m       0.4271            0.4271        1.6800  0.0004  0.1617\n",
      "     48            1.0000        \u001b[32m0.0692\u001b[0m       0.4271            0.4271        1.6881  0.0004  0.1685\n",
      "     49            1.0000        \u001b[32m0.0445\u001b[0m       0.4271            0.4271        1.6961  0.0003  0.1548\n",
      "     50            1.0000        \u001b[32m0.0423\u001b[0m       0.4167            0.4167        1.7040  0.0003  0.1522\n",
      "Fine tuning model for subject 6 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.2081       0.3438            0.3438        1.7073  0.0004  0.1516\n",
      "     32            0.6333        1.0716       0.3438            0.3438        1.6851  0.0005  0.1878\n",
      "     33            0.7000        0.8594       0.3438            0.3438        1.6675  0.0005  0.1648\n",
      "     34            0.8000        0.7466       0.3438            0.3438        1.6512  0.0006  0.1767\n",
      "     35            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6175\u001b[0m       0.3542            0.3542        1.6373  0.0006  0.1663\n",
      "     36            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4704\u001b[0m       0.3333            0.3333        1.6170  0.0007  0.1330\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3789\u001b[0m       0.3333            0.3333        1.5949  0.0007  0.1274\n",
      "     38            1.0000        \u001b[32m0.2518\u001b[0m       0.3333            0.3333        1.5861  0.0007  0.1485\n",
      "     39            1.0000        \u001b[32m0.1752\u001b[0m       0.3750            0.3750        1.5973  0.0007  0.1321\n",
      "     40            1.0000        \u001b[32m0.1624\u001b[0m       0.3854            0.3854        1.6342  0.0007  0.1290\n",
      "     41            0.9667        \u001b[32m0.1356\u001b[0m       0.3750            0.3750        1.6863  0.0007  0.1362\n",
      "     42            0.9667        \u001b[32m0.1217\u001b[0m       0.4062            0.4062        1.7433  0.0007  0.1278\n",
      "     43            0.9667        \u001b[32m0.0979\u001b[0m       0.4062            0.4062        1.7944  0.0006  0.1325\n",
      "     44            0.9667        \u001b[32m0.0771\u001b[0m       0.4062            0.4062        1.8365  0.0006  0.1254\n",
      "     45            0.9667        0.0907       0.4062            0.4062        1.8636  0.0005  0.1262\n",
      "     46            0.9667        \u001b[32m0.0666\u001b[0m       0.4062            0.4062        1.8760  0.0005  0.1254\n",
      "     47            0.9667        \u001b[32m0.0600\u001b[0m       0.3958            0.3958        1.8782  0.0004  0.1316\n",
      "     48            1.0000        0.1015       0.3854            0.3854        1.8763  0.0004  0.1359\n",
      "     49            1.0000        \u001b[32m0.0480\u001b[0m       0.3854            0.3854        1.8683  0.0003  0.1300\n",
      "     50            1.0000        0.0561       0.3958            0.3958        1.8574  0.0003  0.1294\n",
      "Fine tuning model for subject 6 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3333        1.5948       0.3542            0.3542        1.7010  0.0004  0.1275\n",
      "     32            0.4333        1.4874       0.3646            0.3646        1.6611  0.0005  0.1249\n",
      "     33            0.6333        1.3865       0.3438            0.3438        1.6290  0.0005  0.1318\n",
      "     34            \u001b[36m0.8667\u001b[0m        0.9419       0.3646            0.3646        1.6178  0.0006  0.1223\n",
      "     35            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6524\u001b[0m       0.3646            0.3646        1.6425  0.0006  0.1326\n",
      "     36            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4418\u001b[0m       0.4062            0.4062        1.7059  0.0007  0.1338\n",
      "     37            0.9333        \u001b[32m0.3577\u001b[0m       0.4167            0.4167        1.7989  0.0007  0.1289\n",
      "     38            0.9333        \u001b[32m0.3496\u001b[0m       0.4062            0.4062        1.9139  0.0007  0.1331\n",
      "     39            0.9333        \u001b[32m0.3053\u001b[0m       0.4167            0.4167        2.0280  0.0007  0.1186\n",
      "     40            \u001b[36m0.9667\u001b[0m        0.3057       0.3958            0.3958        2.1358  0.0007  0.1346\n",
      "     41            0.9667        \u001b[32m0.2402\u001b[0m       0.3646            0.3646        2.2253  0.0007  0.1277\n",
      "     42            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1835\u001b[0m       0.3646            0.3646        2.2892  0.0007  0.1286\n",
      "     43            1.0000        \u001b[32m0.1815\u001b[0m       0.3646            0.3646        2.3343  0.0006  0.1298\n",
      "     44            1.0000        \u001b[32m0.0971\u001b[0m       0.3542            0.3542        2.3575  0.0006  0.1352\n",
      "     45            1.0000        0.1422       0.3646            0.3646        2.3565  0.0005  0.1281\n",
      "     46            1.0000        0.1057       0.3750            0.3750        2.3420  0.0005  0.1262\n",
      "     47            1.0000        0.1013       0.3958            0.3958        2.3221  0.0004  0.1281\n",
      "     48            1.0000        \u001b[32m0.0728\u001b[0m       0.4062            0.4062        2.2960  0.0004  0.1303\n",
      "     49            1.0000        0.0919       0.3854            0.3854        2.2656  0.0003  0.1264\n",
      "     50            1.0000        \u001b[32m0.0653\u001b[0m       0.3750            0.3750        2.2344  0.0003  0.1227\n",
      "Fine tuning model for subject 6 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.6857       0.3438            0.3438        1.7054  0.0004  0.1341\n",
      "     32            0.4667        1.6974       0.3646            0.3646        1.6763  0.0005  0.1267\n",
      "     33            0.5333        1.4306       0.3646            0.3646        1.6401  0.0005  0.1327\n",
      "     34            0.6000        1.1944       0.4062            0.4062        1.6106  0.0006  0.1306\n",
      "     35            \u001b[36m0.8333\u001b[0m        0.8166       0.3958            0.3958        1.6002  0.0006  0.1381\n",
      "     36            \u001b[36m0.8667\u001b[0m        0.7121       0.3542            0.3542        1.6097  0.0007  0.1337\n",
      "     37            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3826\u001b[0m       0.3333            0.3333        1.6331  0.0007  0.1321\n",
      "     38            \u001b[36m0.9667\u001b[0m        0.5059       0.3333            0.3333        1.6604  0.0007  0.1340\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3409\u001b[0m       0.3125            0.3125        1.6878  0.0007  0.1338\n",
      "     40            1.0000        \u001b[32m0.2855\u001b[0m       0.3125            0.3125        1.7276  0.0007  0.1310\n",
      "     41            1.0000        \u001b[32m0.2297\u001b[0m       0.3125            0.3125        1.7776  0.0007  0.1309\n",
      "     42            1.0000        \u001b[32m0.1713\u001b[0m       0.3021            0.3021        1.8385  0.0007  0.1282\n",
      "     43            1.0000        \u001b[32m0.1367\u001b[0m       0.2917            0.2917        1.9020  0.0006  0.1344\n",
      "     44            1.0000        \u001b[32m0.1343\u001b[0m       0.2812            0.2812        1.9630  0.0006  0.1282\n",
      "     45            1.0000        \u001b[32m0.1061\u001b[0m       0.2604            0.2604        2.0173  0.0005  0.1335\n",
      "     46            1.0000        \u001b[32m0.0937\u001b[0m       0.2500            0.2500        2.0601  0.0005  0.1249\n",
      "     47            1.0000        0.1011       0.2500            0.2500        2.0916  0.0004  0.1274\n",
      "     48            1.0000        0.0982       0.2604            0.2604        2.1104  0.0004  0.1290\n",
      "     49            1.0000        \u001b[32m0.0916\u001b[0m       0.2604            0.2604        2.1162  0.0003  0.1337\n",
      "     50            1.0000        \u001b[32m0.0674\u001b[0m       0.2604            0.2604        2.1122  0.0003  0.1223\n",
      "Fine tuning model for subject 6 with 30 = 30 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.7035       0.3542            0.3542        1.7128  0.0004  0.1302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5333        1.4860       0.3542            0.3542        1.7077  0.0005  0.1301\n",
      "     33            0.6667        1.3289       0.3542            0.3542        1.7048  0.0005  0.1288\n",
      "     34            0.7667        1.0590       0.3229            0.3229        1.6953  0.0006  0.1318\n",
      "     35            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6294\u001b[0m       0.3542            0.3542        1.6897  0.0006  0.1299\n",
      "     36            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3818\u001b[0m       0.3542            0.3542        1.6955  0.0007  0.1348\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3162\u001b[0m       0.3542            0.3542        1.7022  0.0007  0.1282\n",
      "     38            1.0000        \u001b[32m0.2407\u001b[0m       0.3229            0.3229        1.7116  0.0007  0.1263\n",
      "     39            1.0000        0.2574       0.3438            0.3438        1.7254  0.0007  0.1333\n",
      "     40            1.0000        \u001b[32m0.2197\u001b[0m       0.3646            0.3646        1.7524  0.0007  0.1420\n",
      "     41            1.0000        \u001b[32m0.1737\u001b[0m       0.3646            0.3646        1.7842  0.0007  0.1297\n",
      "     42            1.0000        \u001b[32m0.1630\u001b[0m       0.3438            0.3438        1.8109  0.0007  0.1300\n",
      "     43            1.0000        \u001b[32m0.1123\u001b[0m       0.3750            0.3750        1.8376  0.0006  0.1229\n",
      "     44            1.0000        \u001b[32m0.1043\u001b[0m       0.3854            0.3854        1.8609  0.0006  0.1331\n",
      "     45            1.0000        0.1145       0.3854            0.3854        1.8800  0.0005  0.1208\n",
      "     46            1.0000        \u001b[32m0.0841\u001b[0m       0.3854            0.3854        1.8978  0.0005  0.1335\n",
      "     47            1.0000        0.0844       0.3750            0.3750        1.9129  0.0004  0.1316\n",
      "     48            1.0000        \u001b[32m0.0841\u001b[0m       0.3750            0.3750        1.9246  0.0004  0.1328\n",
      "     49            1.0000        \u001b[32m0.0715\u001b[0m       0.3750            0.3750        1.9339  0.0003  0.1268\n",
      "     50            1.0000        \u001b[32m0.0470\u001b[0m       0.3750            0.3750        1.9421  0.0003  0.1268\n",
      "Fine tuning model for subject 6 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4333        1.4829       0.3438            0.3438        1.7078  0.0004  0.1333\n",
      "     32            0.5000        1.2720       0.3542            0.3542        1.6880  0.0005  0.1257\n",
      "     33            0.5667        1.2817       0.3646            0.3646        1.6741  0.0005  0.1328\n",
      "     34            0.6667        0.8913       0.3646            0.3646        1.6662  0.0006  0.1532\n",
      "     35            \u001b[36m0.8333\u001b[0m        0.7361       0.3333            0.3333        1.6634  0.0006  0.1444\n",
      "     36            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5127\u001b[0m       0.3125            0.3125        1.6535  0.0007  0.1542\n",
      "     37            0.9000        \u001b[32m0.4184\u001b[0m       0.3125            0.3125        1.6530  0.0007  0.1477\n",
      "     38            0.9333        \u001b[32m0.3145\u001b[0m       0.3125            0.3125        1.6707  0.0007  0.1588\n",
      "     39            0.9000        \u001b[32m0.3105\u001b[0m       0.3438            0.3438        1.6858  0.0007  0.1542\n",
      "     40            0.9000        \u001b[32m0.2520\u001b[0m       0.3542            0.3542        1.7159  0.0007  0.1584\n",
      "     41            0.9333        \u001b[32m0.1090\u001b[0m       0.3646            0.3646        1.7496  0.0007  0.1577\n",
      "     42            \u001b[36m0.9667\u001b[0m        0.1840       0.3854            0.3854        1.7890  0.0007  0.1470\n",
      "     43            0.9667        0.1551       0.3958            0.3958        1.8184  0.0006  0.1557\n",
      "     44            0.9667        0.1192       0.3958            0.3958        1.8356  0.0006  0.1615\n",
      "     45            0.9667        0.1330       0.4062            0.4062        1.8475  0.0005  0.1556\n",
      "     46            0.9667        \u001b[32m0.0982\u001b[0m       0.4062            0.4062        1.8545  0.0005  0.1599\n",
      "     47            0.9667        0.0985       0.4062            0.4062        1.8558  0.0004  0.1666\n",
      "     48            0.9667        \u001b[32m0.0592\u001b[0m       0.3750            0.3750        1.8542  0.0004  0.1707\n",
      "     49            \u001b[36m1.0000\u001b[0m        0.0621       0.3646            0.3646        1.8518  0.0003  0.1560\n",
      "     50            1.0000        0.0735       0.3750            0.3750        1.8476  0.0003  0.1539\n",
      "Fine tuning model for subject 6 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.6056       0.3542            0.3542        1.7205  0.0004  0.2034\n",
      "     32            0.5667        1.4428       0.3646            0.3646        1.7147  0.0005  0.1277\n",
      "     33            0.6667        1.2867       0.3438            0.3438        1.7142  0.0005  0.1309\n",
      "     34            0.7333        1.0801       0.3333            0.3333        1.7175  0.0006  0.1360\n",
      "     35            0.7667        0.8172       0.3438            0.3438        1.7279  0.0006  0.1440\n",
      "     36            0.8000        \u001b[32m0.6340\u001b[0m       0.3333            0.3333        1.7444  0.0007  0.1333\n",
      "     37            \u001b[36m0.8667\u001b[0m        \u001b[32m0.4166\u001b[0m       0.3229            0.3229        1.7564  0.0007  0.1294\n",
      "     38            0.8667        \u001b[32m0.3237\u001b[0m       0.3333            0.3333        1.7544  0.0007  0.1283\n",
      "     39            \u001b[36m0.9333\u001b[0m        \u001b[32m0.2789\u001b[0m       0.3229            0.3229        1.7496  0.0007  0.1195\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2777\u001b[0m       0.3333            0.3333        1.7449  0.0007  0.1312\n",
      "     41            1.0000        \u001b[32m0.1779\u001b[0m       0.3542            0.3542        1.7355  0.0007  0.1372\n",
      "     42            1.0000        0.2269       0.3646            0.3646        1.7287  0.0007  0.1268\n",
      "     43            1.0000        0.1856       0.3646            0.3646        1.7174  0.0006  0.1298\n",
      "     44            1.0000        \u001b[32m0.1648\u001b[0m       0.3646            0.3646        1.7020  0.0006  0.1309\n",
      "     45            1.0000        \u001b[32m0.1515\u001b[0m       0.3854            0.3854        1.6820  0.0005  0.1270\n",
      "     46            1.0000        \u001b[32m0.0880\u001b[0m       0.3958            0.3958        1.6635  0.0005  0.1375\n",
      "     47            1.0000        0.1295       0.4062            0.4062        1.6464  0.0004  0.1288\n",
      "     48            1.0000        \u001b[32m0.0857\u001b[0m       0.3958            0.3958        1.6318  0.0004  0.1326\n",
      "     49            1.0000        0.0893       0.4062            0.4062        1.6184  0.0003  0.1299\n",
      "     50            1.0000        0.0953       0.4062            0.4062        1.6066  0.0003  0.1261\n",
      "Fine tuning model for subject 6 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.8313       0.3542            0.3542        1.7283  0.0004  0.1325\n",
      "     32            0.4286        1.6179       0.3750            0.3750        1.7408  0.0005  0.1298\n",
      "     33            0.4857        1.6111       0.3542            0.3542        1.7740  0.0005  0.1359\n",
      "     34            0.6000        1.2609       0.3542            0.3542        1.8378  0.0006  0.1339\n",
      "     35            0.7143        1.0149       0.3542            0.3542        1.9097  0.0006  0.1331\n",
      "     36            0.7429        0.7286       0.3021            0.3021        1.9845  0.0007  0.1415\n",
      "     37            0.7429        \u001b[32m0.4852\u001b[0m       0.2917            0.2917        2.0413  0.0007  0.1320\n",
      "     38            0.7714        \u001b[32m0.3692\u001b[0m       0.2917            0.2917        2.0721  0.0007  0.1301\n",
      "     39            0.8000        \u001b[32m0.3508\u001b[0m       0.3021            0.3021        2.0972  0.0007  0.1433\n",
      "     40            \u001b[36m0.8286\u001b[0m        \u001b[32m0.3244\u001b[0m       0.3021            0.3021        2.1238  0.0007  0.1320\n",
      "     41            \u001b[36m0.9429\u001b[0m        \u001b[32m0.2494\u001b[0m       0.3021            0.3021        2.1543  0.0007  0.1353\n",
      "     42            0.9429        \u001b[32m0.2395\u001b[0m       0.3021            0.3021        2.1678  0.0007  0.1338\n",
      "     43            0.9429        \u001b[32m0.1740\u001b[0m       0.3229            0.3229        2.1741  0.0006  0.1282\n",
      "     44            \u001b[36m0.9714\u001b[0m        \u001b[32m0.1642\u001b[0m       0.3333            0.3333        2.1700  0.0006  0.1368\n",
      "     45            0.9714        \u001b[32m0.1590\u001b[0m       0.3542            0.3542        2.1573  0.0005  0.1444\n",
      "     46            0.9714        0.1613       0.3542            0.3542        2.1376  0.0005  0.1396\n",
      "     47            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1280\u001b[0m       0.3333            0.3333        2.1133  0.0004  0.1315\n",
      "     48            1.0000        0.1322       0.3333            0.3333        2.0906  0.0004  0.1291\n",
      "     49            1.0000        \u001b[32m0.0928\u001b[0m       0.3438            0.3438        2.0708  0.0003  0.1341\n",
      "     50            1.0000        0.1086       0.3333            0.3333        2.0524  0.0003  0.1329\n",
      "Fine tuning model for subject 6 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3714        1.6241       0.3542            0.3542        1.7116  0.0004  0.1361\n",
      "     32            0.4286        1.7059       0.3646            0.3646        1.6936  0.0005  0.1336\n",
      "     33            0.5143        1.3293       0.3542            0.3542        1.6816  0.0005  0.1329\n",
      "     34            0.5714        1.3246       0.3229            0.3229        1.6780  0.0006  0.1330\n",
      "     35            0.6000        0.9325       0.3438            0.3438        1.6933  0.0006  0.1281\n",
      "     36            0.7429        0.7409       0.3542            0.3542        1.7201  0.0007  0.1337\n",
      "     37            \u001b[36m0.8286\u001b[0m        \u001b[32m0.5420\u001b[0m       0.3542            0.3542        1.7433  0.0007  0.1335\n",
      "     38            \u001b[36m0.8857\u001b[0m        \u001b[32m0.3944\u001b[0m       0.3333            0.3333        1.7513  0.0007  0.1314\n",
      "     39            \u001b[36m0.9143\u001b[0m        \u001b[32m0.3050\u001b[0m       0.3125            0.3125        1.7457  0.0007  0.1354\n",
      "     40            \u001b[36m0.9714\u001b[0m        0.3132       0.3333            0.3333        1.7278  0.0007  0.1318\n",
      "     41            0.9714        \u001b[32m0.2408\u001b[0m       0.3125            0.3125        1.7142  0.0007  0.1238\n",
      "     42            \u001b[36m1.0000\u001b[0m        0.2605       0.3333            0.3333        1.7032  0.0007  0.1373\n",
      "     43            1.0000        \u001b[32m0.1357\u001b[0m       0.3542            0.3542        1.6960  0.0006  0.1337\n",
      "     44            1.0000        0.1915       0.3542            0.3542        1.6875  0.0006  0.1333\n",
      "     45            1.0000        0.1375       0.3646            0.3646        1.6789  0.0005  0.1386\n",
      "     46            1.0000        \u001b[32m0.1190\u001b[0m       0.3854            0.3854        1.6721  0.0005  0.1318\n",
      "     47            1.0000        0.1494       0.3854            0.3854        1.6649  0.0004  0.1393\n",
      "     48            1.0000        0.1202       0.3646            0.3646        1.6566  0.0004  0.1388\n",
      "     49            1.0000        \u001b[32m0.1140\u001b[0m       0.3542            0.3542        1.6473  0.0003  0.1345\n",
      "     50            1.0000        \u001b[32m0.0709\u001b[0m       0.3542            0.3542        1.6393  0.0003  0.1370\n",
      "Fine tuning model for subject 6 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4571        1.4866       0.3438            0.3438        1.7024  0.0004  0.1375\n",
      "     32            0.4857        1.1830       0.3542            0.3542        1.6737  0.0005  0.1311\n",
      "     33            0.5429        1.1822       0.3750            0.3750        1.6343  0.0005  0.1339\n",
      "     34            0.6286        0.9116       0.3646            0.3646        1.5967  0.0006  0.1386\n",
      "     35            0.7714        0.7210       0.3750            0.3750        1.5713  0.0006  0.1320\n",
      "     36            \u001b[36m0.8571\u001b[0m        \u001b[32m0.5647\u001b[0m       0.3854            0.3854        1.5672  0.0007  0.1385\n",
      "     37            \u001b[36m0.9429\u001b[0m        \u001b[32m0.3583\u001b[0m       0.3750            0.3750        1.5785  0.0007  0.1333\n",
      "     38            \u001b[36m0.9714\u001b[0m        \u001b[32m0.3033\u001b[0m       0.3750            0.3750        1.6026  0.0007  0.1394\n",
      "     39            0.9714        0.3194       0.3958            0.3958        1.6384  0.0007  0.1393\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1871\u001b[0m       0.3958            0.3958        1.6784  0.0007  0.1319\n",
      "     41            1.0000        0.1987       0.3958            0.3958        1.7170  0.0007  0.1277\n",
      "     42            1.0000        0.1973       0.4062            0.4062        1.7552  0.0007  0.1473\n",
      "     43            1.0000        \u001b[32m0.1398\u001b[0m       0.3958            0.3958        1.7888  0.0006  0.1382\n",
      "     44            1.0000        \u001b[32m0.1322\u001b[0m       0.3750            0.3750        1.8155  0.0006  0.1302\n",
      "     45            1.0000        \u001b[32m0.1279\u001b[0m       0.3750            0.3750        1.8391  0.0005  0.1336\n",
      "     46            1.0000        0.1563       0.3542            0.3542        1.8595  0.0005  0.1355\n",
      "     47            1.0000        \u001b[32m0.1248\u001b[0m       0.3438            0.3438        1.8761  0.0004  0.1368\n",
      "     48            1.0000        \u001b[32m0.1005\u001b[0m       0.3542            0.3542        1.8904  0.0004  0.1494\n",
      "     49            1.0000        \u001b[32m0.0962\u001b[0m       0.3438            0.3438        1.9016  0.0003  0.1599\n",
      "     50            1.0000        \u001b[32m0.0637\u001b[0m       0.3438            0.3438        1.9107  0.0003  0.1560\n",
      "Fine tuning model for subject 6 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.7532       0.3438            0.3438        1.7078  0.0004  0.1658\n",
      "     32            0.4000        1.6843       0.3646            0.3646        1.6855  0.0005  0.2179\n",
      "     33            0.4571        1.4122       0.3854            0.3854        1.6551  0.0005  0.2147\n",
      "     34            0.5714        1.0969       0.3750            0.3750        1.6323  0.0006  0.1500\n",
      "     35            0.6000        0.9400       0.3542            0.3542        1.6231  0.0006  0.1834\n",
      "     36            0.6857        0.8378       0.3646            0.3646        1.6308  0.0007  0.1450\n",
      "     37            0.6857        \u001b[32m0.6681\u001b[0m       0.3438            0.3438        1.6630  0.0007  0.1684\n",
      "     38            0.7143        0.7005       0.3125            0.3125        1.7200  0.0007  0.1466\n",
      "     39            0.6571        \u001b[32m0.5138\u001b[0m       0.2812            0.2812        1.7885  0.0007  0.1496\n",
      "     40            0.6286        \u001b[32m0.3830\u001b[0m       0.3021            0.3021        1.8552  0.0007  0.1600\n",
      "     41            0.6286        \u001b[32m0.3506\u001b[0m       0.3021            0.3021        1.8913  0.0007  0.1512\n",
      "     42            0.7143        \u001b[32m0.2994\u001b[0m       0.3021            0.3021        1.9083  0.0007  0.1781\n",
      "     43            0.7143        \u001b[32m0.2151\u001b[0m       0.3021            0.3021        1.9144  0.0006  0.1643\n",
      "     44            0.7143        \u001b[32m0.1882\u001b[0m       0.3125            0.3125        1.9154  0.0006  0.1619\n",
      "     45            0.7429        \u001b[32m0.1798\u001b[0m       0.3125            0.3125        1.8992  0.0005  0.1924\n",
      "     46            \u001b[36m0.8857\u001b[0m        0.1827       0.3438            0.3438        1.8665  0.0005  0.1411\n",
      "     47            \u001b[36m0.9429\u001b[0m        0.1883       0.3229            0.3229        1.8258  0.0004  0.1528\n",
      "     48            0.9429        \u001b[32m0.1533\u001b[0m       0.3021            0.3021        1.7839  0.0004  0.1351\n",
      "     49            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1088\u001b[0m       0.3021            0.3021        1.7451  0.0003  0.1409\n",
      "     50            1.0000        0.1115       0.3021            0.3021        1.7109  0.0003  0.1348\n",
      "Fine tuning model for subject 6 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3714        2.2618       0.3438            0.3438        1.6985  0.0004  0.1383\n",
      "     32            0.3714        2.0692       0.3438            0.3438        1.6627  0.0005  0.1380\n",
      "     33            0.4571        1.9585       0.3750            0.3750        1.6303  0.0005  0.1349\n",
      "     34            0.6000        1.6124       0.3021            0.3021        1.6301  0.0006  0.1336\n",
      "     35            0.7143        1.1744       0.3854            0.3854        1.6809  0.0006  0.1330\n",
      "     36            0.8000        0.8312       0.3542            0.3542        1.7765  0.0007  0.1254\n",
      "     37            0.7429        \u001b[32m0.6095\u001b[0m       0.3333            0.3333        1.9079  0.0007  0.1447\n",
      "     38            0.7714        \u001b[32m0.5429\u001b[0m       0.3021            0.3021        2.0591  0.0007  0.1451\n",
      "     39            0.7714        \u001b[32m0.4065\u001b[0m       0.2917            0.2917        2.1979  0.0007  0.1305\n",
      "     40            0.7714        \u001b[32m0.2761\u001b[0m       0.3021            0.3021        2.3170  0.0007  0.1849\n",
      "     41            0.7429        0.3979       0.2708            0.2708        2.4018  0.0007  0.1485\n",
      "     42            0.7429        0.3342       0.2812            0.2812        2.4499  0.0007  0.1456\n",
      "     43            \u001b[36m0.8286\u001b[0m        0.3692       0.2812            0.2812        2.4647  0.0006  0.1534\n",
      "     44            0.8286        0.3391       0.2812            0.2812        2.4556  0.0006  0.1455\n",
      "     45            \u001b[36m0.8857\u001b[0m        \u001b[32m0.2451\u001b[0m       0.2812            0.2812        2.4338  0.0005  0.1440\n",
      "     46            \u001b[36m0.9714\u001b[0m        \u001b[32m0.1673\u001b[0m       0.2917            0.2917        2.4123  0.0005  0.1394\n",
      "     47            0.9714        \u001b[32m0.1459\u001b[0m       0.3021            0.3021        2.3836  0.0004  0.1519\n",
      "     48            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1327\u001b[0m       0.3021            0.3021        2.3539  0.0004  0.1467\n",
      "     49            1.0000        \u001b[32m0.1114\u001b[0m       0.3021            0.3021        2.3215  0.0003  0.1367\n",
      "     50            1.0000        0.1662       0.2917            0.2917        2.2889  0.0003  0.1378\n",
      "Fine tuning model for subject 6 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4286        1.7845       0.3438            0.3438        1.7161  0.0004  0.1425\n",
      "     32            0.4286        1.5965       0.3438            0.3438        1.7040  0.0005  0.1365\n",
      "     33            0.5429        1.6315       0.3333            0.3333        1.6954  0.0005  0.1344\n",
      "     34            0.6286        1.1975       0.3333            0.3333        1.6953  0.0006  0.1331\n",
      "     35            0.7143        0.9083       0.3333            0.3333        1.6948  0.0006  0.1320\n",
      "     36            \u001b[36m0.8571\u001b[0m        \u001b[32m0.6153\u001b[0m       0.3646            0.3646        1.6956  0.0007  0.1393\n",
      "     37            \u001b[36m0.9143\u001b[0m        0.6876       0.3438            0.3438        1.6966  0.0007  0.1347\n",
      "     38            \u001b[36m0.9429\u001b[0m        \u001b[32m0.4033\u001b[0m       0.3646            0.3646        1.6952  0.0007  0.1367\n",
      "     39            0.9429        \u001b[32m0.2932\u001b[0m       0.3750            0.3750        1.6903  0.0007  0.1414\n",
      "     40            \u001b[36m0.9714\u001b[0m        0.4366       0.3750            0.3750        1.6819  0.0007  0.1495\n",
      "     41            \u001b[36m1.0000\u001b[0m        0.3093       0.3750            0.3750        1.6757  0.0007  0.1381\n",
      "     42            1.0000        \u001b[32m0.2164\u001b[0m       0.3750            0.3750        1.6708  0.0007  0.1333\n",
      "     43            1.0000        0.2181       0.3750            0.3750        1.6680  0.0006  0.1243\n",
      "     44            1.0000        \u001b[32m0.1711\u001b[0m       0.3646            0.3646        1.6640  0.0006  0.1353\n",
      "     45            1.0000        \u001b[32m0.1647\u001b[0m       0.3750            0.3750        1.6640  0.0005  0.1387\n",
      "     46            1.0000        0.1750       0.3750            0.3750        1.6646  0.0005  0.1313\n",
      "     47            1.0000        \u001b[32m0.1210\u001b[0m       0.3542            0.3542        1.6631  0.0004  0.1411\n",
      "     48            1.0000        0.1228       0.3646            0.3646        1.6588  0.0004  0.1357\n",
      "     49            1.0000        0.1570       0.3646            0.3646        1.6541  0.0003  0.1391\n",
      "     50            1.0000        0.1467       0.3646            0.3646        1.6487  0.0003  0.1297\n",
      "Fine tuning model for subject 6 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4286        1.7337       0.3646            0.3646        1.7174  0.0004  0.1368\n",
      "     32            0.4857        1.4505       0.3542            0.3542        1.7196  0.0005  0.1329\n",
      "     33            0.5429        1.4000       0.3646            0.3646        1.7299  0.0005  0.1388\n",
      "     34            0.6286        1.2010       0.3542            0.3542        1.7503  0.0006  0.1389\n",
      "     35            0.7143        0.9165       0.3229            0.3229        1.7682  0.0006  0.1299\n",
      "     36            0.7429        0.6838       0.3125            0.3125        1.7862  0.0007  0.1381\n",
      "     37            \u001b[36m0.8571\u001b[0m        \u001b[32m0.6014\u001b[0m       0.2917            0.2917        1.8152  0.0007  0.1336\n",
      "     38            \u001b[36m0.9143\u001b[0m        \u001b[32m0.5134\u001b[0m       0.3021            0.3021        1.8551  0.0007  0.1296\n",
      "     39            0.9143        \u001b[32m0.3726\u001b[0m       0.2917            0.2917        1.9032  0.0007  0.1292\n",
      "     40            0.9143        \u001b[32m0.3512\u001b[0m       0.2917            0.2917        1.9562  0.0007  0.1340\n",
      "     41            \u001b[36m0.9429\u001b[0m        \u001b[32m0.2799\u001b[0m       0.3021            0.3021        2.0072  0.0007  0.1354\n",
      "     42            0.9429        \u001b[32m0.2632\u001b[0m       0.3125            0.3125        2.0488  0.0007  0.1314\n",
      "     43            0.9429        \u001b[32m0.2189\u001b[0m       0.3438            0.3438        2.0827  0.0006  0.1300\n",
      "     44            0.9429        \u001b[32m0.1845\u001b[0m       0.3438            0.3438        2.1074  0.0006  0.1410\n",
      "     45            0.9429        \u001b[32m0.1267\u001b[0m       0.3542            0.3542        2.1219  0.0005  0.1334\n",
      "     46            \u001b[36m0.9714\u001b[0m        0.1727       0.3542            0.3542        2.1263  0.0005  0.1334\n",
      "     47            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1232\u001b[0m       0.3750            0.3750        2.1194  0.0004  0.1336\n",
      "     48            1.0000        \u001b[32m0.1196\u001b[0m       0.3750            0.3750        2.1020  0.0004  0.1390\n",
      "     49            1.0000        0.1340       0.3750            0.3750        2.0790  0.0003  0.1314\n",
      "     50            1.0000        0.1599       0.3750            0.3750        2.0524  0.0003  0.1304\n",
      "Fine tuning model for subject 6 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4571        1.5407       0.3542            0.3542        1.7139  0.0004  0.1347\n",
      "     32            0.4857        1.5829       0.3542            0.3542        1.7009  0.0005  0.1447\n",
      "     33            0.5143        1.2854       0.3646            0.3646        1.6823  0.0005  0.1336\n",
      "     34            0.6286        0.9910       0.3542            0.3542        1.6693  0.0006  0.1382\n",
      "     35            0.6857        0.8734       0.3646            0.3646        1.6644  0.0006  0.1333\n",
      "     36            0.8000        \u001b[32m0.5743\u001b[0m       0.3750            0.3750        1.6709  0.0007  0.1322\n",
      "     37            \u001b[36m0.8857\u001b[0m        \u001b[32m0.4983\u001b[0m       0.3750            0.3750        1.6908  0.0007  0.1346\n",
      "     38            \u001b[36m0.9429\u001b[0m        \u001b[32m0.3983\u001b[0m       0.3958            0.3958        1.7140  0.0007  0.1553\n",
      "     39            \u001b[36m0.9714\u001b[0m        \u001b[32m0.2658\u001b[0m       0.4062            0.4062        1.7370  0.0007  0.1552\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2379\u001b[0m       0.3958            0.3958        1.7544  0.0007  0.1628\n",
      "     41            0.9714        \u001b[32m0.1880\u001b[0m       0.3646            0.3646        1.7699  0.0007  0.1561\n",
      "     42            0.9714        0.1946       0.3438            0.3438        1.7867  0.0007  0.1600\n",
      "     43            0.9714        0.2186       0.3333            0.3333        1.7998  0.0006  0.1477\n",
      "     44            0.9714        \u001b[32m0.1247\u001b[0m       0.3333            0.3333        1.8088  0.0006  0.1897\n",
      "     45            1.0000        \u001b[32m0.1168\u001b[0m       0.3438            0.3438        1.8172  0.0005  0.1616\n",
      "     46            1.0000        0.1556       0.3646            0.3646        1.8260  0.0005  0.1573\n",
      "     47            1.0000        \u001b[32m0.1162\u001b[0m       0.4062            0.4062        1.8344  0.0004  0.1574\n",
      "     48            1.0000        \u001b[32m0.0816\u001b[0m       0.3958            0.3958        1.8403  0.0004  0.1524\n",
      "     49            1.0000        0.1004       0.3854            0.3854        1.8450  0.0003  0.1654\n",
      "     50            1.0000        0.1022       0.3854            0.3854        1.8467  0.0003  0.1586\n",
      "Fine tuning model for subject 6 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.5964       0.3542            0.3542        1.7167  0.0004  0.1434\n",
      "     32            0.6286        1.3064       0.3646            0.3646        1.7137  0.0005  0.1778\n",
      "     33            0.6571        1.4967       0.3646            0.3646        1.7221  0.0005  0.1494\n",
      "     34            0.7143        1.1947       0.3750            0.3750        1.7430  0.0006  0.1472\n",
      "     35            0.7143        0.8586       0.3542            0.3542        1.7644  0.0006  0.1876\n",
      "     36            0.7714        0.6973       0.3438            0.3438        1.7798  0.0007  0.1518\n",
      "     37            \u001b[36m0.8286\u001b[0m        \u001b[32m0.5619\u001b[0m       0.3542            0.3542        1.7893  0.0007  0.1414\n",
      "     38            \u001b[36m0.8857\u001b[0m        \u001b[32m0.4303\u001b[0m       0.3646            0.3646        1.8013  0.0007  0.1325\n",
      "     39            0.8571        \u001b[32m0.2903\u001b[0m       0.3542            0.3542        1.8159  0.0007  0.1328\n",
      "     40            0.8571        0.3277       0.3750            0.3750        1.8251  0.0007  0.1321\n",
      "     41            0.8286        \u001b[32m0.2741\u001b[0m       0.3854            0.3854        1.8434  0.0007  0.1378\n",
      "     42            0.8857        0.3187       0.3646            0.3646        1.8528  0.0007  0.1363\n",
      "     43            \u001b[36m0.9143\u001b[0m        \u001b[32m0.2225\u001b[0m       0.3854            0.3854        1.8494  0.0006  0.1332\n",
      "     44            \u001b[36m0.9714\u001b[0m        \u001b[32m0.1980\u001b[0m       0.3854            0.3854        1.8384  0.0006  0.1333\n",
      "     45            \u001b[36m1.0000\u001b[0m        0.2634       0.3854            0.3854        1.8162  0.0005  0.1385\n",
      "     46            1.0000        \u001b[32m0.1481\u001b[0m       0.3958            0.3958        1.7950  0.0005  0.1411\n",
      "     47            1.0000        0.1483       0.4062            0.4062        1.7772  0.0004  0.1364\n",
      "     48            1.0000        \u001b[32m0.1222\u001b[0m       0.3854            0.3854        1.7633  0.0004  0.1485\n",
      "     49            1.0000        0.1440       0.3750            0.3750        1.7517  0.0003  0.1391\n",
      "     50            1.0000        \u001b[32m0.1014\u001b[0m       0.3854            0.3854        1.7435  0.0003  0.1355\n",
      "Fine tuning model for subject 6 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.4926       0.3542            0.3542        1.7204  0.0004  0.1281\n",
      "     32            0.4857        1.3855       0.3542            0.3542        1.7119  0.0005  0.1319\n",
      "     33            0.6000        1.2551       0.3438            0.3438        1.7157  0.0005  0.1314\n",
      "     34            0.7143        1.0660       0.3125            0.3125        1.7433  0.0006  0.1327\n",
      "     35            0.8000        0.6938       0.2708            0.2708        1.8156  0.0006  0.1357\n",
      "     36            \u001b[36m0.8857\u001b[0m        \u001b[32m0.5153\u001b[0m       0.3021            0.3021        1.8924  0.0007  0.1390\n",
      "     37            0.8857        \u001b[32m0.2993\u001b[0m       0.3021            0.3021        1.9724  0.0007  0.1366\n",
      "     38            0.8571        0.3234       0.3125            0.3125        2.0212  0.0007  0.1386\n",
      "     39            0.8571        \u001b[32m0.2864\u001b[0m       0.2708            0.2708        2.0401  0.0007  0.1339\n",
      "     40            0.8857        \u001b[32m0.2444\u001b[0m       0.2604            0.2604        2.0422  0.0007  0.1658\n",
      "     41            \u001b[36m1.0000\u001b[0m        0.3429       0.2500            0.2500        2.0428  0.0007  0.1404\n",
      "     42            1.0000        \u001b[32m0.1449\u001b[0m       0.2604            0.2604        2.0389  0.0007  0.1401\n",
      "     43            1.0000        \u001b[32m0.1412\u001b[0m       0.3021            0.3021        2.0359  0.0006  0.1319\n",
      "     44            1.0000        0.1748       0.2917            0.2917        2.0299  0.0006  0.1361\n",
      "     45            1.0000        \u001b[32m0.1120\u001b[0m       0.3021            0.3021        2.0266  0.0005  0.1296\n",
      "     46            1.0000        0.1267       0.3021            0.3021        2.0250  0.0005  0.1328\n",
      "     47            1.0000        \u001b[32m0.0827\u001b[0m       0.3021            0.3021        2.0246  0.0004  0.1325\n",
      "     48            1.0000        \u001b[32m0.0774\u001b[0m       0.2812            0.2812        2.0254  0.0004  0.1349\n",
      "     49            1.0000        \u001b[32m0.0705\u001b[0m       0.2708            0.2708        2.0273  0.0003  0.1278\n",
      "     50            1.0000        0.0888       0.2708            0.2708        2.0288  0.0003  0.1329\n",
      "Fine tuning model for subject 6 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.0129       0.3542            0.3542        1.7084  0.0004  0.1502\n",
      "     32            0.6500        1.1846       0.3542            0.3542        1.6836  0.0005  0.1451\n",
      "     33            0.7250        1.0059       0.3646            0.3646        1.6564  0.0005  0.1458\n",
      "     34            \u001b[36m0.8500\u001b[0m        0.7418       0.3646            0.3646        1.6261  0.0006  0.1574\n",
      "     35            \u001b[36m0.9500\u001b[0m        \u001b[32m0.6689\u001b[0m       0.3542            0.3542        1.6035  0.0006  0.1562\n",
      "     36            \u001b[36m0.9750\u001b[0m        \u001b[32m0.4454\u001b[0m       0.3750            0.3750        1.5854  0.0007  0.1676\n",
      "     37            0.9750        \u001b[32m0.3769\u001b[0m       0.3958            0.3958        1.5712  0.0007  0.1449\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3380\u001b[0m       0.3854            0.3854        1.5622  0.0007  0.1552\n",
      "     39            1.0000        \u001b[32m0.2339\u001b[0m       0.3854            0.3854        1.5619  0.0007  0.1459\n",
      "     40            1.0000        0.2437       0.3854            0.3854        1.5633  0.0007  0.1407\n",
      "     41            1.0000        \u001b[32m0.1685\u001b[0m       0.3750            0.3750        1.5689  0.0007  0.1401\n",
      "     42            1.0000        \u001b[32m0.1602\u001b[0m       0.3438            0.3438        1.5811  0.0007  0.1447\n",
      "     43            1.0000        \u001b[32m0.1336\u001b[0m       0.3646            0.3646        1.5977  0.0006  0.1476\n",
      "     44            1.0000        0.1456       0.3750            0.3750        1.6153  0.0006  0.1429\n",
      "     45            1.0000        0.1726       0.3646            0.3646        1.6337  0.0005  0.1424\n",
      "     46            1.0000        \u001b[32m0.1064\u001b[0m       0.3542            0.3542        1.6481  0.0005  0.1416\n",
      "     47            1.0000        0.1173       0.3542            0.3542        1.6601  0.0004  0.1360\n",
      "     48            1.0000        \u001b[32m0.0884\u001b[0m       0.3542            0.3542        1.6684  0.0004  0.1443\n",
      "     49            1.0000        \u001b[32m0.0624\u001b[0m       0.3542            0.3542        1.6743  0.0003  0.1586\n",
      "     50            1.0000        0.0804       0.3542            0.3542        1.6777  0.0003  0.1504\n",
      "Fine tuning model for subject 6 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3500        1.9713       0.3438            0.3438        1.7102  0.0004  0.1487\n",
      "     32            0.4000        1.6837       0.3438            0.3438        1.6949  0.0005  0.1470\n",
      "     33            0.5500        1.8690       0.3229            0.3229        1.6875  0.0005  0.1539\n",
      "     34            0.6500        1.2459       0.3333            0.3333        1.7027  0.0006  0.1577\n",
      "     35            0.6750        1.1112       0.3333            0.3333        1.7583  0.0006  0.1373\n",
      "     36            0.6750        0.6832       0.3333            0.3333        1.8519  0.0007  0.1452\n",
      "     37            0.7500        \u001b[32m0.5489\u001b[0m       0.3333            0.3333        1.9195  0.0007  0.1386\n",
      "     38            0.7250        \u001b[32m0.5098\u001b[0m       0.3542            0.3542        1.9409  0.0007  0.1496\n",
      "     39            \u001b[36m0.8250\u001b[0m        \u001b[32m0.4709\u001b[0m       0.3646            0.3646        1.9070  0.0007  0.1459\n",
      "     40            \u001b[36m0.9250\u001b[0m        0.4917       0.3542            0.3542        1.8467  0.0007  0.1449\n",
      "     41            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3394\u001b[0m       0.3646            0.3646        1.7978  0.0007  0.1537\n",
      "     42            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3169\u001b[0m       0.3750            0.3750        1.7796  0.0007  0.1442\n",
      "     43            1.0000        \u001b[32m0.3038\u001b[0m       0.3333            0.3333        1.7849  0.0006  0.1393\n",
      "     44            1.0000        \u001b[32m0.1918\u001b[0m       0.3021            0.3021        1.8046  0.0006  0.1440\n",
      "     45            1.0000        0.2011       0.3125            0.3125        1.8266  0.0005  0.1486\n",
      "     46            1.0000        \u001b[32m0.1638\u001b[0m       0.3125            0.3125        1.8450  0.0005  0.1690\n",
      "     47            1.0000        \u001b[32m0.1383\u001b[0m       0.3229            0.3229        1.8571  0.0004  0.1738\n",
      "     48            1.0000        \u001b[32m0.1308\u001b[0m       0.3125            0.3125        1.8617  0.0004  0.1533\n",
      "     49            1.0000        0.1431       0.3021            0.3021        1.8592  0.0003  0.1662\n",
      "     50            1.0000        \u001b[32m0.0921\u001b[0m       0.3021            0.3021        1.8536  0.0003  0.1663\n",
      "Fine tuning model for subject 6 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3750        1.6249       0.3333            0.3333        1.7074  0.0004  0.1850\n",
      "     32            0.3750        1.5307       0.3333            0.3333        1.6935  0.0005  0.1613\n",
      "     33            0.4000        1.4257       0.3646            0.3646        1.6826  0.0005  0.1549\n",
      "     34            0.6000        0.9552       0.3646            0.3646        1.6813  0.0006  0.1737\n",
      "     35            0.8000        0.9621       0.3646            0.3646        1.6795  0.0006  0.1729\n",
      "     36            \u001b[36m0.8750\u001b[0m        \u001b[32m0.5762\u001b[0m       0.3646            0.3646        1.6871  0.0007  0.1670\n",
      "     37            \u001b[36m0.9250\u001b[0m        \u001b[32m0.4812\u001b[0m       0.3438            0.3438        1.7060  0.0007  0.1663\n",
      "     38            \u001b[36m0.9500\u001b[0m        0.4876       0.3333            0.3333        1.7401  0.0007  0.1724\n",
      "     39            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3133\u001b[0m       0.3646            0.3646        1.7891  0.0007  0.1757\n",
      "     40            0.9750        \u001b[32m0.3121\u001b[0m       0.3542            0.3542        1.8474  0.0007  0.1620\n",
      "     41            0.9750        \u001b[32m0.2137\u001b[0m       0.3125            0.3125        1.9082  0.0007  0.1790\n",
      "     42            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1399\u001b[0m       0.3021            0.3021        1.9631  0.0007  0.1615\n",
      "     43            1.0000        0.2130       0.3021            0.3021        2.0128  0.0006  0.2175\n",
      "     44            1.0000        \u001b[32m0.1290\u001b[0m       0.3229            0.3229        2.0521  0.0006  0.1527\n",
      "     45            1.0000        0.1440       0.3333            0.3333        2.0824  0.0005  0.1421\n",
      "     46            1.0000        0.1386       0.3333            0.3333        2.1054  0.0005  0.1443\n",
      "     47            1.0000        \u001b[32m0.1247\u001b[0m       0.3229            0.3229        2.1197  0.0004  0.1391\n",
      "     48            1.0000        \u001b[32m0.1055\u001b[0m       0.3333            0.3333        2.1297  0.0004  0.1461\n",
      "     49            1.0000        \u001b[32m0.1020\u001b[0m       0.3333            0.3333        2.1348  0.0003  0.1468\n",
      "     50            1.0000        \u001b[32m0.0736\u001b[0m       0.3333            0.3333        2.1368  0.0003  0.1407\n",
      "Fine tuning model for subject 6 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4750        1.5990       0.3333            0.3333        1.7115  0.0004  0.1480\n",
      "     32            0.5000        1.4329       0.3750            0.3750        1.7088  0.0005  0.1470\n",
      "     33            0.5500        1.3219       0.3542            0.3542        1.7193  0.0005  0.1400\n",
      "     34            0.6250        1.2713       0.3542            0.3542        1.7502  0.0006  0.1541\n",
      "     35            0.6750        0.8135       0.3438            0.3438        1.7870  0.0006  0.1476\n",
      "     36            0.7500        0.8401       0.3125            0.3125        1.8123  0.0007  0.1463\n",
      "     37            \u001b[36m0.8250\u001b[0m        \u001b[32m0.5510\u001b[0m       0.3125            0.3125        1.8128  0.0007  0.1357\n",
      "     38            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5287\u001b[0m       0.3125            0.3125        1.7972  0.0007  0.1455\n",
      "     39            0.9500        \u001b[32m0.4056\u001b[0m       0.3229            0.3229        1.7763  0.0007  0.1423\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3851\u001b[0m       0.3125            0.3125        1.7516  0.0007  0.1416\n",
      "     41            1.0000        \u001b[32m0.3570\u001b[0m       0.3229            0.3229        1.7343  0.0007  0.1492\n",
      "     42            1.0000        \u001b[32m0.3235\u001b[0m       0.3125            0.3125        1.7210  0.0007  0.1339\n",
      "     43            1.0000        \u001b[32m0.2502\u001b[0m       0.3125            0.3125        1.7165  0.0006  0.1496\n",
      "     44            1.0000        \u001b[32m0.1888\u001b[0m       0.3125            0.3125        1.7177  0.0006  0.1453\n",
      "     45            1.0000        0.1903       0.3333            0.3333        1.7220  0.0005  0.1430\n",
      "     46            1.0000        0.2331       0.3438            0.3438        1.7244  0.0005  0.1443\n",
      "     47            1.0000        \u001b[32m0.1517\u001b[0m       0.3438            0.3438        1.7256  0.0004  0.1394\n",
      "     48            1.0000        0.1538       0.3438            0.3438        1.7256  0.0004  0.1434\n",
      "     49            1.0000        0.1971       0.3542            0.3542        1.7258  0.0003  0.1415\n",
      "     50            1.0000        \u001b[32m0.1472\u001b[0m       0.3542            0.3542        1.7259  0.0003  0.1482\n",
      "Fine tuning model for subject 6 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3250        1.9935       0.3646            0.3646        1.7048  0.0004  0.1389\n",
      "     32            0.3500        1.8167       0.3646            0.3646        1.6624  0.0005  0.1426\n",
      "     33            0.4500        1.7941       0.3646            0.3646        1.6037  0.0005  0.1496\n",
      "     34            0.5750        1.4591       0.3542            0.3542        1.5391  0.0006  0.1395\n",
      "     35            0.8000        1.1294       0.3854            0.3854        1.4969  0.0006  0.1388\n",
      "     36            \u001b[36m0.9000\u001b[0m        0.8417       0.3854            0.3854        1.4905  0.0007  0.1530\n",
      "     37            \u001b[36m0.9250\u001b[0m        \u001b[32m0.5811\u001b[0m       0.4062            0.4062        1.5134  0.0007  0.1399\n",
      "     38            0.9250        \u001b[32m0.4641\u001b[0m       0.4062            0.4062        1.5609  0.0007  0.1408\n",
      "     39            0.9250        0.5211       0.3750            0.3750        1.6307  0.0007  0.1422\n",
      "     40            0.9250        \u001b[32m0.2974\u001b[0m       0.3854            0.3854        1.7069  0.0007  0.1475\n",
      "     41            0.9250        0.3986       0.3750            0.3750        1.7695  0.0007  0.1440\n",
      "     42            \u001b[36m0.9500\u001b[0m        0.3366       0.3750            0.3750        1.8115  0.0007  0.1431\n",
      "     43            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2420\u001b[0m       0.3542            0.3542        1.8430  0.0006  0.1396\n",
      "     44            1.0000        0.2575       0.3646            0.3646        1.8621  0.0006  0.1452\n",
      "     45            1.0000        0.2459       0.3542            0.3542        1.8738  0.0005  0.1522\n",
      "     46            1.0000        \u001b[32m0.1340\u001b[0m       0.3646            0.3646        1.8783  0.0005  0.1429\n",
      "     47            1.0000        0.1687       0.3542            0.3542        1.8768  0.0004  0.1405\n",
      "     48            1.0000        \u001b[32m0.1082\u001b[0m       0.3542            0.3542        1.8727  0.0004  0.1430\n",
      "     49            1.0000        0.1417       0.3542            0.3542        1.8661  0.0003  0.1419\n",
      "     50            1.0000        0.1188       0.3438            0.3438        1.8574  0.0003  0.1440\n",
      "Fine tuning model for subject 6 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3750        1.6312       0.3542            0.3542        1.7150  0.0004  0.1460\n",
      "     32            0.4000        1.4070       0.3542            0.3542        1.6948  0.0005  0.1498\n",
      "     33            0.5250        1.4323       0.3542            0.3542        1.6659  0.0005  0.1360\n",
      "     34            0.7000        1.1287       0.3333            0.3333        1.6404  0.0006  0.1437\n",
      "     35            \u001b[36m0.8250\u001b[0m        0.9431       0.3438            0.3438        1.6309  0.0006  0.1385\n",
      "     36            \u001b[36m0.9250\u001b[0m        0.7895       0.3438            0.3438        1.6466  0.0007  0.1517\n",
      "     37            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5289\u001b[0m       0.3542            0.3542        1.6896  0.0007  0.1459\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3346\u001b[0m       0.3750            0.3750        1.7508  0.0007  0.1402\n",
      "     39            1.0000        \u001b[32m0.2846\u001b[0m       0.3854            0.3854        1.8149  0.0007  0.1360\n",
      "     40            1.0000        \u001b[32m0.2540\u001b[0m       0.3646            0.3646        1.8841  0.0007  0.1433\n",
      "     41            1.0000        0.2657       0.3646            0.3646        1.9420  0.0007  0.1506\n",
      "     42            1.0000        \u001b[32m0.2502\u001b[0m       0.3854            0.3854        1.9812  0.0007  0.1548\n",
      "     43            1.0000        \u001b[32m0.2088\u001b[0m       0.3958            0.3958        2.0018  0.0006  0.1599\n",
      "     44            1.0000        0.2608       0.3854            0.3854        2.0052  0.0006  0.1829\n",
      "     45            1.0000        \u001b[32m0.1641\u001b[0m       0.3854            0.3854        1.9999  0.0005  0.1553\n",
      "     46            1.0000        \u001b[32m0.1199\u001b[0m       0.3854            0.3854        1.9911  0.0005  0.1412\n",
      "     47            1.0000        0.1480       0.3854            0.3854        1.9774  0.0004  0.1646\n",
      "     48            1.0000        0.1504       0.3958            0.3958        1.9607  0.0004  0.1417\n",
      "     49            1.0000        \u001b[32m0.1056\u001b[0m       0.3958            0.3958        1.9456  0.0003  0.1414\n",
      "     50            1.0000        \u001b[32m0.0962\u001b[0m       0.4062            0.4062        1.9329  0.0003  0.1499\n",
      "Fine tuning model for subject 6 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5250        1.2758       0.3438            0.3438        1.7067  0.0004  0.1808\n",
      "     32            0.5750        1.1894       0.3542            0.3542        1.6837  0.0005  0.1721\n",
      "     33            0.6250        1.0589       0.3542            0.3542        1.6664  0.0005  0.1727\n",
      "     34            0.8000        0.7473       0.3229            0.3229        1.6565  0.0006  0.1600\n",
      "     35            \u001b[36m0.8500\u001b[0m        0.7625       0.3125            0.3125        1.6577  0.0006  0.1634\n",
      "     36            \u001b[36m0.8750\u001b[0m        \u001b[32m0.5866\u001b[0m       0.3229            0.3229        1.6702  0.0007  0.1564\n",
      "     37            \u001b[36m0.9250\u001b[0m        \u001b[32m0.4732\u001b[0m       0.3229            0.3229        1.6972  0.0007  0.1865\n",
      "     38            0.9250        \u001b[32m0.4329\u001b[0m       0.3333            0.3333        1.7337  0.0007  0.1600\n",
      "     39            0.9250        \u001b[32m0.3293\u001b[0m       0.3438            0.3438        1.7807  0.0007  0.1872\n",
      "     40            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2220\u001b[0m       0.3438            0.3438        1.8319  0.0007  0.1534\n",
      "     41            0.9500        0.2347       0.3646            0.3646        1.8898  0.0007  0.1634\n",
      "     42            \u001b[36m0.9750\u001b[0m        0.2354       0.3542            0.3542        1.9412  0.0007  0.1567\n",
      "     43            0.9750        \u001b[32m0.1920\u001b[0m       0.3333            0.3333        1.9835  0.0006  0.1655\n",
      "     44            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1522\u001b[0m       0.3438            0.3438        2.0083  0.0006  0.1603\n",
      "     45            1.0000        \u001b[32m0.1278\u001b[0m       0.3333            0.3333        2.0206  0.0005  0.1628\n",
      "     46            1.0000        0.1301       0.3229            0.3229        2.0211  0.0005  0.1796\n",
      "     47            1.0000        \u001b[32m0.1203\u001b[0m       0.3333            0.3333        2.0110  0.0004  0.1647\n",
      "     48            1.0000        0.1353       0.3438            0.3438        1.9935  0.0004  0.1673\n",
      "     49            1.0000        \u001b[32m0.1042\u001b[0m       0.3542            0.3542        1.9716  0.0003  0.1957\n",
      "     50            1.0000        \u001b[32m0.0985\u001b[0m       0.3333            0.3333        1.9472  0.0003  0.1504\n",
      "Fine tuning model for subject 6 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3250        1.8640       0.3542            0.3542        1.7092  0.0004  0.1451\n",
      "     32            0.3750        1.6011       0.3542            0.3542        1.6909  0.0005  0.1435\n",
      "     33            0.4500        1.6614       0.3333            0.3333        1.6649  0.0005  0.1477\n",
      "     34            0.5500        1.3134       0.3333            0.3333        1.6461  0.0006  0.1396\n",
      "     35            0.6250        0.9445       0.3438            0.3438        1.6377  0.0006  0.1512\n",
      "     36            0.7250        0.9017       0.3750            0.3750        1.6439  0.0007  0.1447\n",
      "     37            \u001b[36m0.8250\u001b[0m        0.7393       0.3542            0.3542        1.6718  0.0007  0.1448\n",
      "     38            \u001b[36m0.8500\u001b[0m        \u001b[32m0.6116\u001b[0m       0.3646            0.3646        1.7100  0.0007  0.1397\n",
      "     39            0.8500        \u001b[32m0.4901\u001b[0m       0.3854            0.3854        1.7557  0.0007  0.1497\n",
      "     40            \u001b[36m0.8750\u001b[0m        0.4953       0.3958            0.3958        1.7975  0.0007  0.1452\n",
      "     41            \u001b[36m0.9250\u001b[0m        \u001b[32m0.3754\u001b[0m       0.3958            0.3958        1.8305  0.0007  0.1431\n",
      "     42            0.9250        \u001b[32m0.2914\u001b[0m       0.3854            0.3854        1.8483  0.0007  0.1442\n",
      "     43            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2345\u001b[0m       0.3750            0.3750        1.8512  0.0006  0.1361\n",
      "     44            0.9500        0.2407       0.3750            0.3750        1.8506  0.0006  0.1424\n",
      "     45            0.9500        0.2590       0.3854            0.3854        1.8417  0.0005  0.1414\n",
      "     46            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1797\u001b[0m       0.3750            0.3750        1.8311  0.0005  0.1412\n",
      "     47            1.0000        \u001b[32m0.1486\u001b[0m       0.3854            0.3854        1.8214  0.0004  0.1387\n",
      "     48            1.0000        0.1752       0.3750            0.3750        1.8077  0.0004  0.1435\n",
      "     49            1.0000        0.1708       0.3854            0.3854        1.7935  0.0003  0.1444\n",
      "     50            1.0000        \u001b[32m0.1126\u001b[0m       0.3958            0.3958        1.7812  0.0003  0.1436\n",
      "Fine tuning model for subject 6 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3500        1.3285       0.3438            0.3438        1.7102  0.0004  0.1497\n",
      "     32            0.4500        1.2699       0.3542            0.3542        1.6902  0.0005  0.1441\n",
      "     33            0.5500        1.1326       0.3646            0.3646        1.6667  0.0005  0.1440\n",
      "     34            0.6750        0.9897       0.3854            0.3854        1.6389  0.0006  0.1450\n",
      "     35            0.7750        \u001b[32m0.6478\u001b[0m       0.3958            0.3958        1.6138  0.0006  0.1404\n",
      "     36            \u001b[36m0.8750\u001b[0m        \u001b[32m0.6177\u001b[0m       0.4271            0.4271        1.5877  0.0007  0.1417\n",
      "     37            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5489\u001b[0m       0.4375            0.4375        1.5699  0.0007  0.1438\n",
      "     38            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3278\u001b[0m       0.3854            0.3854        1.5626  0.0007  0.1456\n",
      "     39            0.9750        \u001b[32m0.3066\u001b[0m       0.4167            0.4167        1.5635  0.0007  0.1417\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2900\u001b[0m       0.4271            0.4271        1.5702  0.0007  0.1443\n",
      "     41            1.0000        \u001b[32m0.1891\u001b[0m       0.4271            0.4271        1.5809  0.0007  0.1503\n",
      "     42            1.0000        0.2155       0.4271            0.4271        1.5917  0.0007  0.1442\n",
      "     43            1.0000        \u001b[32m0.1695\u001b[0m       0.4271            0.4271        1.6031  0.0006  0.1435\n",
      "     44            1.0000        0.1998       0.4271            0.4271        1.6094  0.0006  0.1414\n",
      "     45            1.0000        \u001b[32m0.1097\u001b[0m       0.4167            0.4167        1.6153  0.0005  0.1407\n",
      "     46            1.0000        0.1366       0.4167            0.4167        1.6196  0.0005  0.1437\n",
      "     47            1.0000        0.1345       0.4062            0.4062        1.6239  0.0004  0.1342\n",
      "     48            1.0000        0.1496       0.4062            0.4062        1.6266  0.0004  0.1444\n",
      "     49            1.0000        \u001b[32m0.1070\u001b[0m       0.4062            0.4062        1.6276  0.0003  0.1426\n",
      "     50            1.0000        \u001b[32m0.0881\u001b[0m       0.4062            0.4062        1.6282  0.0003  0.1512\n",
      "Fine tuning model for subject 6 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4500        1.5553       0.3542            0.3542        1.7116  0.0004  0.1423\n",
      "     32            0.5000        1.5494       0.3646            0.3646        1.6897  0.0005  0.1428\n",
      "     33            0.6000        1.1781       0.3542            0.3542        1.6513  0.0005  0.1443\n",
      "     34            0.7000        1.0802       0.3646            0.3646        1.5916  0.0006  0.1389\n",
      "     35            \u001b[36m0.8500\u001b[0m        0.7590       0.3333            0.3333        1.5372  0.0006  0.1472\n",
      "     36            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6360\u001b[0m       0.3438            0.3438        1.5033  0.0007  0.1392\n",
      "     37            0.9000        \u001b[32m0.4181\u001b[0m       0.3750            0.3750        1.4870  0.0007  0.1431\n",
      "     38            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3814\u001b[0m       0.3542            0.3542        1.4855  0.0007  0.1436\n",
      "     39            0.9750        \u001b[32m0.3160\u001b[0m       0.3854            0.3854        1.4979  0.0007  0.1445\n",
      "     40            0.9750        \u001b[32m0.2510\u001b[0m       0.3854            0.3854        1.5184  0.0007  0.1394\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1910\u001b[0m       0.3542            0.3542        1.5435  0.0007  0.1473\n",
      "     42            0.9750        0.2459       0.3542            0.3542        1.5714  0.0007  0.1404\n",
      "     43            0.9750        \u001b[32m0.1681\u001b[0m       0.3542            0.3542        1.6004  0.0006  0.1456\n",
      "     44            0.9750        \u001b[32m0.1389\u001b[0m       0.3646            0.3646        1.6265  0.0006  0.1571\n",
      "     45            0.9750        0.1697       0.3542            0.3542        1.6490  0.0005  0.1517\n",
      "     46            0.9750        \u001b[32m0.1261\u001b[0m       0.3750            0.3750        1.6672  0.0005  0.1451\n",
      "     47            1.0000        \u001b[32m0.0872\u001b[0m       0.3646            0.3646        1.6804  0.0004  0.1394\n",
      "     48            1.0000        0.1034       0.3750            0.3750        1.6886  0.0004  0.1418\n",
      "     49            1.0000        0.0954       0.3854            0.3854        1.6937  0.0003  0.1390\n",
      "     50            1.0000        0.1349       0.3750            0.3750        1.6968  0.0003  0.1397\n",
      "Fine tuning model for subject 6 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3556        1.9603       0.3542            0.3542        1.7089  0.0004  0.1566\n",
      "     32            0.3556        1.7733       0.3646            0.3646        1.7033  0.0005  0.1487\n",
      "     33            0.4000        1.5849       0.3542            0.3542        1.6974  0.0005  0.1581\n",
      "     34            0.5556        1.4663       0.3333            0.3333        1.6999  0.0006  0.1551\n",
      "     35            0.6667        1.0661       0.3229            0.3229        1.7170  0.0006  0.1511\n",
      "     36            0.6889        0.8871       0.3229            0.3229        1.7455  0.0007  0.1555\n",
      "     37            0.6667        \u001b[32m0.5736\u001b[0m       0.3542            0.3542        1.7861  0.0007  0.1708\n",
      "     38            0.7778        0.5741       0.3646            0.3646        1.8350  0.0007  0.1829\n",
      "     39            \u001b[36m0.8222\u001b[0m        \u001b[32m0.4854\u001b[0m       0.3646            0.3646        1.8766  0.0007  0.1760\n",
      "     40            0.8222        \u001b[32m0.4657\u001b[0m       0.3438            0.3438        1.9121  0.0007  0.1914\n",
      "     41            \u001b[36m0.8667\u001b[0m        \u001b[32m0.3449\u001b[0m       0.3125            0.3125        1.9432  0.0007  0.1896\n",
      "     42            \u001b[36m0.9556\u001b[0m        \u001b[32m0.2866\u001b[0m       0.3125            0.3125        1.9655  0.0007  0.1720\n",
      "     43            0.9556        \u001b[32m0.2599\u001b[0m       0.3229            0.3229        1.9826  0.0006  0.1685\n",
      "     44            \u001b[36m0.9778\u001b[0m        \u001b[32m0.2222\u001b[0m       0.3333            0.3333        1.9953  0.0006  0.1915\n",
      "     45            0.9778        \u001b[32m0.1678\u001b[0m       0.3333            0.3333        1.9973  0.0005  0.1719\n",
      "     46            0.9778        0.1958       0.3125            0.3125        1.9944  0.0005  0.1920\n",
      "     47            \u001b[36m1.0000\u001b[0m        0.1795       0.3229            0.3229        1.9856  0.0004  0.1937\n",
      "     48            1.0000        \u001b[32m0.1604\u001b[0m       0.3333            0.3333        1.9737  0.0004  0.1998\n",
      "     49            1.0000        0.1738       0.3438            0.3438        1.9581  0.0003  0.2126\n",
      "     50            1.0000        \u001b[32m0.1371\u001b[0m       0.3333            0.3333        1.9413  0.0003  0.1899\n",
      "Fine tuning model for subject 6 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3333        1.7134       0.3438            0.3438        1.7156  0.0004  0.2196\n",
      "     32            0.3556        1.8547       0.3438            0.3438        1.7166  0.0005  0.1557\n",
      "     33            0.4222        1.5969       0.3125            0.3125        1.7314  0.0005  0.1618\n",
      "     34            0.5111        1.2941       0.3229            0.3229        1.7635  0.0006  0.1473\n",
      "     35            0.6222        1.2080       0.2917            0.2917        1.8004  0.0006  0.1528\n",
      "     36            0.7111        0.9169       0.2917            0.2917        1.8388  0.0007  0.1498\n",
      "     37            0.7333        \u001b[32m0.6164\u001b[0m       0.2917            0.2917        1.8872  0.0007  0.1573\n",
      "     38            0.7778        \u001b[32m0.5723\u001b[0m       0.3021            0.3021        1.9396  0.0007  0.1543\n",
      "     39            \u001b[36m0.8222\u001b[0m        \u001b[32m0.4596\u001b[0m       0.3021            0.3021        1.9854  0.0007  0.1515\n",
      "     40            \u001b[36m0.8444\u001b[0m        \u001b[32m0.3774\u001b[0m       0.3125            0.3125        2.0137  0.0007  0.1546\n",
      "     41            0.8222        \u001b[32m0.2985\u001b[0m       0.2917            0.2917        2.0287  0.0007  0.1503\n",
      "     42            0.8222        \u001b[32m0.2557\u001b[0m       0.3125            0.3125        2.0369  0.0007  0.1572\n",
      "     43            0.8444        0.2887       0.3125            0.3125        2.0344  0.0006  0.1587\n",
      "     44            \u001b[36m0.8889\u001b[0m        \u001b[32m0.2232\u001b[0m       0.2917            0.2917        2.0262  0.0006  0.1502\n",
      "     45            \u001b[36m0.9333\u001b[0m        \u001b[32m0.1935\u001b[0m       0.2812            0.2812        2.0122  0.0005  0.1505\n",
      "     46            0.9333        0.2150       0.2604            0.2604        1.9981  0.0005  0.1532\n",
      "     47            \u001b[36m0.9556\u001b[0m        \u001b[32m0.1822\u001b[0m       0.2812            0.2812        1.9843  0.0004  0.1543\n",
      "     48            \u001b[36m0.9778\u001b[0m        \u001b[32m0.1803\u001b[0m       0.2708            0.2708        1.9722  0.0004  0.1509\n",
      "     49            0.9778        \u001b[32m0.1576\u001b[0m       0.2708            0.2708        1.9617  0.0003  0.1558\n",
      "     50            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1308\u001b[0m       0.2812            0.2812        1.9530  0.0003  0.1545\n",
      "Fine tuning model for subject 6 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4667        1.6793       0.3438            0.3438        1.7094  0.0004  0.1588\n",
      "     32            0.4667        1.6589       0.3750            0.3750        1.6885  0.0005  0.1546\n",
      "     33            0.5111        1.3801       0.3750            0.3750        1.6632  0.0005  0.1553\n",
      "     34            0.5556        1.4475       0.3646            0.3646        1.6356  0.0006  0.1531\n",
      "     35            0.6000        1.1978       0.3542            0.3542        1.6017  0.0006  0.1520\n",
      "     36            0.7111        0.9391       0.3646            0.3646        1.5622  0.0007  0.1490\n",
      "     37            0.7778        0.7643       0.3542            0.3542        1.5272  0.0007  0.1510\n",
      "     38            \u001b[36m0.8444\u001b[0m        \u001b[32m0.5783\u001b[0m       0.3750            0.3750        1.4973  0.0007  0.1546\n",
      "     39            \u001b[36m0.8889\u001b[0m        \u001b[32m0.5374\u001b[0m       0.3854            0.3854        1.4754  0.0007  0.1501\n",
      "     40            \u001b[36m0.9556\u001b[0m        \u001b[32m0.3714\u001b[0m       0.4167            0.4167        1.4706  0.0007  0.1499\n",
      "     41            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3256\u001b[0m       0.4062            0.4062        1.4740  0.0007  0.1492\n",
      "     42            0.9778        \u001b[32m0.2999\u001b[0m       0.4167            0.4167        1.4764  0.0007  0.1526\n",
      "     43            0.9778        \u001b[32m0.2578\u001b[0m       0.4167            0.4167        1.4741  0.0006  0.1490\n",
      "     44            0.9778        0.2926       0.4062            0.4062        1.4677  0.0006  0.1468\n",
      "     45            0.9778        0.2610       0.4167            0.4167        1.4611  0.0005  0.1495\n",
      "     46            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2073\u001b[0m       0.4375            0.4375        1.4525  0.0005  0.1510\n",
      "     47            1.0000        0.2873       0.4375            0.4375        1.4413  0.0004  0.1549\n",
      "     48            1.0000        \u001b[32m0.1719\u001b[0m       0.4271            0.4271        1.4322  0.0004  0.1579\n",
      "     49            1.0000        0.2486       0.4271            0.4271        1.4258  0.0003  0.1497\n",
      "     50            1.0000        \u001b[32m0.1686\u001b[0m       0.4167            0.4167        1.4214  0.0003  0.1543\n",
      "Fine tuning model for subject 6 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4444        1.7068       0.3333            0.3333        1.7074  0.0004  0.1546\n",
      "     32            0.4667        1.4210       0.3542            0.3542        1.6886  0.0005  0.1648\n",
      "     33            0.4889        1.4815       0.3646            0.3646        1.6763  0.0005  0.1500\n",
      "     34            0.5333        1.2523       0.3646            0.3646        1.6887  0.0006  0.1543\n",
      "     35            0.6667        1.1255       0.3333            0.3333        1.7186  0.0006  0.1550\n",
      "     36            0.7333        0.8114       0.3438            0.3438        1.7635  0.0007  0.1497\n",
      "     37            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6295\u001b[0m       0.3333            0.3333        1.8103  0.0007  0.1535\n",
      "     38            0.8667        0.6639       0.3229            0.3229        1.8457  0.0007  0.1549\n",
      "     39            \u001b[36m0.8889\u001b[0m        \u001b[32m0.5102\u001b[0m       0.3229            0.3229        1.8652  0.0007  0.1511\n",
      "     40            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3703\u001b[0m       0.3021            0.3021        1.8764  0.0007  0.1745\n",
      "     41            0.9333        0.4344       0.3021            0.3021        1.8777  0.0007  0.1528\n",
      "     42            \u001b[36m0.9556\u001b[0m        0.4127       0.2812            0.2812        1.8686  0.0007  0.1514\n",
      "     43            0.9556        \u001b[32m0.3400\u001b[0m       0.2708            0.2708        1.8501  0.0006  0.1498\n",
      "     44            \u001b[36m0.9778\u001b[0m        \u001b[32m0.2721\u001b[0m       0.2917            0.2917        1.8311  0.0006  0.1650\n",
      "     45            0.9778        \u001b[32m0.2099\u001b[0m       0.3229            0.3229        1.8159  0.0005  0.1549\n",
      "     46            0.9778        0.2284       0.3333            0.3333        1.8034  0.0005  0.1516\n",
      "     47            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1541\u001b[0m       0.3229            0.3229        1.7950  0.0004  0.1531\n",
      "     48            1.0000        0.1669       0.3229            0.3229        1.7899  0.0004  0.1495\n",
      "     49            1.0000        \u001b[32m0.1195\u001b[0m       0.3229            0.3229        1.7860  0.0003  0.1509\n",
      "     50            1.0000        0.1459       0.3333            0.3333        1.7832  0.0003  0.1548\n",
      "Fine tuning model for subject 6 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4667        1.8632       0.3542            0.3542        1.7099  0.0004  0.1548\n",
      "     32            0.4889        1.4058       0.3646            0.3646        1.6899  0.0005  0.1553\n",
      "     33            0.5333        1.2670       0.3958            0.3958        1.6547  0.0005  0.1566\n",
      "     34            0.6444        1.0360       0.3958            0.3958        1.6098  0.0006  0.1492\n",
      "     35            0.7556        0.8521       0.3750            0.3750        1.5701  0.0006  0.1508\n",
      "     36            \u001b[36m0.8889\u001b[0m        0.6964       0.3646            0.3646        1.5473  0.0007  0.1730\n",
      "     37            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5811\u001b[0m       0.3542            0.3542        1.5448  0.0007  0.1601\n",
      "     38            0.9333        \u001b[32m0.4529\u001b[0m       0.3542            0.3542        1.5607  0.0007  0.1743\n",
      "     39            \u001b[36m0.9556\u001b[0m        \u001b[32m0.3838\u001b[0m       0.3854            0.3854        1.5873  0.0007  0.1728\n",
      "     40            0.9556        \u001b[32m0.3263\u001b[0m       0.3750            0.3750        1.6223  0.0007  0.1762\n",
      "     41            0.9556        \u001b[32m0.2451\u001b[0m       0.3646            0.3646        1.6622  0.0007  0.1833\n",
      "     42            0.9556        \u001b[32m0.2193\u001b[0m       0.3958            0.3958        1.7041  0.0007  0.1919\n",
      "     43            0.9556        \u001b[32m0.2061\u001b[0m       0.3854            0.3854        1.7400  0.0006  0.1816\n",
      "     44            0.9556        0.2132       0.3958            0.3958        1.7685  0.0006  0.1794\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1563\u001b[0m       0.4062            0.4062        1.7848  0.0005  0.1736\n",
      "     46            1.0000        0.1738       0.3958            0.3958        1.7925  0.0005  0.1756\n",
      "     47            1.0000        0.2102       0.3854            0.3854        1.7943  0.0004  0.1771\n",
      "     48            1.0000        0.1645       0.3854            0.3854        1.7920  0.0004  0.1873\n",
      "     49            1.0000        \u001b[32m0.1449\u001b[0m       0.3542            0.3542        1.7904  0.0003  0.1617\n",
      "     50            1.0000        \u001b[32m0.1098\u001b[0m       0.3438            0.3438        1.7872  0.0003  0.1738\n",
      "Fine tuning model for subject 6 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4222        1.4181       0.3542            0.3542        1.7174  0.0004  0.1951\n",
      "     32            0.4667        1.7293       0.3646            0.3646        1.7104  0.0005  0.1494\n",
      "     33            0.5111        1.4912       0.3542            0.3542        1.7072  0.0005  0.1585\n",
      "     34            0.6000        1.0647       0.3021            0.3021        1.7162  0.0006  0.1607\n",
      "     35            0.6889        1.0123       0.2917            0.2917        1.7353  0.0006  0.1512\n",
      "     36            0.7333        0.8432       0.2917            0.2917        1.7804  0.0007  0.1476\n",
      "     37            0.7333        \u001b[32m0.5289\u001b[0m       0.3229            0.3229        1.8375  0.0007  0.1550\n",
      "     38            0.8000        \u001b[32m0.4269\u001b[0m       0.3021            0.3021        1.9085  0.0007  0.1468\n",
      "     39            0.7333        \u001b[32m0.3041\u001b[0m       0.2812            0.2812        1.9829  0.0007  0.1547\n",
      "     40            0.7333        0.3214       0.2917            0.2917        2.0542  0.0007  0.1511\n",
      "     41            0.7333        \u001b[32m0.2559\u001b[0m       0.3021            0.3021        2.1153  0.0007  0.1494\n",
      "     42            0.7111        \u001b[32m0.2528\u001b[0m       0.2917            0.2917        2.1536  0.0007  0.1512\n",
      "     43            0.7111        \u001b[32m0.1767\u001b[0m       0.2917            0.2917        2.1748  0.0006  0.1627\n",
      "     44            0.7333        \u001b[32m0.1405\u001b[0m       0.2812            0.2812        2.1847  0.0006  0.1543\n",
      "     45            0.7333        0.1706       0.3021            0.3021        2.1756  0.0005  0.1659\n",
      "     46            0.7556        0.1643       0.3125            0.3125        2.1546  0.0005  0.1603\n",
      "     47            0.8000        \u001b[32m0.0958\u001b[0m       0.3021            0.3021        2.1295  0.0004  0.1723\n",
      "     48            \u001b[36m0.8444\u001b[0m        0.1290       0.2917            0.2917        2.0991  0.0004  0.1764\n",
      "     49            \u001b[36m0.8889\u001b[0m        0.1132       0.3021            0.3021        2.0682  0.0003  0.1562\n",
      "     50            0.8889        0.1148       0.3333            0.3333        2.0407  0.0003  0.1709\n",
      "Fine tuning model for subject 6 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3556        1.7208       0.3646            0.3646        1.7090  0.0004  0.1639\n",
      "     32            0.4000        1.6969       0.3542            0.3542        1.6853  0.0005  0.1531\n",
      "     33            0.4889        1.4968       0.3333            0.3333        1.6674  0.0005  0.1518\n",
      "     34            0.5556        1.1518       0.3542            0.3542        1.6552  0.0006  0.1609\n",
      "     35            0.6889        1.1020       0.3438            0.3438        1.6554  0.0006  0.1494\n",
      "     36            0.7333        0.7554       0.3229            0.3229        1.6588  0.0007  0.1567\n",
      "     37            0.8000        0.8337       0.3021            0.3021        1.6575  0.0007  0.1545\n",
      "     38            \u001b[36m0.8444\u001b[0m        \u001b[32m0.5024\u001b[0m       0.3333            0.3333        1.6580  0.0007  0.1499\n",
      "     39            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4824\u001b[0m       0.3021            0.3021        1.6670  0.0007  0.1538\n",
      "     40            \u001b[36m0.9556\u001b[0m        \u001b[32m0.3571\u001b[0m       0.3333            0.3333        1.6893  0.0007  0.1598\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3020\u001b[0m       0.3542            0.3542        1.7229  0.0007  0.1503\n",
      "     42            0.9778        0.3184       0.3542            0.3542        1.7623  0.0007  0.1530\n",
      "     43            0.9778        \u001b[32m0.2574\u001b[0m       0.3542            0.3542        1.8052  0.0006  0.1602\n",
      "     44            0.9778        \u001b[32m0.2110\u001b[0m       0.3438            0.3438        1.8462  0.0006  0.1535\n",
      "     45            0.9778        0.2312       0.3542            0.3542        1.8785  0.0005  0.1468\n",
      "     46            0.9778        \u001b[32m0.1658\u001b[0m       0.3438            0.3438        1.9047  0.0005  0.1554\n",
      "     47            1.0000        0.2015       0.3438            0.3438        1.9226  0.0004  0.1517\n",
      "     48            1.0000        0.1910       0.3438            0.3438        1.9340  0.0004  0.1547\n",
      "     49            1.0000        \u001b[32m0.1552\u001b[0m       0.3438            0.3438        1.9416  0.0003  0.1557\n",
      "     50            1.0000        \u001b[32m0.1326\u001b[0m       0.3438            0.3438        1.9462  0.0003  0.1519\n",
      "Fine tuning model for subject 6 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.3591       0.3438            0.3438        1.7121  0.0004  0.1522\n",
      "     32            0.4222        1.3806       0.3438            0.3438        1.7083  0.0005  0.1516\n",
      "     33            0.4444        1.3243       0.3542            0.3542        1.7080  0.0005  0.1595\n",
      "     34            0.5333        1.1294       0.3333            0.3333        1.7133  0.0006  0.1552\n",
      "     35            0.6444        0.9818       0.3438            0.3438        1.7244  0.0006  0.1505\n",
      "     36            0.7556        0.7276       0.3438            0.3438        1.7333  0.0007  0.1508\n",
      "     37            \u001b[36m0.8222\u001b[0m        0.7240       0.3438            0.3438        1.7431  0.0007  0.1540\n",
      "     38            \u001b[36m0.8444\u001b[0m        \u001b[32m0.5657\u001b[0m       0.3229            0.3229        1.7505  0.0007  0.1467\n",
      "     39            \u001b[36m0.8889\u001b[0m        \u001b[32m0.5147\u001b[0m       0.3333            0.3333        1.7406  0.0007  0.1507\n",
      "     40            \u001b[36m0.9556\u001b[0m        \u001b[32m0.5136\u001b[0m       0.3229            0.3229        1.7195  0.0007  0.1583\n",
      "     41            0.9556        \u001b[32m0.4326\u001b[0m       0.3333            0.3333        1.6969  0.0007  0.1520\n",
      "     42            0.9556        \u001b[32m0.2813\u001b[0m       0.3333            0.3333        1.6772  0.0007  0.1574\n",
      "     43            \u001b[36m0.9778\u001b[0m        0.2814       0.3438            0.3438        1.6605  0.0006  0.1505\n",
      "     44            0.9778        0.2956       0.3750            0.3750        1.6467  0.0006  0.1589\n",
      "     45            0.9778        \u001b[32m0.2225\u001b[0m       0.3750            0.3750        1.6347  0.0005  0.1559\n",
      "     46            \u001b[36m1.0000\u001b[0m        0.2543       0.3750            0.3750        1.6236  0.0005  0.1452\n",
      "     47            1.0000        \u001b[32m0.2213\u001b[0m       0.3854            0.3854        1.6151  0.0004  0.1511\n",
      "     48            1.0000        \u001b[32m0.1988\u001b[0m       0.3958            0.3958        1.6112  0.0004  0.1477\n",
      "     49            1.0000        \u001b[32m0.1746\u001b[0m       0.3958            0.3958        1.6091  0.0003  0.1501\n",
      "     50            1.0000        \u001b[32m0.1553\u001b[0m       0.3958            0.3958        1.6076  0.0003  0.1486\n",
      "Fine tuning model for subject 6 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4667        1.6241       0.3542            0.3542        1.7158  0.0004  0.1504\n",
      "     32            0.4667        1.7308       0.3854            0.3854        1.7104  0.0005  0.1492\n",
      "     33            0.5333        1.5768       0.3958            0.3958        1.7075  0.0005  0.1536\n",
      "     34            0.6000        1.2858       0.4062            0.4062        1.7162  0.0006  0.1531\n",
      "     35            0.6444        0.9757       0.3750            0.3750        1.7325  0.0006  0.1494\n",
      "     36            0.7111        0.9205       0.3333            0.3333        1.7589  0.0007  0.1814\n",
      "     37            0.6889        0.7905       0.3333            0.3333        1.7984  0.0007  0.1852\n",
      "     38            0.7333        \u001b[32m0.5841\u001b[0m       0.3021            0.3021        1.8566  0.0007  0.1740\n",
      "     39            0.7556        \u001b[32m0.4196\u001b[0m       0.2812            0.2812        1.9192  0.0007  0.1799\n",
      "     40            0.7778        \u001b[32m0.3492\u001b[0m       0.3229            0.3229        1.9804  0.0007  0.1704\n",
      "     41            0.7778        \u001b[32m0.3318\u001b[0m       0.3438            0.3438        2.0371  0.0007  0.1718\n",
      "     42            0.8000        \u001b[32m0.2850\u001b[0m       0.3854            0.3854        2.0737  0.0007  0.1771\n",
      "     43            \u001b[36m0.8222\u001b[0m        \u001b[32m0.2551\u001b[0m       0.3958            0.3958        2.0908  0.0006  0.1787\n",
      "     44            \u001b[36m0.8444\u001b[0m        \u001b[32m0.2238\u001b[0m       0.3854            0.3854        2.0971  0.0006  0.1793\n",
      "     45            \u001b[36m0.8667\u001b[0m        \u001b[32m0.2141\u001b[0m       0.3958            0.3958        2.0830  0.0005  0.1712\n",
      "     46            \u001b[36m0.9111\u001b[0m        \u001b[32m0.1812\u001b[0m       0.4167            0.4167        2.0522  0.0005  0.1757\n",
      "     47            \u001b[36m0.9333\u001b[0m        0.1848       0.3958            0.3958        2.0141  0.0004  0.1691\n",
      "     48            0.9333        \u001b[32m0.1361\u001b[0m       0.4062            0.4062        1.9719  0.0004  0.1654\n",
      "     49            \u001b[36m0.9778\u001b[0m        0.1362       0.4062            0.4062        1.9288  0.0003  0.2168\n",
      "     50            \u001b[36m1.0000\u001b[0m        0.1400       0.3958            0.3958        1.8871  0.0003  0.1714\n",
      "Fine tuning model for subject 6 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4667        1.7668       0.3438            0.3438        1.6936  0.0004  0.2030\n",
      "     32            0.5333        1.4949       0.3438            0.3438        1.6574  0.0005  0.1562\n",
      "     33            0.5778        1.4134       0.3646            0.3646        1.6303  0.0005  0.1664\n",
      "     34            0.6000        1.2442       0.3542            0.3542        1.6075  0.0006  0.1476\n",
      "     35            0.7111        0.9575       0.3646            0.3646        1.5915  0.0006  0.1595\n",
      "     36            0.7556        0.7875       0.3854            0.3854        1.5854  0.0007  0.1526\n",
      "     37            \u001b[36m0.8889\u001b[0m        \u001b[32m0.6453\u001b[0m       0.3958            0.3958        1.5869  0.0007  0.1513\n",
      "     38            \u001b[36m0.9333\u001b[0m        \u001b[32m0.6230\u001b[0m       0.4271            0.4271        1.5802  0.0007  0.1545\n",
      "     39            \u001b[36m0.9556\u001b[0m        \u001b[32m0.4175\u001b[0m       0.3958            0.3958        1.5707  0.0007  0.1463\n",
      "     40            0.9556        \u001b[32m0.4153\u001b[0m       0.3854            0.3854        1.5688  0.0007  0.1602\n",
      "     41            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3274\u001b[0m       0.3854            0.3854        1.5722  0.0007  0.1548\n",
      "     42            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1940\u001b[0m       0.3646            0.3646        1.5812  0.0007  0.1502\n",
      "     43            1.0000        0.2086       0.3750            0.3750        1.5938  0.0006  0.1602\n",
      "     44            1.0000        0.1947       0.3542            0.3542        1.6085  0.0006  0.1525\n",
      "     45            1.0000        \u001b[32m0.1760\u001b[0m       0.3542            0.3542        1.6245  0.0005  0.1517\n",
      "     46            1.0000        \u001b[32m0.1500\u001b[0m       0.3542            0.3542        1.6389  0.0005  0.1543\n",
      "     47            1.0000        \u001b[32m0.1215\u001b[0m       0.3542            0.3542        1.6500  0.0004  0.1549\n",
      "     48            1.0000        \u001b[32m0.1033\u001b[0m       0.3646            0.3646        1.6591  0.0004  0.1440\n",
      "     49            1.0000        0.1038       0.3646            0.3646        1.6657  0.0003  0.1523\n",
      "     50            1.0000        0.1067       0.3646            0.3646        1.6703  0.0003  0.1489\n",
      "Hold out data from subject 7\n",
      "Pre-training model with data from all subjects but subject 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.4029\u001b[0m        \u001b[32m1.5844\u001b[0m       \u001b[35m0.3659\u001b[0m            \u001b[31m0.3659\u001b[0m        \u001b[94m1.3902\u001b[0m  0.0007  7.3679\n",
      "      2            \u001b[36m0.4729\u001b[0m        \u001b[32m1.3958\u001b[0m       \u001b[35m0.3685\u001b[0m            \u001b[31m0.3685\u001b[0m        \u001b[94m1.3257\u001b[0m  0.0007  7.6660\n",
      "      3            \u001b[36m0.5297\u001b[0m        \u001b[32m1.2983\u001b[0m       \u001b[35m0.4284\u001b[0m            \u001b[31m0.4284\u001b[0m        \u001b[94m1.2734\u001b[0m  0.0007  8.0022\n",
      "      4            \u001b[36m0.5552\u001b[0m        \u001b[32m1.2146\u001b[0m       \u001b[35m0.4570\u001b[0m            \u001b[31m0.4570\u001b[0m        \u001b[94m1.2516\u001b[0m  0.0007  7.4029\n",
      "      5            \u001b[36m0.5997\u001b[0m        \u001b[32m1.1700\u001b[0m       \u001b[35m0.4609\u001b[0m            \u001b[31m0.4609\u001b[0m        \u001b[94m1.2024\u001b[0m  0.0007  7.9683\n",
      "      6            \u001b[36m0.6018\u001b[0m        \u001b[32m1.1293\u001b[0m       0.4583            0.4583        1.2266  0.0006  7.7243\n",
      "      7            0.5570        \u001b[32m1.0830\u001b[0m       \u001b[35m0.4661\u001b[0m            \u001b[31m0.4661\u001b[0m        1.2902  0.0006  7.4833\n",
      "      8            \u001b[36m0.6466\u001b[0m        \u001b[32m1.0372\u001b[0m       \u001b[35m0.4896\u001b[0m            \u001b[31m0.4896\u001b[0m        \u001b[94m1.1648\u001b[0m  0.0006  7.9669\n",
      "      9            0.6146        \u001b[32m1.0163\u001b[0m       0.4818            0.4818        1.2141  0.0006  7.7061\n",
      "     10            \u001b[36m0.6628\u001b[0m        \u001b[32m0.9724\u001b[0m       \u001b[35m0.5052\u001b[0m            \u001b[31m0.5052\u001b[0m        1.1720  0.0005  7.3593\n",
      "     11            0.6573        \u001b[32m0.9655\u001b[0m       \u001b[35m0.5182\u001b[0m            \u001b[31m0.5182\u001b[0m        1.1878  0.0005  8.0549\n",
      "     12            \u001b[36m0.7091\u001b[0m        \u001b[32m0.9452\u001b[0m       \u001b[35m0.5273\u001b[0m            \u001b[31m0.5273\u001b[0m        \u001b[94m1.1445\u001b[0m  0.0005  8.1564\n",
      "     13            0.7003        \u001b[32m0.9252\u001b[0m       0.5156            0.5156        \u001b[94m1.1380\u001b[0m  0.0004  7.5209\n",
      "     14            0.6734        \u001b[32m0.8948\u001b[0m       \u001b[35m0.5326\u001b[0m            \u001b[31m0.5326\u001b[0m        1.1620  0.0004  8.1712\n",
      "     15            \u001b[36m0.7281\u001b[0m        \u001b[32m0.8776\u001b[0m       0.5247            0.5247        \u001b[94m1.1111\u001b[0m  0.0004  7.5755\n",
      "     16            \u001b[36m0.7339\u001b[0m        \u001b[32m0.8612\u001b[0m       0.5326            0.5326        1.1380  0.0003  7.4123\n",
      "     17            \u001b[36m0.7391\u001b[0m        \u001b[32m0.8400\u001b[0m       \u001b[35m0.5339\u001b[0m            \u001b[31m0.5339\u001b[0m        1.1221  0.0003  8.0651\n",
      "     18            \u001b[36m0.7409\u001b[0m        \u001b[32m0.7978\u001b[0m       0.5143            0.5143        1.1353  0.0003  7.6005\n",
      "     19            \u001b[36m0.7471\u001b[0m        0.8030       \u001b[35m0.5404\u001b[0m            \u001b[31m0.5404\u001b[0m        1.1373  0.0002  7.4158\n",
      "     20            \u001b[36m0.7552\u001b[0m        \u001b[32m0.7798\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.1201  0.0002  8.1302\n",
      "     21            \u001b[36m0.7732\u001b[0m        \u001b[32m0.7736\u001b[0m       0.5404            0.5404        1.1113  0.0002  8.0855\n",
      "     22            0.7669        \u001b[32m0.7649\u001b[0m       \u001b[35m0.5586\u001b[0m            \u001b[31m0.5586\u001b[0m        \u001b[94m1.1060\u001b[0m  0.0001  7.5696\n",
      "     23            \u001b[36m0.7792\u001b[0m        \u001b[32m0.7586\u001b[0m       0.5534            0.5534        \u001b[94m1.1005\u001b[0m  0.0001  8.3036\n",
      "     24            0.7789        \u001b[32m0.7435\u001b[0m       0.5560            0.5560        1.1080  0.0001  7.9106\n",
      "     25            \u001b[36m0.7914\u001b[0m        \u001b[32m0.7360\u001b[0m       0.5560            0.5560        \u001b[94m1.0974\u001b[0m  0.0001  7.6213\n",
      "     26            0.7867        \u001b[32m0.7331\u001b[0m       0.5469            0.5469        1.1015  0.0000  8.4342\n",
      "     27            \u001b[36m0.7932\u001b[0m        0.7340       0.5469            0.5469        1.0982  0.0000  7.7997\n",
      "     28            \u001b[36m0.7951\u001b[0m        \u001b[32m0.7279\u001b[0m       0.5430            0.5430        1.0987  0.0000  7.5226\n",
      "     29            0.7945        \u001b[32m0.7214\u001b[0m       0.5456            0.5456        1.0985  0.0000  8.2391\n",
      "     30            0.7945        \u001b[32m0.7115\u001b[0m       0.5417            0.5417        1.0985  0.0000  7.7933\n",
      "Before finetuning for subject 7, the baseline accuracy is 0.28125\n",
      "Fine tuning model for subject 7 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.2160       0.3229            0.3229        1.5004  0.0004  0.1312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        0.7490       0.3125            0.3125        1.5146  0.0005  0.1367\n",
      "     33            1.0000        \u001b[32m0.3425\u001b[0m       0.3646            0.3646        1.5478  0.0005  0.1651\n",
      "     34            1.0000        \u001b[32m0.1981\u001b[0m       0.3229            0.3229        1.5848  0.0006  0.1270\n",
      "     35            1.0000        \u001b[32m0.1592\u001b[0m       0.3229            0.3229        1.6181  0.0006  0.1333\n",
      "     36            1.0000        \u001b[32m0.0886\u001b[0m       0.3333            0.3333        1.6488  0.0007  0.1313\n",
      "     37            1.0000        \u001b[32m0.0363\u001b[0m       0.2917            0.2917        1.6784  0.0007  0.1387\n",
      "     38            1.0000        \u001b[32m0.0312\u001b[0m       0.3021            0.3021        1.7091  0.0007  0.1244\n",
      "     39            1.0000        \u001b[32m0.0114\u001b[0m       0.3229            0.3229        1.7421  0.0007  0.1285\n",
      "     40            1.0000        \u001b[32m0.0059\u001b[0m       0.3125            0.3125        1.7756  0.0007  0.1331\n",
      "     41            1.0000        0.0192       0.3125            0.3125        1.8071  0.0007  0.1284\n",
      "     42            1.0000        0.0096       0.3125            0.3125        1.8357  0.0007  0.1345\n",
      "     43            1.0000        \u001b[32m0.0038\u001b[0m       0.3229            0.3229        1.8600  0.0006  0.1314\n",
      "     44            1.0000        0.0039       0.3229            0.3229        1.8793  0.0006  0.1327\n",
      "     45            1.0000        \u001b[32m0.0033\u001b[0m       0.3229            0.3229        1.8937  0.0005  0.1297\n",
      "     46            1.0000        0.0058       0.3333            0.3333        1.9039  0.0005  0.1284\n",
      "     47            1.0000        0.0035       0.3229            0.3229        1.9103  0.0004  0.1281\n",
      "     48            1.0000        0.0037       0.3229            0.3229        1.9138  0.0004  0.1287\n",
      "     49            1.0000        \u001b[32m0.0029\u001b[0m       0.3229            0.3229        1.9149  0.0003  0.1359\n",
      "     50            1.0000        \u001b[32m0.0021\u001b[0m       0.3125            0.3125        1.9143  0.0003  0.1406\n",
      "Fine tuning model for subject 7 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        1.1930       0.3125            0.3125        1.5036  0.0004  0.1298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        0.7691       0.3021            0.3021        1.5112  0.0005  0.1333\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2275\u001b[0m       0.3333            0.3333        1.5244  0.0005  0.1333\n",
      "     34            1.0000        \u001b[32m0.0903\u001b[0m       0.3125            0.3125        1.5381  0.0006  0.1346\n",
      "     35            1.0000        \u001b[32m0.0380\u001b[0m       0.3438            0.3438        1.5499  0.0006  0.1283\n",
      "     36            1.0000        \u001b[32m0.0322\u001b[0m       0.3438            0.3438        1.5584  0.0007  0.1282\n",
      "     37            1.0000        0.1249       0.3333            0.3333        1.5619  0.0007  0.1343\n",
      "     38            1.0000        0.2716       0.3438            0.3438        1.5524  0.0007  0.1311\n",
      "     39            1.0000        \u001b[32m0.0128\u001b[0m       0.3438            0.3438        1.5496  0.0007  0.1284\n",
      "     40            1.0000        \u001b[32m0.0040\u001b[0m       0.3854            0.3854        1.5560  0.0007  0.1387\n",
      "     41            1.0000        \u001b[32m0.0022\u001b[0m       0.3958            0.3958        1.5735  0.0007  0.1301\n",
      "     42            1.0000        0.0031       0.3958            0.3958        1.6012  0.0007  0.1287\n",
      "     43            1.0000        \u001b[32m0.0021\u001b[0m       0.3958            0.3958        1.6353  0.0006  0.1327\n",
      "     44            1.0000        \u001b[32m0.0019\u001b[0m       0.3854            0.3854        1.6709  0.0006  0.1661\n",
      "     45            1.0000        \u001b[32m0.0014\u001b[0m       0.3854            0.3854        1.7039  0.0005  0.1659\n",
      "     46            1.0000        0.0026       0.3854            0.3854        1.7313  0.0005  0.1505\n",
      "     47            1.0000        0.0029       0.3958            0.3958        1.7521  0.0004  0.1484\n",
      "     48            1.0000        0.0022       0.3646            0.3646        1.7664  0.0004  0.1377\n",
      "     49            1.0000        0.0016       0.3750            0.3750        1.7750  0.0003  0.1484\n",
      "     50            1.0000        0.0021       0.3750            0.3750        1.7790  0.0003  0.1291\n",
      "Fine tuning model for subject 7 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.4855       0.3125            0.3125        1.4783  0.0004  0.1312\n",
      "     32            \u001b[36m1.0000\u001b[0m        1.1148       0.3333            0.3333        1.4944  0.0005  0.1347\n",
      "     33            1.0000        0.8277       0.3333            0.3333        1.5627  0.0005  0.1281\n",
      "     34            1.0000        \u001b[32m0.0977\u001b[0m       0.3542            0.3542        1.6652  0.0006  0.1337\n",
      "     35            1.0000        \u001b[32m0.0260\u001b[0m       0.3646            0.3646        1.7997  0.0006  0.1319\n",
      "     36            1.0000        \u001b[32m0.0154\u001b[0m       0.3229            0.3229        1.9704  0.0007  0.1386\n",
      "     37            1.0000        \u001b[32m0.0093\u001b[0m       0.3021            0.3021        2.1665  0.0007  0.1298\n",
      "     38            1.0000        \u001b[32m0.0064\u001b[0m       0.2917            0.2917        2.3663  0.0007  0.1298\n",
      "     39            1.0000        \u001b[32m0.0043\u001b[0m       0.2812            0.2812        2.5474  0.0007  0.1341\n",
      "     40            1.0000        \u001b[32m0.0037\u001b[0m       0.2812            0.2812        2.6942  0.0007  0.1605\n",
      "     41            1.0000        \u001b[32m0.0018\u001b[0m       0.2917            0.2917        2.7989  0.0007  0.1293\n",
      "     42            1.0000        0.0039       0.3021            0.3021        2.8615  0.0007  0.1349\n",
      "     43            1.0000        0.0031       0.3125            0.3125        2.8854  0.0006  0.1322\n",
      "     44            1.0000        0.0035       0.3021            0.3021        2.8765  0.0006  0.1388\n",
      "     45            1.0000        0.0023       0.3021            0.3021        2.8417  0.0005  0.1279\n",
      "     46            1.0000        0.0019       0.2917            0.2917        2.7893  0.0005  0.1425\n",
      "     47            1.0000        \u001b[32m0.0009\u001b[0m       0.3021            0.3021        2.7263  0.0004  0.1307\n",
      "     48            1.0000        0.0070       0.2917            0.2917        2.6600  0.0004  0.1307\n",
      "     49            1.0000        0.0015       0.2917            0.2917        2.5939  0.0003  0.1270\n",
      "     50            1.0000        0.0013       0.2917            0.2917        2.5314  0.0003  0.1300\n",
      "Fine tuning model for subject 7 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.0396       0.3229            0.3229        1.4835  0.0004  0.1291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        1.2443       0.3542            0.3542        1.4659  0.0005  0.1300\n",
      "     33            1.0000        \u001b[32m0.3554\u001b[0m       0.3542            0.3542        1.4792  0.0005  0.1281\n",
      "     34            1.0000        0.4009       0.3542            0.3542        1.5234  0.0006  0.1328\n",
      "     35            1.0000        \u001b[32m0.0859\u001b[0m       0.2917            0.2917        1.6048  0.0006  0.1257\n",
      "     36            1.0000        \u001b[32m0.0463\u001b[0m       0.2708            0.2708        1.7053  0.0007  0.1365\n",
      "     37            1.0000        \u001b[32m0.0191\u001b[0m       0.2500            0.2500        1.8065  0.0007  0.1414\n",
      "     38            1.0000        \u001b[32m0.0104\u001b[0m       0.2396            0.2396        1.8974  0.0007  0.1263\n",
      "     39            1.0000        0.0137       0.2396            0.2396        1.9704  0.0007  0.1425\n",
      "     40            1.0000        \u001b[32m0.0037\u001b[0m       0.2396            0.2396        2.0242  0.0007  0.1681\n",
      "     41            1.0000        0.0054       0.2292            0.2292        2.0594  0.0007  0.1309\n",
      "     42            1.0000        \u001b[32m0.0028\u001b[0m       0.2188            0.2188        2.0779  0.0007  0.1555\n",
      "     43            1.0000        0.0054       0.2292            0.2292        2.0829  0.0006  0.1666\n",
      "     44            1.0000        0.0054       0.2396            0.2396        2.0775  0.0006  0.1651\n",
      "     45            1.0000        0.0040       0.2292            0.2292        2.0649  0.0005  0.1704\n",
      "     46            1.0000        \u001b[32m0.0009\u001b[0m       0.2396            0.2396        2.0481  0.0005  0.1745\n",
      "     47            1.0000        0.0022       0.2396            0.2396        2.0291  0.0004  0.1706\n",
      "     48            1.0000        0.0031       0.2396            0.2396        2.0097  0.0004  0.1736\n",
      "     49            1.0000        0.0020       0.2500            0.2500        1.9909  0.0003  0.1649\n",
      "     50            1.0000        0.0013       0.2500            0.2500        1.9736  0.0003  0.1783\n",
      "Fine tuning model for subject 7 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        1.0384       0.3229            0.3229        1.5122  0.0004  0.1739\n",
      "     32            0.8000        0.8725       0.3333            0.3333        1.5433  0.0005  0.1689\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6421\u001b[0m       0.3646            0.3646        1.5826  0.0005  0.1633\n",
      "     34            1.0000        \u001b[32m0.1921\u001b[0m       0.3021            0.3021        1.6211  0.0006  0.1736\n",
      "     35            1.0000        \u001b[32m0.0909\u001b[0m       0.2708            0.2708        1.6576  0.0006  0.1850\n",
      "     36            1.0000        \u001b[32m0.0458\u001b[0m       0.2917            0.2917        1.6928  0.0007  0.1337\n",
      "     37            1.0000        \u001b[32m0.0149\u001b[0m       0.2708            0.2708        1.7314  0.0007  0.1387\n",
      "     38            1.0000        \u001b[32m0.0094\u001b[0m       0.2812            0.2812        1.7760  0.0007  0.1401\n",
      "     39            1.0000        \u001b[32m0.0082\u001b[0m       0.2812            0.2812        1.8257  0.0007  0.1250\n",
      "     40            1.0000        \u001b[32m0.0056\u001b[0m       0.2500            0.2500        1.8773  0.0007  0.1310\n",
      "     41            1.0000        \u001b[32m0.0032\u001b[0m       0.2292            0.2292        1.9266  0.0007  0.1330\n",
      "     42            1.0000        0.0073       0.2292            0.2292        1.9701  0.0007  0.1342\n",
      "     43            1.0000        0.0074       0.2188            0.2188        2.0059  0.0006  0.1233\n",
      "     44            1.0000        0.0092       0.2083            0.2083        2.0331  0.0006  0.1335\n",
      "     45            1.0000        0.0124       0.2083            0.2083        2.0527  0.0005  0.1317\n",
      "     46            1.0000        0.0064       0.2083            0.2083        2.0657  0.0005  0.1302\n",
      "     47            1.0000        0.0075       0.2292            0.2292        2.0736  0.0004  0.1344\n",
      "     48            1.0000        0.0101       0.2500            0.2500        2.0772  0.0004  0.1331\n",
      "     49            1.0000        0.0036       0.2604            0.2604        2.0785  0.0003  0.1338\n",
      "     50            1.0000        0.0053       0.2604            0.2604        2.0782  0.0003  0.1739\n",
      "Fine tuning model for subject 7 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.1957       0.3125            0.3125        1.4885  0.0004  0.1302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6010\u001b[0m       0.3750            0.3750        1.4913  0.0005  0.1337\n",
      "     33            1.0000        \u001b[32m0.2557\u001b[0m       0.3646            0.3646        1.5164  0.0005  0.1328\n",
      "     34            1.0000        \u001b[32m0.1720\u001b[0m       0.3750            0.3750        1.5665  0.0006  0.1375\n",
      "     35            1.0000        \u001b[32m0.0503\u001b[0m       0.3333            0.3333        1.6329  0.0006  0.1329\n",
      "     36            1.0000        \u001b[32m0.0257\u001b[0m       0.3542            0.3542        1.7085  0.0007  0.1378\n",
      "     37            1.0000        \u001b[32m0.0047\u001b[0m       0.3333            0.3333        1.7887  0.0007  0.1407\n",
      "     38            1.0000        \u001b[32m0.0043\u001b[0m       0.3229            0.3229        1.8701  0.0007  0.1334\n",
      "     39            1.0000        \u001b[32m0.0028\u001b[0m       0.3333            0.3333        1.9495  0.0007  0.1226\n",
      "     40            1.0000        0.0035       0.3229            0.3229        2.0236  0.0007  0.1340\n",
      "     41            1.0000        \u001b[32m0.0014\u001b[0m       0.3438            0.3438        2.0894  0.0007  0.1293\n",
      "     42            1.0000        0.0025       0.3438            0.3438        2.1448  0.0007  0.1264\n",
      "     43            1.0000        0.0040       0.3333            0.3333        2.1893  0.0006  0.1341\n",
      "     44            1.0000        \u001b[32m0.0004\u001b[0m       0.3333            0.3333        2.2222  0.0006  0.1288\n",
      "     45            1.0000        \u001b[32m0.0004\u001b[0m       0.3333            0.3333        2.2447  0.0005  0.1273\n",
      "     46            1.0000        0.0014       0.3229            0.3229        2.2585  0.0005  0.1335\n",
      "     47            1.0000        0.0005       0.2812            0.2812        2.2656  0.0004  0.1296\n",
      "     48            1.0000        0.0005       0.3021            0.3021        2.2679  0.0004  0.1360\n",
      "     49            1.0000        0.0004       0.2917            0.2917        2.2672  0.0003  0.1264\n",
      "     50            1.0000        \u001b[32m0.0003\u001b[0m       0.3021            0.3021        2.2650  0.0003  0.1308\n",
      "Fine tuning model for subject 7 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.9037       0.3125            0.3125        1.4763  0.0004  0.1252\n",
      "     32            \u001b[36m1.0000\u001b[0m        0.8626       0.3542            0.3542        1.4622  0.0005  0.1316\n",
      "     33            1.0000        \u001b[32m0.3571\u001b[0m       0.3854            0.3854        1.4842  0.0005  0.1360\n",
      "     34            1.0000        0.5249       0.3229            0.3229        1.5116  0.0006  0.1261\n",
      "     35            1.0000        \u001b[32m0.0530\u001b[0m       0.3438            0.3438        1.5427  0.0006  0.1409\n",
      "     36            1.0000        \u001b[32m0.0217\u001b[0m       0.3438            0.3438        1.5964  0.0007  0.1281\n",
      "     37            1.0000        0.0424       0.3229            0.3229        1.6881  0.0007  0.1361\n",
      "     38            1.0000        \u001b[32m0.0170\u001b[0m       0.3333            0.3333        1.8192  0.0007  0.1328\n",
      "     39            1.0000        \u001b[32m0.0047\u001b[0m       0.3229            0.3229        1.9737  0.0007  0.1262\n",
      "     40            1.0000        \u001b[32m0.0041\u001b[0m       0.3229            0.3229        2.1290  0.0007  0.1344\n",
      "     41            1.0000        0.0050       0.3125            0.3125        2.2647  0.0007  0.1280\n",
      "     42            1.0000        0.0066       0.3125            0.3125        2.3677  0.0007  0.1376\n",
      "     43            1.0000        0.0042       0.3125            0.3125        2.4336  0.0006  0.1311\n",
      "     44            1.0000        0.0067       0.3125            0.3125        2.4631  0.0006  0.1389\n",
      "     45            1.0000        \u001b[32m0.0035\u001b[0m       0.3229            0.3229        2.4636  0.0005  0.1312\n",
      "     46            1.0000        0.0051       0.3229            0.3229        2.4426  0.0005  0.1256\n",
      "     47            1.0000        0.0074       0.3229            0.3229        2.4081  0.0004  0.1292\n",
      "     48            1.0000        0.0329       0.3229            0.3229        2.3606  0.0004  0.1321\n",
      "     49            1.0000        0.0056       0.3333            0.3333        2.3164  0.0003  0.1267\n",
      "     50            1.0000        0.0056       0.3333            0.3333        2.2780  0.0003  0.1335\n",
      "Fine tuning model for subject 7 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.8864       0.2917            0.2917        1.5076  0.0004  0.1296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        1.3421       0.2917            0.2917        1.5177  0.0005  0.1324\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.8745       0.2812            0.2812        1.5452  0.0005  0.1288\n",
      "     34            1.0000        \u001b[32m0.1501\u001b[0m       0.2708            0.2708        1.5919  0.0006  0.1299\n",
      "     35            1.0000        \u001b[32m0.0513\u001b[0m       0.2812            0.2812        1.6489  0.0006  0.1282\n",
      "     36            1.0000        \u001b[32m0.0156\u001b[0m       0.2917            0.2917        1.7144  0.0007  0.1295\n",
      "     37            1.0000        \u001b[32m0.0058\u001b[0m       0.2917            0.2917        1.7845  0.0007  0.1332\n",
      "     38            1.0000        0.0082       0.2917            0.2917        1.8576  0.0007  0.1335\n",
      "     39            1.0000        0.0077       0.3125            0.3125        1.9340  0.0007  0.1339\n",
      "     40            1.0000        0.0107       0.3125            0.3125        2.0120  0.0007  0.1288\n",
      "     41            1.0000        \u001b[32m0.0043\u001b[0m       0.3125            0.3125        2.0891  0.0007  0.1329\n",
      "     42            1.0000        \u001b[32m0.0013\u001b[0m       0.3125            0.3125        2.1620  0.0007  0.1323\n",
      "     43            1.0000        0.0070       0.2917            0.2917        2.2325  0.0006  0.1333\n",
      "     44            1.0000        0.0053       0.2917            0.2917        2.2981  0.0006  0.1344\n",
      "     45            1.0000        0.0035       0.2708            0.2708        2.3581  0.0005  0.1280\n",
      "     46            1.0000        0.0015       0.2604            0.2604        2.4125  0.0005  0.1319\n",
      "     47            1.0000        0.0017       0.2604            0.2604        2.4614  0.0004  0.1296\n",
      "     48            1.0000        0.0015       0.2500            0.2500        2.5051  0.0004  0.1303\n",
      "     49            1.0000        0.0073       0.2500            0.2500        2.5443  0.0003  0.1316\n",
      "     50            1.0000        0.0044       0.2604            0.2604        2.5791  0.0003  0.1345\n",
      "Fine tuning model for subject 7 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.4760       0.3021            0.3021        1.5011  0.0004  0.1371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        1.2894       0.2812            0.2812        1.5085  0.0005  0.1414\n",
      "     33            1.0000        0.8617       0.3229            0.3229        1.5260  0.0005  0.1339\n",
      "     34            1.0000        \u001b[32m0.2183\u001b[0m       0.3125            0.3125        1.5458  0.0006  0.1324\n",
      "     35            1.0000        \u001b[32m0.0571\u001b[0m       0.3333            0.3333        1.5690  0.0006  0.1498\n",
      "     36            1.0000        \u001b[32m0.0293\u001b[0m       0.3542            0.3542        1.5928  0.0007  0.1591\n",
      "     37            1.0000        0.0325       0.3333            0.3333        1.6193  0.0007  0.1547\n",
      "     38            1.0000        \u001b[32m0.0123\u001b[0m       0.3333            0.3333        1.6452  0.0007  0.1550\n",
      "     39            1.0000        0.0190       0.3333            0.3333        1.6697  0.0007  0.1389\n",
      "     40            1.0000        0.0251       0.3021            0.3021        1.6917  0.0007  0.1317\n",
      "     41            1.0000        0.0179       0.3021            0.3021        1.7081  0.0007  0.1262\n",
      "     42            1.0000        \u001b[32m0.0113\u001b[0m       0.3125            0.3125        1.7184  0.0007  0.1258\n",
      "     43            1.0000        \u001b[32m0.0041\u001b[0m       0.2917            0.2917        1.7228  0.0006  0.1276\n",
      "     44            1.0000        \u001b[32m0.0036\u001b[0m       0.2917            0.2917        1.7225  0.0006  0.1281\n",
      "     45            1.0000        0.0038       0.2812            0.2812        1.7189  0.0005  0.1360\n",
      "     46            1.0000        0.0040       0.2812            0.2812        1.7130  0.0005  0.1438\n",
      "     47            1.0000        0.0054       0.2708            0.2708        1.7060  0.0004  0.1688\n",
      "     48            1.0000        \u001b[32m0.0023\u001b[0m       0.2708            0.2708        1.6986  0.0004  0.1697\n",
      "     49            1.0000        0.0030       0.2604            0.2604        1.6913  0.0003  0.1647\n",
      "     50            1.0000        0.0042       0.2604            0.2604        1.6845  0.0003  0.1714\n",
      "Fine tuning model for subject 7 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.4544       0.3125            0.3125        1.4902  0.0004  0.1752\n",
      "     32            \u001b[36m1.0000\u001b[0m        0.8098       0.3854            0.3854        1.4816  0.0005  0.1865\n",
      "     33            1.0000        \u001b[32m0.1947\u001b[0m       0.3854            0.3854        1.4842  0.0005  0.1649\n",
      "     34            1.0000        0.2033       0.3854            0.3854        1.5055  0.0006  0.1727\n",
      "     35            1.0000        \u001b[32m0.0590\u001b[0m       0.3542            0.3542        1.5420  0.0006  0.1734\n",
      "     36            1.0000        \u001b[32m0.0211\u001b[0m       0.3125            0.3125        1.5888  0.0007  0.1681\n",
      "     37            1.0000        0.0212       0.2812            0.2812        1.6445  0.0007  0.1681\n",
      "     38            1.0000        \u001b[32m0.0209\u001b[0m       0.2708            0.2708        1.7070  0.0007  0.1676\n",
      "     39            1.0000        \u001b[32m0.0120\u001b[0m       0.2812            0.2812        1.7732  0.0007  0.1677\n",
      "     40            1.0000        \u001b[32m0.0034\u001b[0m       0.2500            0.2500        1.8401  0.0007  0.1727\n",
      "     41            1.0000        0.0036       0.2604            0.2604        1.9048  0.0007  0.1649\n",
      "     42            1.0000        \u001b[32m0.0017\u001b[0m       0.2604            0.2604        1.9649  0.0007  0.1477\n",
      "     43            1.0000        0.0063       0.2604            0.2604        2.0185  0.0006  0.1326\n",
      "     44            1.0000        0.0017       0.2396            0.2396        2.0653  0.0006  0.1290\n",
      "     45            1.0000        \u001b[32m0.0011\u001b[0m       0.2188            0.2188        2.1057  0.0005  0.1310\n",
      "     46            1.0000        0.0012       0.2188            0.2188        2.1401  0.0005  0.1316\n",
      "     47            1.0000        \u001b[32m0.0007\u001b[0m       0.2188            0.2188        2.1696  0.0004  0.1340\n",
      "     48            1.0000        \u001b[32m0.0006\u001b[0m       0.2083            0.2083        2.1952  0.0004  0.1315\n",
      "     49            1.0000        0.0051       0.2292            0.2292        2.2178  0.0003  0.1336\n",
      "     50            1.0000        0.0021       0.2292            0.2292        2.2379  0.0003  0.1305\n",
      "Fine tuning model for subject 7 with 10 = 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.8113       0.2812            0.2812        1.4943  0.0004  0.1019\n",
      "     32            0.6000        1.3470       0.3438            0.3438        1.4818  0.0005  0.1187\n",
      "     33            \u001b[36m0.8000\u001b[0m        0.9645       0.3438            0.3438        1.4716  0.0005  0.1068\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6133\u001b[0m       0.3646            0.3646        1.4762  0.0006  0.1170\n",
      "     35            1.0000        \u001b[32m0.3823\u001b[0m       0.3438            0.3438        1.4876  0.0006  0.1139\n",
      "     36            1.0000        \u001b[32m0.2722\u001b[0m       0.3229            0.3229        1.5001  0.0007  0.1176\n",
      "     37            1.0000        \u001b[32m0.1024\u001b[0m       0.3333            0.3333        1.5217  0.0007  0.1085\n",
      "     38            1.0000        \u001b[32m0.0411\u001b[0m       0.3438            0.3438        1.5544  0.0007  0.1086\n",
      "     39            1.0000        \u001b[32m0.0395\u001b[0m       0.3438            0.3438        1.5957  0.0007  0.1104\n",
      "     40            1.0000        \u001b[32m0.0390\u001b[0m       0.3542            0.3542        1.6409  0.0007  0.1129\n",
      "     41            1.0000        \u001b[32m0.0163\u001b[0m       0.3438            0.3438        1.6855  0.0007  0.1117\n",
      "     42            1.0000        \u001b[32m0.0144\u001b[0m       0.3438            0.3438        1.7252  0.0007  0.1177\n",
      "     43            1.0000        0.0514       0.3438            0.3438        1.7560  0.0006  0.1153\n",
      "     44            1.0000        0.0152       0.3333            0.3333        1.7798  0.0006  0.1136\n",
      "     45            1.0000        0.0149       0.3125            0.3125        1.7975  0.0005  0.1172\n",
      "     46            1.0000        0.0147       0.3125            0.3125        1.8101  0.0005  0.1121\n",
      "     47            1.0000        \u001b[32m0.0120\u001b[0m       0.3125            0.3125        1.8191  0.0004  0.1103\n",
      "     48            1.0000        \u001b[32m0.0114\u001b[0m       0.3125            0.3125        1.8264  0.0004  0.1131\n",
      "     49            1.0000        0.0208       0.3125            0.3125        1.8330  0.0003  0.1161\n",
      "     50            1.0000        0.0180       0.3229            0.3229        1.8397  0.0003  0.1100\n",
      "Fine tuning model for subject 7 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        1.6306       0.3333            0.3333        1.4804  0.0004  0.1173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        1.8306       0.3646            0.3646        1.4492  0.0005  0.1131\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.8926       0.3542            0.3542        1.4468  0.0005  0.1120\n",
      "     34            1.0000        \u001b[32m0.4545\u001b[0m       0.3542            0.3542        1.4800  0.0006  0.1122\n",
      "     35            1.0000        \u001b[32m0.2782\u001b[0m       0.3646            0.3646        1.5461  0.0006  0.1117\n",
      "     36            1.0000        \u001b[32m0.0830\u001b[0m       0.3333            0.3333        1.6363  0.0007  0.1143\n",
      "     37            1.0000        0.0915       0.3229            0.3229        1.7374  0.0007  0.1088\n",
      "     38            1.0000        0.0865       0.3125            0.3125        1.8306  0.0007  0.1508\n",
      "     39            1.0000        \u001b[32m0.0768\u001b[0m       0.3021            0.3021        1.9034  0.0007  0.1567\n",
      "     40            1.0000        \u001b[32m0.0179\u001b[0m       0.3229            0.3229        1.9615  0.0007  0.1812\n",
      "     41            1.0000        0.0529       0.2812            0.2812        2.0103  0.0007  0.1646\n",
      "     42            1.0000        0.0285       0.2708            0.2708        2.0369  0.0007  0.1484\n",
      "     43            1.0000        0.0294       0.2708            0.2708        2.0438  0.0006  0.1594\n",
      "     44            1.0000        0.0242       0.2708            0.2708        2.0351  0.0006  0.1428\n",
      "     45            1.0000        0.0259       0.3021            0.3021        2.0149  0.0005  0.1457\n",
      "     46            1.0000        0.0278       0.3125            0.3125        1.9902  0.0005  0.1460\n",
      "     47            1.0000        0.0368       0.3125            0.3125        1.9646  0.0004  0.1455\n",
      "     48            1.0000        0.0181       0.3021            0.3021        1.9404  0.0004  0.1206\n",
      "     49            1.0000        \u001b[32m0.0161\u001b[0m       0.3229            0.3229        1.9192  0.0003  0.1167\n",
      "     50            1.0000        \u001b[32m0.0119\u001b[0m       0.3333            0.3333        1.9010  0.0003  0.1229\n",
      "Fine tuning model for subject 7 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.2226       0.3021            0.3021        1.4766  0.0004  0.1168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        0.8745       0.3542            0.3542        1.4544  0.0005  0.1164\n",
      "     33            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5504\u001b[0m       0.3750            0.3750        1.4458  0.0005  0.1065\n",
      "     34            0.9000        \u001b[32m0.3901\u001b[0m       0.3542            0.3542        1.4561  0.0006  0.1066\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1871\u001b[0m       0.3750            0.3750        1.4748  0.0006  0.1125\n",
      "     36            1.0000        \u001b[32m0.0624\u001b[0m       0.3646            0.3646        1.4886  0.0007  0.1139\n",
      "     37            1.0000        \u001b[32m0.0581\u001b[0m       0.3854            0.3854        1.4926  0.0007  0.1102\n",
      "     38            1.0000        \u001b[32m0.0308\u001b[0m       0.4062            0.4062        1.4930  0.0007  0.1106\n",
      "     39            1.0000        \u001b[32m0.0195\u001b[0m       0.3958            0.3958        1.4962  0.0007  0.1149\n",
      "     40            1.0000        \u001b[32m0.0137\u001b[0m       0.3750            0.3750        1.5058  0.0007  0.1066\n",
      "     41            1.0000        \u001b[32m0.0093\u001b[0m       0.3542            0.3542        1.5217  0.0007  0.1056\n",
      "     42            1.0000        0.0117       0.3438            0.3438        1.5435  0.0007  0.1116\n",
      "     43            1.0000        0.0132       0.3333            0.3333        1.5698  0.0006  0.1117\n",
      "     44            1.0000        0.0180       0.3333            0.3333        1.5985  0.0006  0.1113\n",
      "     45            1.0000        0.0138       0.3229            0.3229        1.6282  0.0005  0.1122\n",
      "     46            1.0000        \u001b[32m0.0067\u001b[0m       0.3125            0.3125        1.6583  0.0005  0.1083\n",
      "     47            1.0000        \u001b[32m0.0043\u001b[0m       0.3333            0.3333        1.6879  0.0004  0.1172\n",
      "     48            1.0000        0.0053       0.3438            0.3438        1.7166  0.0004  0.1097\n",
      "     49            1.0000        0.0080       0.3438            0.3438        1.7442  0.0003  0.1067\n",
      "     50            1.0000        0.0049       0.3333            0.3333        1.7702  0.0003  0.1092\n",
      "Fine tuning model for subject 7 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.0398       0.3021            0.3021        1.4910  0.0004  0.1173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        1.2902       0.3438            0.3438        1.4951  0.0005  0.1130\n",
      "     33            \u001b[36m0.8000\u001b[0m        \u001b[32m0.6264\u001b[0m       0.3229            0.3229        1.5204  0.0005  0.1147\n",
      "     34            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5685\u001b[0m       0.3229            0.3229        1.5602  0.0006  0.1124\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3225\u001b[0m       0.3021            0.3021        1.6102  0.0006  0.1097\n",
      "     36            1.0000        \u001b[32m0.1401\u001b[0m       0.2812            0.2812        1.6627  0.0007  0.1077\n",
      "     37            1.0000        \u001b[32m0.0934\u001b[0m       0.2812            0.2812        1.7151  0.0007  0.1116\n",
      "     38            1.0000        \u001b[32m0.0413\u001b[0m       0.2812            0.2812        1.7625  0.0007  0.1154\n",
      "     39            1.0000        \u001b[32m0.0372\u001b[0m       0.2917            0.2917        1.8025  0.0007  0.1165\n",
      "     40            1.0000        \u001b[32m0.0251\u001b[0m       0.2708            0.2708        1.8332  0.0007  0.1154\n",
      "     41            1.0000        0.0354       0.2604            0.2604        1.8566  0.0007  0.1187\n",
      "     42            1.0000        \u001b[32m0.0250\u001b[0m       0.2812            0.2812        1.8730  0.0007  0.1282\n",
      "     43            1.0000        \u001b[32m0.0210\u001b[0m       0.2708            0.2708        1.8837  0.0006  0.1094\n",
      "     44            1.0000        \u001b[32m0.0132\u001b[0m       0.2812            0.2812        1.8904  0.0006  0.1097\n",
      "     45            1.0000        \u001b[32m0.0129\u001b[0m       0.2917            0.2917        1.8939  0.0005  0.1136\n",
      "     46            1.0000        \u001b[32m0.0114\u001b[0m       0.2604            0.2604        1.8947  0.0005  0.1126\n",
      "     47            1.0000        \u001b[32m0.0074\u001b[0m       0.2604            0.2604        1.8935  0.0004  0.1106\n",
      "     48            1.0000        0.0124       0.2604            0.2604        1.8907  0.0004  0.1131\n",
      "     49            1.0000        0.0082       0.2604            0.2604        1.8869  0.0003  0.1108\n",
      "     50            1.0000        0.0104       0.2500            0.2500        1.8827  0.0003  0.1125\n",
      "Fine tuning model for subject 7 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        1.5567       0.3125            0.3125        1.4726  0.0004  0.1121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.3634       0.3333            0.3333        1.4470  0.0005  0.1130\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.7925       0.3646            0.3646        1.4363  0.0005  0.1105\n",
      "     34            1.0000        \u001b[32m0.5849\u001b[0m       0.3333            0.3333        1.4500  0.0006  0.1093\n",
      "     35            1.0000        \u001b[32m0.2667\u001b[0m       0.3125            0.3125        1.4883  0.0006  0.1123\n",
      "     36            1.0000        \u001b[32m0.2555\u001b[0m       0.3333            0.3333        1.5626  0.0007  0.1119\n",
      "     37            1.0000        \u001b[32m0.1304\u001b[0m       0.3333            0.3333        1.6692  0.0007  0.1126\n",
      "     38            1.0000        \u001b[32m0.0715\u001b[0m       0.3229            0.3229        1.7965  0.0007  0.1143\n",
      "     39            1.0000        \u001b[32m0.0543\u001b[0m       0.3021            0.3021        1.9276  0.0007  0.1322\n",
      "     40            1.0000        \u001b[32m0.0414\u001b[0m       0.3021            0.3021        2.0469  0.0007  0.1390\n",
      "     41            1.0000        \u001b[32m0.0357\u001b[0m       0.3021            0.3021        2.1488  0.0007  0.1411\n",
      "     42            1.0000        0.0377       0.2917            0.2917        2.2252  0.0007  0.1417\n",
      "     43            1.0000        0.0360       0.2917            0.2917        2.2774  0.0006  0.1412\n",
      "     44            1.0000        \u001b[32m0.0249\u001b[0m       0.2917            0.2917        2.3073  0.0006  0.1363\n",
      "     45            1.0000        \u001b[32m0.0153\u001b[0m       0.2917            0.2917        2.3202  0.0005  0.1737\n",
      "     46            1.0000        0.0294       0.2604            0.2604        2.3186  0.0005  0.1367\n",
      "     47            1.0000        0.0255       0.2812            0.2812        2.3076  0.0004  0.1413\n",
      "     48            1.0000        0.0180       0.2500            0.2500        2.2923  0.0004  0.1412\n",
      "     49            1.0000        0.0214       0.2500            0.2500        2.2760  0.0003  0.1333\n",
      "     50            1.0000        \u001b[32m0.0098\u001b[0m       0.2500            0.2500        2.2616  0.0003  0.1449\n",
      "Fine tuning model for subject 7 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        1.7442       0.2917            0.2917        1.4811  0.0004  0.1525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5000        1.6455       0.3438            0.3438        1.4743  0.0005  0.1575\n",
      "     33            \u001b[36m0.9000\u001b[0m        1.1768       0.3542            0.3542        1.4898  0.0005  0.1561\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6741\u001b[0m       0.3438            0.3438        1.5347  0.0006  0.1851\n",
      "     35            1.0000        \u001b[32m0.2224\u001b[0m       0.3438            0.3438        1.5971  0.0006  0.1575\n",
      "     36            1.0000        \u001b[32m0.1369\u001b[0m       0.3438            0.3438        1.6598  0.0007  0.1491\n",
      "     37            1.0000        0.1417       0.3229            0.3229        1.7202  0.0007  0.1426\n",
      "     38            1.0000        \u001b[32m0.0679\u001b[0m       0.3125            0.3125        1.7743  0.0007  0.1169\n",
      "     39            1.0000        0.0694       0.3125            0.3125        1.8217  0.0007  0.1119\n",
      "     40            1.0000        \u001b[32m0.0321\u001b[0m       0.3021            0.3021        1.8625  0.0007  0.1103\n",
      "     41            1.0000        0.0343       0.3125            0.3125        1.8984  0.0007  0.1106\n",
      "     42            1.0000        0.0632       0.3125            0.3125        1.9317  0.0007  0.1107\n",
      "     43            1.0000        0.0506       0.3229            0.3229        1.9593  0.0006  0.1111\n",
      "     44            1.0000        0.0360       0.3021            0.3021        1.9827  0.0006  0.1086\n",
      "     45            1.0000        \u001b[32m0.0196\u001b[0m       0.3229            0.3229        2.0007  0.0005  0.1115\n",
      "     46            1.0000        0.0273       0.3333            0.3333        2.0140  0.0005  0.1094\n",
      "     47            1.0000        0.0274       0.3333            0.3333        2.0243  0.0004  0.1066\n",
      "     48            1.0000        \u001b[32m0.0169\u001b[0m       0.3333            0.3333        2.0316  0.0004  0.1108\n",
      "     49            1.0000        0.0375       0.3333            0.3333        2.0365  0.0003  0.1126\n",
      "     50            1.0000        0.0282       0.3333            0.3333        2.0395  0.0003  0.1152\n",
      "Fine tuning model for subject 7 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.2574       0.3125            0.3125        1.5069  0.0004  0.1190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5000        1.1820       0.2812            0.2812        1.5171  0.0005  0.1139\n",
      "     33            0.7000        0.7512       0.2917            0.2917        1.5372  0.0005  0.1124\n",
      "     34            \u001b[36m1.0000\u001b[0m        0.7562       0.2708            0.2708        1.5651  0.0006  0.1102\n",
      "     35            1.0000        \u001b[32m0.4248\u001b[0m       0.2812            0.2812        1.5977  0.0006  0.1173\n",
      "     36            1.0000        \u001b[32m0.1229\u001b[0m       0.2708            0.2708        1.6392  0.0007  0.1084\n",
      "     37            1.0000        0.1260       0.2917            0.2917        1.6816  0.0007  0.1174\n",
      "     38            1.0000        \u001b[32m0.0506\u001b[0m       0.2708            0.2708        1.7243  0.0007  0.1186\n",
      "     39            1.0000        \u001b[32m0.0301\u001b[0m       0.2917            0.2917        1.7650  0.0007  0.1147\n",
      "     40            1.0000        0.0402       0.3229            0.3229        1.7998  0.0007  0.1121\n",
      "     41            1.0000        0.0312       0.3333            0.3333        1.8251  0.0007  0.1116\n",
      "     42            1.0000        0.0372       0.3333            0.3333        1.8416  0.0007  0.1129\n",
      "     43            1.0000        0.0337       0.3333            0.3333        1.8508  0.0006  0.1115\n",
      "     44            1.0000        0.0797       0.3438            0.3438        1.8497  0.0006  0.1146\n",
      "     45            1.0000        0.0331       0.3333            0.3333        1.8473  0.0005  0.1049\n",
      "     46            1.0000        \u001b[32m0.0139\u001b[0m       0.3333            0.3333        1.8483  0.0005  0.1069\n",
      "     47            1.0000        0.0192       0.3229            0.3229        1.8510  0.0004  0.1156\n",
      "     48            1.0000        \u001b[32m0.0113\u001b[0m       0.3021            0.3021        1.8547  0.0004  0.1149\n",
      "     49            1.0000        \u001b[32m0.0112\u001b[0m       0.3125            0.3125        1.8589  0.0003  0.1082\n",
      "     50            1.0000        \u001b[32m0.0107\u001b[0m       0.3021            0.3021        1.8636  0.0003  0.1118\n",
      "Fine tuning model for subject 7 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        1.5624       0.3333            0.3333        1.4825  0.0004  0.1117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5000        1.4646       0.3125            0.3125        1.4550  0.0005  0.1203\n",
      "     33            \u001b[36m1.0000\u001b[0m        1.0320       0.3750            0.3750        1.4276  0.0005  0.1080\n",
      "     34            1.0000        \u001b[32m0.4619\u001b[0m       0.3438            0.3438        1.4201  0.0006  0.1149\n",
      "     35            1.0000        \u001b[32m0.2308\u001b[0m       0.3333            0.3333        1.4304  0.0006  0.1120\n",
      "     36            1.0000        \u001b[32m0.1435\u001b[0m       0.3333            0.3333        1.4608  0.0007  0.1085\n",
      "     37            1.0000        \u001b[32m0.0652\u001b[0m       0.3333            0.3333        1.5047  0.0007  0.1117\n",
      "     38            1.0000        \u001b[32m0.0565\u001b[0m       0.3333            0.3333        1.5535  0.0007  0.1137\n",
      "     39            1.0000        \u001b[32m0.0475\u001b[0m       0.3333            0.3333        1.6010  0.0007  0.1059\n",
      "     40            1.0000        \u001b[32m0.0405\u001b[0m       0.3333            0.3333        1.6482  0.0007  0.1108\n",
      "     41            1.0000        \u001b[32m0.0192\u001b[0m       0.3229            0.3229        1.6917  0.0007  0.1075\n",
      "     42            1.0000        0.0219       0.3333            0.3333        1.7302  0.0007  0.1136\n",
      "     43            1.0000        \u001b[32m0.0129\u001b[0m       0.3438            0.3438        1.7647  0.0006  0.1050\n",
      "     44            1.0000        0.0179       0.3333            0.3333        1.7950  0.0006  0.1117\n",
      "     45            1.0000        \u001b[32m0.0089\u001b[0m       0.3333            0.3333        1.8230  0.0005  0.1128\n",
      "     46            1.0000        0.0106       0.3229            0.3229        1.8487  0.0005  0.1133\n",
      "     47            1.0000        \u001b[32m0.0040\u001b[0m       0.3125            0.3125        1.8729  0.0004  0.1144\n",
      "     48            1.0000        0.0115       0.3125            0.3125        1.8962  0.0004  0.1160\n",
      "     49            1.0000        0.0240       0.2917            0.2917        1.9178  0.0003  0.1187\n",
      "     50            1.0000        0.0080       0.2812            0.2812        1.9390  0.0003  0.1126\n",
      "Fine tuning model for subject 7 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.1918       0.3021            0.3021        1.4908  0.0004  0.1172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        1.3406       0.3333            0.3333        1.4983  0.0005  0.1172\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6114\u001b[0m       0.3333            0.3333        1.5348  0.0005  0.1107\n",
      "     34            1.0000        \u001b[32m0.4728\u001b[0m       0.3438            0.3438        1.6095  0.0006  0.1142\n",
      "     35            1.0000        \u001b[32m0.3571\u001b[0m       0.3021            0.3021        1.7121  0.0006  0.1114\n",
      "     36            1.0000        \u001b[32m0.1638\u001b[0m       0.2917            0.2917        1.8209  0.0007  0.1117\n",
      "     37            1.0000        \u001b[32m0.0465\u001b[0m       0.2500            0.2500        1.9140  0.0007  0.1140\n",
      "     38            1.0000        \u001b[32m0.0439\u001b[0m       0.2500            0.2500        1.9843  0.0007  0.1073\n",
      "     39            1.0000        0.1093       0.2500            0.2500        2.0299  0.0007  0.1110\n",
      "     40            1.0000        \u001b[32m0.0275\u001b[0m       0.2396            0.2396        2.0604  0.0007  0.1123\n",
      "     41            1.0000        \u001b[32m0.0267\u001b[0m       0.2396            0.2396        2.0823  0.0007  0.1573\n",
      "     42            1.0000        0.0445       0.2500            0.2500        2.0989  0.0007  0.1136\n",
      "     43            1.0000        0.0593       0.2500            0.2500        2.1106  0.0006  0.1167\n",
      "     44            1.0000        0.0398       0.2396            0.2396        2.1209  0.0006  0.1163\n",
      "     45            1.0000        \u001b[32m0.0238\u001b[0m       0.2396            0.2396        2.1299  0.0005  0.1119\n",
      "     46            1.0000        0.0282       0.2396            0.2396        2.1391  0.0005  0.1110\n",
      "     47            1.0000        0.0356       0.2292            0.2292        2.1490  0.0004  0.1117\n",
      "     48            1.0000        0.0260       0.2188            0.2188        2.1566  0.0004  0.1110\n",
      "     49            1.0000        \u001b[32m0.0154\u001b[0m       0.2188            0.2188        2.1633  0.0003  0.1132\n",
      "     50            1.0000        0.0258       0.2188            0.2188        2.1692  0.0003  0.1109\n",
      "Fine tuning model for subject 7 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        1.1855       0.2812            0.2812        1.4919  0.0004  0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        0.9015       0.2917            0.2917        1.4800  0.0005  0.1286\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.8159       0.2917            0.2917        1.4752  0.0005  0.1144\n",
      "     34            1.0000        \u001b[32m0.4121\u001b[0m       0.3333            0.3333        1.4850  0.0006  0.1130\n",
      "     35            1.0000        \u001b[32m0.2326\u001b[0m       0.3229            0.3229        1.5075  0.0006  0.1125\n",
      "     36            1.0000        \u001b[32m0.1191\u001b[0m       0.3125            0.3125        1.5296  0.0007  0.1113\n",
      "     37            1.0000        \u001b[32m0.0733\u001b[0m       0.3021            0.3021        1.5472  0.0007  0.1119\n",
      "     38            1.0000        \u001b[32m0.0389\u001b[0m       0.3021            0.3021        1.5580  0.0007  0.1123\n",
      "     39            1.0000        0.0487       0.3021            0.3021        1.5617  0.0007  0.1168\n",
      "     40            1.0000        0.0462       0.3021            0.3021        1.5629  0.0007  0.1114\n",
      "     41            1.0000        \u001b[32m0.0225\u001b[0m       0.3021            0.3021        1.5614  0.0007  0.1145\n",
      "     42            1.0000        \u001b[32m0.0126\u001b[0m       0.3021            0.3021        1.5585  0.0007  0.1092\n",
      "     43            1.0000        0.0565       0.2917            0.2917        1.5558  0.0006  0.1135\n",
      "     44            1.0000        \u001b[32m0.0069\u001b[0m       0.3125            0.3125        1.5523  0.0006  0.1151\n",
      "     45            1.0000        0.0133       0.3333            0.3333        1.5490  0.0005  0.1157\n",
      "     46            1.0000        0.0128       0.3438            0.3438        1.5461  0.0005  0.1096\n",
      "     47            1.0000        0.0088       0.3646            0.3646        1.5433  0.0004  0.1123\n",
      "     48            1.0000        0.0107       0.3646            0.3646        1.5405  0.0004  0.1108\n",
      "     49            1.0000        \u001b[32m0.0058\u001b[0m       0.3646            0.3646        1.5377  0.0003  0.1077\n",
      "     50            1.0000        0.0081       0.3542            0.3542        1.5351  0.0003  0.1089\n",
      "Fine tuning model for subject 7 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4667        1.4804       0.3125            0.3125        1.4884  0.0004  0.1122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4667        1.1858       0.3333            0.3333        1.4676  0.0005  0.1121\n",
      "     33            \u001b[36m0.8000\u001b[0m        1.0520       0.3646            0.3646        1.4557  0.0005  0.1107\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5977\u001b[0m       0.3646            0.3646        1.4620  0.0006  0.1133\n",
      "     35            1.0000        \u001b[32m0.4086\u001b[0m       0.3646            0.3646        1.4902  0.0006  0.1105\n",
      "     36            1.0000        \u001b[32m0.2842\u001b[0m       0.3542            0.3542        1.5292  0.0007  0.1228\n",
      "     37            1.0000        \u001b[32m0.1594\u001b[0m       0.3646            0.3646        1.5711  0.0007  0.1136\n",
      "     38            1.0000        \u001b[32m0.0798\u001b[0m       0.3542            0.3542        1.6018  0.0007  0.1437\n",
      "     39            1.0000        0.1238       0.3438            0.3438        1.6275  0.0007  0.1727\n",
      "     40            1.0000        0.0859       0.3438            0.3438        1.6473  0.0007  0.1631\n",
      "     41            1.0000        \u001b[32m0.0484\u001b[0m       0.3438            0.3438        1.6638  0.0007  0.1436\n",
      "     42            1.0000        \u001b[32m0.0460\u001b[0m       0.3646            0.3646        1.6828  0.0007  0.1650\n",
      "     43            1.0000        \u001b[32m0.0416\u001b[0m       0.3750            0.3750        1.7004  0.0006  0.1562\n",
      "     44            1.0000        \u001b[32m0.0401\u001b[0m       0.3750            0.3750        1.7165  0.0006  0.1436\n",
      "     45            1.0000        \u001b[32m0.0267\u001b[0m       0.3750            0.3750        1.7327  0.0005  0.1500\n",
      "     46            1.0000        \u001b[32m0.0202\u001b[0m       0.3542            0.3542        1.7462  0.0005  0.1389\n",
      "     47            1.0000        0.0438       0.3438            0.3438        1.7556  0.0004  0.1442\n",
      "     48            1.0000        0.0386       0.3438            0.3438        1.7617  0.0004  0.1273\n",
      "     49            1.0000        0.0284       0.3438            0.3438        1.7652  0.0003  0.1416\n",
      "     50            1.0000        \u001b[32m0.0153\u001b[0m       0.3438            0.3438        1.7664  0.0003  0.1418\n",
      "Fine tuning model for subject 7 with 15 = 15 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2667        1.5753       0.3125            0.3125        1.4812  0.0004  0.1704\n",
      "     32            0.4000        1.6135       0.3438            0.3438        1.4574  0.0005  0.1330\n",
      "     33            0.6667        1.2398       0.3438            0.3438        1.4390  0.0005  0.1428\n",
      "     34            \u001b[36m0.8000\u001b[0m        0.8092       0.3333            0.3333        1.4298  0.0006  0.1486\n",
      "     35            \u001b[36m0.8667\u001b[0m        \u001b[32m0.4414\u001b[0m       0.3750            0.3750        1.4393  0.0006  0.1219\n",
      "     36            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4187\u001b[0m       0.3750            0.3750        1.4713  0.0007  0.1117\n",
      "     37            0.9333        \u001b[32m0.2450\u001b[0m       0.3750            0.3750        1.5183  0.0007  0.1114\n",
      "     38            0.9333        \u001b[32m0.1961\u001b[0m       0.3438            0.3438        1.5667  0.0007  0.1075\n",
      "     39            0.9333        \u001b[32m0.0887\u001b[0m       0.3333            0.3333        1.6157  0.0007  0.1100\n",
      "     40            0.9333        0.1527       0.3333            0.3333        1.6588  0.0007  0.1115\n",
      "     41            0.9333        0.0968       0.3229            0.3229        1.6930  0.0007  0.1116\n",
      "     42            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0693\u001b[0m       0.3229            0.3229        1.7220  0.0007  0.1059\n",
      "     43            1.0000        0.0701       0.3125            0.3125        1.7472  0.0006  0.1053\n",
      "     44            1.0000        0.0760       0.3125            0.3125        1.7654  0.0006  0.1132\n",
      "     45            1.0000        \u001b[32m0.0390\u001b[0m       0.2812            0.2812        1.7783  0.0005  0.1091\n",
      "     46            1.0000        0.0636       0.2917            0.2917        1.7883  0.0005  0.1142\n",
      "     47            1.0000        0.0610       0.2812            0.2812        1.7967  0.0004  0.1284\n",
      "     48            1.0000        \u001b[32m0.0339\u001b[0m       0.2917            0.2917        1.8041  0.0004  0.1255\n",
      "     49            1.0000        \u001b[32m0.0322\u001b[0m       0.3021            0.3021        1.8109  0.0003  0.1539\n",
      "     50            1.0000        0.0390       0.2812            0.2812        1.8161  0.0003  0.1602\n",
      "Fine tuning model for subject 7 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.5743       0.3125            0.3125        1.4799  0.0004  0.1144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4667        1.4979       0.3229            0.3229        1.4621  0.0005  0.1193\n",
      "     33            \u001b[36m0.8000\u001b[0m        1.1139       0.3646            0.3646        1.4801  0.0005  0.1215\n",
      "     34            \u001b[36m0.8667\u001b[0m        0.8269       0.3333            0.3333        1.5435  0.0006  0.1301\n",
      "     35            \u001b[36m0.9333\u001b[0m        \u001b[32m0.6356\u001b[0m       0.3750            0.3750        1.6589  0.0006  0.1165\n",
      "     36            0.8667        \u001b[32m0.2460\u001b[0m       0.3646            0.3646        1.7914  0.0007  0.1110\n",
      "     37            0.8667        \u001b[32m0.1939\u001b[0m       0.3229            0.3229        1.9393  0.0007  0.1078\n",
      "     38            0.8667        \u001b[32m0.0831\u001b[0m       0.2604            0.2604        2.0973  0.0007  0.1249\n",
      "     39            0.8667        0.1222       0.2812            0.2812        2.2374  0.0007  0.1284\n",
      "     40            0.8667        \u001b[32m0.0828\u001b[0m       0.2604            0.2604        2.3574  0.0007  0.1378\n",
      "     41            0.8667        \u001b[32m0.0664\u001b[0m       0.2708            0.2708        2.4526  0.0007  0.1448\n",
      "     42            0.8667        \u001b[32m0.0420\u001b[0m       0.2708            0.2708        2.5199  0.0007  0.1171\n",
      "     43            0.8667        \u001b[32m0.0371\u001b[0m       0.2708            0.2708        2.5565  0.0006  0.1213\n",
      "     44            \u001b[36m1.0000\u001b[0m        0.0449       0.2708            0.2708        2.5627  0.0006  0.1118\n",
      "     45            1.0000        \u001b[32m0.0265\u001b[0m       0.2604            0.2604        2.5442  0.0005  0.1173\n",
      "     46            1.0000        0.0368       0.2500            0.2500        2.5090  0.0005  0.1139\n",
      "     47            1.0000        \u001b[32m0.0155\u001b[0m       0.2604            0.2604        2.4642  0.0004  0.1143\n",
      "     48            1.0000        0.0191       0.2917            0.2917        2.4158  0.0004  0.1147\n",
      "     49            1.0000        0.0214       0.3021            0.3021        2.3690  0.0003  0.1116\n",
      "     50            1.0000        \u001b[32m0.0143\u001b[0m       0.3229            0.3229        2.3241  0.0003  0.1206\n",
      "Fine tuning model for subject 7 with 15 = 15 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5333        1.1838       0.2917            0.2917        1.4863  0.0004  0.1294\n",
      "     32            0.6000        1.2331       0.3438            0.3438        1.4640  0.0005  0.1097\n",
      "     33            \u001b[36m0.8667\u001b[0m        1.0589       0.3750            0.3750        1.4474  0.0005  0.1137\n",
      "     34            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5513\u001b[0m       0.3958            0.3958        1.4417  0.0006  0.1087\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4604\u001b[0m       0.3854            0.3854        1.4514  0.0006  0.1167\n",
      "     36            1.0000        0.4657       0.3854            0.3854        1.4731  0.0007  0.1109\n",
      "     37            1.0000        \u001b[32m0.1693\u001b[0m       0.4062            0.4062        1.5013  0.0007  0.1172\n",
      "     38            1.0000        \u001b[32m0.1043\u001b[0m       0.3958            0.3958        1.5363  0.0007  0.1125\n",
      "     39            1.0000        0.1259       0.3750            0.3750        1.5773  0.0007  0.1108\n",
      "     40            1.0000        \u001b[32m0.0494\u001b[0m       0.3646            0.3646        1.6278  0.0007  0.1108\n",
      "     41            1.0000        0.0788       0.3438            0.3438        1.6835  0.0007  0.1138\n",
      "     42            1.0000        \u001b[32m0.0458\u001b[0m       0.3438            0.3438        1.7397  0.0007  0.1124\n",
      "     43            1.0000        \u001b[32m0.0381\u001b[0m       0.3333            0.3333        1.7905  0.0006  0.1280\n",
      "     44            1.0000        \u001b[32m0.0282\u001b[0m       0.3125            0.3125        1.8343  0.0006  0.1147\n",
      "     45            1.0000        0.0392       0.3125            0.3125        1.8679  0.0005  0.1159\n",
      "     46            1.0000        \u001b[32m0.0274\u001b[0m       0.3125            0.3125        1.8912  0.0005  0.1136\n",
      "     47            1.0000        \u001b[32m0.0245\u001b[0m       0.3125            0.3125        1.9053  0.0004  0.1481\n",
      "     48            1.0000        \u001b[32m0.0188\u001b[0m       0.3021            0.3021        1.9125  0.0004  0.1197\n",
      "     49            1.0000        0.0297       0.3021            0.3021        1.9132  0.0003  0.1124\n",
      "     50            1.0000        \u001b[32m0.0188\u001b[0m       0.3021            0.3021        1.9088  0.0003  0.1214\n",
      "Fine tuning model for subject 7 with 15 = 15 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3333        1.4227       0.3333            0.3333        1.4820  0.0004  0.1168\n",
      "     32            0.4667        1.5595       0.3229            0.3229        1.4658  0.0005  0.1140\n",
      "     33            \u001b[36m0.8000\u001b[0m        1.2155       0.3333            0.3333        1.4590  0.0005  0.1171\n",
      "     34            \u001b[36m0.8667\u001b[0m        0.9754       0.2812            0.2812        1.4751  0.0006  0.1156\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6820\u001b[0m       0.3125            0.3125        1.5243  0.0006  0.1170\n",
      "     36            1.0000        \u001b[32m0.3357\u001b[0m       0.2812            0.2812        1.6059  0.0007  0.1173\n",
      "     37            1.0000        \u001b[32m0.2515\u001b[0m       0.2604            0.2604        1.7155  0.0007  0.1176\n",
      "     38            1.0000        \u001b[32m0.1630\u001b[0m       0.2604            0.2604        1.8423  0.0007  0.1173\n",
      "     39            1.0000        \u001b[32m0.0823\u001b[0m       0.2812            0.2812        1.9686  0.0007  0.1200\n",
      "     40            1.0000        \u001b[32m0.0736\u001b[0m       0.2396            0.2396        2.0838  0.0007  0.1116\n",
      "     41            1.0000        0.0880       0.2500            0.2500        2.1805  0.0007  0.1144\n",
      "     42            1.0000        \u001b[32m0.0489\u001b[0m       0.2500            0.2500        2.2577  0.0007  0.1110\n",
      "     43            1.0000        0.0616       0.2396            0.2396        2.3142  0.0006  0.1088\n",
      "     44            1.0000        \u001b[32m0.0402\u001b[0m       0.2500            0.2500        2.3527  0.0006  0.1125\n",
      "     45            1.0000        0.0404       0.2500            0.2500        2.3768  0.0005  0.1100\n",
      "     46            1.0000        \u001b[32m0.0373\u001b[0m       0.2396            0.2396        2.3873  0.0005  0.1142\n",
      "     47            1.0000        \u001b[32m0.0316\u001b[0m       0.2396            0.2396        2.3895  0.0004  0.1173\n",
      "     48            1.0000        0.0368       0.2396            0.2396        2.3841  0.0004  0.1099\n",
      "     49            1.0000        0.0319       0.2500            0.2500        2.3742  0.0003  0.1134\n",
      "     50            1.0000        \u001b[32m0.0179\u001b[0m       0.2396            0.2396        2.3600  0.0003  0.1153\n",
      "Fine tuning model for subject 7 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.3130       0.3125            0.3125        1.4821  0.0004  0.1134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6667        1.1305       0.3542            0.3542        1.4614  0.0005  0.1205\n",
      "     33            \u001b[36m0.8667\u001b[0m        0.8834       0.3646            0.3646        1.4387  0.0005  0.1098\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4382\u001b[0m       0.3854            0.3854        1.4218  0.0006  0.1318\n",
      "     35            1.0000        \u001b[32m0.3260\u001b[0m       0.4062            0.4062        1.4157  0.0006  0.1176\n",
      "     36            1.0000        \u001b[32m0.2407\u001b[0m       0.3750            0.3750        1.4234  0.0007  0.1177\n",
      "     37            1.0000        \u001b[32m0.1033\u001b[0m       0.3854            0.3854        1.4432  0.0007  0.1127\n",
      "     38            1.0000        0.1116       0.3750            0.3750        1.4752  0.0007  0.1124\n",
      "     39            1.0000        \u001b[32m0.0775\u001b[0m       0.3542            0.3542        1.5158  0.0007  0.1078\n",
      "     40            1.0000        \u001b[32m0.0758\u001b[0m       0.3646            0.3646        1.5615  0.0007  0.1077\n",
      "     41            1.0000        0.0814       0.3646            0.3646        1.6107  0.0007  0.1122\n",
      "     42            1.0000        \u001b[32m0.0422\u001b[0m       0.3646            0.3646        1.6584  0.0007  0.1156\n",
      "     43            1.0000        0.0440       0.3542            0.3542        1.7019  0.0006  0.1012\n",
      "     44            1.0000        \u001b[32m0.0395\u001b[0m       0.3438            0.3438        1.7391  0.0006  0.1131\n",
      "     45            1.0000        0.0454       0.3646            0.3646        1.7691  0.0005  0.1122\n",
      "     46            1.0000        \u001b[32m0.0227\u001b[0m       0.3854            0.3854        1.7927  0.0005  0.1069\n",
      "     47            1.0000        0.0246       0.3854            0.3854        1.8095  0.0004  0.1195\n",
      "     48            1.0000        \u001b[32m0.0159\u001b[0m       0.3854            0.3854        1.8202  0.0004  0.1204\n",
      "     49            1.0000        \u001b[32m0.0152\u001b[0m       0.3854            0.3854        1.8258  0.0003  0.1074\n",
      "     50            1.0000        \u001b[32m0.0142\u001b[0m       0.3646            0.3646        1.8275  0.0003  0.1282\n",
      "Fine tuning model for subject 7 with 15 = 15 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.4668       0.3229            0.3229        1.4773  0.0004  0.1400\n",
      "     32            0.6000        1.3139       0.3333            0.3333        1.4642  0.0005  0.1615\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.7849       0.3542            0.3542        1.4966  0.0005  0.1433\n",
      "     34            1.0000        \u001b[32m0.6350\u001b[0m       0.3646            0.3646        1.5640  0.0006  0.1305\n",
      "     35            1.0000        \u001b[32m0.2904\u001b[0m       0.3646            0.3646        1.6448  0.0006  0.1655\n",
      "     36            1.0000        \u001b[32m0.1590\u001b[0m       0.3854            0.3854        1.7326  0.0007  0.1277\n",
      "     37            1.0000        \u001b[32m0.0929\u001b[0m       0.3750            0.3750        1.8152  0.0007  0.1336\n",
      "     38            1.0000        0.1070       0.3646            0.3646        1.8924  0.0007  0.1405\n",
      "     39            1.0000        \u001b[32m0.0687\u001b[0m       0.3438            0.3438        1.9664  0.0007  0.1403\n",
      "     40            1.0000        \u001b[32m0.0608\u001b[0m       0.3333            0.3333        2.0325  0.0007  0.1443\n",
      "     41            1.0000        \u001b[32m0.0575\u001b[0m       0.3229            0.3229        2.0857  0.0007  0.1400\n",
      "     42            1.0000        \u001b[32m0.0407\u001b[0m       0.3229            0.3229        2.1249  0.0007  0.1359\n",
      "     43            1.0000        \u001b[32m0.0221\u001b[0m       0.3229            0.3229        2.1506  0.0006  0.1437\n",
      "     44            1.0000        0.0269       0.3229            0.3229        2.1638  0.0006  0.1721\n",
      "     45            1.0000        \u001b[32m0.0177\u001b[0m       0.3333            0.3333        2.1663  0.0005  0.1613\n",
      "     46            1.0000        0.0182       0.3542            0.3542        2.1606  0.0005  0.1733\n",
      "     47            1.0000        0.0438       0.3333            0.3333        2.1475  0.0004  0.1396\n",
      "     48            1.0000        \u001b[32m0.0157\u001b[0m       0.3333            0.3333        2.1307  0.0004  0.1090\n",
      "     49            1.0000        0.0344       0.3438            0.3438        2.1130  0.0003  0.1120\n",
      "     50            1.0000        \u001b[32m0.0139\u001b[0m       0.3333            0.3333        2.0956  0.0003  0.1131\n",
      "Fine tuning model for subject 7 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3333        1.6295       0.2917            0.2917        1.5005  0.0004  0.1113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4667        1.4288       0.2917            0.2917        1.5127  0.0005  0.1114\n",
      "     33            0.7333        1.2543       0.2812            0.2812        1.5422  0.0005  0.1115\n",
      "     34            \u001b[36m0.8000\u001b[0m        0.7649       0.2917            0.2917        1.5937  0.0006  0.1079\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4504\u001b[0m       0.3021            0.3021        1.6522  0.0006  0.1113\n",
      "     36            1.0000        \u001b[32m0.2028\u001b[0m       0.3125            0.3125        1.7025  0.0007  0.1135\n",
      "     37            1.0000        \u001b[32m0.0933\u001b[0m       0.2812            0.2812        1.7432  0.0007  0.1126\n",
      "     38            1.0000        0.1473       0.2812            0.2812        1.7777  0.0007  0.1152\n",
      "     39            1.0000        0.1013       0.2917            0.2917        1.8033  0.0007  0.1121\n",
      "     40            1.0000        \u001b[32m0.0758\u001b[0m       0.2917            0.2917        1.8238  0.0007  0.1078\n",
      "     41            1.0000        0.0874       0.2812            0.2812        1.8399  0.0007  0.1637\n",
      "     42            1.0000        \u001b[32m0.0577\u001b[0m       0.2708            0.2708        1.8532  0.0007  0.1172\n",
      "     43            1.0000        0.0639       0.2604            0.2604        1.8645  0.0006  0.1121\n",
      "     44            1.0000        \u001b[32m0.0267\u001b[0m       0.2604            0.2604        1.8740  0.0006  0.1068\n",
      "     45            1.0000        0.0461       0.2708            0.2708        1.8817  0.0005  0.1128\n",
      "     46            1.0000        0.0295       0.2500            0.2500        1.8876  0.0005  0.1132\n",
      "     47            1.0000        0.0283       0.2396            0.2396        1.8911  0.0004  0.1183\n",
      "     48            1.0000        0.0764       0.2396            0.2396        1.8925  0.0004  0.1131\n",
      "     49            1.0000        0.0272       0.2396            0.2396        1.8929  0.0003  0.1115\n",
      "     50            1.0000        0.0311       0.2500            0.2500        1.8921  0.0003  0.1112\n",
      "Fine tuning model for subject 7 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.7993       0.3229            0.3229        1.4948  0.0004  0.1125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5333        1.7351       0.3333            0.3333        1.4939  0.0005  0.1176\n",
      "     33            0.7333        1.4492       0.3333            0.3333        1.5415  0.0005  0.1082\n",
      "     34            0.7333        0.8290       0.3229            0.3229        1.6554  0.0006  0.1113\n",
      "     35            \u001b[36m0.8000\u001b[0m        \u001b[32m0.5158\u001b[0m       0.3229            0.3229        1.8225  0.0006  0.1112\n",
      "     36            \u001b[36m0.8667\u001b[0m        \u001b[32m0.3384\u001b[0m       0.2917            0.2917        2.0014  0.0007  0.1112\n",
      "     37            0.8667        \u001b[32m0.2840\u001b[0m       0.2604            0.2604        2.1351  0.0007  0.1075\n",
      "     38            \u001b[36m0.9333\u001b[0m        \u001b[32m0.1629\u001b[0m       0.2708            0.2708        2.2161  0.0007  0.1126\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1111\u001b[0m       0.2604            0.2604        2.2545  0.0007  0.1126\n",
      "     40            1.0000        \u001b[32m0.1101\u001b[0m       0.2500            0.2500        2.2579  0.0007  0.1083\n",
      "     41            1.0000        \u001b[32m0.0767\u001b[0m       0.2812            0.2812        2.2404  0.0007  0.1108\n",
      "     42            1.0000        \u001b[32m0.0446\u001b[0m       0.2708            0.2708        2.2162  0.0007  0.1098\n",
      "     43            1.0000        0.0528       0.2812            0.2812        2.1879  0.0006  0.1133\n",
      "     44            1.0000        0.0476       0.2812            0.2812        2.1593  0.0006  0.1076\n",
      "     45            1.0000        \u001b[32m0.0439\u001b[0m       0.2812            0.2812        2.1321  0.0005  0.1160\n",
      "     46            1.0000        \u001b[32m0.0393\u001b[0m       0.2708            0.2708        2.1078  0.0005  0.1132\n",
      "     47            1.0000        \u001b[32m0.0342\u001b[0m       0.2604            0.2604        2.0842  0.0004  0.1078\n",
      "     48            1.0000        0.0468       0.2604            0.2604        2.0601  0.0004  0.1117\n",
      "     49            1.0000        0.0363       0.2708            0.2708        2.0368  0.0003  0.1122\n",
      "     50            1.0000        \u001b[32m0.0215\u001b[0m       0.2917            0.2917        2.0142  0.0003  0.1130\n",
      "Fine tuning model for subject 7 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3333        1.5065       0.3333            0.3333        1.4777  0.0004  0.1113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.5483       0.3750            0.3750        1.4586  0.0005  0.1139\n",
      "     33            \u001b[36m0.8000\u001b[0m        0.8975       0.3750            0.3750        1.4803  0.0005  0.1129\n",
      "     34            \u001b[36m1.0000\u001b[0m        0.7459       0.3438            0.3438        1.5542  0.0006  0.1107\n",
      "     35            1.0000        \u001b[32m0.4041\u001b[0m       0.3542            0.3542        1.6525  0.0006  0.1115\n",
      "     36            1.0000        \u001b[32m0.2665\u001b[0m       0.3646            0.3646        1.7524  0.0007  0.1045\n",
      "     37            1.0000        \u001b[32m0.1801\u001b[0m       0.3333            0.3333        1.8349  0.0007  0.1093\n",
      "     38            1.0000        \u001b[32m0.0984\u001b[0m       0.3125            0.3125        1.9006  0.0007  0.1102\n",
      "     39            1.0000        \u001b[32m0.0932\u001b[0m       0.3125            0.3125        1.9567  0.0007  0.1117\n",
      "     40            1.0000        \u001b[32m0.0711\u001b[0m       0.3333            0.3333        1.9986  0.0007  0.1156\n",
      "     41            1.0000        0.0900       0.3333            0.3333        2.0290  0.0007  0.1165\n",
      "     42            1.0000        0.0908       0.3333            0.3333        2.0463  0.0007  0.1329\n",
      "     43            1.0000        0.0727       0.3333            0.3333        2.0575  0.0006  0.1346\n",
      "     44            1.0000        \u001b[32m0.0584\u001b[0m       0.3229            0.3229        2.0659  0.0006  0.1411\n",
      "     45            1.0000        0.0735       0.3229            0.3229        2.0732  0.0005  0.1238\n",
      "     46            1.0000        \u001b[32m0.0405\u001b[0m       0.3229            0.3229        2.0779  0.0005  0.1166\n",
      "     47            1.0000        \u001b[32m0.0324\u001b[0m       0.3333            0.3333        2.0802  0.0004  0.1159\n",
      "     48            1.0000        0.0347       0.3333            0.3333        2.0818  0.0004  0.1140\n",
      "     49            1.0000        \u001b[32m0.0226\u001b[0m       0.3438            0.3438        2.0830  0.0003  0.1175\n",
      "     50            1.0000        0.0316       0.3438            0.3438        2.0826  0.0003  0.1077\n",
      "Fine tuning model for subject 7 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2500        1.5069       0.3021            0.3021        1.4909  0.0004  0.1104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5000        1.4014       0.3021            0.3021        1.4876  0.0005  0.1163\n",
      "     33            0.7000        1.3531       0.2708            0.2708        1.5080  0.0005  0.1152\n",
      "     34            0.7500        0.7907       0.2604            0.2604        1.5573  0.0006  0.1157\n",
      "     35            0.7500        \u001b[32m0.6275\u001b[0m       0.2812            0.2812        1.6385  0.0006  0.1123\n",
      "     36            \u001b[36m0.8500\u001b[0m        \u001b[32m0.4013\u001b[0m       0.2917            0.2917        1.7499  0.0007  0.1147\n",
      "     37            0.8500        0.4556       0.2812            0.2812        1.8623  0.0007  0.1188\n",
      "     38            \u001b[36m0.9000\u001b[0m        \u001b[32m0.2850\u001b[0m       0.2708            0.2708        1.9594  0.0007  0.1103\n",
      "     39            \u001b[36m0.9500\u001b[0m        \u001b[32m0.1453\u001b[0m       0.2708            0.2708        2.0091  0.0007  0.1149\n",
      "     40            0.9500        0.1578       0.2812            0.2812        2.0339  0.0007  0.1156\n",
      "     41            0.9500        \u001b[32m0.1436\u001b[0m       0.2708            0.2708        2.0282  0.0007  0.1091\n",
      "     42            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0545\u001b[0m       0.2708            0.2708        2.0062  0.0007  0.1167\n",
      "     43            1.0000        \u001b[32m0.0489\u001b[0m       0.2812            0.2812        1.9783  0.0006  0.1172\n",
      "     44            1.0000        \u001b[32m0.0483\u001b[0m       0.2604            0.2604        1.9499  0.0006  0.1234\n",
      "     45            1.0000        0.0594       0.2604            0.2604        1.9194  0.0005  0.1189\n",
      "     46            1.0000        0.0500       0.2604            0.2604        1.8913  0.0005  0.1169\n",
      "     47            1.0000        0.0617       0.2708            0.2708        1.8650  0.0004  0.1291\n",
      "     48            1.0000        \u001b[32m0.0278\u001b[0m       0.2812            0.2812        1.8421  0.0004  0.1156\n",
      "     49            1.0000        0.0438       0.2917            0.2917        1.8217  0.0003  0.1191\n",
      "     50            1.0000        0.0458       0.2917            0.2917        1.8041  0.0003  0.1117\n",
      "Fine tuning model for subject 7 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.4357       0.3438            0.3438        1.4792  0.0004  0.1171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        0.9356       0.3750            0.3750        1.4584  0.0005  0.1108\n",
      "     33            0.7000        0.9120       0.3542            0.3542        1.4520  0.0005  0.1147\n",
      "     34            \u001b[36m0.9000\u001b[0m        \u001b[32m0.7041\u001b[0m       0.3438            0.3438        1.4717  0.0006  0.1153\n",
      "     35            0.9000        \u001b[32m0.5260\u001b[0m       0.3542            0.3542        1.5143  0.0006  0.1184\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3095\u001b[0m       0.3333            0.3333        1.5518  0.0007  0.1123\n",
      "     37            1.0000        \u001b[32m0.2431\u001b[0m       0.3438            0.3438        1.5828  0.0007  0.1151\n",
      "     38            1.0000        \u001b[32m0.1758\u001b[0m       0.3646            0.3646        1.6032  0.0007  0.1112\n",
      "     39            1.0000        \u001b[32m0.1247\u001b[0m       0.3646            0.3646        1.6181  0.0007  0.1170\n",
      "     40            1.0000        \u001b[32m0.1008\u001b[0m       0.3542            0.3542        1.6415  0.0007  0.1192\n",
      "     41            1.0000        0.1160       0.3646            0.3646        1.6747  0.0007  0.1174\n",
      "     42            1.0000        0.1099       0.3438            0.3438        1.7157  0.0007  0.1143\n",
      "     43            1.0000        0.1155       0.3646            0.3646        1.7549  0.0006  0.1152\n",
      "     44            1.0000        \u001b[32m0.0812\u001b[0m       0.3750            0.3750        1.7915  0.0006  0.1542\n",
      "     45            1.0000        0.0996       0.3750            0.3750        1.8231  0.0005  0.1408\n",
      "     46            1.0000        \u001b[32m0.0711\u001b[0m       0.3750            0.3750        1.8467  0.0005  0.1298\n",
      "     47            1.0000        \u001b[32m0.0485\u001b[0m       0.3750            0.3750        1.8616  0.0004  0.1448\n",
      "     48            1.0000        \u001b[32m0.0477\u001b[0m       0.3854            0.3854        1.8670  0.0004  0.1337\n",
      "     49            1.0000        \u001b[32m0.0303\u001b[0m       0.3750            0.3750        1.8662  0.0003  0.1653\n",
      "     50            1.0000        0.0317       0.3854            0.3854        1.8610  0.0003  0.1726\n",
      "Fine tuning model for subject 7 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.3698       0.3333            0.3333        1.4853  0.0004  0.1460\n",
      "     32            0.6000        1.2410       0.3125            0.3125        1.4656  0.0005  0.1462\n",
      "     33            \u001b[36m0.8500\u001b[0m        0.9343       0.3333            0.3333        1.4490  0.0005  0.1516\n",
      "     34            \u001b[36m0.9000\u001b[0m        0.9565       0.3229            0.3229        1.4455  0.0006  0.1349\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5994\u001b[0m       0.3646            0.3646        1.4595  0.0006  0.1335\n",
      "     36            1.0000        \u001b[32m0.4228\u001b[0m       0.3750            0.3750        1.4889  0.0007  0.1470\n",
      "     37            1.0000        \u001b[32m0.3604\u001b[0m       0.3438            0.3438        1.5234  0.0007  0.1343\n",
      "     38            1.0000        \u001b[32m0.2394\u001b[0m       0.3125            0.3125        1.5610  0.0007  0.1553\n",
      "     39            1.0000        \u001b[32m0.1478\u001b[0m       0.2917            0.2917        1.6006  0.0007  0.1360\n",
      "     40            1.0000        \u001b[32m0.1218\u001b[0m       0.3021            0.3021        1.6419  0.0007  0.1479\n",
      "     41            1.0000        \u001b[32m0.1178\u001b[0m       0.3229            0.3229        1.6785  0.0007  0.1531\n",
      "     42            1.0000        \u001b[32m0.0617\u001b[0m       0.3229            0.3229        1.7090  0.0007  0.1174\n",
      "     43            1.0000        \u001b[32m0.0574\u001b[0m       0.3125            0.3125        1.7321  0.0006  0.1171\n",
      "     44            1.0000        0.0634       0.3125            0.3125        1.7472  0.0006  0.1093\n",
      "     45            1.0000        \u001b[32m0.0569\u001b[0m       0.3333            0.3333        1.7557  0.0005  0.1175\n",
      "     46            1.0000        \u001b[32m0.0490\u001b[0m       0.3438            0.3438        1.7573  0.0005  0.1212\n",
      "     47            1.0000        \u001b[32m0.0459\u001b[0m       0.3542            0.3542        1.7517  0.0004  0.1274\n",
      "     48            1.0000        0.0529       0.3542            0.3542        1.7419  0.0004  0.1158\n",
      "     49            1.0000        0.0596       0.3542            0.3542        1.7292  0.0003  0.1134\n",
      "     50            1.0000        \u001b[32m0.0426\u001b[0m       0.3542            0.3542        1.7149  0.0003  0.1196\n",
      "Fine tuning model for subject 7 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5500        1.2337       0.2917            0.2917        1.4860  0.0004  0.1163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6500        1.1636       0.3333            0.3333        1.4765  0.0005  0.1184\n",
      "     33            0.7000        0.8594       0.3438            0.3438        1.4724  0.0005  0.1172\n",
      "     34            \u001b[36m0.9500\u001b[0m        0.7312       0.3333            0.3333        1.4783  0.0006  0.1242\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5430\u001b[0m       0.3438            0.3438        1.4908  0.0006  0.1161\n",
      "     36            1.0000        \u001b[32m0.4392\u001b[0m       0.3542            0.3542        1.5078  0.0007  0.1121\n",
      "     37            1.0000        \u001b[32m0.2319\u001b[0m       0.3438            0.3438        1.5331  0.0007  0.1223\n",
      "     38            1.0000        \u001b[32m0.1387\u001b[0m       0.3438            0.3438        1.5696  0.0007  0.1075\n",
      "     39            1.0000        0.1632       0.3021            0.3021        1.6175  0.0007  0.1169\n",
      "     40            1.0000        \u001b[32m0.0935\u001b[0m       0.2812            0.2812        1.6690  0.0007  0.1219\n",
      "     41            1.0000        0.1180       0.2812            0.2812        1.7112  0.0007  0.1168\n",
      "     42            1.0000        0.1105       0.3125            0.3125        1.7406  0.0007  0.1160\n",
      "     43            1.0000        0.1043       0.3125            0.3125        1.7591  0.0006  0.1188\n",
      "     44            1.0000        \u001b[32m0.0743\u001b[0m       0.3125            0.3125        1.7692  0.0006  0.1135\n",
      "     45            1.0000        \u001b[32m0.0610\u001b[0m       0.3125            0.3125        1.7761  0.0005  0.1135\n",
      "     46            1.0000        \u001b[32m0.0491\u001b[0m       0.3229            0.3229        1.7809  0.0005  0.1129\n",
      "     47            1.0000        \u001b[32m0.0406\u001b[0m       0.3438            0.3438        1.7838  0.0004  0.1122\n",
      "     48            1.0000        \u001b[32m0.0352\u001b[0m       0.3438            0.3438        1.7857  0.0004  0.1172\n",
      "     49            1.0000        0.0438       0.3333            0.3333        1.7873  0.0003  0.1166\n",
      "     50            1.0000        0.0367       0.3333            0.3333        1.7890  0.0003  0.1143\n",
      "Fine tuning model for subject 7 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4500        1.3728       0.3229            0.3229        1.4808  0.0004  0.1184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6500        1.3194       0.3333            0.3333        1.4707  0.0005  0.1173\n",
      "     33            \u001b[36m0.8000\u001b[0m        0.8781       0.3542            0.3542        1.4945  0.0005  0.1151\n",
      "     34            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6437\u001b[0m       0.3125            0.3125        1.5356  0.0006  0.1128\n",
      "     35            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5583\u001b[0m       0.3125            0.3125        1.5751  0.0006  0.1162\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2776\u001b[0m       0.3125            0.3125        1.6018  0.0007  0.1101\n",
      "     37            1.0000        \u001b[32m0.1901\u001b[0m       0.3229            0.3229        1.6129  0.0007  0.1098\n",
      "     38            1.0000        \u001b[32m0.1450\u001b[0m       0.3333            0.3333        1.6038  0.0007  0.1149\n",
      "     39            1.0000        \u001b[32m0.1322\u001b[0m       0.3646            0.3646        1.5872  0.0007  0.1138\n",
      "     40            1.0000        \u001b[32m0.0939\u001b[0m       0.3646            0.3646        1.5747  0.0007  0.1147\n",
      "     41            1.0000        0.0950       0.3854            0.3854        1.5675  0.0007  0.1167\n",
      "     42            1.0000        \u001b[32m0.0644\u001b[0m       0.3750            0.3750        1.5656  0.0007  0.1132\n",
      "     43            1.0000        0.0670       0.3854            0.3854        1.5688  0.0006  0.1145\n",
      "     44            1.0000        \u001b[32m0.0494\u001b[0m       0.3854            0.3854        1.5748  0.0006  0.1125\n",
      "     45            1.0000        \u001b[32m0.0330\u001b[0m       0.3958            0.3958        1.5838  0.0005  0.1088\n",
      "     46            1.0000        \u001b[32m0.0279\u001b[0m       0.3958            0.3958        1.5941  0.0005  0.1161\n",
      "     47            1.0000        0.0490       0.3958            0.3958        1.6061  0.0004  0.1176\n",
      "     48            1.0000        0.0299       0.3854            0.3854        1.6183  0.0004  0.1216\n",
      "     49            1.0000        0.0372       0.3750            0.3750        1.6310  0.0003  0.1209\n",
      "     50            1.0000        0.0403       0.3750            0.3750        1.6439  0.0003  0.1156\n",
      "Fine tuning model for subject 7 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.4797       0.3021            0.3021        1.4853  0.0004  0.1129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.5000        1.5506       0.3125            0.3125        1.4614  0.0005  0.1154\n",
      "     33            0.7500        1.4213       0.3542            0.3542        1.4392  0.0005  0.1119\n",
      "     34            \u001b[36m0.8500\u001b[0m        0.8130       0.3542            0.3542        1.4281  0.0006  0.1106\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4469\u001b[0m       0.3646            0.3646        1.4394  0.0006  0.1147\n",
      "     36            1.0000        0.5606       0.3438            0.3438        1.4674  0.0007  0.1128\n",
      "     37            1.0000        \u001b[32m0.2651\u001b[0m       0.3542            0.3542        1.4944  0.0007  0.1168\n",
      "     38            1.0000        \u001b[32m0.2297\u001b[0m       0.3958            0.3958        1.5170  0.0007  0.1151\n",
      "     39            1.0000        \u001b[32m0.1003\u001b[0m       0.3958            0.3958        1.5342  0.0007  0.1084\n",
      "     40            1.0000        0.1338       0.3958            0.3958        1.5480  0.0007  0.1171\n",
      "     41            1.0000        0.1187       0.4062            0.4062        1.5673  0.0007  0.1147\n",
      "     42            1.0000        0.1429       0.3958            0.3958        1.5812  0.0007  0.1122\n",
      "     43            1.0000        0.1009       0.3958            0.3958        1.5919  0.0006  0.1135\n",
      "     44            1.0000        \u001b[32m0.0438\u001b[0m       0.3750            0.3750        1.6016  0.0006  0.1187\n",
      "     45            1.0000        \u001b[32m0.0401\u001b[0m       0.3958            0.3958        1.6108  0.0005  0.1139\n",
      "     46            1.0000        0.1042       0.3958            0.3958        1.6129  0.0005  0.1177\n",
      "     47            1.0000        0.0568       0.3958            0.3958        1.6157  0.0004  0.1194\n",
      "     48            1.0000        0.0413       0.3958            0.3958        1.6193  0.0004  0.1125\n",
      "     49            1.0000        0.0472       0.3958            0.3958        1.6231  0.0003  0.1261\n",
      "     50            1.0000        \u001b[32m0.0389\u001b[0m       0.3958            0.3958        1.6263  0.0003  0.1159\n",
      "Fine tuning model for subject 7 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.2850       0.3125            0.3125        1.4781  0.0004  0.1169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6500        0.9984       0.3646            0.3646        1.4492  0.0005  0.1129\n",
      "     33            \u001b[36m0.8000\u001b[0m        1.0737       0.3333            0.3333        1.4323  0.0005  0.1059\n",
      "     34            0.8000        \u001b[32m0.5102\u001b[0m       0.3750            0.3750        1.4333  0.0006  0.1135\n",
      "     35            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4072\u001b[0m       0.3958            0.3958        1.4429  0.0006  0.1173\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2847\u001b[0m       0.3854            0.3854        1.4494  0.0007  0.1139\n",
      "     37            1.0000        \u001b[32m0.2559\u001b[0m       0.3854            0.3854        1.4426  0.0007  0.1147\n",
      "     38            1.0000        \u001b[32m0.1358\u001b[0m       0.4167            0.4167        1.4267  0.0007  0.1269\n",
      "     39            1.0000        \u001b[32m0.1305\u001b[0m       0.4375            0.4375        1.4064  0.0007  0.1204\n",
      "     40            1.0000        \u001b[32m0.0821\u001b[0m       0.3958            0.3958        1.3935  0.0007  0.1172\n",
      "     41            1.0000        \u001b[32m0.0808\u001b[0m       0.4271            0.4271        1.3928  0.0007  0.1201\n",
      "     42            1.0000        \u001b[32m0.0519\u001b[0m       0.4062            0.4062        1.4035  0.0007  0.1205\n",
      "     43            1.0000        0.0701       0.3854            0.3854        1.4198  0.0006  0.1284\n",
      "     44            1.0000        0.0585       0.3750            0.3750        1.4379  0.0006  0.1295\n",
      "     45            1.0000        0.0541       0.3646            0.3646        1.4545  0.0005  0.1289\n",
      "     46            1.0000        \u001b[32m0.0338\u001b[0m       0.3750            0.3750        1.4684  0.0005  0.1389\n",
      "     47            1.0000        \u001b[32m0.0289\u001b[0m       0.3854            0.3854        1.4786  0.0004  0.1279\n",
      "     48            1.0000        \u001b[32m0.0287\u001b[0m       0.3542            0.3542        1.4857  0.0004  0.1172\n",
      "     49            1.0000        0.0429       0.3542            0.3542        1.4909  0.0003  0.1052\n",
      "     50            1.0000        0.0407       0.3646            0.3646        1.4943  0.0003  0.1134\n",
      "Fine tuning model for subject 7 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3500        1.5295       0.2917            0.2917        1.4860  0.0004  0.1155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4000        1.3329       0.3125            0.3125        1.4705  0.0005  0.1199\n",
      "     33            0.6500        0.8690       0.3229            0.3229        1.4688  0.0005  0.1219\n",
      "     34            \u001b[36m0.8000\u001b[0m        0.9059       0.3333            0.3333        1.4982  0.0006  0.1450\n",
      "     35            \u001b[36m0.9500\u001b[0m        \u001b[32m0.6041\u001b[0m       0.3438            0.3438        1.5614  0.0006  0.1288\n",
      "     36            0.9500        \u001b[32m0.4418\u001b[0m       0.3438            0.3438        1.6469  0.0007  0.1377\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3143\u001b[0m       0.3229            0.3229        1.7274  0.0007  0.1351\n",
      "     38            1.0000        \u001b[32m0.2300\u001b[0m       0.3229            0.3229        1.7987  0.0007  0.1558\n",
      "     39            1.0000        \u001b[32m0.1661\u001b[0m       0.3229            0.3229        1.8535  0.0007  0.1759\n",
      "     40            1.0000        \u001b[32m0.1050\u001b[0m       0.3125            0.3125        1.9068  0.0007  0.1688\n",
      "     41            1.0000        0.1456       0.3021            0.3021        1.9533  0.0007  0.1323\n",
      "     42            1.0000        \u001b[32m0.0580\u001b[0m       0.3125            0.3125        1.9916  0.0007  0.1564\n",
      "     43            1.0000        0.0903       0.2917            0.2917        2.0230  0.0006  0.1436\n",
      "     44            1.0000        0.0965       0.2917            0.2917        2.0337  0.0006  0.1406\n",
      "     45            1.0000        0.0624       0.3021            0.3021        2.0313  0.0005  0.1441\n",
      "     46            1.0000        \u001b[32m0.0353\u001b[0m       0.3021            0.3021        2.0166  0.0005  0.1398\n",
      "     47            1.0000        0.0991       0.3229            0.3229        1.9863  0.0004  0.1420\n",
      "     48            1.0000        \u001b[32m0.0352\u001b[0m       0.3229            0.3229        1.9530  0.0004  0.1457\n",
      "     49            1.0000        0.0386       0.3229            0.3229        1.9195  0.0003  0.1785\n",
      "     50            1.0000        0.0358       0.3229            0.3229        1.8874  0.0003  0.1413\n",
      "Fine tuning model for subject 7 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5500        1.2652       0.3125            0.3125        1.4899  0.0004  0.1575\n",
      "     32            0.6000        1.3282       0.3438            0.3438        1.4751  0.0005  0.1273\n",
      "     33            0.6500        1.3699       0.3646            0.3646        1.4832  0.0005  0.1173\n",
      "     34            0.7500        0.8248       0.3750            0.3750        1.5321  0.0006  0.1130\n",
      "     35            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5516\u001b[0m       0.3750            0.3750        1.6115  0.0006  0.1149\n",
      "     36            0.9000        \u001b[32m0.4163\u001b[0m       0.3542            0.3542        1.7029  0.0007  0.1184\n",
      "     37            0.9000        \u001b[32m0.2697\u001b[0m       0.3542            0.3542        1.7834  0.0007  0.1296\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2670\u001b[0m       0.3229            0.3229        1.8381  0.0007  0.1127\n",
      "     39            1.0000        \u001b[32m0.1382\u001b[0m       0.3333            0.3333        1.8597  0.0007  0.1137\n",
      "     40            1.0000        \u001b[32m0.1144\u001b[0m       0.3229            0.3229        1.8646  0.0007  0.1202\n",
      "     41            1.0000        \u001b[32m0.1003\u001b[0m       0.3229            0.3229        1.8566  0.0007  0.1135\n",
      "     42            1.0000        \u001b[32m0.0911\u001b[0m       0.3125            0.3125        1.8538  0.0007  0.1119\n",
      "     43            1.0000        \u001b[32m0.0873\u001b[0m       0.3125            0.3125        1.8564  0.0006  0.1157\n",
      "     44            1.0000        \u001b[32m0.0549\u001b[0m       0.3021            0.3021        1.8594  0.0006  0.1183\n",
      "     45            1.0000        0.1080       0.3021            0.3021        1.8587  0.0005  0.1152\n",
      "     46            1.0000        0.0566       0.3021            0.3021        1.8575  0.0005  0.1153\n",
      "     47            1.0000        0.0739       0.3021            0.3021        1.8571  0.0004  0.1147\n",
      "     48            1.0000        0.0664       0.3125            0.3125        1.8566  0.0004  0.1162\n",
      "     49            1.0000        \u001b[32m0.0435\u001b[0m       0.3125            0.3125        1.8564  0.0003  0.1172\n",
      "     50            1.0000        0.0584       0.3125            0.3125        1.8561  0.0003  0.1175\n",
      "Fine tuning model for subject 7 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.2838       0.3125            0.3125        1.4871  0.0004  0.1164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4000        1.2693       0.3438            0.3438        1.4842  0.0005  0.1177\n",
      "     33            0.7000        1.1801       0.3333            0.3333        1.5038  0.0005  0.1142\n",
      "     34            \u001b[36m0.9000\u001b[0m        0.8574       0.3438            0.3438        1.5428  0.0006  0.1116\n",
      "     35            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5107\u001b[0m       0.3854            0.3854        1.5910  0.0006  0.1175\n",
      "     36            0.9500        \u001b[32m0.4507\u001b[0m       0.3854            0.3854        1.6280  0.0007  0.1212\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2481\u001b[0m       0.3750            0.3750        1.6597  0.0007  0.1124\n",
      "     38            1.0000        \u001b[32m0.2025\u001b[0m       0.3646            0.3646        1.6968  0.0007  0.1153\n",
      "     39            1.0000        \u001b[32m0.1488\u001b[0m       0.3542            0.3542        1.7208  0.0007  0.1172\n",
      "     40            1.0000        \u001b[32m0.0864\u001b[0m       0.3438            0.3438        1.7381  0.0007  0.1133\n",
      "     41            1.0000        \u001b[32m0.0843\u001b[0m       0.3542            0.3542        1.7563  0.0007  0.1150\n",
      "     42            1.0000        \u001b[32m0.0631\u001b[0m       0.3542            0.3542        1.7720  0.0007  0.1213\n",
      "     43            1.0000        \u001b[32m0.0537\u001b[0m       0.3646            0.3646        1.7861  0.0006  0.1092\n",
      "     44            1.0000        0.0954       0.3646            0.3646        1.7979  0.0006  0.1154\n",
      "     45            1.0000        0.0630       0.3750            0.3750        1.8064  0.0005  0.1203\n",
      "     46            1.0000        \u001b[32m0.0474\u001b[0m       0.3646            0.3646        1.8121  0.0005  0.1109\n",
      "     47            1.0000        \u001b[32m0.0461\u001b[0m       0.3750            0.3750        1.8147  0.0004  0.1170\n",
      "     48            1.0000        \u001b[32m0.0403\u001b[0m       0.4062            0.4062        1.8146  0.0004  0.1510\n",
      "     49            1.0000        0.0706       0.4062            0.4062        1.8122  0.0003  0.1176\n",
      "     50            1.0000        0.0444       0.4062            0.4062        1.8075  0.0003  0.1112\n",
      "Fine tuning model for subject 7 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3600        1.6892       0.3021            0.3021        1.4821  0.0004  0.1257\n",
      "     32            0.5200        1.3731       0.3542            0.3542        1.4647  0.0005  0.1251\n",
      "     33            0.5600        1.3608       0.3646            0.3646        1.4551  0.0005  0.1252\n",
      "     34            0.7600        1.0679       0.3542            0.3542        1.4628  0.0006  0.1278\n",
      "     35            \u001b[36m0.8400\u001b[0m        \u001b[32m0.7009\u001b[0m       0.3333            0.3333        1.4937  0.0006  0.1192\n",
      "     36            \u001b[36m0.9200\u001b[0m        \u001b[32m0.5207\u001b[0m       0.3438            0.3438        1.5355  0.0007  0.1212\n",
      "     37            \u001b[36m0.9600\u001b[0m        \u001b[32m0.3796\u001b[0m       0.3125            0.3125        1.5914  0.0007  0.1257\n",
      "     38            0.9600        \u001b[32m0.2269\u001b[0m       0.3125            0.3125        1.6530  0.0007  0.1300\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2180\u001b[0m       0.2812            0.2812        1.7127  0.0007  0.1205\n",
      "     40            1.0000        \u001b[32m0.1955\u001b[0m       0.3021            0.3021        1.7759  0.0007  0.1240\n",
      "     41            1.0000        \u001b[32m0.1823\u001b[0m       0.3021            0.3021        1.8393  0.0007  0.1244\n",
      "     42            1.0000        \u001b[32m0.1389\u001b[0m       0.3021            0.3021        1.8996  0.0007  0.1250\n",
      "     43            1.0000        \u001b[32m0.1114\u001b[0m       0.2708            0.2708        1.9516  0.0006  0.1224\n",
      "     44            1.0000        \u001b[32m0.0764\u001b[0m       0.2604            0.2604        1.9905  0.0006  0.1249\n",
      "     45            1.0000        0.0766       0.2500            0.2500        2.0164  0.0005  0.1203\n",
      "     46            1.0000        \u001b[32m0.0720\u001b[0m       0.2604            0.2604        2.0341  0.0005  0.1170\n",
      "     47            1.0000        \u001b[32m0.0632\u001b[0m       0.2604            0.2604        2.0456  0.0004  0.1226\n",
      "     48            1.0000        \u001b[32m0.0568\u001b[0m       0.2917            0.2917        2.0500  0.0004  0.1240\n",
      "     49            1.0000        \u001b[32m0.0469\u001b[0m       0.2917            0.2917        2.0504  0.0003  0.1270\n",
      "     50            1.0000        0.0553       0.2917            0.2917        2.0467  0.0003  0.1250\n",
      "Fine tuning model for subject 7 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2000        1.8653       0.3125            0.3125        1.4871  0.0004  0.1253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.2800        1.7671       0.2917            0.2917        1.4837  0.0005  0.1272\n",
      "     33            0.5200        1.5029       0.2917            0.2917        1.4899  0.0005  0.1242\n",
      "     34            0.7600        1.2363       0.3438            0.3438        1.5004  0.0006  0.1238\n",
      "     35            0.7600        0.8734       0.3229            0.3229        1.5134  0.0006  0.1225\n",
      "     36            \u001b[36m0.8400\u001b[0m        \u001b[32m0.6598\u001b[0m       0.3021            0.3021        1.5166  0.0007  0.1232\n",
      "     37            \u001b[36m0.9200\u001b[0m        \u001b[32m0.5018\u001b[0m       0.3229            0.3229        1.5118  0.0007  0.1217\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3507\u001b[0m       0.3438            0.3438        1.5082  0.0007  0.1222\n",
      "     39            1.0000        0.3763       0.3438            0.3438        1.5024  0.0007  0.1246\n",
      "     40            1.0000        \u001b[32m0.2707\u001b[0m       0.3646            0.3646        1.4977  0.0007  0.1241\n",
      "     41            1.0000        \u001b[32m0.2194\u001b[0m       0.3958            0.3958        1.4963  0.0007  0.1252\n",
      "     42            1.0000        \u001b[32m0.1639\u001b[0m       0.3958            0.3958        1.4992  0.0007  0.1224\n",
      "     43            1.0000        \u001b[32m0.1340\u001b[0m       0.3750            0.3750        1.5045  0.0006  0.1206\n",
      "     44            1.0000        \u001b[32m0.1234\u001b[0m       0.3646            0.3646        1.5100  0.0006  0.1292\n",
      "     45            1.0000        \u001b[32m0.0769\u001b[0m       0.3646            0.3646        1.5158  0.0005  0.1225\n",
      "     46            1.0000        0.1081       0.3646            0.3646        1.5209  0.0005  0.1282\n",
      "     47            1.0000        0.1298       0.3646            0.3646        1.5250  0.0004  0.1199\n",
      "     48            1.0000        0.1018       0.3750            0.3750        1.5279  0.0004  0.1326\n",
      "     49            1.0000        0.0987       0.3854            0.3854        1.5290  0.0003  0.1227\n",
      "     50            1.0000        0.0842       0.3854            0.3854        1.5293  0.0003  0.1268\n",
      "Fine tuning model for subject 7 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3600        1.4820       0.3229            0.3229        1.4726  0.0004  0.1257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.4000        1.4437       0.3229            0.3229        1.4384  0.0005  0.1239\n",
      "     33            0.6000        1.2861       0.3438            0.3438        1.4138  0.0005  0.1216\n",
      "     34            \u001b[36m0.8400\u001b[0m        0.8642       0.3438            0.3438        1.4158  0.0006  0.1239\n",
      "     35            \u001b[36m1.0000\u001b[0m        0.9466       0.3958            0.3958        1.4626  0.0006  0.1173\n",
      "     36            1.0000        \u001b[32m0.5701\u001b[0m       0.3750            0.3750        1.5581  0.0007  0.1190\n",
      "     37            0.9600        \u001b[32m0.3629\u001b[0m       0.3750            0.3750        1.6767  0.0007  0.1245\n",
      "     38            0.9600        \u001b[32m0.2934\u001b[0m       0.3646            0.3646        1.7970  0.0007  0.1269\n",
      "     39            0.9600        \u001b[32m0.2244\u001b[0m       0.3333            0.3333        1.8895  0.0007  0.1215\n",
      "     40            0.9200        \u001b[32m0.1839\u001b[0m       0.3438            0.3438        1.9732  0.0007  0.1503\n",
      "     41            0.9600        \u001b[32m0.1066\u001b[0m       0.3438            0.3438        2.0346  0.0007  0.1288\n",
      "     42            1.0000        0.1115       0.3438            0.3438        2.0722  0.0007  0.1419\n",
      "     43            1.0000        \u001b[32m0.0990\u001b[0m       0.3542            0.3542        2.0926  0.0006  0.1494\n",
      "     44            1.0000        \u001b[32m0.0813\u001b[0m       0.3438            0.3438        2.0987  0.0006  0.1439\n",
      "     45            1.0000        \u001b[32m0.0725\u001b[0m       0.3438            0.3438        2.0930  0.0005  0.1426\n",
      "     46            1.0000        \u001b[32m0.0563\u001b[0m       0.3333            0.3333        2.0756  0.0005  0.1944\n",
      "     47            1.0000        0.0818       0.3542            0.3542        2.0455  0.0004  0.1516\n",
      "     48            1.0000        0.0641       0.3542            0.3542        2.0094  0.0004  0.1516\n",
      "     49            1.0000        \u001b[32m0.0530\u001b[0m       0.3750            0.3750        1.9711  0.0003  0.1446\n",
      "     50            1.0000        \u001b[32m0.0443\u001b[0m       0.3854            0.3854        1.9315  0.0003  0.1583\n",
      "Fine tuning model for subject 7 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6400        1.6079       0.2917            0.2917        1.4906  0.0004  0.1388\n",
      "     32            0.7200        1.4266       0.3021            0.3021        1.4772  0.0005  0.1614\n",
      "     33            \u001b[36m0.8400\u001b[0m        1.2433       0.3125            0.3125        1.4732  0.0005  0.1635\n",
      "     34            \u001b[36m0.8800\u001b[0m        0.9047       0.3229            0.3229        1.4760  0.0006  0.1733\n",
      "     35            \u001b[36m0.9600\u001b[0m        \u001b[32m0.6457\u001b[0m       0.3021            0.3021        1.4821  0.0006  0.1417\n",
      "     36            0.9600        \u001b[32m0.4902\u001b[0m       0.3125            0.3125        1.4950  0.0007  0.1595\n",
      "     37            0.9600        \u001b[32m0.2743\u001b[0m       0.3438            0.3438        1.5202  0.0007  0.2020\n",
      "     38            0.9600        \u001b[32m0.2083\u001b[0m       0.3125            0.3125        1.5560  0.0007  0.1358\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1883\u001b[0m       0.2812            0.2812        1.5980  0.0007  0.1280\n",
      "     40            1.0000        \u001b[32m0.1809\u001b[0m       0.2708            0.2708        1.6436  0.0007  0.1291\n",
      "     41            1.0000        0.1877       0.2917            0.2917        1.6843  0.0007  0.1234\n",
      "     42            1.0000        \u001b[32m0.1419\u001b[0m       0.3021            0.3021        1.7134  0.0007  0.1225\n",
      "     43            0.9600        \u001b[32m0.1402\u001b[0m       0.3125            0.3125        1.7293  0.0006  0.1212\n",
      "     44            0.9600        \u001b[32m0.0921\u001b[0m       0.3229            0.3229        1.7404  0.0006  0.1287\n",
      "     45            0.9600        \u001b[32m0.0731\u001b[0m       0.3125            0.3125        1.7450  0.0005  0.1209\n",
      "     46            0.9600        \u001b[32m0.0554\u001b[0m       0.3125            0.3125        1.7478  0.0005  0.1304\n",
      "     47            1.0000        0.0623       0.3125            0.3125        1.7459  0.0004  0.1221\n",
      "     48            1.0000        0.0709       0.3125            0.3125        1.7424  0.0004  0.1234\n",
      "     49            1.0000        \u001b[32m0.0466\u001b[0m       0.3125            0.3125        1.7385  0.0003  0.1222\n",
      "     50            1.0000        0.0539       0.3125            0.3125        1.7349  0.0003  0.1228\n",
      "Fine tuning model for subject 7 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4400        1.4143       0.3125            0.3125        1.4936  0.0004  0.1280\n",
      "     32            0.4400        1.0719       0.3542            0.3542        1.4866  0.0005  0.1242\n",
      "     33            0.5200        1.1295       0.3646            0.3646        1.4878  0.0005  0.1201\n",
      "     34            0.7600        0.9864       0.3750            0.3750        1.4996  0.0006  0.1294\n",
      "     35            \u001b[36m0.8400\u001b[0m        0.7363       0.3750            0.3750        1.5201  0.0006  0.1214\n",
      "     36            \u001b[36m0.9200\u001b[0m        \u001b[32m0.5026\u001b[0m       0.3646            0.3646        1.5435  0.0007  0.1261\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3849\u001b[0m       0.3438            0.3438        1.5690  0.0007  0.1297\n",
      "     38            1.0000        \u001b[32m0.3039\u001b[0m       0.3333            0.3333        1.5936  0.0007  0.1262\n",
      "     39            1.0000        \u001b[32m0.2614\u001b[0m       0.3229            0.3229        1.6175  0.0007  0.1239\n",
      "     40            1.0000        \u001b[32m0.1926\u001b[0m       0.2812            0.2812        1.6328  0.0007  0.1277\n",
      "     41            1.0000        \u001b[32m0.1368\u001b[0m       0.2812            0.2812        1.6438  0.0007  0.1219\n",
      "     42            1.0000        \u001b[32m0.1318\u001b[0m       0.2812            0.2812        1.6526  0.0007  0.1154\n",
      "     43            1.0000        \u001b[32m0.1155\u001b[0m       0.2604            0.2604        1.6638  0.0006  0.1216\n",
      "     44            1.0000        \u001b[32m0.1076\u001b[0m       0.2604            0.2604        1.6733  0.0006  0.1284\n",
      "     45            1.0000        \u001b[32m0.0936\u001b[0m       0.2708            0.2708        1.6830  0.0005  0.1250\n",
      "     46            1.0000        0.1095       0.2708            0.2708        1.6897  0.0005  0.1212\n",
      "     47            1.0000        \u001b[32m0.0507\u001b[0m       0.2708            0.2708        1.6985  0.0004  0.1280\n",
      "     48            1.0000        0.0903       0.2708            0.2708        1.7068  0.0004  0.1265\n",
      "     49            1.0000        0.0662       0.2500            0.2500        1.7156  0.0003  0.1225\n",
      "     50            1.0000        0.0527       0.2396            0.2396        1.7257  0.0003  0.1195\n",
      "Fine tuning model for subject 7 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2400        1.4585       0.3021            0.3021        1.4851  0.0004  0.1271\n",
      "     32            0.4400        1.4132       0.3229            0.3229        1.4688  0.0005  0.1237\n",
      "     33            0.6800        1.0021       0.3333            0.3333        1.4640  0.0005  0.1166\n",
      "     34            \u001b[36m0.8400\u001b[0m        0.8800       0.3229            0.3229        1.4722  0.0006  0.1202\n",
      "     35            \u001b[36m0.9600\u001b[0m        \u001b[32m0.5626\u001b[0m       0.3229            0.3229        1.4913  0.0006  0.1244\n",
      "     36            0.9600        \u001b[32m0.4681\u001b[0m       0.3438            0.3438        1.5063  0.0007  0.1232\n",
      "     37            0.9600        \u001b[32m0.3276\u001b[0m       0.3438            0.3438        1.5205  0.0007  0.1222\n",
      "     38            0.9600        0.3922       0.3333            0.3333        1.5391  0.0007  0.1223\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2347\u001b[0m       0.3438            0.3438        1.5486  0.0007  0.1287\n",
      "     40            1.0000        \u001b[32m0.1652\u001b[0m       0.3542            0.3542        1.5587  0.0007  0.1283\n",
      "     41            1.0000        0.2083       0.3646            0.3646        1.5678  0.0007  0.1263\n",
      "     42            1.0000        \u001b[32m0.1161\u001b[0m       0.3438            0.3438        1.5788  0.0007  0.1241\n",
      "     43            1.0000        0.1477       0.3542            0.3542        1.5836  0.0006  0.1283\n",
      "     44            1.0000        \u001b[32m0.1080\u001b[0m       0.3750            0.3750        1.5844  0.0006  0.1236\n",
      "     45            1.0000        \u001b[32m0.0853\u001b[0m       0.3750            0.3750        1.5832  0.0005  0.1227\n",
      "     46            1.0000        0.1463       0.3854            0.3854        1.5778  0.0005  0.1258\n",
      "     47            1.0000        \u001b[32m0.0763\u001b[0m       0.3958            0.3958        1.5707  0.0004  0.1224\n",
      "     48            1.0000        \u001b[32m0.0674\u001b[0m       0.3958            0.3958        1.5639  0.0004  0.1262\n",
      "     49            1.0000        0.0706       0.4062            0.4062        1.5557  0.0003  0.1239\n",
      "     50            1.0000        \u001b[32m0.0615\u001b[0m       0.4062            0.4062        1.5482  0.0003  0.1188\n",
      "Fine tuning model for subject 7 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6400        1.6786       0.3229            0.3229        1.4915  0.0004  0.1233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6800        1.5475       0.3438            0.3438        1.4840  0.0005  0.1228\n",
      "     33            0.7600        1.1238       0.3438            0.3438        1.4936  0.0005  0.1258\n",
      "     34            \u001b[36m0.8800\u001b[0m        0.8784       0.3021            0.3021        1.5191  0.0006  0.1223\n",
      "     35            \u001b[36m0.9200\u001b[0m        \u001b[32m0.6708\u001b[0m       0.3229            0.3229        1.5505  0.0006  0.1259\n",
      "     36            \u001b[36m0.9600\u001b[0m        \u001b[32m0.3669\u001b[0m       0.3229            0.3229        1.5899  0.0007  0.1203\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3150\u001b[0m       0.3333            0.3333        1.6488  0.0007  0.1278\n",
      "     38            1.0000        \u001b[32m0.2449\u001b[0m       0.3646            0.3646        1.7199  0.0007  0.1175\n",
      "     39            1.0000        \u001b[32m0.2043\u001b[0m       0.3542            0.3542        1.7961  0.0007  0.1203\n",
      "     40            1.0000        \u001b[32m0.1424\u001b[0m       0.3438            0.3438        1.8727  0.0007  0.1260\n",
      "     41            1.0000        0.1535       0.3646            0.3646        1.9434  0.0007  0.1286\n",
      "     42            1.0000        \u001b[32m0.1402\u001b[0m       0.3542            0.3542        2.0042  0.0007  0.1240\n",
      "     43            1.0000        \u001b[32m0.0892\u001b[0m       0.3542            0.3542        2.0538  0.0006  0.1196\n",
      "     44            1.0000        0.1450       0.3750            0.3750        2.0889  0.0006  0.1242\n",
      "     45            1.0000        \u001b[32m0.0738\u001b[0m       0.3542            0.3542        2.1116  0.0005  0.1220\n",
      "     46            1.0000        0.0894       0.3542            0.3542        2.1219  0.0005  0.1293\n",
      "     47            1.0000        \u001b[32m0.0664\u001b[0m       0.3646            0.3646        2.1248  0.0004  0.1195\n",
      "     48            1.0000        0.0834       0.3854            0.3854        2.1237  0.0004  0.1311\n",
      "     49            1.0000        0.0819       0.4062            0.4062        2.1169  0.0003  0.1243\n",
      "     50            1.0000        \u001b[32m0.0605\u001b[0m       0.3958            0.3958        2.1096  0.0003  0.1267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning model for subject 7 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4400        1.3481       0.3125            0.3125        1.4883  0.0004  0.1224\n",
      "     32            0.5200        1.3416       0.3229            0.3229        1.4765  0.0005  0.1172\n",
      "     33            0.6400        1.0820       0.3125            0.3125        1.4792  0.0005  0.1243\n",
      "     34            0.7200        0.8520       0.3333            0.3333        1.5131  0.0006  0.1224\n",
      "     35            \u001b[36m0.8000\u001b[0m        0.7539       0.3542            0.3542        1.5783  0.0006  0.1252\n",
      "     36            0.8000        \u001b[32m0.4447\u001b[0m       0.3646            0.3646        1.6665  0.0007  0.1229\n",
      "     37            \u001b[36m0.9200\u001b[0m        \u001b[32m0.3593\u001b[0m       0.3542            0.3542        1.7452  0.0007  0.1241\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2171\u001b[0m       0.3542            0.3542        1.8070  0.0007  0.1218\n",
      "     39            1.0000        0.2292       0.3542            0.3542        1.8553  0.0007  0.1231\n",
      "     40            1.0000        \u001b[32m0.1706\u001b[0m       0.3542            0.3542        1.8937  0.0007  0.1426\n",
      "     41            1.0000        \u001b[32m0.1335\u001b[0m       0.3438            0.3438        1.9269  0.0007  0.1332\n",
      "     42            1.0000        \u001b[32m0.1319\u001b[0m       0.3229            0.3229        1.9519  0.0007  0.1585\n",
      "     43            1.0000        \u001b[32m0.1188\u001b[0m       0.3333            0.3333        1.9729  0.0006  0.1423\n",
      "     44            1.0000        0.1336       0.3229            0.3229        1.9835  0.0006  0.1584\n",
      "     45            1.0000        \u001b[32m0.0985\u001b[0m       0.3229            0.3229        1.9817  0.0005  0.1370\n",
      "     46            1.0000        \u001b[32m0.0764\u001b[0m       0.3229            0.3229        1.9700  0.0005  0.1806\n",
      "     47            1.0000        0.0968       0.3229            0.3229        1.9501  0.0004  0.1339\n",
      "     48            1.0000        \u001b[32m0.0737\u001b[0m       0.3125            0.3125        1.9257  0.0004  0.1403\n",
      "     49            1.0000        \u001b[32m0.0700\u001b[0m       0.3542            0.3542        1.9008  0.0003  0.1447\n",
      "     50            1.0000        \u001b[32m0.0545\u001b[0m       0.3542            0.3542        1.8759  0.0003  0.1518\n",
      "Fine tuning model for subject 7 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3200        1.5061       0.2917            0.2917        1.4856  0.0004  0.1549\n",
      "     32            0.4800        1.3833       0.3333            0.3333        1.4663  0.0005  0.1471\n",
      "     33            0.6000        1.2268       0.3438            0.3438        1.4538  0.0005  0.1528\n",
      "     34            0.7200        1.0501       0.3646            0.3646        1.4388  0.0006  0.1433\n",
      "     35            \u001b[36m0.8800\u001b[0m        0.8618       0.4375            0.4375        1.4306  0.0006  0.1804\n",
      "     36            \u001b[36m0.9200\u001b[0m        \u001b[32m0.6538\u001b[0m       0.4479            0.4479        1.4368  0.0007  0.1760\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5500\u001b[0m       0.4062            0.4062        1.4487  0.0007  0.1719\n",
      "     38            1.0000        \u001b[32m0.3935\u001b[0m       0.3958            0.3958        1.4666  0.0007  0.1419\n",
      "     39            1.0000        \u001b[32m0.2810\u001b[0m       0.3958            0.3958        1.4805  0.0007  0.1248\n",
      "     40            1.0000        \u001b[32m0.1789\u001b[0m       0.3542            0.3542        1.4955  0.0007  0.1376\n",
      "     41            1.0000        0.2032       0.3438            0.3438        1.5106  0.0007  0.1490\n",
      "     42            1.0000        \u001b[32m0.1508\u001b[0m       0.3438            0.3438        1.5191  0.0007  0.1479\n",
      "     43            1.0000        \u001b[32m0.1129\u001b[0m       0.3542            0.3542        1.5236  0.0006  0.1278\n",
      "     44            1.0000        \u001b[32m0.0610\u001b[0m       0.3646            0.3646        1.5248  0.0006  0.1386\n",
      "     45            1.0000        0.0645       0.3542            0.3542        1.5235  0.0005  0.1333\n",
      "     46            1.0000        0.0928       0.3542            0.3542        1.5208  0.0005  0.1302\n",
      "     47            1.0000        0.0805       0.3542            0.3542        1.5168  0.0004  0.1273\n",
      "     48            1.0000        0.0766       0.3542            0.3542        1.5124  0.0004  0.1280\n",
      "     49            1.0000        0.0869       0.3646            0.3646        1.5076  0.0003  0.1446\n",
      "     50            1.0000        0.0723       0.3958            0.3958        1.5032  0.0003  0.1432\n",
      "Fine tuning model for subject 7 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3600        1.5535       0.3021            0.3021        1.4826  0.0004  0.1387\n",
      "     32            0.4400        1.5106       0.3229            0.3229        1.4645  0.0005  0.2263\n",
      "     33            0.5200        1.4381       0.3646            0.3646        1.4516  0.0005  0.1541\n",
      "     34            0.6800        1.1277       0.3750            0.3750        1.4525  0.0006  0.5003\n",
      "     35            \u001b[36m0.8800\u001b[0m        0.8024       0.3542            0.3542        1.4698  0.0006  0.2863\n",
      "     36            \u001b[36m0.9600\u001b[0m        \u001b[32m0.6104\u001b[0m       0.3438            0.3438        1.4965  0.0007  0.2410\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4839\u001b[0m       0.3750            0.3750        1.5204  0.0007  0.1991\n",
      "     38            1.0000        \u001b[32m0.3418\u001b[0m       0.3750            0.3750        1.5372  0.0007  0.2030\n",
      "     39            1.0000        \u001b[32m0.3096\u001b[0m       0.3958            0.3958        1.5486  0.0007  0.1992\n",
      "     40            1.0000        \u001b[32m0.2068\u001b[0m       0.3958            0.3958        1.5652  0.0007  0.1975\n",
      "     41            1.0000        \u001b[32m0.1179\u001b[0m       0.3854            0.3854        1.5873  0.0007  0.2312\n",
      "     42            1.0000        \u001b[32m0.0910\u001b[0m       0.3750            0.3750        1.6132  0.0007  0.1986\n",
      "     43            1.0000        0.0933       0.3646            0.3646        1.6433  0.0006  0.1795\n",
      "     44            1.0000        0.1086       0.3750            0.3750        1.6741  0.0006  0.1976\n",
      "     45            1.0000        \u001b[32m0.0672\u001b[0m       0.3542            0.3542        1.7042  0.0005  0.1877\n",
      "     46            1.0000        0.0867       0.3646            0.3646        1.7320  0.0005  0.1970\n",
      "     47            1.0000        \u001b[32m0.0604\u001b[0m       0.3646            0.3646        1.7572  0.0004  0.1707\n",
      "     48            1.0000        \u001b[32m0.0461\u001b[0m       0.3750            0.3750        1.7813  0.0004  0.1482\n",
      "     49            1.0000        0.0525       0.3542            0.3542        1.8025  0.0003  0.1668\n",
      "     50            1.0000        0.0528       0.3542            0.3542        1.8223  0.0003  0.2202\n",
      "Fine tuning model for subject 7 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5333        1.6663       0.3021            0.3021        1.4752  0.0004  0.1541\n",
      "     32            0.6000        1.4002       0.3438            0.3438        1.4496  0.0005  0.1383\n",
      "     33            0.7333        1.2903       0.3438            0.3438        1.4382  0.0005  0.1364\n",
      "     34            \u001b[36m0.8000\u001b[0m        0.9706       0.3229            0.3229        1.4543  0.0006  0.1557\n",
      "     35            \u001b[36m0.9000\u001b[0m        0.7172       0.3021            0.3021        1.4904  0.0006  0.1473\n",
      "     36            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5240\u001b[0m       0.2396            0.2396        1.5321  0.0007  0.1394\n",
      "     37            \u001b[36m0.9667\u001b[0m        0.5460       0.2708            0.2708        1.5665  0.0007  0.1323\n",
      "     38            0.9667        \u001b[32m0.2863\u001b[0m       0.2500            0.2500        1.5937  0.0007  0.1321\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2571\u001b[0m       0.2292            0.2292        1.6158  0.0007  0.1352\n",
      "     40            1.0000        0.2587       0.2812            0.2812        1.6314  0.0007  0.1278\n",
      "     41            1.0000        0.3103       0.3229            0.3229        1.6398  0.0007  0.1336\n",
      "     42            1.0000        \u001b[32m0.1192\u001b[0m       0.3229            0.3229        1.6508  0.0007  0.1280\n",
      "     43            1.0000        0.2165       0.3333            0.3333        1.6571  0.0006  0.1303\n",
      "     44            1.0000        \u001b[32m0.1083\u001b[0m       0.3542            0.3542        1.6647  0.0006  0.1299\n",
      "     45            1.0000        0.1909       0.3646            0.3646        1.6688  0.0005  0.1366\n",
      "     46            1.0000        0.1191       0.3646            0.3646        1.6714  0.0005  0.1300\n",
      "     47            1.0000        \u001b[32m0.1065\u001b[0m       0.3542            0.3542        1.6732  0.0004  0.1319\n",
      "     48            1.0000        \u001b[32m0.0809\u001b[0m       0.3750            0.3750        1.6751  0.0004  0.1365\n",
      "     49            1.0000        0.1068       0.3854            0.3854        1.6775  0.0003  0.1337\n",
      "     50            1.0000        0.0871       0.3750            0.3750        1.6800  0.0003  0.1629\n",
      "Fine tuning model for subject 7 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3667        1.4981       0.2917            0.2917        1.4841  0.0004  0.1641\n",
      "     32            0.6333        1.5931       0.3229            0.3229        1.4730  0.0005  0.1414\n",
      "     33            0.6667        1.3036       0.3229            0.3229        1.4732  0.0005  0.1628\n",
      "     34            0.7000        1.0565       0.3229            0.3229        1.4872  0.0006  0.1306\n",
      "     35            0.7667        0.8120       0.3438            0.3438        1.5221  0.0006  0.1539\n",
      "     36            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6518\u001b[0m       0.3438            0.3438        1.5675  0.0007  0.1405\n",
      "     37            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5348\u001b[0m       0.3333            0.3333        1.6149  0.0007  0.1337\n",
      "     38            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4615\u001b[0m       0.3333            0.3333        1.6652  0.0007  0.1424\n",
      "     39            0.9667        \u001b[32m0.3486\u001b[0m       0.3333            0.3333        1.7140  0.0007  0.1236\n",
      "     40            0.9667        \u001b[32m0.2796\u001b[0m       0.3021            0.3021        1.7613  0.0007  0.1339\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2789\u001b[0m       0.2917            0.2917        1.8041  0.0007  0.1363\n",
      "     42            1.0000        \u001b[32m0.1961\u001b[0m       0.2708            0.2708        1.8359  0.0007  0.1360\n",
      "     43            1.0000        \u001b[32m0.1886\u001b[0m       0.2708            0.2708        1.8601  0.0006  0.1333\n",
      "     44            1.0000        \u001b[32m0.1235\u001b[0m       0.2604            0.2604        1.8750  0.0006  0.1591\n",
      "     45            1.0000        0.1427       0.2604            0.2604        1.8831  0.0005  0.1387\n",
      "     46            1.0000        \u001b[32m0.0980\u001b[0m       0.2604            0.2604        1.8817  0.0005  0.1659\n",
      "     47            1.0000        \u001b[32m0.0963\u001b[0m       0.2812            0.2812        1.8733  0.0004  0.1530\n",
      "     48            1.0000        \u001b[32m0.0872\u001b[0m       0.2604            0.2604        1.8570  0.0004  0.1681\n",
      "     49            1.0000        \u001b[32m0.0567\u001b[0m       0.2708            0.2708        1.8369  0.0003  0.2057\n",
      "     50            1.0000        0.0796       0.2917            0.2917        1.8151  0.0003  0.1499\n",
      "Fine tuning model for subject 7 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4667        1.2587       0.3125            0.3125        1.4848  0.0004  0.1791\n",
      "     32            0.5333        1.4256       0.3125            0.3125        1.4653  0.0005  0.1625\n",
      "     33            0.5667        1.3243       0.3646            0.3646        1.4524  0.0005  0.1650\n",
      "     34            0.7667        1.1359       0.3646            0.3646        1.4450  0.0006  0.1441\n",
      "     35            \u001b[36m0.9333\u001b[0m        0.7888       0.4062            0.4062        1.4421  0.0006  0.1379\n",
      "     36            \u001b[36m0.9667\u001b[0m        \u001b[32m0.5982\u001b[0m       0.3854            0.3854        1.4416  0.0007  0.1810\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4249\u001b[0m       0.3542            0.3542        1.4363  0.0007  0.1871\n",
      "     38            1.0000        \u001b[32m0.3628\u001b[0m       0.3542            0.3542        1.4281  0.0007  0.1625\n",
      "     39            1.0000        \u001b[32m0.2661\u001b[0m       0.3438            0.3438        1.4184  0.0007  0.1941\n",
      "     40            1.0000        0.3510       0.3542            0.3542        1.4098  0.0007  0.1324\n",
      "     41            1.0000        \u001b[32m0.2107\u001b[0m       0.3542            0.3542        1.4110  0.0007  0.1323\n",
      "     42            1.0000        0.2604       0.3542            0.3542        1.4167  0.0007  0.1340\n",
      "     43            1.0000        \u001b[32m0.1577\u001b[0m       0.3542            0.3542        1.4235  0.0006  0.1339\n",
      "     44            1.0000        0.1838       0.3229            0.3229        1.4299  0.0006  0.1572\n",
      "     45            1.0000        \u001b[32m0.1276\u001b[0m       0.3229            0.3229        1.4368  0.0005  0.1250\n",
      "     46            1.0000        0.1418       0.3021            0.3021        1.4418  0.0005  0.1364\n",
      "     47            1.0000        0.1280       0.2917            0.2917        1.4454  0.0004  0.1164\n",
      "     48            1.0000        \u001b[32m0.1002\u001b[0m       0.3021            0.3021        1.4476  0.0004  0.1369\n",
      "     49            1.0000        0.1128       0.2708            0.2708        1.4493  0.0003  0.1457\n",
      "     50            1.0000        \u001b[32m0.0828\u001b[0m       0.2708            0.2708        1.4506  0.0003  0.1576\n",
      "Fine tuning model for subject 7 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5333        1.1476       0.2917            0.2917        1.4906  0.0004  0.1329\n",
      "     32            0.5667        1.2598       0.3021            0.3021        1.4833  0.0005  0.1485\n",
      "     33            0.6333        1.2935       0.2917            0.2917        1.4787  0.0005  0.1552\n",
      "     34            0.7333        1.0017       0.3333            0.3333        1.4761  0.0006  0.1442\n",
      "     35            \u001b[36m0.8000\u001b[0m        0.7419       0.3333            0.3333        1.4734  0.0006  0.1329\n",
      "     36            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5578\u001b[0m       0.3542            0.3542        1.4714  0.0007  0.1535\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4392\u001b[0m       0.3333            0.3333        1.4704  0.0007  0.1332\n",
      "     38            1.0000        \u001b[32m0.2945\u001b[0m       0.3333            0.3333        1.4812  0.0007  0.1232\n",
      "     39            1.0000        \u001b[32m0.2791\u001b[0m       0.3333            0.3333        1.4905  0.0007  0.1261\n",
      "     40            1.0000        \u001b[32m0.2303\u001b[0m       0.3542            0.3542        1.5007  0.0007  0.1208\n",
      "     41            1.0000        \u001b[32m0.2148\u001b[0m       0.3542            0.3542        1.5107  0.0007  0.1307\n",
      "     42            1.0000        \u001b[32m0.1724\u001b[0m       0.3646            0.3646        1.5139  0.0007  0.1523\n",
      "     43            1.0000        \u001b[32m0.1273\u001b[0m       0.3646            0.3646        1.5112  0.0006  0.1387\n",
      "     44            1.0000        0.1281       0.3750            0.3750        1.5061  0.0006  0.1282\n",
      "     45            1.0000        0.1457       0.3646            0.3646        1.5008  0.0005  0.1494\n",
      "     46            1.0000        0.1353       0.3750            0.3750        1.4990  0.0005  0.1395\n",
      "     47            1.0000        \u001b[32m0.1071\u001b[0m       0.3646            0.3646        1.4960  0.0004  0.1350\n",
      "     48            1.0000        0.1090       0.3646            0.3646        1.4941  0.0004  0.1405\n",
      "     49            1.0000        0.1187       0.3542            0.3542        1.4917  0.0003  0.1674\n",
      "     50            1.0000        \u001b[32m0.0806\u001b[0m       0.3646            0.3646        1.4876  0.0003  0.1310\n",
      "Fine tuning model for subject 7 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3333        1.5383       0.3125            0.3125        1.4879  0.0004  0.1157\n",
      "     32            0.3333        1.2203       0.3229            0.3229        1.4746  0.0005  0.1756\n",
      "     33            0.4667        1.1988       0.3125            0.3125        1.4722  0.0005  0.1130\n",
      "     34            \u001b[36m0.8000\u001b[0m        0.8418       0.3333            0.3333        1.4792  0.0006  0.1197\n",
      "     35            \u001b[36m0.8667\u001b[0m        0.7961       0.3542            0.3542        1.4980  0.0006  0.1601\n",
      "     36            \u001b[36m0.9333\u001b[0m        \u001b[32m0.6094\u001b[0m       0.3438            0.3438        1.5214  0.0007  0.1453\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4846\u001b[0m       0.3646            0.3646        1.5389  0.0007  0.2266\n",
      "     38            1.0000        \u001b[32m0.2439\u001b[0m       0.3646            0.3646        1.5546  0.0007  0.1278\n",
      "     39            1.0000        \u001b[32m0.1739\u001b[0m       0.3542            0.3542        1.5681  0.0007  0.1496\n",
      "     40            1.0000        \u001b[32m0.1657\u001b[0m       0.3646            0.3646        1.5810  0.0007  0.1576\n",
      "     41            1.0000        \u001b[32m0.1507\u001b[0m       0.3646            0.3646        1.5970  0.0007  0.1468\n",
      "     42            1.0000        0.2033       0.3750            0.3750        1.6164  0.0007  0.1602\n",
      "     43            1.0000        \u001b[32m0.0987\u001b[0m       0.3750            0.3750        1.6346  0.0006  0.1396\n",
      "     44            1.0000        \u001b[32m0.0890\u001b[0m       0.3854            0.3854        1.6513  0.0006  0.1669\n",
      "     45            1.0000        \u001b[32m0.0698\u001b[0m       0.3438            0.3438        1.6660  0.0005  0.1581\n",
      "     46            1.0000        0.0770       0.3333            0.3333        1.6780  0.0005  0.1417\n",
      "     47            1.0000        0.1060       0.3333            0.3333        1.6856  0.0004  0.1685\n",
      "     48            1.0000        \u001b[32m0.0537\u001b[0m       0.3438            0.3438        1.6910  0.0004  0.1301\n",
      "     49            1.0000        0.0717       0.3438            0.3438        1.6939  0.0003  0.1582\n",
      "     50            1.0000        0.0737       0.3542            0.3542        1.6954  0.0003  0.1458\n",
      "Fine tuning model for subject 7 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3667        1.6743       0.2917            0.2917        1.4765  0.0004  0.1678\n",
      "     32            0.4333        1.6507       0.3125            0.3125        1.4460  0.0005  0.1338\n",
      "     33            0.6333        1.3111       0.3333            0.3333        1.4272  0.0005  0.1683\n",
      "     34            0.7000        1.0260       0.3438            0.3438        1.4353  0.0006  0.1452\n",
      "     35            \u001b[36m0.8000\u001b[0m        0.7490       0.3646            0.3646        1.4521  0.0006  0.1303\n",
      "     36            \u001b[36m0.8667\u001b[0m        \u001b[32m0.5824\u001b[0m       0.3438            0.3438        1.4497  0.0007  0.1315\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4175\u001b[0m       0.3438            0.3438        1.4374  0.0007  0.1394\n",
      "     38            1.0000        \u001b[32m0.2857\u001b[0m       0.3646            0.3646        1.4333  0.0007  0.1334\n",
      "     39            1.0000        \u001b[32m0.2284\u001b[0m       0.3854            0.3854        1.4537  0.0007  0.1200\n",
      "     40            0.9667        0.2403       0.3854            0.3854        1.4937  0.0007  0.1344\n",
      "     41            0.9667        \u001b[32m0.1827\u001b[0m       0.3854            0.3854        1.5483  0.0007  0.1482\n",
      "     42            0.9667        \u001b[32m0.1326\u001b[0m       0.3646            0.3646        1.6069  0.0007  0.1246\n",
      "     43            0.9667        \u001b[32m0.1265\u001b[0m       0.3542            0.3542        1.6538  0.0006  0.1365\n",
      "     44            0.9667        0.1333       0.3646            0.3646        1.6892  0.0006  0.1447\n",
      "     45            0.9667        0.1394       0.3750            0.3750        1.7083  0.0005  0.1509\n",
      "     46            0.9667        \u001b[32m0.0752\u001b[0m       0.3854            0.3854        1.7199  0.0005  0.1316\n",
      "     47            0.9667        0.0911       0.3854            0.3854        1.7247  0.0004  0.1348\n",
      "     48            1.0000        0.1141       0.3854            0.3854        1.7227  0.0004  0.1490\n",
      "     49            1.0000        0.0946       0.3854            0.3854        1.7181  0.0003  0.1274\n",
      "     50            1.0000        \u001b[32m0.0736\u001b[0m       0.3854            0.3854        1.7124  0.0003  0.1578\n",
      "Fine tuning model for subject 7 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4667        1.2835       0.3021            0.3021        1.4910  0.0004  0.1460\n",
      "     32            0.6000        1.1756       0.3229            0.3229        1.4866  0.0005  0.1520\n",
      "     33            0.7000        1.0564       0.3438            0.3438        1.4971  0.0005  0.1597\n",
      "     34            0.7667        0.8962       0.3438            0.3438        1.5248  0.0006  0.1654\n",
      "     35            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6249\u001b[0m       0.3333            0.3333        1.5670  0.0006  0.1591\n",
      "     36            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5449\u001b[0m       0.2708            0.2708        1.6238  0.0007  0.1833\n",
      "     37            0.9333        \u001b[32m0.4682\u001b[0m       0.3021            0.3021        1.6741  0.0007  0.1475\n",
      "     38            0.9333        \u001b[32m0.4537\u001b[0m       0.2917            0.2917        1.7072  0.0007  0.1667\n",
      "     39            0.9333        \u001b[32m0.2646\u001b[0m       0.2812            0.2812        1.7131  0.0007  0.1590\n",
      "     40            \u001b[36m0.9667\u001b[0m        \u001b[32m0.2308\u001b[0m       0.2917            0.2917        1.7126  0.0007  0.1568\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1616\u001b[0m       0.2917            0.2917        1.7124  0.0007  0.1321\n",
      "     42            1.0000        \u001b[32m0.1491\u001b[0m       0.2812            0.2812        1.7159  0.0007  0.1558\n",
      "     43            1.0000        \u001b[32m0.1149\u001b[0m       0.2812            0.2812        1.7213  0.0006  0.1600\n",
      "     44            1.0000        0.1272       0.2812            0.2812        1.7280  0.0006  0.1769\n",
      "     45            1.0000        \u001b[32m0.1059\u001b[0m       0.2708            0.2708        1.7358  0.0005  0.1641\n",
      "     46            1.0000        0.1207       0.2604            0.2604        1.7413  0.0005  0.1646\n",
      "     47            1.0000        0.1508       0.2604            0.2604        1.7422  0.0004  0.2465\n",
      "     48            1.0000        \u001b[32m0.0932\u001b[0m       0.2708            0.2708        1.7415  0.0004  0.2917\n",
      "     49            1.0000        \u001b[32m0.0809\u001b[0m       0.2812            0.2812        1.7404  0.0003  0.2156\n",
      "     50            1.0000        \u001b[32m0.0769\u001b[0m       0.2708            0.2708        1.7378  0.0003  0.1529\n",
      "Fine tuning model for subject 7 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2667        1.6837       0.3021            0.3021        1.4815  0.0004  0.1585\n",
      "     32            0.2667        1.6109       0.3333            0.3333        1.4605  0.0005  0.1807\n",
      "     33            0.4333        1.4568       0.3750            0.3750        1.4540  0.0005  0.1564\n",
      "     34            0.6333        1.1025       0.3542            0.3542        1.4833  0.0006  0.1600\n",
      "     35            0.7333        0.9073       0.3542            0.3542        1.5576  0.0006  0.1548\n",
      "     36            0.7667        0.7232       0.3750            0.3750        1.6679  0.0007  0.1482\n",
      "     37            0.7667        \u001b[32m0.5733\u001b[0m       0.3542            0.3542        1.8032  0.0007  0.1527\n",
      "     38            \u001b[36m0.8333\u001b[0m        \u001b[32m0.4332\u001b[0m       0.3333            0.3333        1.9220  0.0007  0.1961\n",
      "     39            0.8333        \u001b[32m0.3868\u001b[0m       0.2708            0.2708        2.0263  0.0007  0.1550\n",
      "     40            \u001b[36m0.8667\u001b[0m        \u001b[32m0.2414\u001b[0m       0.2917            0.2917        2.1118  0.0007  0.1420\n",
      "     41            \u001b[36m0.9000\u001b[0m        \u001b[32m0.2368\u001b[0m       0.2812            0.2812        2.1863  0.0007  0.1463\n",
      "     42            0.8667        \u001b[32m0.2362\u001b[0m       0.2812            0.2812        2.2347  0.0007  0.1664\n",
      "     43            0.8667        \u001b[32m0.1378\u001b[0m       0.2812            0.2812        2.2622  0.0006  0.1780\n",
      "     44            0.8667        0.1842       0.2812            0.2812        2.2658  0.0006  0.1425\n",
      "     45            0.9000        \u001b[32m0.1190\u001b[0m       0.2812            0.2812        2.2480  0.0005  0.1568\n",
      "     46            0.9000        0.1890       0.2812            0.2812        2.2097  0.0005  0.1641\n",
      "     47            \u001b[36m0.9333\u001b[0m        \u001b[32m0.0989\u001b[0m       0.2604            0.2604        2.1608  0.0004  0.1780\n",
      "     48            0.9333        0.1060       0.2812            0.2812        2.1080  0.0004  0.1540\n",
      "     49            \u001b[36m1.0000\u001b[0m        0.1337       0.2917            0.2917        2.0551  0.0003  0.1543\n",
      "     50            1.0000        0.1328       0.2917            0.2917        2.0060  0.0003  0.1434\n",
      "Fine tuning model for subject 7 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5333        1.7045       0.3125            0.3125        1.4843  0.0004  0.1339\n",
      "     32            0.6000        1.5026       0.3333            0.3333        1.4624  0.0005  0.1507\n",
      "     33            0.6667        1.1909       0.3438            0.3438        1.4424  0.0005  0.1312\n",
      "     34            0.7000        1.1533       0.3229            0.3229        1.4272  0.0006  0.1304\n",
      "     35            \u001b[36m0.8000\u001b[0m        0.9631       0.3229            0.3229        1.4196  0.0006  0.1322\n",
      "     36            \u001b[36m0.9667\u001b[0m        0.7954       0.3333            0.3333        1.4249  0.0007  0.1322\n",
      "     37            0.9667        \u001b[32m0.4625\u001b[0m       0.3750            0.3750        1.4441  0.0007  0.1306\n",
      "     38            0.9667        \u001b[32m0.3231\u001b[0m       0.3750            0.3750        1.4812  0.0007  0.1439\n",
      "     39            0.9667        \u001b[32m0.3094\u001b[0m       0.3958            0.3958        1.5310  0.0007  0.1399\n",
      "     40            0.9667        \u001b[32m0.2113\u001b[0m       0.3646            0.3646        1.5877  0.0007  0.1611\n",
      "     41            0.9667        \u001b[32m0.1960\u001b[0m       0.3646            0.3646        1.6414  0.0007  0.1402\n",
      "     42            0.9667        \u001b[32m0.1620\u001b[0m       0.3646            0.3646        1.6843  0.0007  0.1451\n",
      "     43            0.9667        \u001b[32m0.1420\u001b[0m       0.3854            0.3854        1.7151  0.0006  0.1386\n",
      "     44            0.9667        \u001b[32m0.1386\u001b[0m       0.3854            0.3854        1.7345  0.0006  0.1379\n",
      "     45            0.9667        \u001b[32m0.1151\u001b[0m       0.3750            0.3750        1.7448  0.0005  0.1390\n",
      "     46            \u001b[36m1.0000\u001b[0m        0.1300       0.3542            0.3542        1.7491  0.0005  0.1406\n",
      "     47            1.0000        \u001b[32m0.0982\u001b[0m       0.3542            0.3542        1.7515  0.0004  0.1308\n",
      "     48            1.0000        \u001b[32m0.0813\u001b[0m       0.3542            0.3542        1.7543  0.0004  0.1344\n",
      "     49            1.0000        \u001b[32m0.0725\u001b[0m       0.3854            0.3854        1.7586  0.0003  0.1232\n",
      "     50            1.0000        0.0793       0.3958            0.3958        1.7647  0.0003  0.1310\n",
      "Fine tuning model for subject 7 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3667        1.5267       0.2917            0.2917        1.4859  0.0004  0.1331\n",
      "     32            0.3667        1.3180       0.2917            0.2917        1.4729  0.0005  0.1270\n",
      "     33            0.5333        1.2412       0.3333            0.3333        1.4779  0.0005  0.1280\n",
      "     34            0.6333        1.1320       0.3542            0.3542        1.5007  0.0006  0.1325\n",
      "     35            0.7333        0.7876       0.3646            0.3646        1.5259  0.0006  0.1289\n",
      "     36            \u001b[36m0.8000\u001b[0m        \u001b[32m0.5844\u001b[0m       0.3438            0.3438        1.5478  0.0007  0.1357\n",
      "     37            \u001b[36m0.8667\u001b[0m        \u001b[32m0.4436\u001b[0m       0.3542            0.3542        1.5561  0.0007  0.1313\n",
      "     38            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3437\u001b[0m       0.3958            0.3958        1.5491  0.0007  0.1251\n",
      "     39            0.9667        \u001b[32m0.2627\u001b[0m       0.3958            0.3958        1.5394  0.0007  0.1322\n",
      "     40            0.9667        \u001b[32m0.2483\u001b[0m       0.3750            0.3750        1.5245  0.0007  0.1256\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1925\u001b[0m       0.3542            0.3542        1.5107  0.0007  0.1248\n",
      "     42            1.0000        \u001b[32m0.1669\u001b[0m       0.3438            0.3438        1.5011  0.0007  0.1349\n",
      "     43            1.0000        \u001b[32m0.1298\u001b[0m       0.3438            0.3438        1.4959  0.0006  0.1315\n",
      "     44            1.0000        0.1394       0.3438            0.3438        1.4952  0.0006  0.1280\n",
      "     45            1.0000        0.1417       0.3750            0.3750        1.4990  0.0005  0.1257\n",
      "     46            1.0000        0.1477       0.3646            0.3646        1.5085  0.0005  0.1310\n",
      "     47            1.0000        \u001b[32m0.0859\u001b[0m       0.3542            0.3542        1.5206  0.0004  0.1508\n",
      "     48            1.0000        \u001b[32m0.0759\u001b[0m       0.3542            0.3542        1.5339  0.0004  0.1729\n",
      "     49            1.0000        \u001b[32m0.0656\u001b[0m       0.3646            0.3646        1.5473  0.0003  0.1788\n",
      "     50            1.0000        \u001b[32m0.0588\u001b[0m       0.3646            0.3646        1.5603  0.0003  0.1378\n",
      "Fine tuning model for subject 7 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3429        1.3505       0.3333            0.3333        1.4864  0.0004  0.1761\n",
      "     32            0.4571        1.1922       0.3646            0.3646        1.4789  0.0005  0.2200\n",
      "     33            0.6000        1.2368       0.3542            0.3542        1.4884  0.0005  0.1413\n",
      "     34            0.6000        1.1417       0.3542            0.3542        1.5118  0.0006  0.1409\n",
      "     35            0.7429        0.8614       0.3333            0.3333        1.5447  0.0006  0.1368\n",
      "     36            0.7429        \u001b[32m0.6736\u001b[0m       0.3229            0.3229        1.5798  0.0007  0.1316\n",
      "     37            \u001b[36m0.8571\u001b[0m        \u001b[32m0.5555\u001b[0m       0.3333            0.3333        1.6051  0.0007  0.1390\n",
      "     38            \u001b[36m0.9143\u001b[0m        \u001b[32m0.5060\u001b[0m       0.3438            0.3438        1.6166  0.0007  0.1355\n",
      "     39            \u001b[36m0.9429\u001b[0m        \u001b[32m0.2986\u001b[0m       0.3333            0.3333        1.6363  0.0007  0.1397\n",
      "     40            0.9429        0.3184       0.2917            0.2917        1.6577  0.0007  0.1626\n",
      "     41            \u001b[36m1.0000\u001b[0m        0.3574       0.2708            0.2708        1.6798  0.0007  0.1595\n",
      "     42            1.0000        \u001b[32m0.2359\u001b[0m       0.2812            0.2812        1.6998  0.0007  0.1490\n",
      "     43            1.0000        \u001b[32m0.2087\u001b[0m       0.2917            0.2917        1.7202  0.0006  0.1805\n",
      "     44            1.0000        \u001b[32m0.1366\u001b[0m       0.3021            0.3021        1.7397  0.0006  0.1586\n",
      "     45            1.0000        0.1486       0.2708            0.2708        1.7575  0.0005  0.2075\n",
      "     46            1.0000        \u001b[32m0.1266\u001b[0m       0.2812            0.2812        1.7751  0.0005  0.2027\n",
      "     47            1.0000        \u001b[32m0.1145\u001b[0m       0.2917            0.2917        1.7917  0.0004  0.1512\n",
      "     48            1.0000        \u001b[32m0.1090\u001b[0m       0.3021            0.3021        1.8052  0.0004  0.1688\n",
      "     49            1.0000        0.1226       0.2917            0.2917        1.8162  0.0003  0.1522\n",
      "     50            1.0000        0.1170       0.2917            0.2917        1.8255  0.0003  0.1584\n",
      "Fine tuning model for subject 7 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3143        1.6214       0.3021            0.3021        1.4873  0.0004  0.1887\n",
      "     32            0.4000        1.5995       0.3229            0.3229        1.4669  0.0005  0.1932\n",
      "     33            0.5143        1.3788       0.3646            0.3646        1.4427  0.0005  0.1827\n",
      "     34            0.6286        1.1188       0.3750            0.3750        1.4179  0.0006  0.2216\n",
      "     35            \u001b[36m0.8000\u001b[0m        0.9007       0.4062            0.4062        1.4032  0.0006  0.1599\n",
      "     36            \u001b[36m0.8571\u001b[0m        \u001b[32m0.6779\u001b[0m       0.4167            0.4167        1.4041  0.0007  0.1384\n",
      "     37            \u001b[36m0.9143\u001b[0m        \u001b[32m0.6148\u001b[0m       0.3646            0.3646        1.4205  0.0007  0.1494\n",
      "     38            \u001b[36m0.9714\u001b[0m        \u001b[32m0.5126\u001b[0m       0.4167            0.4167        1.4459  0.0007  0.1606\n",
      "     39            0.9714        \u001b[32m0.4169\u001b[0m       0.4167            0.4167        1.4835  0.0007  0.1475\n",
      "     40            0.9714        \u001b[32m0.2453\u001b[0m       0.4375            0.4375        1.5227  0.0007  0.1447\n",
      "     41            0.9714        0.2818       0.4375            0.4375        1.5652  0.0007  0.1444\n",
      "     42            0.9714        \u001b[32m0.2201\u001b[0m       0.4375            0.4375        1.6042  0.0007  0.1422\n",
      "     43            0.9714        \u001b[32m0.1675\u001b[0m       0.4271            0.4271        1.6329  0.0006  0.1399\n",
      "     44            0.9714        0.2556       0.4375            0.4375        1.6484  0.0006  0.1431\n",
      "     45            0.9714        0.1764       0.4375            0.4375        1.6523  0.0005  0.1413\n",
      "     46            0.9714        \u001b[32m0.1208\u001b[0m       0.4479            0.4479        1.6503  0.0005  0.1386\n",
      "     47            \u001b[36m1.0000\u001b[0m        0.1697       0.4583            0.4583        1.6423  0.0004  0.1387\n",
      "     48            1.0000        0.1539       0.4583            0.4583        1.6307  0.0004  0.1361\n",
      "     49            1.0000        0.1524       0.4688            0.4688        1.6180  0.0003  0.1517\n",
      "     50            1.0000        \u001b[32m0.1194\u001b[0m       0.4792            0.4792        1.6061  0.0003  0.1404\n",
      "Fine tuning model for subject 7 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6571        1.2272       0.2917            0.2917        1.4836  0.0004  0.1504\n",
      "     32            0.6571        1.1980       0.3125            0.3125        1.4703  0.0005  0.1387\n",
      "     33            0.7143        1.1053       0.3229            0.3229        1.4673  0.0005  0.1353\n",
      "     34            \u001b[36m0.8000\u001b[0m        1.0085       0.3125            0.3125        1.4770  0.0006  0.1541\n",
      "     35            \u001b[36m0.9143\u001b[0m        0.8323       0.3021            0.3021        1.5074  0.0006  0.1351\n",
      "     36            \u001b[36m0.9429\u001b[0m        \u001b[32m0.6149\u001b[0m       0.3229            0.3229        1.5585  0.0007  0.1386\n",
      "     37            \u001b[36m0.9714\u001b[0m        \u001b[32m0.4996\u001b[0m       0.3438            0.3438        1.6273  0.0007  0.1385\n",
      "     38            0.9714        \u001b[32m0.3306\u001b[0m       0.3333            0.3333        1.6958  0.0007  0.1353\n",
      "     39            0.9714        \u001b[32m0.2858\u001b[0m       0.3333            0.3333        1.7546  0.0007  0.1414\n",
      "     40            0.9714        0.2880       0.3333            0.3333        1.7982  0.0007  0.1338\n",
      "     41            0.9714        \u001b[32m0.2308\u001b[0m       0.3542            0.3542        1.8244  0.0007  0.1314\n",
      "     42            \u001b[36m1.0000\u001b[0m        0.2352       0.3438            0.3438        1.8313  0.0007  0.1386\n",
      "     43            1.0000        \u001b[32m0.2238\u001b[0m       0.3333            0.3333        1.8262  0.0006  0.1292\n",
      "     44            1.0000        \u001b[32m0.1845\u001b[0m       0.3125            0.3125        1.8117  0.0006  0.1344\n",
      "     45            1.0000        \u001b[32m0.1731\u001b[0m       0.3125            0.3125        1.7943  0.0005  0.1383\n",
      "     46            1.0000        \u001b[32m0.1336\u001b[0m       0.3021            0.3021        1.7764  0.0005  0.1385\n",
      "     47            1.0000        \u001b[32m0.1175\u001b[0m       0.3021            0.3021        1.7590  0.0004  0.1419\n",
      "     48            1.0000        0.1285       0.3229            0.3229        1.7445  0.0004  0.1320\n",
      "     49            1.0000        0.1211       0.3438            0.3438        1.7329  0.0003  0.1380\n",
      "     50            1.0000        \u001b[32m0.1123\u001b[0m       0.3333            0.3333        1.7248  0.0003  0.1389\n",
      "Fine tuning model for subject 7 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3429        1.4592       0.2812            0.2812        1.4937  0.0004  0.1421\n",
      "     32            0.4571        1.3671       0.2812            0.2812        1.4908  0.0005  0.1386\n",
      "     33            0.5143        1.3634       0.2917            0.2917        1.4992  0.0005  0.1375\n",
      "     34            0.6571        1.1921       0.2917            0.2917        1.5183  0.0006  0.1449\n",
      "     35            0.7429        0.8489       0.3021            0.3021        1.5481  0.0006  0.1504\n",
      "     36            0.7714        \u001b[32m0.6139\u001b[0m       0.3021            0.3021        1.5893  0.0007  0.1684\n",
      "     37            \u001b[36m0.8000\u001b[0m        \u001b[32m0.5487\u001b[0m       0.2917            0.2917        1.6292  0.0007  0.1429\n",
      "     38            \u001b[36m0.8286\u001b[0m        \u001b[32m0.4249\u001b[0m       0.3021            0.3021        1.6591  0.0007  0.1439\n",
      "     39            0.8286        \u001b[32m0.3549\u001b[0m       0.3125            0.3125        1.6781  0.0007  0.1386\n",
      "     40            \u001b[36m0.8857\u001b[0m        \u001b[32m0.2968\u001b[0m       0.3125            0.3125        1.6867  0.0007  0.1364\n",
      "     41            0.8857        \u001b[32m0.2530\u001b[0m       0.3125            0.3125        1.6805  0.0007  0.1366\n",
      "     42            0.8857        \u001b[32m0.1932\u001b[0m       0.3438            0.3438        1.6641  0.0007  0.1394\n",
      "     43            \u001b[36m0.9714\u001b[0m        0.2066       0.3854            0.3854        1.6402  0.0006  0.1366\n",
      "     44            0.9714        \u001b[32m0.1548\u001b[0m       0.3854            0.3854        1.6118  0.0006  0.1439\n",
      "     45            0.9714        \u001b[32m0.1433\u001b[0m       0.3854            0.3854        1.5785  0.0005  0.1364\n",
      "     46            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1098\u001b[0m       0.3958            0.3958        1.5461  0.0005  0.1389\n",
      "     47            1.0000        0.1167       0.3542            0.3542        1.5155  0.0004  0.1397\n",
      "     48            1.0000        0.1102       0.3229            0.3229        1.4925  0.0004  0.1354\n",
      "     49            1.0000        \u001b[32m0.0922\u001b[0m       0.3333            0.3333        1.4749  0.0003  0.1378\n",
      "     50            1.0000        0.0988       0.3438            0.3438        1.4619  0.0003  0.1367\n",
      "Fine tuning model for subject 7 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5143        1.2596       0.3125            0.3125        1.4807  0.0004  0.1464\n",
      "     32            0.6000        1.2005       0.3438            0.3438        1.4518  0.0005  0.1369\n",
      "     33            0.6571        1.1818       0.3229            0.3229        1.4267  0.0005  0.1333\n",
      "     34            0.7714        0.9385       0.3333            0.3333        1.4067  0.0006  0.1447\n",
      "     35            \u001b[36m0.8000\u001b[0m        0.8253       0.3646            0.3646        1.3785  0.0006  0.1402\n",
      "     36            \u001b[36m0.9143\u001b[0m        \u001b[32m0.6299\u001b[0m       0.3854            0.3854        1.3417  0.0007  0.1326\n",
      "     37            \u001b[36m0.9714\u001b[0m        \u001b[32m0.3273\u001b[0m       0.3958            0.3958        1.3028  0.0007  0.1330\n",
      "     38            \u001b[36m1.0000\u001b[0m        0.4095       0.4271            0.4271        1.2669  0.0007  0.1389\n",
      "     39            1.0000        \u001b[32m0.2501\u001b[0m       0.4479            0.4479        1.2434  0.0007  0.1433\n",
      "     40            1.0000        0.2566       0.4688            0.4688        1.2372  0.0007  0.1338\n",
      "     41            1.0000        \u001b[32m0.2039\u001b[0m       0.4792            0.4792        1.2495  0.0007  0.1544\n",
      "     42            1.0000        \u001b[32m0.2029\u001b[0m       0.4896            0.4896        1.2701  0.0007  0.1386\n",
      "     43            1.0000        \u001b[32m0.1625\u001b[0m       0.5000            0.5000        1.2964  0.0006  0.1367\n",
      "     44            1.0000        \u001b[32m0.1398\u001b[0m       0.4896            0.4896        1.3196  0.0006  0.1387\n",
      "     45            1.0000        \u001b[32m0.1381\u001b[0m       0.4792            0.4792        1.3380  0.0005  0.1349\n",
      "     46            1.0000        \u001b[32m0.1367\u001b[0m       0.4792            0.4792        1.3479  0.0005  0.1327\n",
      "     47            1.0000        \u001b[32m0.1227\u001b[0m       0.4583            0.4583        1.3521  0.0004  0.1340\n",
      "     48            1.0000        \u001b[32m0.1155\u001b[0m       0.4792            0.4792        1.3515  0.0004  0.1611\n",
      "     49            1.0000        \u001b[32m0.0804\u001b[0m       0.4792            0.4792        1.3470  0.0003  0.1475\n",
      "     50            1.0000        0.0832       0.5000            0.5000        1.3411  0.0003  0.1782\n",
      "Fine tuning model for subject 7 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4286        1.4016       0.3021            0.3021        1.4896  0.0004  0.1630\n",
      "     32            0.4857        1.3836       0.3229            0.3229        1.4761  0.0005  0.1532\n",
      "     33            0.5714        1.2650       0.3646            0.3646        1.4721  0.0005  0.1945\n",
      "     34            0.6571        0.9846       0.3854            0.3854        1.4808  0.0006  0.1653\n",
      "     35            \u001b[36m0.8286\u001b[0m        0.8442       0.3646            0.3646        1.5057  0.0006  0.1435\n",
      "     36            0.8286        \u001b[32m0.6525\u001b[0m       0.4167            0.4167        1.5308  0.0007  0.1627\n",
      "     37            \u001b[36m0.8857\u001b[0m        \u001b[32m0.5048\u001b[0m       0.4062            0.4062        1.5467  0.0007  0.1720\n",
      "     38            \u001b[36m0.9714\u001b[0m        \u001b[32m0.4636\u001b[0m       0.4062            0.4062        1.5570  0.0007  0.1588\n",
      "     39            0.9714        \u001b[32m0.2888\u001b[0m       0.3646            0.3646        1.5664  0.0007  0.1559\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2585\u001b[0m       0.3854            0.3854        1.5812  0.0007  0.1609\n",
      "     41            1.0000        \u001b[32m0.2183\u001b[0m       0.3542            0.3542        1.6009  0.0007  0.1562\n",
      "     42            1.0000        \u001b[32m0.1693\u001b[0m       0.3438            0.3438        1.6236  0.0007  0.2080\n",
      "     43            1.0000        \u001b[32m0.1629\u001b[0m       0.3542            0.3542        1.6467  0.0006  0.1721\n",
      "     44            1.0000        \u001b[32m0.1592\u001b[0m       0.3438            0.3438        1.6675  0.0006  0.1741\n",
      "     45            1.0000        \u001b[32m0.1312\u001b[0m       0.3438            0.3438        1.6837  0.0005  0.1436\n",
      "     46            1.0000        0.1492       0.3333            0.3333        1.6969  0.0005  0.1345\n",
      "     47            1.0000        \u001b[32m0.1297\u001b[0m       0.3333            0.3333        1.7075  0.0004  0.1385\n",
      "     48            1.0000        \u001b[32m0.1054\u001b[0m       0.3438            0.3438        1.7154  0.0004  0.1388\n",
      "     49            1.0000        \u001b[32m0.1036\u001b[0m       0.3438            0.3438        1.7218  0.0003  0.1320\n",
      "     50            1.0000        \u001b[32m0.0860\u001b[0m       0.3438            0.3438        1.7268  0.0003  0.1376\n",
      "Fine tuning model for subject 7 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3714        1.5745       0.2917            0.2917        1.4876  0.0004  0.1402\n",
      "     32            0.4286        1.4516       0.3021            0.3021        1.4715  0.0005  0.1394\n",
      "     33            0.4571        1.4324       0.3125            0.3125        1.4606  0.0005  0.1428\n",
      "     34            0.5714        1.1198       0.3125            0.3125        1.4608  0.0006  0.1417\n",
      "     35            \u001b[36m0.8000\u001b[0m        0.8047       0.3125            0.3125        1.4726  0.0006  0.1354\n",
      "     36            \u001b[36m0.8571\u001b[0m        \u001b[32m0.6635\u001b[0m       0.3125            0.3125        1.4918  0.0007  0.1383\n",
      "     37            0.8571        \u001b[32m0.4722\u001b[0m       0.3333            0.3333        1.5037  0.0007  0.1489\n",
      "     38            \u001b[36m0.9429\u001b[0m        \u001b[32m0.4583\u001b[0m       0.3125            0.3125        1.5039  0.0007  0.1398\n",
      "     39            0.9429        \u001b[32m0.4501\u001b[0m       0.3333            0.3333        1.4933  0.0007  0.1376\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2970\u001b[0m       0.3542            0.3542        1.4840  0.0007  0.1349\n",
      "     41            1.0000        0.3438       0.3438            0.3438        1.4822  0.0007  0.1353\n",
      "     42            1.0000        \u001b[32m0.2752\u001b[0m       0.3333            0.3333        1.4851  0.0007  0.1322\n",
      "     43            1.0000        \u001b[32m0.2131\u001b[0m       0.3125            0.3125        1.4874  0.0006  0.1405\n",
      "     44            1.0000        \u001b[32m0.1817\u001b[0m       0.3125            0.3125        1.4845  0.0006  0.1382\n",
      "     45            1.0000        0.1914       0.3229            0.3229        1.4837  0.0005  0.1449\n",
      "     46            1.0000        \u001b[32m0.1465\u001b[0m       0.3333            0.3333        1.4822  0.0005  0.1396\n",
      "     47            1.0000        \u001b[32m0.1452\u001b[0m       0.3333            0.3333        1.4785  0.0004  0.1354\n",
      "     48            1.0000        \u001b[32m0.1304\u001b[0m       0.3438            0.3438        1.4743  0.0004  0.1374\n",
      "     49            1.0000        0.1699       0.3229            0.3229        1.4697  0.0003  0.1423\n",
      "     50            1.0000        \u001b[32m0.1186\u001b[0m       0.3333            0.3333        1.4647  0.0003  0.1396\n",
      "Fine tuning model for subject 7 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5429        1.2365       0.3021            0.3021        1.4963  0.0004  0.1347\n",
      "     32            0.5429        1.3917       0.3021            0.3021        1.5009  0.0005  0.1360\n",
      "     33            0.6286        1.2699       0.3021            0.3021        1.5116  0.0005  0.1335\n",
      "     34            0.7429        1.0337       0.2917            0.2917        1.5228  0.0006  0.1397\n",
      "     35            \u001b[36m0.8000\u001b[0m        0.7470       0.2604            0.2604        1.5370  0.0006  0.1428\n",
      "     36            \u001b[36m0.8857\u001b[0m        \u001b[32m0.6861\u001b[0m       0.3125            0.3125        1.5593  0.0007  0.1359\n",
      "     37            \u001b[36m0.9143\u001b[0m        \u001b[32m0.5867\u001b[0m       0.3229            0.3229        1.5850  0.0007  0.1335\n",
      "     38            \u001b[36m0.9429\u001b[0m        \u001b[32m0.4658\u001b[0m       0.3125            0.3125        1.5928  0.0007  0.1429\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2920\u001b[0m       0.2917            0.2917        1.5836  0.0007  0.1375\n",
      "     40            1.0000        \u001b[32m0.2573\u001b[0m       0.3125            0.3125        1.5602  0.0007  0.1430\n",
      "     41            1.0000        0.2683       0.3229            0.3229        1.5305  0.0007  0.1456\n",
      "     42            1.0000        \u001b[32m0.2084\u001b[0m       0.3542            0.3542        1.5024  0.0007  0.1350\n",
      "     43            1.0000        0.2356       0.3542            0.3542        1.4802  0.0006  0.1307\n",
      "     44            1.0000        \u001b[32m0.2050\u001b[0m       0.3438            0.3438        1.4646  0.0006  0.1326\n",
      "     45            1.0000        \u001b[32m0.1879\u001b[0m       0.3333            0.3333        1.4548  0.0005  0.1349\n",
      "     46            1.0000        \u001b[32m0.1398\u001b[0m       0.3333            0.3333        1.4517  0.0005  0.1360\n",
      "     47            1.0000        0.1933       0.3438            0.3438        1.4542  0.0004  0.1369\n",
      "     48            1.0000        \u001b[32m0.1198\u001b[0m       0.3542            0.3542        1.4604  0.0004  0.1350\n",
      "     49            1.0000        0.1555       0.3542            0.3542        1.4673  0.0003  0.1337\n",
      "     50            1.0000        \u001b[32m0.1014\u001b[0m       0.3542            0.3542        1.4721  0.0003  0.1356\n",
      "Fine tuning model for subject 7 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4286        1.5588       0.3229            0.3229        1.4817  0.0004  0.1343\n",
      "     32            0.4857        1.2805       0.3438            0.3438        1.4594  0.0005  0.1316\n",
      "     33            0.5714        1.0792       0.3542            0.3542        1.4403  0.0005  0.1409\n",
      "     34            0.6857        1.0692       0.3438            0.3438        1.4218  0.0006  0.1280\n",
      "     35            0.7429        0.8642       0.4271            0.4271        1.4116  0.0006  0.1354\n",
      "     36            \u001b[36m0.9143\u001b[0m        \u001b[32m0.7077\u001b[0m       0.4167            0.4167        1.4180  0.0007  0.1384\n",
      "     37            \u001b[36m0.9714\u001b[0m        \u001b[32m0.5933\u001b[0m       0.4062            0.4062        1.4380  0.0007  0.1335\n",
      "     38            0.9714        \u001b[32m0.4198\u001b[0m       0.3646            0.3646        1.4713  0.0007  0.1309\n",
      "     39            0.9714        \u001b[32m0.3371\u001b[0m       0.3542            0.3542        1.5060  0.0007  0.1339\n",
      "     40            \u001b[36m1.0000\u001b[0m        0.3762       0.3542            0.3542        1.5411  0.0007  0.1331\n",
      "     41            1.0000        \u001b[32m0.2726\u001b[0m       0.3333            0.3333        1.5742  0.0007  0.1359\n",
      "     42            1.0000        \u001b[32m0.1994\u001b[0m       0.3125            0.3125        1.5995  0.0007  0.1398\n",
      "     43            1.0000        \u001b[32m0.1903\u001b[0m       0.2917            0.2917        1.6195  0.0006  0.1333\n",
      "     44            1.0000        \u001b[32m0.1760\u001b[0m       0.2917            0.2917        1.6347  0.0006  0.1387\n",
      "     45            1.0000        \u001b[32m0.1669\u001b[0m       0.2917            0.2917        1.6419  0.0005  0.1390\n",
      "     46            1.0000        \u001b[32m0.1192\u001b[0m       0.2917            0.2917        1.6464  0.0005  0.1403\n",
      "     47            1.0000        0.1243       0.3021            0.3021        1.6484  0.0004  0.1452\n",
      "     48            1.0000        0.1394       0.3125            0.3125        1.6497  0.0004  0.1561\n",
      "     49            1.0000        0.1210       0.3229            0.3229        1.6516  0.0003  0.1623\n",
      "     50            1.0000        \u001b[32m0.0829\u001b[0m       0.3333            0.3333        1.6550  0.0003  0.1439\n",
      "Fine tuning model for subject 7 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6286        1.1443       0.3333            0.3333        1.4870  0.0004  0.1443\n",
      "     32            0.7429        1.2320       0.3542            0.3542        1.4818  0.0005  0.1419\n",
      "     33            0.7714        0.9767       0.3438            0.3438        1.4827  0.0005  0.1334\n",
      "     34            \u001b[36m0.8571\u001b[0m        0.9457       0.3333            0.3333        1.4871  0.0006  0.1335\n",
      "     35            \u001b[36m0.8857\u001b[0m        0.7133       0.3333            0.3333        1.4922  0.0006  0.1334\n",
      "     36            \u001b[36m0.9429\u001b[0m        \u001b[32m0.6297\u001b[0m       0.3438            0.3438        1.4954  0.0007  0.1426\n",
      "     37            0.9429        \u001b[32m0.3330\u001b[0m       0.3750            0.3750        1.5004  0.0007  0.1447\n",
      "     38            \u001b[36m0.9714\u001b[0m        0.3632       0.3438            0.3438        1.5063  0.0007  0.1574\n",
      "     39            0.9714        \u001b[32m0.2778\u001b[0m       0.3438            0.3438        1.5132  0.0007  0.1865\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2523\u001b[0m       0.3646            0.3646        1.5214  0.0007  0.1904\n",
      "     41            1.0000        \u001b[32m0.2089\u001b[0m       0.3750            0.3750        1.5310  0.0007  0.1550\n",
      "     42            1.0000        \u001b[32m0.1446\u001b[0m       0.3646            0.3646        1.5408  0.0007  0.1634\n",
      "     43            1.0000        0.2166       0.3750            0.3750        1.5496  0.0006  0.1720\n",
      "     44            1.0000        \u001b[32m0.1417\u001b[0m       0.3646            0.3646        1.5571  0.0006  0.1598\n",
      "     45            1.0000        \u001b[32m0.1256\u001b[0m       0.3542            0.3542        1.5643  0.0005  0.1554\n",
      "     46            1.0000        \u001b[32m0.1027\u001b[0m       0.3542            0.3542        1.5706  0.0005  0.1659\n",
      "     47            1.0000        \u001b[32m0.0915\u001b[0m       0.3542            0.3542        1.5766  0.0004  0.1512\n",
      "     48            1.0000        0.0957       0.3646            0.3646        1.5818  0.0004  0.1486\n",
      "     49            1.0000        \u001b[32m0.0806\u001b[0m       0.3542            0.3542        1.5866  0.0003  0.1586\n",
      "     50            1.0000        \u001b[32m0.0775\u001b[0m       0.3438            0.3438        1.5908  0.0003  0.1624\n",
      "Fine tuning model for subject 7 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3500        1.5197       0.2917            0.2917        1.4863  0.0004  0.2173\n",
      "     32            0.3750        1.5377       0.3229            0.3229        1.4666  0.0005  0.1768\n",
      "     33            0.5000        1.2286       0.3438            0.3438        1.4518  0.0005  0.2269\n",
      "     34            0.5750        1.2249       0.3438            0.3438        1.4491  0.0006  0.1576\n",
      "     35            0.7500        0.9293       0.3438            0.3438        1.4662  0.0006  0.1491\n",
      "     36            0.7750        0.8378       0.3229            0.3229        1.5138  0.0007  0.1560\n",
      "     37            0.7750        \u001b[32m0.6738\u001b[0m       0.3229            0.3229        1.5896  0.0007  0.1429\n",
      "     38            0.7500        \u001b[32m0.4868\u001b[0m       0.3125            0.3125        1.6942  0.0007  0.1475\n",
      "     39            0.7000        \u001b[32m0.4096\u001b[0m       0.3021            0.3021        1.7935  0.0007  0.1491\n",
      "     40            0.7000        0.4438       0.3021            0.3021        1.8871  0.0007  0.1414\n",
      "     41            0.7250        0.4157       0.3021            0.3021        1.9345  0.0007  0.1493\n",
      "     42            0.7750        \u001b[32m0.2941\u001b[0m       0.3021            0.3021        1.9445  0.0007  0.1493\n",
      "     43            \u001b[36m0.8000\u001b[0m        0.3079       0.3125            0.3125        1.9185  0.0006  0.1565\n",
      "     44            \u001b[36m0.8500\u001b[0m        \u001b[32m0.2590\u001b[0m       0.3229            0.3229        1.8709  0.0006  0.1500\n",
      "     45            \u001b[36m0.9250\u001b[0m        \u001b[32m0.1911\u001b[0m       0.3229            0.3229        1.8219  0.0005  0.1479\n",
      "     46            \u001b[36m0.9500\u001b[0m        0.2113       0.3333            0.3333        1.7674  0.0005  0.1562\n",
      "     47            \u001b[36m0.9750\u001b[0m        \u001b[32m0.1609\u001b[0m       0.3229            0.3229        1.7191  0.0004  0.1495\n",
      "     48            \u001b[36m1.0000\u001b[0m        0.1852       0.3333            0.3333        1.6742  0.0004  0.1424\n",
      "     49            1.0000        \u001b[32m0.1271\u001b[0m       0.3333            0.3333        1.6351  0.0003  0.1441\n",
      "     50            1.0000        \u001b[32m0.1221\u001b[0m       0.3438            0.3438        1.6039  0.0003  0.1422\n",
      "Fine tuning model for subject 7 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.5426       0.2917            0.2917        1.4843  0.0004  0.1489\n",
      "     32            0.5000        1.4992       0.3333            0.3333        1.4614  0.0005  0.1622\n",
      "     33            0.5250        1.3521       0.3646            0.3646        1.4426  0.0005  0.1440\n",
      "     34            0.6500        1.1585       0.3438            0.3438        1.4446  0.0006  0.1528\n",
      "     35            0.7250        1.1114       0.3542            0.3542        1.4623  0.0006  0.1494\n",
      "     36            0.7750        0.7781       0.3542            0.3542        1.4885  0.0007  0.1457\n",
      "     37            \u001b[36m0.8750\u001b[0m        \u001b[32m0.5843\u001b[0m       0.3750            0.3750        1.5221  0.0007  0.1492\n",
      "     38            0.8750        0.6177       0.3958            0.3958        1.5575  0.0007  0.1433\n",
      "     39            \u001b[36m0.9250\u001b[0m        \u001b[32m0.5344\u001b[0m       0.3750            0.3750        1.5666  0.0007  0.1454\n",
      "     40            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4804\u001b[0m       0.4062            0.4062        1.5744  0.0007  0.1457\n",
      "     41            \u001b[36m0.9750\u001b[0m        \u001b[32m0.4025\u001b[0m       0.4271            0.4271        1.5825  0.0007  0.1438\n",
      "     42            0.9750        \u001b[32m0.3400\u001b[0m       0.4062            0.4062        1.5988  0.0007  0.1421\n",
      "     43            0.9750        \u001b[32m0.2783\u001b[0m       0.3750            0.3750        1.6162  0.0006  0.1458\n",
      "     44            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2032\u001b[0m       0.3438            0.3438        1.6330  0.0006  0.1472\n",
      "     45            1.0000        0.2276       0.3438            0.3438        1.6475  0.0005  0.1441\n",
      "     46            1.0000        0.2195       0.3438            0.3438        1.6611  0.0005  0.1429\n",
      "     47            1.0000        \u001b[32m0.1760\u001b[0m       0.3438            0.3438        1.6708  0.0004  0.1435\n",
      "     48            1.0000        \u001b[32m0.1612\u001b[0m       0.3438            0.3438        1.6789  0.0004  0.1514\n",
      "     49            1.0000        \u001b[32m0.1466\u001b[0m       0.3750            0.3750        1.6857  0.0003  0.1452\n",
      "     50            1.0000        \u001b[32m0.1093\u001b[0m       0.3750            0.3750        1.6906  0.0003  0.1444\n",
      "Fine tuning model for subject 7 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4250        1.6022       0.3021            0.3021        1.4890  0.0004  0.1543\n",
      "     32            0.5000        1.3435       0.3333            0.3333        1.4831  0.0005  0.1440\n",
      "     33            0.4750        1.3207       0.3438            0.3438        1.4893  0.0005  0.1440\n",
      "     34            0.6000        1.0349       0.3333            0.3333        1.5021  0.0006  0.1433\n",
      "     35            \u001b[36m0.8250\u001b[0m        0.8788       0.3333            0.3333        1.5221  0.0006  0.1447\n",
      "     36            \u001b[36m0.9250\u001b[0m        \u001b[32m0.6281\u001b[0m       0.3438            0.3438        1.5491  0.0007  0.1438\n",
      "     37            \u001b[36m0.9500\u001b[0m        \u001b[32m0.6037\u001b[0m       0.3125            0.3125        1.5814  0.0007  0.1443\n",
      "     38            0.9500        \u001b[32m0.4296\u001b[0m       0.2917            0.2917        1.6225  0.0007  0.1430\n",
      "     39            0.9500        \u001b[32m0.3889\u001b[0m       0.2812            0.2812        1.6661  0.0007  0.1539\n",
      "     40            0.9500        \u001b[32m0.2708\u001b[0m       0.2708            0.2708        1.7121  0.0007  0.1445\n",
      "     41            0.9500        \u001b[32m0.2507\u001b[0m       0.2604            0.2604        1.7486  0.0007  0.1390\n",
      "     42            0.9250        \u001b[32m0.2440\u001b[0m       0.2812            0.2812        1.7696  0.0007  0.1437\n",
      "     43            0.9250        \u001b[32m0.1910\u001b[0m       0.2708            0.2708        1.7816  0.0006  0.1449\n",
      "     44            0.9250        0.2014       0.2917            0.2917        1.7823  0.0006  0.1444\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1895\u001b[0m       0.2812            0.2812        1.7806  0.0005  0.1428\n",
      "     46            1.0000        \u001b[32m0.1733\u001b[0m       0.3021            0.3021        1.7772  0.0005  0.1464\n",
      "     47            1.0000        \u001b[32m0.1386\u001b[0m       0.3125            0.3125        1.7712  0.0004  0.1774\n",
      "     48            1.0000        \u001b[32m0.1257\u001b[0m       0.3125            0.3125        1.7653  0.0004  0.1917\n",
      "     49            1.0000        \u001b[32m0.1186\u001b[0m       0.3438            0.3438        1.7601  0.0003  0.1531\n",
      "     50            1.0000        0.1199       0.3438            0.3438        1.7551  0.0003  0.1461\n",
      "Fine tuning model for subject 7 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4500        1.5626       0.2917            0.2917        1.4889  0.0004  0.1448\n",
      "     32            0.5250        1.3096       0.3125            0.3125        1.4779  0.0005  0.1441\n",
      "     33            0.5750        1.1096       0.3333            0.3333        1.4778  0.0005  0.1477\n",
      "     34            0.6500        1.1843       0.3438            0.3438        1.4835  0.0006  0.1469\n",
      "     35            \u001b[36m0.8250\u001b[0m        0.8994       0.3229            0.3229        1.5060  0.0006  0.1431\n",
      "     36            \u001b[36m0.8500\u001b[0m        0.7965       0.3229            0.3229        1.5493  0.0007  0.1428\n",
      "     37            0.8250        \u001b[32m0.6493\u001b[0m       0.3229            0.3229        1.6280  0.0007  0.1446\n",
      "     38            0.8500        \u001b[32m0.4712\u001b[0m       0.3125            0.3125        1.7102  0.0007  0.1435\n",
      "     39            0.8000        \u001b[32m0.4154\u001b[0m       0.2812            0.2812        1.7826  0.0007  0.1406\n",
      "     40            0.8000        \u001b[32m0.4142\u001b[0m       0.2917            0.2917        1.8257  0.0007  0.1452\n",
      "     41            0.8500        \u001b[32m0.3388\u001b[0m       0.2917            0.2917        1.8303  0.0007  0.1490\n",
      "     42            \u001b[36m0.8750\u001b[0m        \u001b[32m0.2680\u001b[0m       0.2917            0.2917        1.8137  0.0007  0.1746\n",
      "     43            0.8750        \u001b[32m0.2269\u001b[0m       0.2917            0.2917        1.7928  0.0006  0.1857\n",
      "     44            \u001b[36m0.9250\u001b[0m        \u001b[32m0.1510\u001b[0m       0.2812            0.2812        1.7634  0.0006  0.2004\n",
      "     45            \u001b[36m0.9750\u001b[0m        0.2652       0.2604            0.2604        1.7247  0.0005  0.1991\n",
      "     46            \u001b[36m1.0000\u001b[0m        0.1606       0.2708            0.2708        1.6923  0.0005  0.1825\n",
      "     47            1.0000        0.1560       0.3021            0.3021        1.6661  0.0004  0.1709\n",
      "     48            1.0000        0.1512       0.3125            0.3125        1.6437  0.0004  0.1706\n",
      "     49            1.0000        \u001b[32m0.1246\u001b[0m       0.2917            0.2917        1.6258  0.0003  0.1927\n",
      "     50            1.0000        0.1437       0.3021            0.3021        1.6121  0.0003  0.1725\n",
      "Fine tuning model for subject 7 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3500        1.7608       0.3021            0.3021        1.4758  0.0004  0.2076\n",
      "     32            0.4000        1.5476       0.3333            0.3333        1.4460  0.0005  0.1957\n",
      "     33            0.4500        1.6635       0.3438            0.3438        1.4190  0.0005  0.2070\n",
      "     34            0.6750        1.3624       0.3646            0.3646        1.3984  0.0006  0.2088\n",
      "     35            0.7500        1.1148       0.3333            0.3333        1.3938  0.0006  0.1988\n",
      "     36            0.7750        1.0093       0.3646            0.3646        1.3963  0.0007  0.1532\n",
      "     37            \u001b[36m0.8250\u001b[0m        \u001b[32m0.6816\u001b[0m       0.3438            0.3438        1.4042  0.0007  0.1468\n",
      "     38            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5136\u001b[0m       0.3229            0.3229        1.4075  0.0007  0.1512\n",
      "     39            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4855\u001b[0m       0.3229            0.3229        1.4074  0.0007  0.1578\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3318\u001b[0m       0.3542            0.3542        1.4087  0.0007  0.1507\n",
      "     41            1.0000        \u001b[32m0.2812\u001b[0m       0.3542            0.3542        1.4137  0.0007  0.1495\n",
      "     42            1.0000        0.2908       0.3542            0.3542        1.4209  0.0007  0.1477\n",
      "     43            1.0000        \u001b[32m0.2577\u001b[0m       0.3542            0.3542        1.4301  0.0006  0.1440\n",
      "     44            1.0000        \u001b[32m0.2536\u001b[0m       0.3854            0.3854        1.4414  0.0006  0.1444\n",
      "     45            1.0000        \u001b[32m0.1816\u001b[0m       0.3958            0.3958        1.4500  0.0005  0.1430\n",
      "     46            1.0000        0.1952       0.3854            0.3854        1.4576  0.0005  0.1450\n",
      "     47            1.0000        \u001b[32m0.1795\u001b[0m       0.3854            0.3854        1.4632  0.0004  0.1443\n",
      "     48            1.0000        0.1893       0.3958            0.3958        1.4664  0.0004  0.1476\n",
      "     49            1.0000        \u001b[32m0.1369\u001b[0m       0.4062            0.4062        1.4698  0.0003  0.1514\n",
      "     50            1.0000        0.1668       0.3958            0.3958        1.4725  0.0003  0.1460\n",
      "Fine tuning model for subject 7 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.8616       0.3021            0.3021        1.4831  0.0004  0.1402\n",
      "     32            0.4250        1.6473       0.2917            0.2917        1.4625  0.0005  0.1494\n",
      "     33            0.5750        1.4464       0.2917            0.2917        1.4505  0.0005  0.1415\n",
      "     34            0.6750        1.2074       0.2812            0.2812        1.4452  0.0006  0.1478\n",
      "     35            0.7750        1.2244       0.2812            0.2812        1.4399  0.0006  0.1448\n",
      "     36            \u001b[36m0.8500\u001b[0m        0.7775       0.3333            0.3333        1.4419  0.0007  0.1416\n",
      "     37            \u001b[36m0.9000\u001b[0m        0.7424       0.3438            0.3438        1.4479  0.0007  0.1402\n",
      "     38            \u001b[36m0.9250\u001b[0m        \u001b[32m0.5686\u001b[0m       0.3333            0.3333        1.4635  0.0007  0.1436\n",
      "     39            0.9250        \u001b[32m0.4915\u001b[0m       0.3333            0.3333        1.4950  0.0007  0.1400\n",
      "     40            0.9000        \u001b[32m0.3988\u001b[0m       0.3542            0.3542        1.5424  0.0007  0.1467\n",
      "     41            0.9000        \u001b[32m0.2869\u001b[0m       0.3542            0.3542        1.5966  0.0007  0.1430\n",
      "     42            0.9250        \u001b[32m0.2724\u001b[0m       0.3750            0.3750        1.6516  0.0007  0.1448\n",
      "     43            0.9250        \u001b[32m0.2414\u001b[0m       0.3438            0.3438        1.7001  0.0006  0.1431\n",
      "     44            0.9250        \u001b[32m0.1987\u001b[0m       0.3438            0.3438        1.7411  0.0006  0.1486\n",
      "     45            \u001b[36m0.9500\u001b[0m        \u001b[32m0.1931\u001b[0m       0.3333            0.3333        1.7688  0.0005  0.1447\n",
      "     46            \u001b[36m0.9750\u001b[0m        \u001b[32m0.1618\u001b[0m       0.3333            0.3333        1.7871  0.0005  0.1405\n",
      "     47            0.9750        0.1674       0.3333            0.3333        1.7982  0.0004  0.1491\n",
      "     48            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1096\u001b[0m       0.3646            0.3646        1.8043  0.0004  0.1452\n",
      "     49            1.0000        0.1518       0.3750            0.3750        1.8055  0.0003  0.1418\n",
      "     50            1.0000        0.1129       0.3646            0.3646        1.8047  0.0003  0.1466\n",
      "Fine tuning model for subject 7 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.4261       0.3125            0.3125        1.4818  0.0004  0.1459\n",
      "     32            0.5250        1.3077       0.3542            0.3542        1.4568  0.0005  0.1413\n",
      "     33            0.6000        1.1223       0.3542            0.3542        1.4380  0.0005  0.1473\n",
      "     34            0.7250        1.1008       0.3646            0.3646        1.4276  0.0006  0.1510\n",
      "     35            \u001b[36m0.8000\u001b[0m        0.8057       0.3958            0.3958        1.4383  0.0006  0.1395\n",
      "     36            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6921\u001b[0m       0.3646            0.3646        1.4760  0.0007  0.1444\n",
      "     37            \u001b[36m0.9250\u001b[0m        \u001b[32m0.5458\u001b[0m       0.3646            0.3646        1.5330  0.0007  0.1449\n",
      "     38            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4392\u001b[0m       0.3542            0.3542        1.6026  0.0007  0.1433\n",
      "     39            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3413\u001b[0m       0.3438            0.3438        1.6630  0.0007  0.1428\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2632\u001b[0m       0.3333            0.3333        1.7082  0.0007  0.1390\n",
      "     41            1.0000        \u001b[32m0.2350\u001b[0m       0.3438            0.3438        1.7398  0.0007  0.1397\n",
      "     42            1.0000        \u001b[32m0.1669\u001b[0m       0.3542            0.3542        1.7577  0.0007  0.1444\n",
      "     43            1.0000        0.1859       0.3542            0.3542        1.7681  0.0006  0.1434\n",
      "     44            1.0000        \u001b[32m0.1577\u001b[0m       0.3542            0.3542        1.7691  0.0006  0.1442\n",
      "     45            1.0000        \u001b[32m0.1426\u001b[0m       0.3646            0.3646        1.7640  0.0005  0.1445\n",
      "     46            1.0000        0.1551       0.3646            0.3646        1.7563  0.0005  0.1428\n",
      "     47            1.0000        \u001b[32m0.1267\u001b[0m       0.3542            0.3542        1.7442  0.0004  0.1418\n",
      "     48            1.0000        \u001b[32m0.0999\u001b[0m       0.3542            0.3542        1.7315  0.0004  0.1458\n",
      "     49            1.0000        0.1399       0.3542            0.3542        1.7198  0.0003  0.1456\n",
      "     50            1.0000        0.1494       0.3750            0.3750        1.7068  0.0003  0.1460\n",
      "Fine tuning model for subject 7 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.5927       0.3021            0.3021        1.4822  0.0004  0.1492\n",
      "     32            0.4000        1.4414       0.3229            0.3229        1.4587  0.0005  0.1502\n",
      "     33            0.5250        1.3913       0.3646            0.3646        1.4356  0.0005  0.1441\n",
      "     34            0.6750        1.1690       0.3542            0.3542        1.4139  0.0006  0.1367\n",
      "     35            0.7500        1.0268       0.3542            0.3542        1.3945  0.0006  0.1419\n",
      "     36            \u001b[36m0.8250\u001b[0m        0.8108       0.3750            0.3750        1.3898  0.0007  0.1497\n",
      "     37            \u001b[36m0.8500\u001b[0m        \u001b[32m0.6315\u001b[0m       0.3646            0.3646        1.4021  0.0007  0.1447\n",
      "     38            \u001b[36m0.9250\u001b[0m        \u001b[32m0.5226\u001b[0m       0.3438            0.3438        1.4340  0.0007  0.1567\n",
      "     39            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4203\u001b[0m       0.3854            0.3854        1.4792  0.0007  0.1438\n",
      "     40            0.9500        \u001b[32m0.3076\u001b[0m       0.3750            0.3750        1.5368  0.0007  0.1465\n",
      "     41            0.9500        0.3321       0.3750            0.3750        1.5979  0.0007  0.1540\n",
      "     42            0.9500        \u001b[32m0.2983\u001b[0m       0.3646            0.3646        1.6459  0.0007  0.1505\n",
      "     43            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2878\u001b[0m       0.3750            0.3750        1.6822  0.0006  0.1422\n",
      "     44            0.9500        \u001b[32m0.2271\u001b[0m       0.3750            0.3750        1.6993  0.0006  0.1431\n",
      "     45            0.9750        \u001b[32m0.2221\u001b[0m       0.3646            0.3646        1.7034  0.0005  0.1650\n",
      "     46            0.9750        \u001b[32m0.1775\u001b[0m       0.3438            0.3438        1.7018  0.0005  0.1618\n",
      "     47            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1362\u001b[0m       0.3542            0.3542        1.6937  0.0004  0.1694\n",
      "     48            1.0000        0.1544       0.3750            0.3750        1.6822  0.0004  0.1650\n",
      "     49            1.0000        \u001b[32m0.1115\u001b[0m       0.3958            0.3958        1.6695  0.0003  0.1459\n",
      "     50            1.0000        0.1265       0.3854            0.3854        1.6554  0.0003  0.1710\n",
      "Fine tuning model for subject 7 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3000        1.4495       0.3021            0.3021        1.4869  0.0004  0.1673\n",
      "     32            0.3250        1.5819       0.3646            0.3646        1.4746  0.0005  0.1618\n",
      "     33            0.4750        1.3307       0.3333            0.3333        1.4696  0.0005  0.1745\n",
      "     34            0.5750        1.1180       0.3438            0.3438        1.4799  0.0006  0.1581\n",
      "     35            0.6750        0.9325       0.3750            0.3750        1.4999  0.0006  0.1546\n",
      "     36            0.7750        0.7715       0.3646            0.3646        1.5224  0.0007  0.1717\n",
      "     37            \u001b[36m0.8250\u001b[0m        \u001b[32m0.6078\u001b[0m       0.3125            0.3125        1.5370  0.0007  0.1888\n",
      "     38            \u001b[36m0.8750\u001b[0m        \u001b[32m0.4934\u001b[0m       0.3229            0.3229        1.5368  0.0007  0.2028\n",
      "     39            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3869\u001b[0m       0.3021            0.3021        1.5288  0.0007  0.1681\n",
      "     40            0.9750        \u001b[32m0.3585\u001b[0m       0.3021            0.3021        1.5194  0.0007  0.1618\n",
      "     41            0.9750        \u001b[32m0.3490\u001b[0m       0.3021            0.3021        1.5173  0.0007  0.2057\n",
      "     42            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2495\u001b[0m       0.3229            0.3229        1.5201  0.0007  0.1584\n",
      "     43            1.0000        \u001b[32m0.2058\u001b[0m       0.3438            0.3438        1.5224  0.0006  0.1541\n",
      "     44            1.0000        \u001b[32m0.1832\u001b[0m       0.3333            0.3333        1.5235  0.0006  0.1492\n",
      "     45            1.0000        0.2026       0.3438            0.3438        1.5211  0.0005  0.1455\n",
      "     46            1.0000        \u001b[32m0.1793\u001b[0m       0.3438            0.3438        1.5209  0.0005  0.1441\n",
      "     47            1.0000        0.1873       0.3542            0.3542        1.5182  0.0004  0.1422\n",
      "     48            1.0000        \u001b[32m0.1510\u001b[0m       0.3646            0.3646        1.5143  0.0004  0.1439\n",
      "     49            1.0000        0.1598       0.3750            0.3750        1.5078  0.0003  0.1444\n",
      "     50            1.0000        0.1534       0.3750            0.3750        1.5010  0.0003  0.1482\n",
      "Fine tuning model for subject 7 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2750        1.9862       0.3021            0.3021        1.4858  0.0004  0.1434\n",
      "     32            0.3000        1.9001       0.3229            0.3229        1.4696  0.0005  0.1450\n",
      "     33            0.3000        1.5701       0.3125            0.3125        1.4549  0.0005  0.1470\n",
      "     34            0.3500        1.5851       0.3229            0.3229        1.4481  0.0006  0.1520\n",
      "     35            0.4500        1.2744       0.3333            0.3333        1.4553  0.0006  0.1568\n",
      "     36            0.5750        0.9406       0.3750            0.3750        1.4769  0.0007  0.1591\n",
      "     37            0.6500        0.7865       0.3542            0.3542        1.5145  0.0007  0.1595\n",
      "     38            0.7250        \u001b[32m0.6221\u001b[0m       0.3438            0.3438        1.5593  0.0007  0.1540\n",
      "     39            0.7750        \u001b[32m0.4449\u001b[0m       0.3438            0.3438        1.6233  0.0007  0.1742\n",
      "     40            \u001b[36m0.8000\u001b[0m        \u001b[32m0.3653\u001b[0m       0.3542            0.3542        1.6933  0.0007  0.1491\n",
      "     41            \u001b[36m0.8750\u001b[0m        \u001b[32m0.3006\u001b[0m       0.3750            0.3750        1.7622  0.0007  0.1463\n",
      "     42            0.8750        \u001b[32m0.2342\u001b[0m       0.3542            0.3542        1.8300  0.0007  0.1426\n",
      "     43            \u001b[36m0.9000\u001b[0m        \u001b[32m0.2207\u001b[0m       0.3542            0.3542        1.8855  0.0006  0.1424\n",
      "     44            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2118\u001b[0m       0.3750            0.3750        1.9263  0.0006  0.1474\n",
      "     45            0.9500        0.2129       0.3750            0.3750        1.9458  0.0005  0.1460\n",
      "     46            0.9500        \u001b[32m0.1489\u001b[0m       0.3646            0.3646        1.9464  0.0005  0.1435\n",
      "     47            0.9500        0.1660       0.3646            0.3646        1.9299  0.0004  0.1490\n",
      "     48            0.9500        0.1732       0.3542            0.3542        1.9039  0.0004  0.1473\n",
      "     49            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1397\u001b[0m       0.3542            0.3542        1.8722  0.0003  0.1385\n",
      "     50            1.0000        \u001b[32m0.1368\u001b[0m       0.3542            0.3542        1.8396  0.0003  0.1446\n",
      "Fine tuning model for subject 7 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3556        1.4904       0.3125            0.3125        1.4911  0.0004  0.1525\n",
      "     32            0.4000        1.5024       0.3333            0.3333        1.4881  0.0005  0.1571\n",
      "     33            0.4667        1.5709       0.3229            0.3229        1.4988  0.0005  0.1532\n",
      "     34            0.5333        1.2365       0.2917            0.2917        1.5256  0.0006  0.1651\n",
      "     35            0.6222        0.9101       0.2917            0.2917        1.5839  0.0006  0.1552\n",
      "     36            0.6222        0.7407       0.2917            0.2917        1.6569  0.0007  0.1527\n",
      "     37            0.6222        \u001b[32m0.6856\u001b[0m       0.2500            0.2500        1.7239  0.0007  0.1532\n",
      "     38            0.6889        \u001b[32m0.5465\u001b[0m       0.2604            0.2604        1.7541  0.0007  0.1548\n",
      "     39            0.7333        \u001b[32m0.4261\u001b[0m       0.2812            0.2812        1.7353  0.0007  0.1544\n",
      "     40            \u001b[36m0.8889\u001b[0m        \u001b[32m0.4001\u001b[0m       0.2917            0.2917        1.6790  0.0007  0.1522\n",
      "     41            0.8889        \u001b[32m0.2786\u001b[0m       0.3333            0.3333        1.6160  0.0007  0.1580\n",
      "     42            \u001b[36m0.9333\u001b[0m        \u001b[32m0.2659\u001b[0m       0.3542            0.3542        1.5541  0.0007  0.1532\n",
      "     43            0.9333        \u001b[32m0.2584\u001b[0m       0.3750            0.3750        1.5016  0.0006  0.1461\n",
      "     44            0.9333        \u001b[32m0.1730\u001b[0m       0.3646            0.3646        1.4667  0.0006  0.1491\n",
      "     45            \u001b[36m1.0000\u001b[0m        0.1965       0.3958            0.3958        1.4471  0.0005  0.1559\n",
      "     46            1.0000        \u001b[32m0.1625\u001b[0m       0.4167            0.4167        1.4341  0.0005  0.1548\n",
      "     47            1.0000        \u001b[32m0.1465\u001b[0m       0.4167            0.4167        1.4252  0.0004  0.1491\n",
      "     48            1.0000        0.1918       0.4167            0.4167        1.4194  0.0004  0.1496\n",
      "     49            1.0000        0.1744       0.4271            0.4271        1.4158  0.0003  0.1510\n",
      "     50            1.0000        \u001b[32m0.1167\u001b[0m       0.4167            0.4167        1.4133  0.0003  0.1509\n",
      "Fine tuning model for subject 7 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.3501       0.3229            0.3229        1.4870  0.0004  0.1573\n",
      "     32            0.4889        1.3666       0.3333            0.3333        1.4711  0.0005  0.1596\n",
      "     33            0.5111        1.1413       0.3542            0.3542        1.4604  0.0005  0.1526\n",
      "     34            0.5556        1.0989       0.3438            0.3438        1.4608  0.0006  0.1651\n",
      "     35            0.6222        0.8241       0.3438            0.3438        1.4662  0.0006  0.1516\n",
      "     36            0.7556        0.8133       0.3438            0.3438        1.4678  0.0007  0.1523\n",
      "     37            \u001b[36m0.8222\u001b[0m        \u001b[32m0.6346\u001b[0m       0.3438            0.3438        1.4642  0.0007  0.1494\n",
      "     38            \u001b[36m0.8889\u001b[0m        \u001b[32m0.4388\u001b[0m       0.3542            0.3542        1.4607  0.0007  0.1626\n",
      "     39            0.8889        \u001b[32m0.4066\u001b[0m       0.3542            0.3542        1.4588  0.0007  0.1540\n",
      "     40            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3191\u001b[0m       0.3750            0.3750        1.4578  0.0007  0.1530\n",
      "     41            \u001b[36m0.9556\u001b[0m        \u001b[32m0.3170\u001b[0m       0.3646            0.3646        1.4584  0.0007  0.1506\n",
      "     42            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2755\u001b[0m       0.3438            0.3438        1.4594  0.0007  0.1550\n",
      "     43            1.0000        \u001b[32m0.2656\u001b[0m       0.3333            0.3333        1.4614  0.0006  0.1539\n",
      "     44            1.0000        \u001b[32m0.2214\u001b[0m       0.3438            0.3438        1.4632  0.0006  0.1612\n",
      "     45            1.0000        0.2425       0.3438            0.3438        1.4654  0.0005  0.1553\n",
      "     46            1.0000        \u001b[32m0.1847\u001b[0m       0.3438            0.3438        1.4677  0.0005  0.1607\n",
      "     47            1.0000        \u001b[32m0.1687\u001b[0m       0.3438            0.3438        1.4700  0.0004  0.2024\n",
      "     48            1.0000        \u001b[32m0.1395\u001b[0m       0.3750            0.3750        1.4726  0.0004  0.1781\n",
      "     49            1.0000        0.1616       0.3750            0.3750        1.4754  0.0003  0.1812\n",
      "     50            1.0000        \u001b[32m0.1296\u001b[0m       0.3750            0.3750        1.4783  0.0003  0.1861\n",
      "Fine tuning model for subject 7 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.4757       0.3229            0.3229        1.4824  0.0004  0.1997\n",
      "     32            0.4000        1.4811       0.3438            0.3438        1.4620  0.0005  0.1655\n",
      "     33            0.4667        1.2804       0.3646            0.3646        1.4475  0.0005  0.1840\n",
      "     34            0.6222        1.1622       0.3750            0.3750        1.4457  0.0006  0.1891\n",
      "     35            0.7111        0.9096       0.3646            0.3646        1.4614  0.0006  0.1689\n",
      "     36            \u001b[36m0.8667\u001b[0m        0.7757       0.3542            0.3542        1.4884  0.0007  0.1663\n",
      "     37            \u001b[36m0.8889\u001b[0m        \u001b[32m0.6448\u001b[0m       0.3542            0.3542        1.5165  0.0007  0.1744\n",
      "     38            \u001b[36m0.9111\u001b[0m        \u001b[32m0.5548\u001b[0m       0.3438            0.3438        1.5425  0.0007  0.1975\n",
      "     39            0.9111        \u001b[32m0.5340\u001b[0m       0.3542            0.3542        1.5654  0.0007  0.2361\n",
      "     40            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4159\u001b[0m       0.3646            0.3646        1.5847  0.0007  0.1681\n",
      "     41            0.9333        0.4197       0.3750            0.3750        1.5937  0.0007  0.2115\n",
      "     42            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3952\u001b[0m       0.3750            0.3750        1.5964  0.0007  0.1705\n",
      "     43            0.9778        \u001b[32m0.2749\u001b[0m       0.3646            0.3646        1.6005  0.0006  0.1657\n",
      "     44            0.9778        0.2933       0.3750            0.3750        1.5994  0.0006  0.1549\n",
      "     45            0.9778        \u001b[32m0.2072\u001b[0m       0.3854            0.3854        1.5933  0.0005  0.1564\n",
      "     46            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1687\u001b[0m       0.3854            0.3854        1.5850  0.0005  0.1533\n",
      "     47            1.0000        0.2621       0.3854            0.3854        1.5748  0.0004  0.1516\n",
      "     48            1.0000        0.2038       0.3958            0.3958        1.5654  0.0004  0.1504\n",
      "     49            1.0000        0.2429       0.4062            0.4062        1.5559  0.0003  0.1478\n",
      "     50            1.0000        0.1958       0.4271            0.4271        1.5471  0.0003  0.1509\n",
      "Fine tuning model for subject 7 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4444        1.2971       0.2917            0.2917        1.4870  0.0004  0.1529\n",
      "     32            0.4889        1.4368       0.3021            0.3021        1.4717  0.0005  0.1604\n",
      "     33            0.6000        1.2338       0.3125            0.3125        1.4574  0.0005  0.1502\n",
      "     34            0.6667        1.0849       0.3333            0.3333        1.4507  0.0006  0.1529\n",
      "     35            \u001b[36m0.8444\u001b[0m        0.9792       0.3125            0.3125        1.4528  0.0006  0.1540\n",
      "     36            \u001b[36m0.8667\u001b[0m        0.7519       0.3125            0.3125        1.4693  0.0007  0.1443\n",
      "     37            0.8667        \u001b[32m0.5106\u001b[0m       0.3021            0.3021        1.4927  0.0007  0.1610\n",
      "     38            \u001b[36m0.8889\u001b[0m        0.5647       0.2708            0.2708        1.5314  0.0007  0.1578\n",
      "     39            \u001b[36m0.9111\u001b[0m        \u001b[32m0.3669\u001b[0m       0.3229            0.3229        1.5797  0.0007  0.1600\n",
      "     40            \u001b[36m0.9333\u001b[0m        0.4271       0.3229            0.3229        1.6205  0.0007  0.1562\n",
      "     41            \u001b[36m0.9556\u001b[0m        \u001b[32m0.3264\u001b[0m       0.3229            0.3229        1.6562  0.0007  0.1497\n",
      "     42            0.9556        0.3333       0.3333            0.3333        1.6782  0.0007  0.1517\n",
      "     43            0.9556        \u001b[32m0.3145\u001b[0m       0.3333            0.3333        1.6866  0.0006  0.1514\n",
      "     44            \u001b[36m0.9778\u001b[0m        \u001b[32m0.2545\u001b[0m       0.3438            0.3438        1.6978  0.0006  0.1521\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1942\u001b[0m       0.3438            0.3438        1.7071  0.0005  0.1522\n",
      "     46            1.0000        0.2187       0.3438            0.3438        1.7159  0.0005  0.1484\n",
      "     47            1.0000        \u001b[32m0.1797\u001b[0m       0.3438            0.3438        1.7219  0.0004  0.1490\n",
      "     48            1.0000        \u001b[32m0.1658\u001b[0m       0.3542            0.3542        1.7266  0.0004  0.1537\n",
      "     49            1.0000        \u001b[32m0.1540\u001b[0m       0.3646            0.3646        1.7264  0.0003  0.1468\n",
      "     50            1.0000        \u001b[32m0.1420\u001b[0m       0.3542            0.3542        1.7229  0.0003  0.1514\n",
      "Fine tuning model for subject 7 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4222        1.5264       0.3229            0.3229        1.4863  0.0004  0.1624\n",
      "     32            0.4444        1.3430       0.3542            0.3542        1.4675  0.0005  0.1546\n",
      "     33            0.6000        1.1780       0.3542            0.3542        1.4541  0.0005  0.1666\n",
      "     34            0.7333        0.7950       0.3229            0.3229        1.4481  0.0006  0.1558\n",
      "     35            \u001b[36m0.8444\u001b[0m        0.8327       0.3542            0.3542        1.4527  0.0006  0.1570\n",
      "     36            \u001b[36m0.8889\u001b[0m        \u001b[32m0.6444\u001b[0m       0.3750            0.3750        1.4656  0.0007  0.1705\n",
      "     37            \u001b[36m0.9556\u001b[0m        \u001b[32m0.5199\u001b[0m       0.3542            0.3542        1.4804  0.0007  0.1797\n",
      "     38            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3738\u001b[0m       0.3646            0.3646        1.4953  0.0007  0.1547\n",
      "     39            0.9778        \u001b[32m0.3272\u001b[0m       0.3646            0.3646        1.5114  0.0007  0.1800\n",
      "     40            0.9778        0.3437       0.3646            0.3646        1.5321  0.0007  0.1578\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2970\u001b[0m       0.3750            0.3750        1.5537  0.0007  0.1537\n",
      "     42            1.0000        \u001b[32m0.1910\u001b[0m       0.3750            0.3750        1.5731  0.0007  0.1574\n",
      "     43            1.0000        0.2067       0.3854            0.3854        1.5894  0.0006  0.1552\n",
      "     44            1.0000        \u001b[32m0.1742\u001b[0m       0.3646            0.3646        1.6017  0.0006  0.1580\n",
      "     45            1.0000        0.2090       0.3646            0.3646        1.6121  0.0005  0.1576\n",
      "     46            1.0000        0.1927       0.3750            0.3750        1.6204  0.0005  0.1558\n",
      "     47            1.0000        \u001b[32m0.1266\u001b[0m       0.3750            0.3750        1.6270  0.0004  0.1596\n",
      "     48            1.0000        0.1512       0.3750            0.3750        1.6327  0.0004  0.1535\n",
      "     49            1.0000        \u001b[32m0.1231\u001b[0m       0.3646            0.3646        1.6376  0.0003  0.1567\n",
      "     50            1.0000        0.1270       0.3542            0.3542        1.6420  0.0003  0.1531\n",
      "Fine tuning model for subject 7 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5333        1.5733       0.3229            0.3229        1.4818  0.0004  0.1579\n",
      "     32            0.5778        1.3566       0.3646            0.3646        1.4712  0.0005  0.1506\n",
      "     33            0.6889        1.1284       0.3333            0.3333        1.4828  0.0005  0.1607\n",
      "     34            0.7111        0.9818       0.3854            0.3854        1.5210  0.0006  0.1503\n",
      "     35            0.6889        0.7884       0.3750            0.3750        1.5750  0.0006  0.1515\n",
      "     36            0.7556        \u001b[32m0.6023\u001b[0m       0.3646            0.3646        1.6289  0.0007  0.1550\n",
      "     37            0.7778        \u001b[32m0.4052\u001b[0m       0.3646            0.3646        1.6770  0.0007  0.1571\n",
      "     38            \u001b[36m0.8444\u001b[0m        0.4617       0.3542            0.3542        1.7219  0.0007  0.1566\n",
      "     39            \u001b[36m0.8667\u001b[0m        \u001b[32m0.3755\u001b[0m       0.3333            0.3333        1.7608  0.0007  0.1489\n",
      "     40            \u001b[36m0.8889\u001b[0m        0.4408       0.3021            0.3021        1.7937  0.0007  0.1542\n",
      "     41            0.8889        \u001b[32m0.3058\u001b[0m       0.3125            0.3125        1.8262  0.0007  0.1529\n",
      "     42            \u001b[36m0.9333\u001b[0m        \u001b[32m0.2333\u001b[0m       0.3125            0.3125        1.8519  0.0007  0.1577\n",
      "     43            0.9333        0.2361       0.3021            0.3021        1.8699  0.0006  0.1549\n",
      "     44            \u001b[36m0.9778\u001b[0m        \u001b[32m0.1912\u001b[0m       0.3125            0.3125        1.8767  0.0006  0.1593\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1841\u001b[0m       0.3021            0.3021        1.8773  0.0005  0.1756\n",
      "     46            1.0000        0.1854       0.3021            0.3021        1.8733  0.0005  0.1782\n",
      "     47            1.0000        \u001b[32m0.1569\u001b[0m       0.3021            0.3021        1.8612  0.0004  0.1754\n",
      "     48            1.0000        \u001b[32m0.1510\u001b[0m       0.2917            0.2917        1.8425  0.0004  0.1809\n",
      "     49            1.0000        \u001b[32m0.1290\u001b[0m       0.2812            0.2812        1.8226  0.0003  0.1624\n",
      "     50            1.0000        0.1329       0.2812            0.2812        1.8006  0.0003  0.2118\n",
      "Fine tuning model for subject 7 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.2889        1.4850       0.2917            0.2917        1.4767  0.0004  0.1863\n",
      "     32            0.3111        1.5145       0.3333            0.3333        1.4530  0.0005  0.1909\n",
      "     33            0.4444        1.4095       0.3646            0.3646        1.4333  0.0005  0.1951\n",
      "     34            0.4667        1.2436       0.3750            0.3750        1.4207  0.0006  0.1979\n",
      "     35            0.6222        1.1760       0.3958            0.3958        1.4109  0.0006  0.1832\n",
      "     36            0.7556        0.9760       0.4167            0.4167        1.4128  0.0007  0.1890\n",
      "     37            \u001b[36m0.8444\u001b[0m        0.7294       0.3958            0.3958        1.4157  0.0007  0.1713\n",
      "     38            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6812\u001b[0m       0.4062            0.4062        1.4199  0.0007  0.1889\n",
      "     39            \u001b[36m0.9111\u001b[0m        \u001b[32m0.5846\u001b[0m       0.3854            0.3854        1.4251  0.0007  0.2348\n",
      "     40            \u001b[36m0.9556\u001b[0m        \u001b[32m0.4282\u001b[0m       0.3854            0.3854        1.4392  0.0007  0.1518\n",
      "     41            0.9556        0.4340       0.3646            0.3646        1.4485  0.0007  0.1603\n",
      "     42            0.9556        \u001b[32m0.3455\u001b[0m       0.3542            0.3542        1.4578  0.0007  0.1494\n",
      "     43            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2835\u001b[0m       0.3750            0.3750        1.4646  0.0006  0.1493\n",
      "     44            1.0000        \u001b[32m0.2812\u001b[0m       0.3750            0.3750        1.4682  0.0006  0.1540\n",
      "     45            1.0000        \u001b[32m0.2483\u001b[0m       0.3854            0.3854        1.4703  0.0005  0.1619\n",
      "     46            1.0000        \u001b[32m0.2389\u001b[0m       0.3958            0.3958        1.4728  0.0005  0.1535\n",
      "     47            1.0000        0.2737       0.3958            0.3958        1.4686  0.0004  0.1631\n",
      "     48            1.0000        \u001b[32m0.1531\u001b[0m       0.3958            0.3958        1.4645  0.0004  0.1565\n",
      "     49            1.0000        \u001b[32m0.1352\u001b[0m       0.4062            0.4062        1.4608  0.0003  0.1580\n",
      "     50            1.0000        0.1414       0.4062            0.4062        1.4568  0.0003  0.1525\n",
      "Fine tuning model for subject 7 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4889        1.4308       0.3021            0.3021        1.4884  0.0004  0.1588\n",
      "     32            0.5556        1.5439       0.3229            0.3229        1.4753  0.0005  0.1540\n",
      "     33            0.5556        1.2348       0.3333            0.3333        1.4643  0.0005  0.1550\n",
      "     34            0.6222        1.1999       0.3333            0.3333        1.4573  0.0006  0.2331\n",
      "     35            0.7556        0.9060       0.3646            0.3646        1.4582  0.0006  0.1591\n",
      "     36            \u001b[36m0.8222\u001b[0m        0.8150       0.3438            0.3438        1.4726  0.0007  0.1563\n",
      "     37            \u001b[36m0.8667\u001b[0m        0.7215       0.3854            0.3854        1.4982  0.0007  0.1537\n",
      "     38            \u001b[36m0.8889\u001b[0m        \u001b[32m0.6241\u001b[0m       0.4062            0.4062        1.5191  0.0007  0.1550\n",
      "     39            \u001b[36m0.9556\u001b[0m        \u001b[32m0.5149\u001b[0m       0.4062            0.4062        1.5350  0.0007  0.1502\n",
      "     40            0.9556        \u001b[32m0.4982\u001b[0m       0.3958            0.3958        1.5368  0.0007  0.1542\n",
      "     41            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3854\u001b[0m       0.3542            0.3542        1.5316  0.0007  0.1514\n",
      "     42            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3167\u001b[0m       0.3542            0.3542        1.5221  0.0007  0.1534\n",
      "     43            1.0000        0.3245       0.3542            0.3542        1.5096  0.0006  0.1550\n",
      "     44            1.0000        \u001b[32m0.2231\u001b[0m       0.3542            0.3542        1.5006  0.0006  0.1498\n",
      "     45            1.0000        0.2512       0.3438            0.3438        1.4931  0.0005  0.1533\n",
      "     46            1.0000        \u001b[32m0.1867\u001b[0m       0.3542            0.3542        1.4877  0.0005  0.1559\n",
      "     47            1.0000        \u001b[32m0.1475\u001b[0m       0.3333            0.3333        1.4833  0.0004  0.1607\n",
      "     48            1.0000        0.1841       0.3646            0.3646        1.4806  0.0004  0.1511\n",
      "     49            1.0000        \u001b[32m0.1358\u001b[0m       0.3750            0.3750        1.4792  0.0003  0.1599\n",
      "     50            1.0000        0.1556       0.3646            0.3646        1.4783  0.0003  0.1540\n",
      "Fine tuning model for subject 7 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4667        1.4885       0.3333            0.3333        1.4893  0.0004  0.1531\n",
      "     32            0.5111        1.4756       0.3333            0.3333        1.4838  0.0005  0.1579\n",
      "     33            0.5556        1.2215       0.3229            0.3229        1.4836  0.0005  0.1540\n",
      "     34            0.6000        1.0372       0.3438            0.3438        1.4952  0.0006  0.1502\n",
      "     35            0.6000        0.9275       0.3750            0.3750        1.5194  0.0006  0.1582\n",
      "     36            0.6667        \u001b[32m0.5718\u001b[0m       0.3438            0.3438        1.5523  0.0007  0.1535\n",
      "     37            0.7556        \u001b[32m0.5622\u001b[0m       0.3646            0.3646        1.5815  0.0007  0.1538\n",
      "     38            \u001b[36m0.8222\u001b[0m        \u001b[32m0.5435\u001b[0m       0.3542            0.3542        1.6018  0.0007  0.1551\n",
      "     39            \u001b[36m0.8667\u001b[0m        \u001b[32m0.4483\u001b[0m       0.3438            0.3438        1.6151  0.0007  0.1531\n",
      "     40            \u001b[36m0.8889\u001b[0m        \u001b[32m0.4344\u001b[0m       0.3438            0.3438        1.6224  0.0007  0.1544\n",
      "     41            \u001b[36m0.9111\u001b[0m        \u001b[32m0.3682\u001b[0m       0.3542            0.3542        1.6218  0.0007  0.1502\n",
      "     42            0.9111        \u001b[32m0.3668\u001b[0m       0.3854            0.3854        1.6179  0.0007  0.1597\n",
      "     43            0.9111        \u001b[32m0.3386\u001b[0m       0.3854            0.3854        1.6190  0.0006  0.1549\n",
      "     44            \u001b[36m0.9333\u001b[0m        \u001b[32m0.2127\u001b[0m       0.3958            0.3958        1.6175  0.0006  0.1536\n",
      "     45            \u001b[36m0.9556\u001b[0m        0.2550       0.3958            0.3958        1.6126  0.0005  0.1652\n",
      "     46            \u001b[36m1.0000\u001b[0m        0.2574       0.4271            0.4271        1.6092  0.0005  0.1493\n",
      "     47            1.0000        0.2417       0.4271            0.4271        1.6065  0.0004  0.1520\n",
      "     48            1.0000        \u001b[32m0.2010\u001b[0m       0.4375            0.4375        1.6039  0.0004  0.1513\n",
      "     49            1.0000        \u001b[32m0.1560\u001b[0m       0.4375            0.4375        1.5995  0.0003  0.1553\n",
      "     50            1.0000        0.1897       0.4375            0.4375        1.5955  0.0003  0.1568\n",
      "Fine tuning model for subject 7 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4000        1.4873       0.2917            0.2917        1.4924  0.0004  0.1614\n",
      "     32            0.4000        1.2919       0.3021            0.3021        1.4852  0.0005  0.1543\n",
      "     33            0.4000        1.1296       0.3229            0.3229        1.4855  0.0005  0.1516\n",
      "     34            0.6000        1.0692       0.3125            0.3125        1.4909  0.0006  0.1669\n",
      "     35            0.7111        0.8575       0.3333            0.3333        1.4977  0.0006  0.1719\n",
      "     36            \u001b[36m0.8000\u001b[0m        0.7190       0.3125            0.3125        1.5110  0.0007  0.1628\n",
      "     37            \u001b[36m0.8444\u001b[0m        \u001b[32m0.5793\u001b[0m       0.3021            0.3021        1.5269  0.0007  0.1672\n",
      "     38            \u001b[36m0.8889\u001b[0m        \u001b[32m0.5301\u001b[0m       0.3333            0.3333        1.5470  0.0007  0.1506\n",
      "     39            \u001b[36m0.9111\u001b[0m        \u001b[32m0.3930\u001b[0m       0.3229            0.3229        1.5666  0.0007  0.1455\n",
      "     40            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3193\u001b[0m       0.3125            0.3125        1.5829  0.0007  0.1524\n",
      "     41            \u001b[36m0.9778\u001b[0m        \u001b[32m0.2997\u001b[0m       0.3125            0.3125        1.5961  0.0007  0.1544\n",
      "     42            0.9778        \u001b[32m0.2506\u001b[0m       0.3125            0.3125        1.6088  0.0007  0.1601\n",
      "     43            0.9778        0.2633       0.3125            0.3125        1.6208  0.0006  0.1708\n",
      "     44            0.9778        0.2787       0.3021            0.3021        1.6339  0.0006  0.1636\n",
      "     45            0.9778        \u001b[32m0.2103\u001b[0m       0.3438            0.3438        1.6474  0.0005  0.1599\n",
      "     46            \u001b[36m1.0000\u001b[0m        0.2180       0.3438            0.3438        1.6608  0.0005  0.1712\n",
      "     47            1.0000        \u001b[32m0.1790\u001b[0m       0.3333            0.3333        1.6757  0.0004  0.1980\n",
      "     48            1.0000        \u001b[32m0.1437\u001b[0m       0.3229            0.3229        1.6915  0.0004  0.1998\n",
      "     49            1.0000        0.1734       0.3125            0.3125        1.7068  0.0003  0.1733\n",
      "     50            1.0000        \u001b[32m0.1386\u001b[0m       0.3229            0.3229        1.7209  0.0003  0.1807\n",
      "Hold out data from subject 8\n",
      "Pre-training model with data from all subjects but subject 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3901\u001b[0m        \u001b[32m1.6037\u001b[0m       \u001b[35m0.3372\u001b[0m            \u001b[31m0.3372\u001b[0m        \u001b[94m1.3891\u001b[0m  0.0007  7.9953\n",
      "      2            \u001b[36m0.4500\u001b[0m        \u001b[32m1.4250\u001b[0m       \u001b[35m0.3633\u001b[0m            \u001b[31m0.3633\u001b[0m        \u001b[94m1.3440\u001b[0m  0.0007  8.0856\n",
      "      3            \u001b[36m0.5107\u001b[0m        \u001b[32m1.3281\u001b[0m       \u001b[35m0.4102\u001b[0m            \u001b[31m0.4102\u001b[0m        \u001b[94m1.2872\u001b[0m  0.0007  7.3888\n",
      "      4            \u001b[36m0.5479\u001b[0m        \u001b[32m1.2714\u001b[0m       \u001b[35m0.4479\u001b[0m            \u001b[31m0.4479\u001b[0m        \u001b[94m1.2468\u001b[0m  0.0007  7.8833\n",
      "      5            \u001b[36m0.5779\u001b[0m        \u001b[32m1.2172\u001b[0m       0.4349            0.4349        \u001b[94m1.2343\u001b[0m  0.0007  7.7785\n",
      "      6            0.5708        \u001b[32m1.1757\u001b[0m       0.4193            0.4193        1.2558  0.0006  7.4772\n",
      "      7            0.5404        \u001b[32m1.1310\u001b[0m       0.4154            0.4154        1.3390  0.0006  7.8583\n",
      "      8            \u001b[36m0.6372\u001b[0m        \u001b[32m1.0922\u001b[0m       \u001b[35m0.4805\u001b[0m            \u001b[31m0.4805\u001b[0m        \u001b[94m1.1794\u001b[0m  0.0006  7.7641\n",
      "      9            \u001b[36m0.6458\u001b[0m        \u001b[32m1.0429\u001b[0m       0.4779            0.4779        1.1797  0.0006  7.3438\n",
      "     10            \u001b[36m0.6755\u001b[0m        \u001b[32m1.0151\u001b[0m       \u001b[35m0.4818\u001b[0m            \u001b[31m0.4818\u001b[0m        \u001b[94m1.1718\u001b[0m  0.0005  8.0856\n",
      "     11            0.6682        \u001b[32m0.9884\u001b[0m       \u001b[35m0.4870\u001b[0m            \u001b[31m0.4870\u001b[0m        1.1871  0.0005  7.7226\n",
      "     12            \u001b[36m0.6826\u001b[0m        \u001b[32m0.9697\u001b[0m       0.4805            0.4805        \u001b[94m1.1636\u001b[0m  0.0005  7.7236\n",
      "     13            \u001b[36m0.7096\u001b[0m        \u001b[32m0.9382\u001b[0m       \u001b[35m0.5026\u001b[0m            \u001b[31m0.5026\u001b[0m        \u001b[94m1.1351\u001b[0m  0.0004  8.3800\n",
      "     14            0.6687        \u001b[32m0.9214\u001b[0m       0.4974            0.4974        1.2246  0.0004  7.8271\n",
      "     15            \u001b[36m0.7310\u001b[0m        \u001b[32m0.9030\u001b[0m       \u001b[35m0.5182\u001b[0m            \u001b[31m0.5182\u001b[0m        1.1382  0.0004  7.5278\n",
      "     16            \u001b[36m0.7422\u001b[0m        \u001b[32m0.8819\u001b[0m       0.5052            0.5052        1.1522  0.0003  8.1427\n",
      "     17            \u001b[36m0.7612\u001b[0m        \u001b[32m0.8564\u001b[0m       0.5052            0.5052        \u001b[94m1.1331\u001b[0m  0.0003  7.9223\n",
      "     18            0.7549        \u001b[32m0.8252\u001b[0m       \u001b[35m0.5221\u001b[0m            \u001b[31m0.5221\u001b[0m        1.1363  0.0003  7.6052\n",
      "     19            0.7279        \u001b[32m0.8223\u001b[0m       0.4948            0.4948        1.1729  0.0002  8.2940\n",
      "     20            \u001b[36m0.7628\u001b[0m        \u001b[32m0.7970\u001b[0m       0.5156            0.5156        1.1335  0.0002  7.9899\n",
      "     21            \u001b[36m0.7883\u001b[0m        \u001b[32m0.7904\u001b[0m       \u001b[35m0.5299\u001b[0m            \u001b[31m0.5299\u001b[0m        \u001b[94m1.1089\u001b[0m  0.0002  7.6277\n",
      "     22            0.7794        \u001b[32m0.7789\u001b[0m       \u001b[35m0.5352\u001b[0m            \u001b[31m0.5352\u001b[0m        1.1136  0.0001  8.1161\n",
      "     23            \u001b[36m0.7927\u001b[0m        \u001b[32m0.7784\u001b[0m       0.5312            0.5312        \u001b[94m1.1039\u001b[0m  0.0001  7.9652\n",
      "     24            \u001b[36m0.8005\u001b[0m        \u001b[32m0.7503\u001b[0m       \u001b[35m0.5417\u001b[0m            \u001b[31m0.5417\u001b[0m        \u001b[94m1.1003\u001b[0m  0.0001  7.7839\n",
      "     25            0.8005        \u001b[32m0.7406\u001b[0m       0.5404            0.5404        \u001b[94m1.0955\u001b[0m  0.0001  8.2346\n",
      "     26            0.8005        0.7436       0.5352            0.5352        1.1023  0.0000  8.0389\n",
      "     27            \u001b[36m0.8091\u001b[0m        \u001b[32m0.7383\u001b[0m       0.5339            0.5339        1.0978  0.0000  7.7542\n",
      "     28            0.8091        \u001b[32m0.7238\u001b[0m       0.5312            0.5312        1.0975  0.0000  7.9203\n",
      "     29            \u001b[36m0.8107\u001b[0m        0.7400       0.5352            0.5352        1.0976  0.0000  8.0260\n",
      "     30            \u001b[36m0.8109\u001b[0m        \u001b[32m0.7230\u001b[0m       0.5286            0.5286        1.0969  0.0000  7.6168\n",
      "Before finetuning for subject 8, the baseline accuracy is 0.6041666666666666\n",
      "Fine tuning model for subject 8 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.8000        0.8542       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m1.0124\u001b[0m  0.0004  0.1612\n",
      "     32            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3513\u001b[0m       0.5625            0.5625        1.0361  0.0005  0.1751\n",
      "     33            1.0000        \u001b[32m0.3459\u001b[0m       0.5729            0.5729        1.0654  0.0005  0.1805\n",
      "     34            1.0000        \u001b[32m0.3096\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.0966  0.0006  0.1778\n",
      "     35            1.0000        \u001b[32m0.0642\u001b[0m       0.5729            0.5729        1.1329  0.0006  0.1612\n",
      "     36            1.0000        \u001b[32m0.0124\u001b[0m       0.5417            0.5417        1.1736  0.0007  0.1739\n",
      "     37            1.0000        0.0173       0.4792            0.4792        1.2197  0.0007  0.1576\n",
      "     38            1.0000        \u001b[32m0.0033\u001b[0m       0.4688            0.4688        1.2687  0.0007  0.1216\n",
      "     39            1.0000        0.0072       0.4479            0.4479        1.3184  0.0007  0.1230\n",
      "     40            1.0000        \u001b[32m0.0020\u001b[0m       0.4479            0.4479        1.3662  0.0007  0.1347\n",
      "     41            1.0000        0.0022       0.4479            0.4479        1.4097  0.0007  0.1212\n",
      "     42            1.0000        0.0068       0.4479            0.4479        1.4485  0.0007  0.1399\n",
      "     43            1.0000        0.0025       0.4375            0.4375        1.4800  0.0006  0.1291\n",
      "     44            1.0000        \u001b[32m0.0017\u001b[0m       0.4271            0.4271        1.5036  0.0006  0.1273\n",
      "     45            1.0000        \u001b[32m0.0017\u001b[0m       0.4167            0.4167        1.5199  0.0005  0.1265\n",
      "     46            1.0000        0.0024       0.4167            0.4167        1.5301  0.0005  0.1340\n",
      "     47            1.0000        0.0028       0.4167            0.4167        1.5351  0.0004  0.1296\n",
      "     48            1.0000        0.0031       0.4167            0.4167        1.5359  0.0004  0.1372\n",
      "     49            1.0000        0.0059       0.4271            0.4271        1.5333  0.0003  0.1276\n",
      "     50            1.0000        \u001b[32m0.0015\u001b[0m       0.4271            0.4271        1.5287  0.0003  0.1349\n",
      "Fine tuning model for subject 8 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.8000        0.8108       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0031\u001b[0m  0.0004  0.1388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6151\u001b[0m       0.5521            0.5521        1.0143  0.0005  0.1388\n",
      "     33            1.0000        \u001b[32m0.3299\u001b[0m       0.5417            0.5417        1.0503  0.0005  0.1291\n",
      "     34            1.0000        \u001b[32m0.0807\u001b[0m       0.5208            0.5208        1.1070  0.0006  0.1441\n",
      "     35            1.0000        \u001b[32m0.0331\u001b[0m       0.5312            0.5312        1.1786  0.0006  0.1286\n",
      "     36            1.0000        \u001b[32m0.0306\u001b[0m       0.5104            0.5104        1.2654  0.0007  0.1308\n",
      "     37            1.0000        \u001b[32m0.0286\u001b[0m       0.5000            0.5000        1.3601  0.0007  0.1315\n",
      "     38            1.0000        \u001b[32m0.0150\u001b[0m       0.4792            0.4792        1.4501  0.0007  0.1289\n",
      "     39            1.0000        \u001b[32m0.0063\u001b[0m       0.4688            0.4688        1.5283  0.0007  0.1338\n",
      "     40            1.0000        \u001b[32m0.0045\u001b[0m       0.4583            0.4583        1.5885  0.0007  0.1313\n",
      "     41            1.0000        0.0128       0.4375            0.4375        1.6387  0.0007  0.1319\n",
      "     42            1.0000        0.0058       0.4167            0.4167        1.6730  0.0007  0.1562\n",
      "     43            1.0000        \u001b[32m0.0022\u001b[0m       0.4167            0.4167        1.6928  0.0006  0.1461\n",
      "     44            1.0000        0.0126       0.4167            0.4167        1.7028  0.0006  0.1384\n",
      "     45            1.0000        0.0036       0.4271            0.4271        1.7044  0.0005  0.1381\n",
      "     46            1.0000        0.0119       0.4062            0.4062        1.7024  0.0005  0.1288\n",
      "     47            1.0000        0.0051       0.4167            0.4167        1.6938  0.0004  0.1329\n",
      "     48            1.0000        0.0027       0.4271            0.4271        1.6812  0.0004  0.1306\n",
      "     49            1.0000        \u001b[32m0.0013\u001b[0m       0.4271            0.4271        1.6661  0.0003  0.1336\n",
      "     50            1.0000        0.0020       0.4167            0.4167        1.6498  0.0003  0.1362\n",
      "Fine tuning model for subject 8 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.8000        1.1712       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0196\u001b[0m  0.0004  0.1477\n",
      "     32            0.8000        \u001b[32m0.7033\u001b[0m       0.5625            0.5625        1.0715  0.0005  0.1331\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3072\u001b[0m       0.5104            0.5104        1.1437  0.0005  0.1376\n",
      "     34            1.0000        \u001b[32m0.1134\u001b[0m       0.4688            0.4688        1.2215  0.0006  0.1297\n",
      "     35            1.0000        \u001b[32m0.0865\u001b[0m       0.4375            0.4375        1.2837  0.0006  0.1448\n",
      "     36            1.0000        \u001b[32m0.0379\u001b[0m       0.4271            0.4271        1.3217  0.0007  0.1445\n",
      "     37            1.0000        \u001b[32m0.0077\u001b[0m       0.4062            0.4062        1.3433  0.0007  0.1330\n",
      "     38            1.0000        0.0184       0.3854            0.3854        1.3551  0.0007  0.1370\n",
      "     39            1.0000        0.0123       0.3542            0.3542        1.3648  0.0007  0.1348\n",
      "     40            1.0000        0.0124       0.3958            0.3958        1.3755  0.0007  0.1352\n",
      "     41            1.0000        \u001b[32m0.0052\u001b[0m       0.4062            0.4062        1.3886  0.0007  0.1381\n",
      "     42            1.0000        0.0139       0.4271            0.4271        1.4030  0.0007  0.1367\n",
      "     43            1.0000        \u001b[32m0.0032\u001b[0m       0.4062            0.4062        1.4176  0.0006  0.1290\n",
      "     44            1.0000        0.0049       0.3542            0.3542        1.4314  0.0006  0.1285\n",
      "     45            1.0000        0.0088       0.3646            0.3646        1.4436  0.0005  0.1280\n",
      "     46            1.0000        0.0033       0.3542            0.3542        1.4540  0.0005  0.1312\n",
      "     47            1.0000        0.0051       0.3646            0.3646        1.4627  0.0004  0.1335\n",
      "     48            1.0000        0.0049       0.3646            0.3646        1.4702  0.0004  0.1300\n",
      "     49            1.0000        \u001b[32m0.0030\u001b[0m       0.3646            0.3646        1.4768  0.0003  0.1351\n",
      "     50            1.0000        0.0055       0.3646            0.3646        1.4831  0.0003  0.1281\n",
      "Fine tuning model for subject 8 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        2.1089       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0221\u001b[0m  0.0004  0.1516\n",
      "     32            0.8000        1.4809       0.5729            0.5729        1.0862  0.0005  0.1375\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4991\u001b[0m       0.5104            0.5104        1.1985  0.0005  0.1257\n",
      "     34            1.0000        \u001b[32m0.3828\u001b[0m       0.4271            0.4271        1.3576  0.0006  0.1254\n",
      "     35            1.0000        \u001b[32m0.0458\u001b[0m       0.4062            0.4062        1.5461  0.0006  0.1330\n",
      "     36            1.0000        0.0859       0.3542            0.3542        1.7402  0.0007  0.1290\n",
      "     37            1.0000        \u001b[32m0.0184\u001b[0m       0.3333            0.3333        1.9127  0.0007  0.1299\n",
      "     38            1.0000        0.0184       0.3125            0.3125        2.0475  0.0007  0.1310\n",
      "     39            1.0000        0.0397       0.3021            0.3021        2.1474  0.0007  0.1306\n",
      "     40            1.0000        \u001b[32m0.0079\u001b[0m       0.3021            0.3021        2.2102  0.0007  0.1260\n",
      "     41            1.0000        0.0247       0.3125            0.3125        2.2462  0.0007  0.1350\n",
      "     42            1.0000        0.0286       0.3125            0.3125        2.2592  0.0007  0.1321\n",
      "     43            1.0000        0.0231       0.3021            0.3021        2.2527  0.0006  0.1309\n",
      "     44            1.0000        \u001b[32m0.0058\u001b[0m       0.3125            0.3125        2.2303  0.0006  0.1288\n",
      "     45            1.0000        \u001b[32m0.0038\u001b[0m       0.3021            0.3021        2.1968  0.0005  0.1330\n",
      "     46            1.0000        0.0089       0.2917            0.2917        2.1563  0.0005  0.1287\n",
      "     47            1.0000        0.0096       0.2917            0.2917        2.1115  0.0004  0.1321\n",
      "     48            1.0000        \u001b[32m0.0036\u001b[0m       0.2917            0.2917        2.0651  0.0004  0.1308\n",
      "     49            1.0000        0.0056       0.2708            0.2708        2.0194  0.0003  0.1257\n",
      "     50            1.0000        0.0055       0.2604            0.2604        1.9759  0.0003  0.1282\n",
      "Fine tuning model for subject 8 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m1.0000\u001b[0m        0.8797       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m1.0062\u001b[0m  0.0004  0.1294\n",
      "     32            1.0000        \u001b[32m0.4050\u001b[0m       0.5833            0.5833        1.0212  0.0005  0.1366\n",
      "     33            1.0000        \u001b[32m0.2036\u001b[0m       0.5833            0.5833        1.0351  0.0005  0.1306\n",
      "     34            1.0000        \u001b[32m0.0484\u001b[0m       0.5938            0.5938        1.0501  0.0006  0.1419\n",
      "     35            1.0000        0.0516       0.6042            0.6042        1.0689  0.0006  0.1339\n",
      "     36            1.0000        \u001b[32m0.0353\u001b[0m       0.5833            0.5833        1.0893  0.0007  0.1307\n",
      "     37            1.0000        \u001b[32m0.0097\u001b[0m       0.5521            0.5521        1.1103  0.0007  0.1374\n",
      "     38            1.0000        \u001b[32m0.0043\u001b[0m       0.5312            0.5312        1.1319  0.0007  0.1807\n",
      "     39            1.0000        0.0061       0.5312            0.5312        1.1543  0.0007  0.1366\n",
      "     40            1.0000        0.0078       0.5208            0.5208        1.1772  0.0007  0.1411\n",
      "     41            1.0000        \u001b[32m0.0030\u001b[0m       0.5000            0.5000        1.1990  0.0007  0.1299\n",
      "     42            1.0000        0.0033       0.4792            0.4792        1.2192  0.0007  0.1312\n",
      "     43            1.0000        \u001b[32m0.0009\u001b[0m       0.4792            0.4792        1.2373  0.0006  0.1313\n",
      "     44            1.0000        0.0046       0.4792            0.4792        1.2533  0.0006  0.1504\n",
      "     45            1.0000        0.0028       0.4688            0.4688        1.2670  0.0005  0.1704\n",
      "     46            1.0000        0.0017       0.4792            0.4792        1.2784  0.0005  0.1699\n",
      "     47            1.0000        0.0020       0.4792            0.4792        1.2877  0.0004  0.1711\n",
      "     48            1.0000        0.0031       0.4792            0.4792        1.2951  0.0004  0.1713\n",
      "     49            1.0000        0.0028       0.4896            0.4896        1.3008  0.0003  0.1772\n",
      "     50            1.0000        0.0027       0.4896            0.4896        1.3052  0.0003  0.1740\n",
      "Fine tuning model for subject 8 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5696\u001b[0m       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m1.0133\u001b[0m  0.0004  0.1694\n",
      "     32            1.0000        \u001b[32m0.2889\u001b[0m       0.5938            0.5938        1.0826  0.0005  0.1763\n",
      "     33            1.0000        \u001b[32m0.0872\u001b[0m       0.5208            0.5208        1.2021  0.0005  0.1707\n",
      "     34            1.0000        \u001b[32m0.0756\u001b[0m       0.4896            0.4896        1.3427  0.0006  0.1686\n",
      "     35            1.0000        \u001b[32m0.0548\u001b[0m       0.4792            0.4792        1.4845  0.0006  0.1843\n",
      "     36            1.0000        \u001b[32m0.0348\u001b[0m       0.4688            0.4688        1.6137  0.0007  0.1906\n",
      "     37            1.0000        \u001b[32m0.0196\u001b[0m       0.4583            0.4583        1.7163  0.0007  0.1995\n",
      "     38            1.0000        \u001b[32m0.0130\u001b[0m       0.4062            0.4062        1.7875  0.0007  0.1434\n",
      "     39            1.0000        \u001b[32m0.0106\u001b[0m       0.3854            0.3854        1.8303  0.0007  0.1567\n",
      "     40            1.0000        0.0134       0.3854            0.3854        1.8510  0.0007  0.1350\n",
      "     41            1.0000        \u001b[32m0.0040\u001b[0m       0.3854            0.3854        1.8554  0.0007  0.1293\n",
      "     42            1.0000        0.0089       0.3854            0.3854        1.8483  0.0007  0.1304\n",
      "     43            1.0000        0.0128       0.3854            0.3854        1.8335  0.0006  0.1356\n",
      "     44            1.0000        0.0043       0.4062            0.4062        1.8165  0.0006  0.1330\n",
      "     45            1.0000        0.0059       0.4167            0.4167        1.7998  0.0005  0.1321\n",
      "     46            1.0000        0.0083       0.4167            0.4167        1.7853  0.0005  0.1306\n",
      "     47            1.0000        \u001b[32m0.0035\u001b[0m       0.4167            0.4167        1.7721  0.0004  0.1297\n",
      "     48            1.0000        0.0078       0.4167            0.4167        1.7603  0.0004  0.1366\n",
      "     49            1.0000        \u001b[32m0.0019\u001b[0m       0.4167            0.4167        1.7501  0.0003  0.1387\n",
      "     50            1.0000        0.0023       0.4062            0.4062        1.7414  0.0003  0.1424\n",
      "Fine tuning model for subject 8 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.8000        0.9737       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m1.0139\u001b[0m  0.0004  0.1305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3801\u001b[0m       0.5833            0.5833        1.0451  0.0005  0.1276\n",
      "     33            1.0000        \u001b[32m0.2180\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.0957  0.0005  0.1293\n",
      "     34            1.0000        \u001b[32m0.0699\u001b[0m       0.5521            0.5521        1.1570  0.0006  0.1869\n",
      "     35            1.0000        0.1053       0.5000            0.5000        1.2013  0.0006  0.1728\n",
      "     36            1.0000        \u001b[32m0.0162\u001b[0m       0.5000            0.5000        1.2451  0.0007  0.1563\n",
      "     37            1.0000        0.0384       0.4583            0.4583        1.2864  0.0007  0.1505\n",
      "     38            1.0000        0.0282       0.3958            0.3958        1.3323  0.0007  0.1502\n",
      "     39            1.0000        \u001b[32m0.0099\u001b[0m       0.3854            0.3854        1.3796  0.0007  0.1435\n",
      "     40            1.0000        \u001b[32m0.0087\u001b[0m       0.3646            0.3646        1.4249  0.0007  0.1486\n",
      "     41            1.0000        0.0238       0.3438            0.3438        1.4650  0.0007  0.1428\n",
      "     42            1.0000        0.0128       0.3438            0.3438        1.4984  0.0007  0.1445\n",
      "     43            1.0000        \u001b[32m0.0062\u001b[0m       0.3438            0.3438        1.5255  0.0006  0.1424\n",
      "     44            1.0000        0.0085       0.3438            0.3438        1.5438  0.0006  0.1319\n",
      "     45            1.0000        \u001b[32m0.0040\u001b[0m       0.3542            0.3542        1.5560  0.0005  0.1306\n",
      "     46            1.0000        0.0088       0.3542            0.3542        1.5624  0.0005  0.1326\n",
      "     47            1.0000        \u001b[32m0.0014\u001b[0m       0.3750            0.3750        1.5652  0.0004  0.1414\n",
      "     48            1.0000        0.0019       0.4062            0.4062        1.5657  0.0004  0.2395\n",
      "     49            1.0000        0.0046       0.4062            0.4062        1.5649  0.0003  0.1780\n",
      "     50            1.0000        0.0019       0.4062            0.4062        1.5639  0.0003  0.5910\n",
      "Fine tuning model for subject 8 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m1.0000\u001b[0m        0.7851       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0180\u001b[0m  0.0004  0.2247\n",
      "     32            1.0000        \u001b[32m0.6038\u001b[0m       0.5833            0.5833        1.0755  0.0005  0.2081\n",
      "     33            1.0000        \u001b[32m0.1430\u001b[0m       0.5625            0.5625        1.1730  0.0005  0.1708\n",
      "     34            1.0000        \u001b[32m0.0770\u001b[0m       0.5208            0.5208        1.2739  0.0006  0.1496\n",
      "     35            1.0000        \u001b[32m0.0303\u001b[0m       0.4896            0.4896        1.3543  0.0006  0.1783\n",
      "     36            1.0000        \u001b[32m0.0150\u001b[0m       0.4688            0.4688        1.4024  0.0007  0.1291\n",
      "     37            1.0000        \u001b[32m0.0143\u001b[0m       0.4688            0.4688        1.4187  0.0007  0.1507\n",
      "     38            1.0000        \u001b[32m0.0070\u001b[0m       0.4792            0.4792        1.4124  0.0007  0.1675\n",
      "     39            1.0000        0.0122       0.4896            0.4896        1.3906  0.0007  0.1863\n",
      "     40            1.0000        0.0082       0.5208            0.5208        1.3663  0.0007  0.1316\n",
      "     41            1.0000        0.0079       0.5312            0.5312        1.3435  0.0007  0.1275\n",
      "     42            1.0000        \u001b[32m0.0048\u001b[0m       0.5312            0.5312        1.3243  0.0007  0.1204\n",
      "     43            1.0000        0.0156       0.5417            0.5417        1.3084  0.0006  0.1207\n",
      "     44            1.0000        0.0098       0.5417            0.5417        1.3001  0.0006  0.1281\n",
      "     45            1.0000        0.0055       0.5417            0.5417        1.2950  0.0005  0.1278\n",
      "     46            1.0000        0.0057       0.5312            0.5312        1.2925  0.0005  0.1338\n",
      "     47            1.0000        \u001b[32m0.0025\u001b[0m       0.5208            0.5208        1.2921  0.0004  0.1288\n",
      "     48            1.0000        \u001b[32m0.0018\u001b[0m       0.5104            0.5104        1.2930  0.0004  0.1541\n",
      "     49            1.0000        \u001b[32m0.0012\u001b[0m       0.4896            0.4896        1.2950  0.0003  0.1414\n",
      "     50            1.0000        0.0090       0.4896            0.4896        1.2975  0.0003  0.1266\n",
      "Fine tuning model for subject 8 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.8000        1.1003       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0019\u001b[0m  0.0004  0.1329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        \u001b[32m0.7076\u001b[0m       0.5938            0.5938        1.0190  0.0005  0.1349\n",
      "     33            1.0000        \u001b[32m0.1503\u001b[0m       0.5938            0.5938        1.0695  0.0005  0.1299\n",
      "     34            1.0000        \u001b[32m0.1274\u001b[0m       0.5521            0.5521        1.1622  0.0006  0.1340\n",
      "     35            1.0000        \u001b[32m0.0409\u001b[0m       0.4688            0.4688        1.2818  0.0006  0.1312\n",
      "     36            1.0000        \u001b[32m0.0286\u001b[0m       0.4271            0.4271        1.4023  0.0007  0.1355\n",
      "     37            1.0000        0.0803       0.4062            0.4062        1.5094  0.0007  0.1577\n",
      "     38            1.0000        0.0307       0.3750            0.3750        1.5954  0.0007  0.1403\n",
      "     39            1.0000        \u001b[32m0.0266\u001b[0m       0.3646            0.3646        1.6605  0.0007  0.1589\n",
      "     40            1.0000        0.0309       0.3542            0.3542        1.7025  0.0007  0.1339\n",
      "     41            1.0000        \u001b[32m0.0084\u001b[0m       0.3438            0.3438        1.7249  0.0007  0.1247\n",
      "     42            1.0000        0.0241       0.3229            0.3229        1.7373  0.0007  0.1334\n",
      "     43            1.0000        0.0139       0.3333            0.3333        1.7395  0.0006  0.1611\n",
      "     44            1.0000        0.0151       0.3333            0.3333        1.7335  0.0006  0.1386\n",
      "     45            1.0000        0.0164       0.3438            0.3438        1.7200  0.0005  0.1340\n",
      "     46            1.0000        \u001b[32m0.0048\u001b[0m       0.3542            0.3542        1.7026  0.0005  0.1399\n",
      "     47            1.0000        \u001b[32m0.0035\u001b[0m       0.3542            0.3542        1.6836  0.0004  0.1571\n",
      "     48            1.0000        \u001b[32m0.0024\u001b[0m       0.3333            0.3333        1.6648  0.0004  0.1309\n",
      "     49            1.0000        0.0026       0.3438            0.3438        1.6475  0.0003  0.1339\n",
      "     50            1.0000        0.0072       0.3438            0.3438        1.6327  0.0003  0.1493\n",
      "Fine tuning model for subject 8 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.8000        \u001b[32m0.7155\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m1.0055\u001b[0m  0.0004  0.1331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        0.7926       0.5625            0.5625        1.0281  0.0005  0.1346\n",
      "     33            1.0000        \u001b[32m0.3410\u001b[0m       0.5729            0.5729        1.0786  0.0005  0.1552\n",
      "     34            1.0000        \u001b[32m0.1370\u001b[0m       0.5000            0.5000        1.1575  0.0006  0.1267\n",
      "     35            1.0000        \u001b[32m0.0459\u001b[0m       0.4375            0.4375        1.2384  0.0006  0.1281\n",
      "     36            1.0000        \u001b[32m0.0347\u001b[0m       0.4062            0.4062        1.3164  0.0007  0.1331\n",
      "     37            1.0000        \u001b[32m0.0085\u001b[0m       0.3854            0.3854        1.3805  0.0007  0.1282\n",
      "     38            1.0000        0.0343       0.3854            0.3854        1.4360  0.0007  0.1231\n",
      "     39            1.0000        0.0255       0.3542            0.3542        1.4793  0.0007  0.1525\n",
      "     40            1.0000        \u001b[32m0.0084\u001b[0m       0.3750            0.3750        1.5089  0.0007  0.1578\n",
      "     41            1.0000        \u001b[32m0.0048\u001b[0m       0.3646            0.3646        1.5292  0.0007  0.1606\n",
      "     42            1.0000        0.0050       0.3438            0.3438        1.5436  0.0007  0.1932\n",
      "     43            1.0000        0.0082       0.3542            0.3542        1.5560  0.0006  0.1729\n",
      "     44            1.0000        \u001b[32m0.0034\u001b[0m       0.3438            0.3438        1.5670  0.0006  0.1620\n",
      "     45            1.0000        0.0038       0.3750            0.3750        1.5777  0.0005  0.1587\n",
      "     46            1.0000        0.0038       0.3750            0.3750        1.5886  0.0005  0.1578\n",
      "     47            1.0000        \u001b[32m0.0016\u001b[0m       0.3646            0.3646        1.5998  0.0004  0.1642\n",
      "     48            1.0000        0.0040       0.3542            0.3542        1.6118  0.0004  0.1667\n",
      "     49            1.0000        0.0026       0.3438            0.3438        1.6237  0.0003  0.1994\n",
      "     50            1.0000        0.0031       0.3542            0.3542        1.6355  0.0003  0.1650\n",
      "Fine tuning model for subject 8 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.8000        0.7998       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        \u001b[94m0.9969\u001b[0m  0.0004  0.1168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        0.7423       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.0218  0.0005  0.1807\n",
      "     33            1.0000        \u001b[32m0.5095\u001b[0m       0.5312            0.5312        1.1045  0.0005  0.1686\n",
      "     34            0.9000        \u001b[32m0.2206\u001b[0m       0.5208            0.5208        1.2287  0.0006  0.1272\n",
      "     35            0.9000        \u001b[32m0.1023\u001b[0m       0.4896            0.4896        1.3404  0.0006  0.1144\n",
      "     36            0.9000        0.2053       0.5104            0.5104        1.4298  0.0007  0.1206\n",
      "     37            1.0000        0.1263       0.5104            0.5104        1.5047  0.0007  0.1121\n",
      "     38            1.0000        \u001b[32m0.0443\u001b[0m       0.5104            0.5104        1.5708  0.0007  0.1478\n",
      "     39            1.0000        0.0578       0.5000            0.5000        1.6306  0.0007  0.1262\n",
      "     40            1.0000        0.0489       0.4792            0.4792        1.6806  0.0007  0.1154\n",
      "     41            1.0000        \u001b[32m0.0243\u001b[0m       0.4792            0.4792        1.7177  0.0007  0.1219\n",
      "     42            1.0000        0.0346       0.4792            0.4792        1.7364  0.0007  0.1012\n",
      "     43            1.0000        \u001b[32m0.0203\u001b[0m       0.4792            0.4792        1.7368  0.0006  0.0973\n",
      "     44            1.0000        \u001b[32m0.0148\u001b[0m       0.4792            0.4792        1.7239  0.0006  0.1191\n",
      "     45            1.0000        \u001b[32m0.0097\u001b[0m       0.4896            0.4896        1.7016  0.0005  0.1182\n",
      "     46            1.0000        0.0150       0.4792            0.4792        1.6683  0.0005  0.1133\n",
      "     47            1.0000        0.0160       0.4896            0.4896        1.6328  0.0004  0.0964\n",
      "     48            1.0000        0.0190       0.4896            0.4896        1.5970  0.0004  0.1209\n",
      "     49            1.0000        0.0278       0.4792            0.4792        1.5605  0.0003  0.1180\n",
      "     50            1.0000        0.0219       0.4792            0.4792        1.5266  0.0003  0.1232\n",
      "Fine tuning model for subject 8 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.3215       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m1.0020\u001b[0m  0.0004  0.1187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        1.2461       0.6146            0.6146        1.0160  0.0005  0.1308\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6805\u001b[0m       0.5729            0.5729        1.0508  0.0005  0.1167\n",
      "     34            1.0000        \u001b[32m0.4610\u001b[0m       0.5208            0.5208        1.1190  0.0006  0.1198\n",
      "     35            1.0000        \u001b[32m0.3197\u001b[0m       0.4583            0.4583        1.2188  0.0006  0.1279\n",
      "     36            1.0000        \u001b[32m0.1153\u001b[0m       0.4271            0.4271        1.3467  0.0007  0.1217\n",
      "     37            1.0000        \u001b[32m0.0544\u001b[0m       0.4062            0.4062        1.4941  0.0007  0.0985\n",
      "     38            1.0000        \u001b[32m0.0398\u001b[0m       0.3854            0.3854        1.6477  0.0007  0.1159\n",
      "     39            1.0000        \u001b[32m0.0336\u001b[0m       0.3750            0.3750        1.7895  0.0007  0.1289\n",
      "     40            1.0000        \u001b[32m0.0312\u001b[0m       0.3646            0.3646        1.9089  0.0007  0.1174\n",
      "     41            1.0000        \u001b[32m0.0125\u001b[0m       0.3646            0.3646        2.0026  0.0007  0.0992\n",
      "     42            1.0000        \u001b[32m0.0108\u001b[0m       0.3646            0.3646        2.0692  0.0007  0.1150\n",
      "     43            1.0000        0.0138       0.3646            0.3646        2.1090  0.0006  0.1197\n",
      "     44            1.0000        0.0181       0.3646            0.3646        2.1260  0.0006  0.1337\n",
      "     45            1.0000        0.0130       0.3854            0.3854        2.1240  0.0005  0.1384\n",
      "     46            1.0000        \u001b[32m0.0097\u001b[0m       0.3854            0.3854        2.1068  0.0005  0.1069\n",
      "     47            1.0000        \u001b[32m0.0064\u001b[0m       0.3854            0.3854        2.0782  0.0004  0.1160\n",
      "     48            1.0000        \u001b[32m0.0031\u001b[0m       0.3854            0.3854        2.0415  0.0004  0.1320\n",
      "     49            1.0000        0.0056       0.3958            0.3958        1.9996  0.0003  0.1479\n",
      "     50            1.0000        0.0155       0.3958            0.3958        1.9568  0.0003  0.1145\n",
      "Fine tuning model for subject 8 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        1.7091       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        \u001b[94m0.9969\u001b[0m  0.0004  0.1097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.9000\u001b[0m        1.0406       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.0148  0.0005  0.1138\n",
      "     33            0.9000        \u001b[32m0.6291\u001b[0m       0.5521            0.5521        1.0794  0.0005  0.1139\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2144\u001b[0m       0.5208            0.5208        1.1599  0.0006  0.1037\n",
      "     35            1.0000        0.3864       0.5104            0.5104        1.2180  0.0006  0.1185\n",
      "     36            1.0000        \u001b[32m0.1019\u001b[0m       0.5104            0.5104        1.2592  0.0007  0.1028\n",
      "     37            1.0000        \u001b[32m0.0669\u001b[0m       0.4896            0.4896        1.2871  0.0007  0.0965\n",
      "     38            1.0000        0.0740       0.4688            0.4688        1.3107  0.0007  0.1211\n",
      "     39            1.0000        \u001b[32m0.0407\u001b[0m       0.4583            0.4583        1.3311  0.0007  0.1007\n",
      "     40            1.0000        0.0671       0.4583            0.4583        1.3496  0.0007  0.1237\n",
      "     41            1.0000        \u001b[32m0.0390\u001b[0m       0.4792            0.4792        1.3677  0.0007  0.1160\n",
      "     42            1.0000        \u001b[32m0.0214\u001b[0m       0.4583            0.4583        1.3842  0.0007  0.1460\n",
      "     43            1.0000        0.0282       0.4583            0.4583        1.3993  0.0006  0.1139\n",
      "     44            1.0000        \u001b[32m0.0201\u001b[0m       0.4479            0.4479        1.4124  0.0006  0.1199\n",
      "     45            1.0000        \u001b[32m0.0122\u001b[0m       0.4375            0.4375        1.4243  0.0005  0.1032\n",
      "     46            1.0000        \u001b[32m0.0115\u001b[0m       0.4479            0.4479        1.4361  0.0005  0.1083\n",
      "     47            1.0000        \u001b[32m0.0060\u001b[0m       0.4271            0.4271        1.4484  0.0004  0.1176\n",
      "     48            1.0000        0.0113       0.4375            0.4375        1.4627  0.0004  0.1183\n",
      "     49            1.0000        0.0104       0.4375            0.4375        1.4796  0.0003  0.1245\n",
      "     50            1.0000        0.0096       0.4479            0.4479        1.4991  0.0003  0.1304\n",
      "Fine tuning model for subject 8 with 10 = 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m1.0000\u001b[0m        0.8867       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        \u001b[94m1.0021\u001b[0m  0.0004  0.0996\n",
      "     32            1.0000        \u001b[32m0.6920\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.0247  0.0005  0.1442\n",
      "     33            1.0000        \u001b[32m0.2822\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        1.0619  0.0005  0.1250\n",
      "     34            1.0000        \u001b[32m0.1334\u001b[0m       0.6146            0.6146        1.0982  0.0006  0.1162\n",
      "     35            1.0000        0.1490       0.6042            0.6042        1.1489  0.0006  0.1014\n",
      "     36            1.0000        \u001b[32m0.0543\u001b[0m       0.6042            0.6042        1.2126  0.0007  0.0957\n",
      "     37            1.0000        \u001b[32m0.0424\u001b[0m       0.5833            0.5833        1.2887  0.0007  0.1207\n",
      "     38            1.0000        \u001b[32m0.0411\u001b[0m       0.5417            0.5417        1.3671  0.0007  0.1531\n",
      "     39            1.0000        \u001b[32m0.0146\u001b[0m       0.5104            0.5104        1.4400  0.0007  0.1296\n",
      "     40            1.0000        \u001b[32m0.0114\u001b[0m       0.5104            0.5104        1.5020  0.0007  0.1207\n",
      "     41            1.0000        0.0131       0.5000            0.5000        1.5487  0.0007  0.1153\n",
      "     42            1.0000        \u001b[32m0.0103\u001b[0m       0.4896            0.4896        1.5789  0.0007  0.1174\n",
      "     43            1.0000        \u001b[32m0.0056\u001b[0m       0.4792            0.4792        1.5928  0.0006  0.1215\n",
      "     44            1.0000        0.0083       0.4792            0.4792        1.5924  0.0006  0.1123\n",
      "     45            1.0000        0.0186       0.5000            0.5000        1.5786  0.0005  0.1065\n",
      "     46            1.0000        0.0095       0.4896            0.4896        1.5578  0.0005  0.1197\n",
      "     47            1.0000        \u001b[32m0.0045\u001b[0m       0.4792            0.4792        1.5330  0.0004  0.1173\n",
      "     48            1.0000        0.0068       0.4896            0.4896        1.5061  0.0004  0.1168\n",
      "     49            1.0000        \u001b[32m0.0030\u001b[0m       0.4896            0.4896        1.4795  0.0003  0.1331\n",
      "     50            1.0000        0.0060       0.5000            0.5000        1.4544  0.0003  0.1414\n",
      "Fine tuning model for subject 8 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.1330       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m1.0076\u001b[0m  0.0004  0.1337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        0.8709       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        1.0297  0.0005  0.1152\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4843\u001b[0m       0.5938            0.5938        1.0775  0.0005  0.1168\n",
      "     34            1.0000        \u001b[32m0.4093\u001b[0m       0.5417            0.5417        1.1431  0.0006  0.1184\n",
      "     35            1.0000        \u001b[32m0.1136\u001b[0m       0.5000            0.5000        1.2117  0.0006  0.1214\n",
      "     36            1.0000        \u001b[32m0.0929\u001b[0m       0.4896            0.4896        1.2908  0.0007  0.1041\n",
      "     37            1.0000        0.0998       0.4583            0.4583        1.3643  0.0007  0.1014\n",
      "     38            1.0000        \u001b[32m0.0737\u001b[0m       0.4479            0.4479        1.4379  0.0007  0.1198\n",
      "     39            1.0000        \u001b[32m0.0236\u001b[0m       0.4375            0.4375        1.5079  0.0007  0.1268\n",
      "     40            1.0000        \u001b[32m0.0232\u001b[0m       0.4375            0.4375        1.5723  0.0007  0.1289\n",
      "     41            1.0000        0.0276       0.4271            0.4271        1.6303  0.0007  0.1154\n",
      "     42            1.0000        0.0281       0.4271            0.4271        1.6812  0.0007  0.1291\n",
      "     43            1.0000        \u001b[32m0.0144\u001b[0m       0.4271            0.4271        1.7203  0.0006  0.1035\n",
      "     44            1.0000        0.0207       0.4062            0.4062        1.7473  0.0006  0.1226\n",
      "     45            1.0000        0.0149       0.4271            0.4271        1.7624  0.0005  0.1340\n",
      "     46            1.0000        0.0148       0.4271            0.4271        1.7662  0.0005  0.1173\n",
      "     47            1.0000        0.0194       0.4167            0.4167        1.7620  0.0004  0.1150\n",
      "     48            1.0000        0.0167       0.4167            0.4167        1.7520  0.0004  0.1188\n",
      "     49            1.0000        0.0229       0.4271            0.4271        1.7382  0.0003  0.1171\n",
      "     50            1.0000        0.0146       0.4375            0.4375        1.7222  0.0003  0.1040\n",
      "Fine tuning model for subject 8 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        1.1883       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m1.0118\u001b[0m  0.0004  0.1091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        \u001b[32m0.7090\u001b[0m       0.5521            0.5521        1.0331  0.0005  0.1499\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6007\u001b[0m       0.5625            0.5625        1.0621  0.0005  0.1334\n",
      "     34            1.0000        \u001b[32m0.5200\u001b[0m       0.5521            0.5521        1.1041  0.0006  0.1483\n",
      "     35            1.0000        \u001b[32m0.1879\u001b[0m       0.5104            0.5104        1.1554  0.0006  0.1466\n",
      "     36            1.0000        \u001b[32m0.1150\u001b[0m       0.5104            0.5104        1.2120  0.0007  0.1333\n",
      "     37            1.0000        \u001b[32m0.0552\u001b[0m       0.5000            0.5000        1.2647  0.0007  0.1418\n",
      "     38            1.0000        \u001b[32m0.0537\u001b[0m       0.4896            0.4896        1.3155  0.0007  0.1657\n",
      "     39            1.0000        \u001b[32m0.0330\u001b[0m       0.4479            0.4479        1.3662  0.0007  0.1364\n",
      "     40            1.0000        \u001b[32m0.0172\u001b[0m       0.4375            0.4375        1.4152  0.0007  0.1378\n",
      "     41            1.0000        0.0309       0.4167            0.4167        1.4627  0.0007  0.1485\n",
      "     42            1.0000        0.0305       0.4167            0.4167        1.5071  0.0007  0.1834\n",
      "     43            1.0000        0.0203       0.3854            0.3854        1.5459  0.0006  0.1985\n",
      "     44            1.0000        \u001b[32m0.0172\u001b[0m       0.3854            0.3854        1.5748  0.0006  0.2151\n",
      "     45            1.0000        \u001b[32m0.0094\u001b[0m       0.3854            0.3854        1.5952  0.0005  0.1290\n",
      "     46            1.0000        0.0169       0.3958            0.3958        1.6060  0.0005  0.1378\n",
      "     47            1.0000        \u001b[32m0.0089\u001b[0m       0.3854            0.3854        1.6092  0.0004  0.1521\n",
      "     48            1.0000        \u001b[32m0.0050\u001b[0m       0.3854            0.3854        1.6075  0.0004  0.1354\n",
      "     49            1.0000        0.0059       0.3854            0.3854        1.6020  0.0003  0.1822\n",
      "     50            1.0000        0.0109       0.3854            0.3854        1.5939  0.0003  0.1190\n",
      "Fine tuning model for subject 8 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.1132       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0020\u001b[0m  0.0004  0.0984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.1507       0.5938            0.5938        \u001b[94m0.9987\u001b[0m  0.0005  0.1176\n",
      "     33            \u001b[36m0.9000\u001b[0m        0.7540       0.6042            0.6042        1.0018  0.0005  0.1164\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4678\u001b[0m       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        1.0229  0.0006  0.1121\n",
      "     35            1.0000        \u001b[32m0.2389\u001b[0m       0.5938            0.5938        1.0674  0.0006  0.0999\n",
      "     36            1.0000        \u001b[32m0.0760\u001b[0m       0.5521            0.5521        1.1316  0.0007  0.1071\n",
      "     37            1.0000        0.1234       0.5208            0.5208        1.2044  0.0007  0.1169\n",
      "     38            1.0000        0.0816       0.4792            0.4792        1.2836  0.0007  0.1198\n",
      "     39            1.0000        0.1253       0.4375            0.4375        1.3695  0.0007  0.1141\n",
      "     40            1.0000        \u001b[32m0.0323\u001b[0m       0.4062            0.4062        1.4505  0.0007  0.1030\n",
      "     41            1.0000        \u001b[32m0.0189\u001b[0m       0.3958            0.3958        1.5214  0.0007  0.1013\n",
      "     42            1.0000        0.0287       0.3958            0.3958        1.5760  0.0007  0.1199\n",
      "     43            1.0000        \u001b[32m0.0095\u001b[0m       0.3958            0.3958        1.6128  0.0006  0.1189\n",
      "     44            1.0000        0.0097       0.3958            0.3958        1.6317  0.0006  0.1023\n",
      "     45            1.0000        \u001b[32m0.0092\u001b[0m       0.3958            0.3958        1.6352  0.0005  0.1153\n",
      "     46            1.0000        \u001b[32m0.0073\u001b[0m       0.4062            0.4062        1.6269  0.0005  0.1036\n",
      "     47            1.0000        \u001b[32m0.0061\u001b[0m       0.4062            0.4062        1.6114  0.0004  0.1223\n",
      "     48            1.0000        \u001b[32m0.0056\u001b[0m       0.4167            0.4167        1.5926  0.0004  0.1003\n",
      "     49            1.0000        0.0079       0.4167            0.4167        1.5733  0.0003  0.1150\n",
      "     50            1.0000        \u001b[32m0.0036\u001b[0m       0.4479            0.4479        1.5561  0.0003  0.1183\n",
      "Fine tuning model for subject 8 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.9000\u001b[0m        1.0643       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9985\u001b[0m  0.0004  0.1015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.9000        0.9436       0.6042            0.6042        1.0031  0.0005  0.1432\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4486\u001b[0m       0.5833            0.5833        1.0203  0.0005  0.1437\n",
      "     34            1.0000        \u001b[32m0.1921\u001b[0m       0.5833            0.5833        1.0521  0.0006  0.1156\n",
      "     35            1.0000        \u001b[32m0.1266\u001b[0m       0.5521            0.5521        1.1000  0.0006  0.1230\n",
      "     36            1.0000        \u001b[32m0.0676\u001b[0m       0.5104            0.5104        1.1613  0.0007  0.1000\n",
      "     37            1.0000        \u001b[32m0.0566\u001b[0m       0.5208            0.5208        1.2284  0.0007  0.1539\n",
      "     38            1.0000        \u001b[32m0.0368\u001b[0m       0.5000            0.5000        1.2941  0.0007  0.1546\n",
      "     39            1.0000        \u001b[32m0.0354\u001b[0m       0.4792            0.4792        1.3517  0.0007  0.1543\n",
      "     40            1.0000        0.0559       0.4583            0.4583        1.3925  0.0007  0.1665\n",
      "     41            1.0000        \u001b[32m0.0224\u001b[0m       0.4583            0.4583        1.4223  0.0007  0.1701\n",
      "     42            1.0000        \u001b[32m0.0220\u001b[0m       0.4479            0.4479        1.4415  0.0007  0.1688\n",
      "     43            1.0000        0.0255       0.4479            0.4479        1.4516  0.0006  0.1289\n",
      "     44            1.0000        \u001b[32m0.0155\u001b[0m       0.4479            0.4479        1.4550  0.0006  0.1296\n",
      "     45            1.0000        0.0193       0.4479            0.4479        1.4519  0.0005  0.1049\n",
      "     46            1.0000        \u001b[32m0.0103\u001b[0m       0.4375            0.4375        1.4440  0.0005  0.1172\n",
      "     47            1.0000        \u001b[32m0.0094\u001b[0m       0.4688            0.4688        1.4336  0.0004  0.1207\n",
      "     48            1.0000        0.0182       0.4792            0.4792        1.4215  0.0004  0.1239\n",
      "     49            1.0000        \u001b[32m0.0082\u001b[0m       0.4896            0.4896        1.4085  0.0003  0.0998\n",
      "     50            1.0000        0.0096       0.4792            0.4792        1.3955  0.0003  0.1304\n",
      "Fine tuning model for subject 8 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.0007       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m1.0054\u001b[0m  0.0004  0.1179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        1.1920       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.0400  0.0005  0.1246\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4657\u001b[0m       0.5521            0.5521        1.1411  0.0005  0.1093\n",
      "     34            1.0000        \u001b[32m0.3966\u001b[0m       0.4896            0.4896        1.3139  0.0006  0.1030\n",
      "     35            1.0000        \u001b[32m0.1342\u001b[0m       0.4375            0.4375        1.5123  0.0006  0.1220\n",
      "     36            1.0000        \u001b[32m0.0700\u001b[0m       0.3646            0.3646        1.6720  0.0007  0.1011\n",
      "     37            1.0000        \u001b[32m0.0620\u001b[0m       0.3229            0.3229        1.7699  0.0007  0.1144\n",
      "     38            1.0000        \u001b[32m0.0280\u001b[0m       0.2917            0.2917        1.8122  0.0007  0.1033\n",
      "     39            1.0000        0.0336       0.2812            0.2812        1.8181  0.0007  0.1206\n",
      "     40            1.0000        0.0289       0.2917            0.2917        1.8044  0.0007  0.1167\n",
      "     41            1.0000        0.0639       0.3021            0.3021        1.7867  0.0007  0.1023\n",
      "     42            1.0000        0.0675       0.3021            0.3021        1.7690  0.0007  0.1469\n",
      "     43            1.0000        0.0606       0.3125            0.3125        1.7543  0.0006  0.1247\n",
      "     44            1.0000        0.0315       0.3021            0.3021        1.7444  0.0006  0.1864\n",
      "     45            1.0000        0.0289       0.3125            0.3125        1.7391  0.0005  0.1287\n",
      "     46            1.0000        \u001b[32m0.0231\u001b[0m       0.3438            0.3438        1.7372  0.0005  0.1141\n",
      "     47            1.0000        0.0289       0.3542            0.3542        1.7362  0.0004  0.0964\n",
      "     48            1.0000        \u001b[32m0.0195\u001b[0m       0.3646            0.3646        1.7361  0.0004  0.1179\n",
      "     49            1.0000        \u001b[32m0.0159\u001b[0m       0.3750            0.3750        1.7361  0.0003  0.1192\n",
      "     50            1.0000        \u001b[32m0.0145\u001b[0m       0.3750            0.3750        1.7367  0.0003  0.1131\n",
      "Fine tuning model for subject 8 with 10 = 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.3899       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m1.0074\u001b[0m  0.0004  0.1459\n",
      "     32            0.8000        1.0778       0.5729            0.5729        1.0657  0.0005  0.1297\n",
      "     33            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6350\u001b[0m       0.5312            0.5312        1.1915  0.0005  0.1405\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3969\u001b[0m       0.4688            0.4688        1.3646  0.0006  0.1384\n",
      "     35            1.0000        \u001b[32m0.1371\u001b[0m       0.4167            0.4167        1.5286  0.0006  0.1355\n",
      "     36            1.0000        \u001b[32m0.0757\u001b[0m       0.3854            0.3854        1.6605  0.0007  0.1426\n",
      "     37            1.0000        0.1235       0.3750            0.3750        1.7712  0.0007  0.1295\n",
      "     38            1.0000        0.0818       0.3646            0.3646        1.8550  0.0007  0.1063\n",
      "     39            1.0000        0.0802       0.3542            0.3542        1.9247  0.0007  0.1034\n",
      "     40            1.0000        \u001b[32m0.0262\u001b[0m       0.3438            0.3438        1.9775  0.0007  0.1370\n",
      "     41            1.0000        \u001b[32m0.0189\u001b[0m       0.3333            0.3333        2.0153  0.0007  0.1215\n",
      "     42            1.0000        0.0385       0.3438            0.3438        2.0429  0.0007  0.1226\n",
      "     43            1.0000        0.0417       0.3229            0.3229        2.0611  0.0006  0.1294\n",
      "     44            1.0000        \u001b[32m0.0176\u001b[0m       0.3438            0.3438        2.0713  0.0006  0.1272\n",
      "     45            1.0000        \u001b[32m0.0156\u001b[0m       0.3438            0.3438        2.0738  0.0005  0.1046\n",
      "     46            1.0000        \u001b[32m0.0077\u001b[0m       0.3646            0.3646        2.0696  0.0005  0.1203\n",
      "     47            1.0000        0.0220       0.3542            0.3542        2.0595  0.0004  0.1216\n",
      "     48            1.0000        0.0145       0.3542            0.3542        2.0450  0.0004  0.1032\n",
      "     49            1.0000        0.0162       0.3646            0.3646        2.0275  0.0003  0.1384\n",
      "     50            1.0000        0.0212       0.3646            0.3646        2.0076  0.0003  0.1254\n",
      "Fine tuning model for subject 8 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.8000        0.9311       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9953\u001b[0m  0.0004  0.1121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.9333\u001b[0m        \u001b[32m0.6357\u001b[0m       0.5938            0.5938        0.9993  0.0005  0.1337\n",
      "     33            0.9333        \u001b[32m0.4988\u001b[0m       0.6042            0.6042        1.0240  0.0005  0.1145\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2293\u001b[0m       0.5938            0.5938        1.0752  0.0006  0.1120\n",
      "     35            1.0000        0.2521       0.5000            0.5000        1.1489  0.0006  0.1208\n",
      "     36            1.0000        \u001b[32m0.1737\u001b[0m       0.4792            0.4792        1.2355  0.0007  0.0968\n",
      "     37            1.0000        \u001b[32m0.0803\u001b[0m       0.4271            0.4271        1.3160  0.0007  0.1147\n",
      "     38            1.0000        \u001b[32m0.0550\u001b[0m       0.4167            0.4167        1.3864  0.0007  0.1112\n",
      "     39            1.0000        0.0726       0.4375            0.4375        1.4461  0.0007  0.1196\n",
      "     40            1.0000        \u001b[32m0.0471\u001b[0m       0.4271            0.4271        1.4906  0.0007  0.0975\n",
      "     41            1.0000        0.0528       0.4271            0.4271        1.5172  0.0007  0.1159\n",
      "     42            1.0000        \u001b[32m0.0340\u001b[0m       0.4271            0.4271        1.5296  0.0007  0.1153\n",
      "     43            1.0000        0.0503       0.4167            0.4167        1.5290  0.0006  0.1226\n",
      "     44            1.0000        \u001b[32m0.0301\u001b[0m       0.4271            0.4271        1.5244  0.0006  0.1480\n",
      "     45            1.0000        \u001b[32m0.0177\u001b[0m       0.4375            0.4375        1.5177  0.0005  0.1456\n",
      "     46            1.0000        0.0243       0.4479            0.4479        1.5093  0.0005  0.1428\n",
      "     47            1.0000        \u001b[32m0.0141\u001b[0m       0.4479            0.4479        1.5018  0.0004  0.1232\n",
      "     48            1.0000        \u001b[32m0.0112\u001b[0m       0.4479            0.4479        1.4951  0.0004  0.1498\n",
      "     49            1.0000        0.0189       0.4375            0.4375        1.4883  0.0003  0.1516\n",
      "     50            1.0000        \u001b[32m0.0067\u001b[0m       0.4479            0.4479        1.4824  0.0003  0.1652\n",
      "Fine tuning model for subject 8 with 15 = 15 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.8000        1.1757       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0079\u001b[0m  0.0004  0.1153\n",
      "     32            0.8000        1.1889       0.5729            0.5729        1.0312  0.0005  0.1449\n",
      "     33            \u001b[36m0.9333\u001b[0m        0.9133       0.5833            0.5833        1.1008  0.0005  0.1349\n",
      "     34            0.9333        \u001b[32m0.3864\u001b[0m       0.5000            0.5000        1.2301  0.0006  0.1349\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2361\u001b[0m       0.4479            0.4479        1.4116  0.0006  0.1365\n",
      "     36            1.0000        \u001b[32m0.1650\u001b[0m       0.4375            0.4375        1.6001  0.0007  0.1354\n",
      "     37            1.0000        \u001b[32m0.1139\u001b[0m       0.4062            0.4062        1.7569  0.0007  0.2184\n",
      "     38            1.0000        0.1543       0.3854            0.3854        1.8454  0.0007  0.1639\n",
      "     39            1.0000        \u001b[32m0.1080\u001b[0m       0.3854            0.3854        1.8731  0.0007  0.1344\n",
      "     40            1.0000        \u001b[32m0.0684\u001b[0m       0.3646            0.3646        1.8594  0.0007  0.1521\n",
      "     41            1.0000        \u001b[32m0.0630\u001b[0m       0.3750            0.3750        1.8289  0.0007  0.1793\n",
      "     42            1.0000        \u001b[32m0.0465\u001b[0m       0.3958            0.3958        1.7928  0.0007  0.1470\n",
      "     43            1.0000        0.0559       0.3958            0.3958        1.7732  0.0006  0.1195\n",
      "     44            1.0000        \u001b[32m0.0412\u001b[0m       0.4062            0.4062        1.7572  0.0006  0.1309\n",
      "     45            1.0000        \u001b[32m0.0307\u001b[0m       0.4271            0.4271        1.7355  0.0005  0.1027\n",
      "     46            1.0000        0.0403       0.4271            0.4271        1.7131  0.0005  0.1159\n",
      "     47            1.0000        0.0333       0.4062            0.4062        1.6900  0.0004  0.1210\n",
      "     48            1.0000        0.0310       0.4167            0.4167        1.6657  0.0004  0.1203\n",
      "     49            1.0000        \u001b[32m0.0303\u001b[0m       0.4271            0.4271        1.6449  0.0003  0.1081\n",
      "     50            1.0000        0.0383       0.4583            0.4583        1.6264  0.0003  0.1319\n",
      "Fine tuning model for subject 8 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3333        1.7968       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0016\u001b[0m  0.0004  0.0916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6667        1.7290       0.5833            0.5833        1.0156  0.0005  0.1219\n",
      "     33            \u001b[36m0.9333\u001b[0m        1.1428       0.5521            0.5521        1.0604  0.0005  0.1173\n",
      "     34            0.9333        \u001b[32m0.6319\u001b[0m       0.5208            0.5208        1.1567  0.0006  0.1176\n",
      "     35            0.9333        \u001b[32m0.4459\u001b[0m       0.4583            0.4583        1.2937  0.0006  0.1530\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2300\u001b[0m       0.4375            0.4375        1.4466  0.0007  0.1302\n",
      "     37            1.0000        \u001b[32m0.2009\u001b[0m       0.4479            0.4479        1.6010  0.0007  0.1540\n",
      "     38            1.0000        \u001b[32m0.1800\u001b[0m       0.4479            0.4479        1.7368  0.0007  0.1279\n",
      "     39            1.0000        \u001b[32m0.1461\u001b[0m       0.4375            0.4375        1.8610  0.0007  0.1480\n",
      "     40            0.9333        \u001b[32m0.0869\u001b[0m       0.4479            0.4479        1.9614  0.0007  0.1157\n",
      "     41            0.9333        \u001b[32m0.0466\u001b[0m       0.4375            0.4375        2.0363  0.0007  0.1332\n",
      "     42            0.9333        0.0753       0.4271            0.4271        2.0842  0.0007  0.1901\n",
      "     43            0.9333        0.0697       0.4062            0.4062        2.1116  0.0006  0.1599\n",
      "     44            0.9333        0.0621       0.4062            0.4062        2.1148  0.0006  0.1822\n",
      "     45            0.9333        \u001b[32m0.0328\u001b[0m       0.3958            0.3958        2.1025  0.0005  0.1271\n",
      "     46            1.0000        0.0463       0.4062            0.4062        2.0790  0.0005  0.1193\n",
      "     47            1.0000        \u001b[32m0.0313\u001b[0m       0.3854            0.3854        2.0472  0.0004  0.1293\n",
      "     48            1.0000        \u001b[32m0.0206\u001b[0m       0.3854            0.3854        2.0092  0.0004  0.1341\n",
      "     49            1.0000        0.0214       0.4062            0.4062        1.9691  0.0003  0.1604\n",
      "     50            1.0000        0.0296       0.3958            0.3958        1.9289  0.0003  0.1331\n",
      "Fine tuning model for subject 8 with 15 = 15 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5333        0.9683       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0096\u001b[0m  0.0004  0.1329\n",
      "     32            0.6667        1.0117       0.5729            0.5729        1.0379  0.0005  0.1232\n",
      "     33            \u001b[36m0.8667\u001b[0m        0.9735       0.5417            0.5417        1.0950  0.0005  0.1176\n",
      "     34            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4044\u001b[0m       0.5000            0.5000        1.1909  0.0006  0.1838\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3608\u001b[0m       0.4688            0.4688        1.3106  0.0006  0.1532\n",
      "     36            0.9333        \u001b[32m0.1483\u001b[0m       0.4271            0.4271        1.4512  0.0007  0.1259\n",
      "     37            0.9333        0.2750       0.4271            0.4271        1.5915  0.0007  0.1205\n",
      "     38            0.9333        \u001b[32m0.1240\u001b[0m       0.4479            0.4479        1.7184  0.0007  0.1205\n",
      "     39            0.9333        \u001b[32m0.0976\u001b[0m       0.4479            0.4479        1.8268  0.0007  0.1259\n",
      "     40            0.9333        0.1095       0.4583            0.4583        1.9001  0.0007  0.1207\n",
      "     41            0.9333        \u001b[32m0.0801\u001b[0m       0.4479            0.4479        1.9557  0.0007  0.1282\n",
      "     42            0.9333        0.0984       0.4479            0.4479        1.9679  0.0007  0.1273\n",
      "     43            1.0000        \u001b[32m0.0374\u001b[0m       0.4479            0.4479        1.9577  0.0006  0.1215\n",
      "     44            1.0000        0.0385       0.4375            0.4375        1.9321  0.0006  0.1297\n",
      "     45            1.0000        0.0402       0.4271            0.4271        1.8962  0.0005  0.1219\n",
      "     46            1.0000        \u001b[32m0.0223\u001b[0m       0.4375            0.4375        1.8518  0.0005  0.1729\n",
      "     47            1.0000        0.0253       0.4375            0.4375        1.8032  0.0004  0.1659\n",
      "     48            1.0000        0.0268       0.4375            0.4375        1.7521  0.0004  0.1194\n",
      "     49            1.0000        \u001b[32m0.0160\u001b[0m       0.4375            0.4375        1.7022  0.0003  0.1202\n",
      "     50            1.0000        0.0262       0.4375            0.4375        1.6532  0.0003  0.1218\n",
      "Fine tuning model for subject 8 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        1.0746       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0116\u001b[0m  0.0004  0.1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8667\u001b[0m        1.0952       0.5938            0.5938        1.0324  0.0005  0.1191\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6047\u001b[0m       0.5417            0.5417        1.0779  0.0005  0.1240\n",
      "     34            1.0000        \u001b[32m0.5132\u001b[0m       0.5208            0.5208        1.1584  0.0006  0.1184\n",
      "     35            1.0000        \u001b[32m0.1865\u001b[0m       0.4792            0.4792        1.2815  0.0006  0.1208\n",
      "     36            1.0000        \u001b[32m0.1073\u001b[0m       0.4271            0.4271        1.4160  0.0007  0.1246\n",
      "     37            1.0000        \u001b[32m0.0940\u001b[0m       0.3854            0.3854        1.5397  0.0007  0.1415\n",
      "     38            1.0000        0.0945       0.3646            0.3646        1.6286  0.0007  0.1244\n",
      "     39            1.0000        \u001b[32m0.0828\u001b[0m       0.3438            0.3438        1.6969  0.0007  0.1236\n",
      "     40            1.0000        \u001b[32m0.0696\u001b[0m       0.3333            0.3333        1.7396  0.0007  0.1125\n",
      "     41            1.0000        \u001b[32m0.0351\u001b[0m       0.3125            0.3125        1.7664  0.0007  0.1105\n",
      "     42            1.0000        0.0393       0.3021            0.3021        1.7813  0.0007  0.1157\n",
      "     43            1.0000        0.0432       0.3021            0.3021        1.7880  0.0006  0.1145\n",
      "     44            1.0000        \u001b[32m0.0268\u001b[0m       0.3125            0.3125        1.7916  0.0006  0.1185\n",
      "     45            1.0000        0.0451       0.3125            0.3125        1.7951  0.0005  0.1116\n",
      "     46            1.0000        0.0287       0.3125            0.3125        1.8002  0.0005  0.1153\n",
      "     47            1.0000        0.1005       0.3125            0.3125        1.8020  0.0004  0.1139\n",
      "     48            1.0000        \u001b[32m0.0196\u001b[0m       0.3229            0.3229        1.8040  0.0004  0.1081\n",
      "     49            1.0000        0.0290       0.3229            0.3229        1.8072  0.0003  0.1101\n",
      "     50            1.0000        0.0299       0.3229            0.3229        1.8108  0.0003  0.1171\n",
      "Fine tuning model for subject 8 with 15 = 15 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.2675       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0047\u001b[0m  0.0004  0.1124\n",
      "     32            0.6000        1.2678       0.5729            0.5729        1.0175  0.0005  0.1166\n",
      "     33            \u001b[36m0.9333\u001b[0m        0.9411       0.5625            0.5625        1.0497  0.0005  0.1011\n",
      "     34            0.9333        \u001b[32m0.5386\u001b[0m       0.5208            0.5208        1.1048  0.0006  0.1121\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2264\u001b[0m       0.5312            0.5312        1.1567  0.0006  0.1075\n",
      "     36            1.0000        \u001b[32m0.1467\u001b[0m       0.4792            0.4792        1.2136  0.0007  0.1069\n",
      "     37            1.0000        \u001b[32m0.1053\u001b[0m       0.4271            0.4271        1.2844  0.0007  0.1154\n",
      "     38            1.0000        \u001b[32m0.0847\u001b[0m       0.3958            0.3958        1.3704  0.0007  0.1116\n",
      "     39            1.0000        0.0877       0.4062            0.4062        1.4732  0.0007  0.1153\n",
      "     40            1.0000        \u001b[32m0.0572\u001b[0m       0.3542            0.3542        1.5822  0.0007  0.1096\n",
      "     41            1.0000        \u001b[32m0.0445\u001b[0m       0.3229            0.3229        1.6817  0.0007  0.1174\n",
      "     42            1.0000        \u001b[32m0.0375\u001b[0m       0.3021            0.3021        1.7620  0.0007  0.1135\n",
      "     43            1.0000        0.0728       0.2917            0.2917        1.8188  0.0006  0.1114\n",
      "     44            1.0000        \u001b[32m0.0324\u001b[0m       0.3021            0.3021        1.8582  0.0006  0.1100\n",
      "     45            1.0000        \u001b[32m0.0178\u001b[0m       0.2917            0.2917        1.8806  0.0005  0.1078\n",
      "     46            1.0000        \u001b[32m0.0132\u001b[0m       0.3021            0.3021        1.8889  0.0005  0.1066\n",
      "     47            1.0000        0.0341       0.3229            0.3229        1.8854  0.0004  0.1116\n",
      "     48            1.0000        0.0282       0.3229            0.3229        1.8741  0.0004  0.1138\n",
      "     49            1.0000        0.0169       0.3125            0.3125        1.8576  0.0003  0.1109\n",
      "     50            1.0000        0.0178       0.3125            0.3125        1.8386  0.0003  0.1245\n",
      "Fine tuning model for subject 8 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        1.1838       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0110\u001b[0m  0.0004  0.1267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6667        0.9853       0.5833            0.5833        1.0364  0.0005  0.1493\n",
      "     33            \u001b[36m0.8667\u001b[0m        0.8256       0.5625            0.5625        1.0853  0.0005  0.1409\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4423\u001b[0m       0.5312            0.5312        1.1599  0.0006  0.1371\n",
      "     35            1.0000        \u001b[32m0.3018\u001b[0m       0.4792            0.4792        1.2405  0.0006  0.1365\n",
      "     36            1.0000        \u001b[32m0.1989\u001b[0m       0.4688            0.4688        1.3004  0.0007  0.1667\n",
      "     37            1.0000        \u001b[32m0.1935\u001b[0m       0.4583            0.4583        1.3379  0.0007  0.1349\n",
      "     38            1.0000        \u001b[32m0.1230\u001b[0m       0.4792            0.4792        1.3623  0.0007  0.1434\n",
      "     39            1.0000        \u001b[32m0.1009\u001b[0m       0.4688            0.4688        1.3807  0.0007  0.1300\n",
      "     40            1.0000        \u001b[32m0.0857\u001b[0m       0.5000            0.5000        1.4002  0.0007  0.1544\n",
      "     41            1.0000        \u001b[32m0.0402\u001b[0m       0.5000            0.5000        1.4212  0.0007  0.1398\n",
      "     42            1.0000        \u001b[32m0.0384\u001b[0m       0.5000            0.5000        1.4418  0.0007  0.1426\n",
      "     43            1.0000        \u001b[32m0.0306\u001b[0m       0.5104            0.5104        1.4578  0.0006  0.1448\n",
      "     44            1.0000        0.0378       0.4896            0.4896        1.4661  0.0006  0.1365\n",
      "     45            1.0000        0.0447       0.4688            0.4688        1.4676  0.0005  0.1729\n",
      "     46            1.0000        0.0538       0.4688            0.4688        1.4613  0.0005  0.1413\n",
      "     47            1.0000        \u001b[32m0.0251\u001b[0m       0.4688            0.4688        1.4523  0.0004  0.1707\n",
      "     48            1.0000        0.0299       0.4792            0.4792        1.4418  0.0004  0.1575\n",
      "     49            1.0000        0.0254       0.4896            0.4896        1.4316  0.0003  0.1144\n",
      "     50            1.0000        0.0311       0.4792            0.4792        1.4222  0.0003  0.1105\n",
      "Fine tuning model for subject 8 with 15 = 15 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7333        1.1580       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9972\u001b[0m  0.0004  0.1117\n",
      "     32            0.7333        1.1219       0.5938            0.5938        1.0286  0.0005  0.1175\n",
      "     33            \u001b[36m0.9333\u001b[0m        \u001b[32m0.6495\u001b[0m       0.5625            0.5625        1.1295  0.0005  0.1121\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4880\u001b[0m       0.5312            0.5312        1.2672  0.0006  0.1281\n",
      "     35            0.9333        \u001b[32m0.3300\u001b[0m       0.5208            0.5208        1.3818  0.0006  0.1030\n",
      "     36            1.0000        \u001b[32m0.2506\u001b[0m       0.4896            0.4896        1.4281  0.0007  0.1123\n",
      "     37            1.0000        \u001b[32m0.0901\u001b[0m       0.4896            0.4896        1.4508  0.0007  0.1142\n",
      "     38            1.0000        0.1136       0.4792            0.4792        1.4718  0.0007  0.1117\n",
      "     39            1.0000        \u001b[32m0.0881\u001b[0m       0.4792            0.4792        1.4938  0.0007  0.1152\n",
      "     40            1.0000        0.1422       0.4792            0.4792        1.5223  0.0007  0.1137\n",
      "     41            1.0000        0.1162       0.4583            0.4583        1.5550  0.0007  0.1124\n",
      "     42            1.0000        \u001b[32m0.0541\u001b[0m       0.4479            0.4479        1.5837  0.0007  0.1169\n",
      "     43            1.0000        0.0551       0.4479            0.4479        1.6043  0.0006  0.1101\n",
      "     44            1.0000        \u001b[32m0.0352\u001b[0m       0.4479            0.4479        1.6124  0.0006  0.1126\n",
      "     45            1.0000        \u001b[32m0.0276\u001b[0m       0.4479            0.4479        1.6095  0.0005  0.1173\n",
      "     46            1.0000        0.0425       0.4479            0.4479        1.5981  0.0005  0.1104\n",
      "     47            1.0000        0.0288       0.4583            0.4583        1.5802  0.0004  0.1223\n",
      "     48            1.0000        0.0339       0.4583            0.4583        1.5584  0.0004  0.1225\n",
      "     49            1.0000        \u001b[32m0.0248\u001b[0m       0.4583            0.4583        1.5354  0.0003  0.1093\n",
      "     50            1.0000        \u001b[32m0.0208\u001b[0m       0.4688            0.4688        1.5127  0.0003  0.1132\n",
      "Fine tuning model for subject 8 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        1.4522       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m0.9963\u001b[0m  0.0004  0.1121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7333        1.1450       0.5833            0.5833        1.0016  0.0005  0.1125\n",
      "     33            \u001b[36m0.8667\u001b[0m        0.9615       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.0201  0.0005  0.1118\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6512\u001b[0m       0.6042            0.6042        1.0514  0.0006  0.1118\n",
      "     35            1.0000        \u001b[32m0.1987\u001b[0m       0.5938            0.5938        1.1005  0.0006  0.1127\n",
      "     36            1.0000        0.2114       0.5312            0.5312        1.1752  0.0007  0.1075\n",
      "     37            1.0000        \u001b[32m0.1209\u001b[0m       0.5104            0.5104        1.2795  0.0007  0.1041\n",
      "     38            1.0000        \u001b[32m0.0661\u001b[0m       0.5208            0.5208        1.4069  0.0007  0.1085\n",
      "     39            1.0000        0.1061       0.5000            0.5000        1.5341  0.0007  0.1121\n",
      "     40            1.0000        0.0729       0.5000            0.5000        1.6443  0.0007  0.1034\n",
      "     41            1.0000        0.0688       0.5000            0.5000        1.7332  0.0007  0.1101\n",
      "     42            1.0000        \u001b[32m0.0495\u001b[0m       0.4896            0.4896        1.7913  0.0007  0.1176\n",
      "     43            1.0000        0.0633       0.4896            0.4896        1.8234  0.0006  0.1241\n",
      "     44            1.0000        \u001b[32m0.0379\u001b[0m       0.4688            0.4688        1.8329  0.0006  0.1282\n",
      "     45            1.0000        \u001b[32m0.0299\u001b[0m       0.4583            0.4583        1.8242  0.0005  0.1346\n",
      "     46            1.0000        \u001b[32m0.0272\u001b[0m       0.4792            0.4792        1.8018  0.0005  0.1174\n",
      "     47            1.0000        \u001b[32m0.0160\u001b[0m       0.4792            0.4792        1.7699  0.0004  0.1078\n",
      "     48            1.0000        0.0162       0.4896            0.4896        1.7320  0.0004  0.1055\n",
      "     49            1.0000        0.0297       0.4896            0.4896        1.6902  0.0003  0.1134\n",
      "     50            1.0000        \u001b[32m0.0119\u001b[0m       0.4896            0.4896        1.6473  0.0003  0.1133\n",
      "Fine tuning model for subject 8 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        1.2875       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m1.0061\u001b[0m  0.0004  0.1167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7333        1.2608       0.5208            0.5208        1.0194  0.0005  0.1117\n",
      "     33            \u001b[36m0.9333\u001b[0m        1.0598       0.5521            0.5521        1.0468  0.0005  0.1123\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6386\u001b[0m       0.5312            0.5312        1.0875  0.0006  0.1211\n",
      "     35            1.0000        \u001b[32m0.2660\u001b[0m       0.5104            0.5104        1.1396  0.0006  0.1117\n",
      "     36            1.0000        \u001b[32m0.2141\u001b[0m       0.4792            0.4792        1.1954  0.0007  0.1126\n",
      "     37            1.0000        \u001b[32m0.2068\u001b[0m       0.4479            0.4479        1.2569  0.0007  0.1147\n",
      "     38            1.0000        \u001b[32m0.0891\u001b[0m       0.4688            0.4688        1.3226  0.0007  0.1172\n",
      "     39            1.0000        \u001b[32m0.0393\u001b[0m       0.4479            0.4479        1.3832  0.0007  0.1100\n",
      "     40            1.0000        0.0726       0.4271            0.4271        1.4357  0.0007  0.1118\n",
      "     41            1.0000        0.0674       0.4271            0.4271        1.4766  0.0007  0.1128\n",
      "     42            1.0000        \u001b[32m0.0327\u001b[0m       0.4271            0.4271        1.5062  0.0007  0.1132\n",
      "     43            1.0000        \u001b[32m0.0261\u001b[0m       0.4375            0.4375        1.5258  0.0006  0.1125\n",
      "     44            1.0000        0.0263       0.4167            0.4167        1.5388  0.0006  0.1107\n",
      "     45            1.0000        0.0278       0.4062            0.4062        1.5458  0.0005  0.1094\n",
      "     46            1.0000        \u001b[32m0.0183\u001b[0m       0.4062            0.4062        1.5483  0.0005  0.1081\n",
      "     47            1.0000        \u001b[32m0.0182\u001b[0m       0.4062            0.4062        1.5482  0.0004  0.1107\n",
      "     48            1.0000        0.0239       0.4167            0.4167        1.5464  0.0004  0.1135\n",
      "     49            1.0000        \u001b[32m0.0165\u001b[0m       0.4271            0.4271        1.5440  0.0003  0.1171\n",
      "     50            1.0000        0.0267       0.4271            0.4271        1.5415  0.0003  0.1094\n",
      "Fine tuning model for subject 8 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.0311       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0001\u001b[0m  0.0004  0.1108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        1.0056       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        1.0088  0.0005  0.1178\n",
      "     33            0.8000        0.8985       0.5729            0.5729        1.0290  0.0005  0.1124\n",
      "     34            \u001b[36m0.9500\u001b[0m        \u001b[32m0.6039\u001b[0m       0.5938            0.5938        1.0609  0.0006  0.1120\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4894\u001b[0m       0.5521            0.5521        1.1070  0.0006  0.1124\n",
      "     36            1.0000        \u001b[32m0.3081\u001b[0m       0.5521            0.5521        1.1517  0.0007  0.1127\n",
      "     37            1.0000        \u001b[32m0.2678\u001b[0m       0.5312            0.5312        1.1846  0.0007  0.1175\n",
      "     38            1.0000        \u001b[32m0.1660\u001b[0m       0.4792            0.4792        1.2113  0.0007  0.1171\n",
      "     39            1.0000        \u001b[32m0.1209\u001b[0m       0.5000            0.5000        1.2442  0.0007  0.1132\n",
      "     40            1.0000        0.1619       0.5208            0.5208        1.2700  0.0007  0.1170\n",
      "     41            1.0000        \u001b[32m0.0920\u001b[0m       0.5208            0.5208        1.2968  0.0007  0.1170\n",
      "     42            1.0000        0.1148       0.5000            0.5000        1.3166  0.0007  0.1139\n",
      "     43            1.0000        \u001b[32m0.0701\u001b[0m       0.4896            0.4896        1.3311  0.0006  0.1103\n",
      "     44            1.0000        0.0768       0.4792            0.4792        1.3414  0.0006  0.1115\n",
      "     45            1.0000        \u001b[32m0.0640\u001b[0m       0.5000            0.5000        1.3513  0.0005  0.1128\n",
      "     46            1.0000        \u001b[32m0.0631\u001b[0m       0.5000            0.5000        1.3555  0.0005  0.1121\n",
      "     47            1.0000        \u001b[32m0.0351\u001b[0m       0.5104            0.5104        1.3560  0.0004  0.1174\n",
      "     48            1.0000        0.0654       0.5104            0.5104        1.3539  0.0004  0.1201\n",
      "     49            1.0000        \u001b[32m0.0294\u001b[0m       0.5417            0.5417        1.3483  0.0003  0.1132\n",
      "     50            1.0000        \u001b[32m0.0287\u001b[0m       0.5417            0.5417        1.3406  0.0003  0.1140\n",
      "Fine tuning model for subject 8 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6500        1.1474       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m1.0026\u001b[0m  0.0004  0.1174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8500\u001b[0m        0.8549       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.0346  0.0005  0.1212\n",
      "     33            \u001b[36m0.9000\u001b[0m        0.8787       0.5625            0.5625        1.0941  0.0005  0.1121\n",
      "     34            0.8500        \u001b[32m0.6126\u001b[0m       0.5417            0.5417        1.1819  0.0006  0.1166\n",
      "     35            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4177\u001b[0m       0.5208            0.5208        1.2382  0.0006  0.1118\n",
      "     36            0.9500        0.4219       0.5208            0.5208        1.2374  0.0007  0.1156\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2293\u001b[0m       0.5000            0.5000        1.2123  0.0007  0.1178\n",
      "     38            1.0000        \u001b[32m0.1436\u001b[0m       0.4896            0.4896        1.1899  0.0007  0.1116\n",
      "     39            1.0000        \u001b[32m0.1339\u001b[0m       0.4792            0.4792        1.2002  0.0007  0.1118\n",
      "     40            1.0000        \u001b[32m0.0900\u001b[0m       0.5000            0.5000        1.2289  0.0007  0.1212\n",
      "     41            1.0000        0.0978       0.5104            0.5104        1.2567  0.0007  0.1098\n",
      "     42            1.0000        \u001b[32m0.0834\u001b[0m       0.5208            0.5208        1.2661  0.0007  0.1151\n",
      "     43            1.0000        \u001b[32m0.0665\u001b[0m       0.5208            0.5208        1.2659  0.0006  0.1155\n",
      "     44            1.0000        \u001b[32m0.0588\u001b[0m       0.5208            0.5208        1.2621  0.0006  0.1189\n",
      "     45            1.0000        \u001b[32m0.0561\u001b[0m       0.5208            0.5208        1.2585  0.0005  0.1225\n",
      "     46            1.0000        \u001b[32m0.0365\u001b[0m       0.5208            0.5208        1.2499  0.0005  0.1313\n",
      "     47            1.0000        \u001b[32m0.0282\u001b[0m       0.5000            0.5000        1.2397  0.0004  0.1472\n",
      "     48            1.0000        0.0340       0.4896            0.4896        1.2299  0.0004  0.1347\n",
      "     49            1.0000        0.0303       0.4896            0.4896        1.2194  0.0003  0.1393\n",
      "     50            1.0000        \u001b[32m0.0206\u001b[0m       0.5104            0.5104        1.2092  0.0003  0.1382\n",
      "Fine tuning model for subject 8 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.8000        1.0596       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m1.0018\u001b[0m  0.0004  0.1493\n",
      "     32            0.8000        0.9485       0.6042            0.6042        1.0077  0.0005  0.1421\n",
      "     33            \u001b[36m0.9000\u001b[0m        \u001b[32m0.7175\u001b[0m       0.6042            0.6042        1.0232  0.0005  0.1486\n",
      "     34            0.9000        \u001b[32m0.5461\u001b[0m       0.5729            0.5729        1.0442  0.0006  0.1364\n",
      "     35            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4371\u001b[0m       0.5729            0.5729        1.0713  0.0006  0.1408\n",
      "     36            0.9500        \u001b[32m0.3396\u001b[0m       0.5625            0.5625        1.0973  0.0007  0.1321\n",
      "     37            0.9500        \u001b[32m0.1782\u001b[0m       0.5729            0.5729        1.1174  0.0007  0.1436\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1437\u001b[0m       0.5833            0.5833        1.1269  0.0007  0.1481\n",
      "     39            1.0000        0.1591       0.5729            0.5729        1.1297  0.0007  0.1294\n",
      "     40            1.0000        \u001b[32m0.0667\u001b[0m       0.5938            0.5938        1.1275  0.0007  0.1755\n",
      "     41            1.0000        0.1007       0.6042            0.6042        1.1254  0.0007  0.1414\n",
      "     42            1.0000        \u001b[32m0.0415\u001b[0m       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        1.1265  0.0007  0.1420\n",
      "     43            1.0000        0.0492       0.6146            0.6146        1.1293  0.0006  0.1494\n",
      "     44            1.0000        0.0442       0.5938            0.5938        1.1338  0.0006  0.1191\n",
      "     45            1.0000        \u001b[32m0.0364\u001b[0m       0.5729            0.5729        1.1388  0.0005  0.1120\n",
      "     46            1.0000        \u001b[32m0.0350\u001b[0m       0.5417            0.5417        1.1441  0.0005  0.1109\n",
      "     47            1.0000        0.0367       0.5417            0.5417        1.1493  0.0004  0.1158\n",
      "     48            1.0000        0.0370       0.5521            0.5521        1.1545  0.0004  0.1114\n",
      "     49            1.0000        \u001b[32m0.0176\u001b[0m       0.5521            0.5521        1.1589  0.0003  0.1168\n",
      "     50            1.0000        \u001b[32m0.0173\u001b[0m       0.5521            0.5521        1.1625  0.0003  0.1144\n",
      "Fine tuning model for subject 8 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.5288       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9993\u001b[0m  0.0004  0.1131\n",
      "     32            0.7000        1.0753       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        1.0022  0.0005  0.1083\n",
      "     33            0.7500        0.8861       0.6250            0.6250        1.0225  0.0005  0.1171\n",
      "     34            \u001b[36m0.9000\u001b[0m        0.7359       0.6146            0.6146        1.0669  0.0006  0.1142\n",
      "     35            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4594\u001b[0m       0.5938            0.5938        1.1152  0.0006  0.1125\n",
      "     36            0.9500        \u001b[32m0.2878\u001b[0m       0.5729            0.5729        1.1762  0.0007  0.1089\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2193\u001b[0m       0.5417            0.5417        1.2228  0.0007  0.1147\n",
      "     38            1.0000        \u001b[32m0.2117\u001b[0m       0.5417            0.5417        1.2548  0.0007  0.1130\n",
      "     39            1.0000        \u001b[32m0.0690\u001b[0m       0.5312            0.5312        1.2830  0.0007  0.1172\n",
      "     40            1.0000        0.0776       0.5312            0.5312        1.3107  0.0007  0.1122\n",
      "     41            1.0000        \u001b[32m0.0561\u001b[0m       0.5312            0.5312        1.3420  0.0007  0.1138\n",
      "     42            1.0000        0.0767       0.5312            0.5312        1.3660  0.0007  0.1279\n",
      "     43            1.0000        0.0768       0.5104            0.5104        1.3860  0.0006  0.1199\n",
      "     44            1.0000        0.0607       0.5104            0.5104        1.4031  0.0006  0.1126\n",
      "     45            1.0000        \u001b[32m0.0557\u001b[0m       0.5208            0.5208        1.4144  0.0005  0.1154\n",
      "     46            1.0000        0.0797       0.5208            0.5208        1.4229  0.0005  0.1241\n",
      "     47            1.0000        \u001b[32m0.0426\u001b[0m       0.5208            0.5208        1.4291  0.0004  0.1366\n",
      "     48            1.0000        \u001b[32m0.0346\u001b[0m       0.5104            0.5104        1.4318  0.0004  0.1162\n",
      "     49            1.0000        0.0369       0.5208            0.5208        1.4325  0.0003  0.1177\n",
      "     50            1.0000        0.0444       0.5208            0.5208        1.4321  0.0003  0.1084\n",
      "Fine tuning model for subject 8 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6500        1.1275       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0058\u001b[0m  0.0004  0.1113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7500        1.3691       0.5833            0.5833        1.0364  0.0005  0.1112\n",
      "     33            \u001b[36m0.9000\u001b[0m        0.7403       0.5417            0.5417        1.1031  0.0005  0.1127\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5457\u001b[0m       0.4792            0.4792        1.2205  0.0006  0.1164\n",
      "     35            1.0000        \u001b[32m0.3425\u001b[0m       0.4583            0.4583        1.3634  0.0006  0.1117\n",
      "     36            1.0000        \u001b[32m0.3065\u001b[0m       0.4271            0.4271        1.4980  0.0007  0.1130\n",
      "     37            1.0000        \u001b[32m0.1686\u001b[0m       0.4167            0.4167        1.6183  0.0007  0.1170\n",
      "     38            1.0000        0.1795       0.4062            0.4062        1.7232  0.0007  0.1154\n",
      "     39            1.0000        \u001b[32m0.1109\u001b[0m       0.4062            0.4062        1.8171  0.0007  0.1149\n",
      "     40            1.0000        0.1115       0.4167            0.4167        1.8960  0.0007  0.1145\n",
      "     41            1.0000        \u001b[32m0.1014\u001b[0m       0.3854            0.3854        1.9621  0.0007  0.1126\n",
      "     42            1.0000        \u001b[32m0.0671\u001b[0m       0.3646            0.3646        2.0140  0.0007  0.1123\n",
      "     43            1.0000        0.0986       0.3646            0.3646        2.0509  0.0006  0.1184\n",
      "     44            1.0000        0.0862       0.3542            0.3542        2.0731  0.0006  0.1116\n",
      "     45            1.0000        \u001b[32m0.0339\u001b[0m       0.3646            0.3646        2.0837  0.0005  0.1167\n",
      "     46            1.0000        \u001b[32m0.0249\u001b[0m       0.3646            0.3646        2.0849  0.0005  0.1133\n",
      "     47            1.0000        0.0377       0.3646            0.3646        2.0792  0.0004  0.1137\n",
      "     48            1.0000        0.0267       0.3646            0.3646        2.0681  0.0004  0.1114\n",
      "     49            1.0000        0.0427       0.3646            0.3646        2.0551  0.0003  0.1165\n",
      "     50            1.0000        0.0288       0.3646            0.3646        2.0388  0.0003  0.1117\n",
      "Fine tuning model for subject 8 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7500        0.9607       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0065\u001b[0m  0.0004  0.1241\n",
      "     32            0.7500        0.7385       0.5625            0.5625        1.0219  0.0005  0.1167\n",
      "     33            \u001b[36m0.9000\u001b[0m        \u001b[32m0.7108\u001b[0m       0.5521            0.5521        1.0517  0.0005  0.1175\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5003\u001b[0m       0.5833            0.5833        1.0871  0.0006  0.1144\n",
      "     35            1.0000        \u001b[32m0.4260\u001b[0m       0.5625            0.5625        1.1299  0.0006  0.1071\n",
      "     36            1.0000        \u001b[32m0.2228\u001b[0m       0.5312            0.5312        1.1680  0.0007  0.1158\n",
      "     37            1.0000        \u001b[32m0.2205\u001b[0m       0.5104            0.5104        1.2058  0.0007  0.1136\n",
      "     38            1.0000        \u001b[32m0.1810\u001b[0m       0.4896            0.4896        1.2513  0.0007  0.1159\n",
      "     39            1.0000        \u001b[32m0.0912\u001b[0m       0.4583            0.4583        1.3101  0.0007  0.1178\n",
      "     40            1.0000        0.1212       0.4479            0.4479        1.3745  0.0007  0.1157\n",
      "     41            1.0000        \u001b[32m0.0642\u001b[0m       0.4479            0.4479        1.4364  0.0007  0.1148\n",
      "     42            1.0000        0.0901       0.4271            0.4271        1.4911  0.0007  0.1180\n",
      "     43            1.0000        \u001b[32m0.0478\u001b[0m       0.3958            0.3958        1.5323  0.0006  0.1166\n",
      "     44            1.0000        \u001b[32m0.0429\u001b[0m       0.3958            0.3958        1.5589  0.0006  0.1171\n",
      "     45            1.0000        0.0513       0.3958            0.3958        1.5760  0.0005  0.1172\n",
      "     46            1.0000        \u001b[32m0.0325\u001b[0m       0.3958            0.3958        1.5822  0.0005  0.1281\n",
      "     47            1.0000        0.0368       0.4062            0.4062        1.5785  0.0004  0.1311\n",
      "     48            1.0000        \u001b[32m0.0268\u001b[0m       0.4062            0.4062        1.5675  0.0004  0.1594\n",
      "     49            1.0000        \u001b[32m0.0239\u001b[0m       0.3854            0.3854        1.5519  0.0003  0.1084\n",
      "     50            1.0000        0.0295       0.3854            0.3854        1.5344  0.0003  0.1108\n",
      "Fine tuning model for subject 8 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        0.9619       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9976\u001b[0m  0.0004  0.1153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.0508       0.6042            0.6042        0.9998  0.0005  0.1141\n",
      "     33            \u001b[36m0.9500\u001b[0m        \u001b[32m0.6498\u001b[0m       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        1.0131  0.0005  0.1171\n",
      "     34            0.9500        \u001b[32m0.4954\u001b[0m       0.5833            0.5833        1.0428  0.0006  0.1190\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3211\u001b[0m       0.5833            0.5833        1.0681  0.0006  0.1173\n",
      "     36            1.0000        \u001b[32m0.2350\u001b[0m       0.5938            0.5938        1.1035  0.0007  0.1190\n",
      "     37            1.0000        \u001b[32m0.2014\u001b[0m       0.5312            0.5312        1.1678  0.0007  0.1162\n",
      "     38            0.9500        \u001b[32m0.1167\u001b[0m       0.5000            0.5000        1.2509  0.0007  0.1126\n",
      "     39            0.9500        0.1179       0.4688            0.4688        1.3469  0.0007  0.1158\n",
      "     40            0.9500        \u001b[32m0.1115\u001b[0m       0.4792            0.4792        1.4295  0.0007  0.1224\n",
      "     41            1.0000        \u001b[32m0.0973\u001b[0m       0.4792            0.4792        1.4928  0.0007  0.1128\n",
      "     42            1.0000        \u001b[32m0.0772\u001b[0m       0.4792            0.4792        1.5283  0.0007  0.1124\n",
      "     43            1.0000        \u001b[32m0.0728\u001b[0m       0.4479            0.4479        1.5424  0.0006  0.1119\n",
      "     44            1.0000        \u001b[32m0.0567\u001b[0m       0.4375            0.4375        1.5435  0.0006  0.1098\n",
      "     45            1.0000        \u001b[32m0.0497\u001b[0m       0.4688            0.4688        1.5342  0.0005  0.1154\n",
      "     46            1.0000        \u001b[32m0.0439\u001b[0m       0.4583            0.4583        1.5189  0.0005  0.1103\n",
      "     47            1.0000        0.0471       0.4583            0.4583        1.4987  0.0004  0.1124\n",
      "     48            1.0000        \u001b[32m0.0362\u001b[0m       0.4479            0.4479        1.4745  0.0004  0.1100\n",
      "     49            1.0000        0.0586       0.4688            0.4688        1.4479  0.0003  0.1107\n",
      "     50            1.0000        0.0387       0.4688            0.4688        1.4209  0.0003  0.1098\n",
      "Fine tuning model for subject 8 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.3517       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0034\u001b[0m  0.0004  0.1120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6500        1.1511       0.6042            0.6042        1.0182  0.0005  0.1202\n",
      "     33            0.7000        1.1655       0.5833            0.5833        1.0561  0.0005  0.1098\n",
      "     34            0.7500        \u001b[32m0.6993\u001b[0m       0.5104            0.5104        1.1125  0.0006  0.1246\n",
      "     35            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4338\u001b[0m       0.5000            0.5000        1.1863  0.0006  0.1124\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3359\u001b[0m       0.4479            0.4479        1.2652  0.0007  0.1148\n",
      "     37            1.0000        \u001b[32m0.2471\u001b[0m       0.4271            0.4271        1.3373  0.0007  0.1196\n",
      "     38            1.0000        \u001b[32m0.2007\u001b[0m       0.4271            0.4271        1.3978  0.0007  0.1291\n",
      "     39            1.0000        \u001b[32m0.1389\u001b[0m       0.4167            0.4167        1.4387  0.0007  0.1378\n",
      "     40            1.0000        \u001b[32m0.1074\u001b[0m       0.4167            0.4167        1.4557  0.0007  0.1360\n",
      "     41            1.0000        \u001b[32m0.0949\u001b[0m       0.4167            0.4167        1.4619  0.0007  0.1278\n",
      "     42            1.0000        0.1500       0.3958            0.3958        1.4637  0.0007  0.1391\n",
      "     43            1.0000        \u001b[32m0.0861\u001b[0m       0.3958            0.3958        1.4580  0.0006  0.1274\n",
      "     44            1.0000        \u001b[32m0.0735\u001b[0m       0.3854            0.3854        1.4474  0.0006  0.1670\n",
      "     45            1.0000        \u001b[32m0.0563\u001b[0m       0.3854            0.3854        1.4381  0.0005  0.1297\n",
      "     46            1.0000        0.0593       0.3854            0.3854        1.4274  0.0005  0.1348\n",
      "     47            1.0000        \u001b[32m0.0415\u001b[0m       0.4062            0.4062        1.4178  0.0004  0.1366\n",
      "     48            1.0000        \u001b[32m0.0317\u001b[0m       0.4062            0.4062        1.4085  0.0004  0.1492\n",
      "     49            1.0000        \u001b[32m0.0256\u001b[0m       0.4167            0.4167        1.4007  0.0003  0.1388\n",
      "     50            1.0000        0.0457       0.4271            0.4271        1.3940  0.0003  0.1398\n",
      "Fine tuning model for subject 8 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        1.0899       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m0.9977\u001b[0m  0.0004  0.1383\n",
      "     32            \u001b[36m0.8500\u001b[0m        0.7554       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.0066  0.0005  0.1472\n",
      "     33            \u001b[36m0.9500\u001b[0m        \u001b[32m0.6955\u001b[0m       0.5833            0.5833        1.0293  0.0005  0.1290\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5202\u001b[0m       0.5938            0.5938        1.0632  0.0006  0.1337\n",
      "     35            1.0000        \u001b[32m0.2767\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.1044  0.0006  0.1636\n",
      "     36            1.0000        0.2924       0.5833            0.5833        1.1637  0.0007  0.1417\n",
      "     37            1.0000        \u001b[32m0.2253\u001b[0m       0.5000            0.5000        1.2468  0.0007  0.1167\n",
      "     38            1.0000        \u001b[32m0.1304\u001b[0m       0.4792            0.4792        1.3431  0.0007  0.1148\n",
      "     39            1.0000        \u001b[32m0.1048\u001b[0m       0.4375            0.4375        1.4413  0.0007  0.1146\n",
      "     40            1.0000        0.1123       0.4167            0.4167        1.5210  0.0007  0.1107\n",
      "     41            1.0000        \u001b[32m0.0872\u001b[0m       0.4167            0.4167        1.5747  0.0007  0.1221\n",
      "     42            1.0000        \u001b[32m0.0633\u001b[0m       0.4271            0.4271        1.6008  0.0007  0.1168\n",
      "     43            1.0000        0.0653       0.4375            0.4375        1.6065  0.0006  0.1097\n",
      "     44            1.0000        0.0716       0.4375            0.4375        1.6050  0.0006  0.1142\n",
      "     45            1.0000        \u001b[32m0.0582\u001b[0m       0.4375            0.4375        1.5909  0.0005  0.1146\n",
      "     46            1.0000        \u001b[32m0.0516\u001b[0m       0.4375            0.4375        1.5659  0.0005  0.1127\n",
      "     47            1.0000        \u001b[32m0.0442\u001b[0m       0.4375            0.4375        1.5346  0.0004  0.1117\n",
      "     48            1.0000        \u001b[32m0.0333\u001b[0m       0.4375            0.4375        1.5004  0.0004  0.1147\n",
      "     49            1.0000        0.0440       0.4375            0.4375        1.4647  0.0003  0.1118\n",
      "     50            1.0000        0.0379       0.4479            0.4479        1.4301  0.0003  0.1125\n",
      "Fine tuning model for subject 8 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        \u001b[32m0.6604\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0075\u001b[0m  0.0004  0.1172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        0.8360       0.5729            0.5729        1.0231  0.0005  0.1133\n",
      "     33            \u001b[36m0.8500\u001b[0m        0.6635       0.5729            0.5729        1.0673  0.0005  0.1170\n",
      "     34            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4599\u001b[0m       0.5417            0.5417        1.1469  0.0006  0.1271\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3114\u001b[0m       0.5312            0.5312        1.2297  0.0006  0.1161\n",
      "     36            1.0000        \u001b[32m0.1518\u001b[0m       0.5000            0.5000        1.3015  0.0007  0.1163\n",
      "     37            1.0000        0.1670       0.5000            0.5000        1.3318  0.0007  0.1155\n",
      "     38            1.0000        \u001b[32m0.1006\u001b[0m       0.5104            0.5104        1.3285  0.0007  0.1174\n",
      "     39            1.0000        \u001b[32m0.0666\u001b[0m       0.4896            0.4896        1.3136  0.0007  0.1161\n",
      "     40            1.0000        0.0670       0.5000            0.5000        1.2974  0.0007  0.1170\n",
      "     41            1.0000        \u001b[32m0.0650\u001b[0m       0.5104            0.5104        1.2839  0.0007  0.1158\n",
      "     42            1.0000        \u001b[32m0.0361\u001b[0m       0.4792            0.4792        1.2780  0.0007  0.1152\n",
      "     43            1.0000        \u001b[32m0.0315\u001b[0m       0.4792            0.4792        1.2777  0.0006  0.1112\n",
      "     44            1.0000        \u001b[32m0.0295\u001b[0m       0.4688            0.4688        1.2803  0.0006  0.1164\n",
      "     45            1.0000        \u001b[32m0.0201\u001b[0m       0.4688            0.4688        1.2833  0.0005  0.1112\n",
      "     46            1.0000        0.0348       0.4688            0.4688        1.2853  0.0005  0.1109\n",
      "     47            1.0000        0.0272       0.4792            0.4792        1.2857  0.0004  0.1145\n",
      "     48            1.0000        0.0338       0.4792            0.4792        1.2851  0.0004  0.1160\n",
      "     49            1.0000        0.0229       0.4792            0.4792        1.2841  0.0003  0.1161\n",
      "     50            1.0000        0.0205       0.4688            0.4688        1.2830  0.0003  0.1154\n",
      "Fine tuning model for subject 8 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7200        0.9219       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9987\u001b[0m  0.0004  0.1176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        0.8026       0.5938            0.5938        1.0019  0.0005  0.1193\n",
      "     33            \u001b[36m0.8800\u001b[0m        \u001b[32m0.6881\u001b[0m       0.6042            0.6042        1.0222  0.0005  0.1181\n",
      "     34            0.8800        \u001b[32m0.5377\u001b[0m       0.5521            0.5521        1.0597  0.0006  0.1233\n",
      "     35            \u001b[36m0.9600\u001b[0m        \u001b[32m0.3558\u001b[0m       0.5729            0.5729        1.1101  0.0006  0.1216\n",
      "     36            0.9600        \u001b[32m0.1753\u001b[0m       0.5625            0.5625        1.1549  0.0007  0.1192\n",
      "     37            \u001b[36m1.0000\u001b[0m        0.2802       0.5521            0.5521        1.1813  0.0007  0.1207\n",
      "     38            1.0000        \u001b[32m0.1388\u001b[0m       0.5312            0.5312        1.1896  0.0007  0.1259\n",
      "     39            1.0000        \u001b[32m0.1181\u001b[0m       0.5208            0.5208        1.1883  0.0007  0.1188\n",
      "     40            1.0000        0.1304       0.4896            0.4896        1.1860  0.0007  0.1212\n",
      "     41            1.0000        \u001b[32m0.0990\u001b[0m       0.4792            0.4792        1.1900  0.0007  0.1232\n",
      "     42            1.0000        \u001b[32m0.0823\u001b[0m       0.4583            0.4583        1.1977  0.0007  0.1221\n",
      "     43            1.0000        \u001b[32m0.0591\u001b[0m       0.4688            0.4688        1.2097  0.0006  0.1353\n",
      "     44            1.0000        \u001b[32m0.0530\u001b[0m       0.4479            0.4479        1.2217  0.0006  0.1174\n",
      "     45            1.0000        0.0623       0.4792            0.4792        1.2333  0.0005  0.1242\n",
      "     46            1.0000        \u001b[32m0.0526\u001b[0m       0.4688            0.4688        1.2424  0.0005  0.1176\n",
      "     47            1.0000        0.0777       0.4688            0.4688        1.2478  0.0004  0.1207\n",
      "     48            1.0000        \u001b[32m0.0435\u001b[0m       0.4583            0.4583        1.2517  0.0004  0.1174\n",
      "     49            1.0000        \u001b[32m0.0361\u001b[0m       0.4792            0.4792        1.2539  0.0003  0.1218\n",
      "     50            1.0000        \u001b[32m0.0312\u001b[0m       0.4792            0.4792        1.2546  0.0003  0.1213\n",
      "Fine tuning model for subject 8 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4400        1.4469       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        \u001b[94m0.9968\u001b[0m  0.0004  0.1221\n",
      "     32            0.5600        1.4286       0.5729            0.5729        1.0011  0.0005  0.1218\n",
      "     33            0.6000        1.3010       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.0281  0.0005  0.1191\n",
      "     34            0.8000        \u001b[32m0.7148\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.0699  0.0006  0.1299\n",
      "     35            \u001b[36m0.8800\u001b[0m        0.8425       0.5521            0.5521        1.1114  0.0006  0.1170\n",
      "     36            0.8800        \u001b[32m0.4068\u001b[0m       0.5417            0.5417        1.1545  0.0007  0.1163\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2771\u001b[0m       0.5417            0.5417        1.2229  0.0007  0.1223\n",
      "     38            1.0000        \u001b[32m0.1991\u001b[0m       0.4792            0.4792        1.3205  0.0007  0.1264\n",
      "     39            1.0000        \u001b[32m0.1674\u001b[0m       0.4271            0.4271        1.4400  0.0007  0.1168\n",
      "     40            1.0000        \u001b[32m0.1277\u001b[0m       0.4271            0.4271        1.5717  0.0007  0.1175\n",
      "     41            1.0000        \u001b[32m0.1022\u001b[0m       0.4375            0.4375        1.6983  0.0007  0.1225\n",
      "     42            1.0000        \u001b[32m0.0819\u001b[0m       0.4271            0.4271        1.7978  0.0007  0.1209\n",
      "     43            1.0000        0.0985       0.4167            0.4167        1.8654  0.0006  0.1248\n",
      "     44            1.0000        0.0910       0.4167            0.4167        1.9084  0.0006  0.1175\n",
      "     45            1.0000        \u001b[32m0.0731\u001b[0m       0.3854            0.3854        1.9306  0.0005  0.1219\n",
      "     46            1.0000        \u001b[32m0.0686\u001b[0m       0.3854            0.3854        1.9270  0.0005  0.1213\n",
      "     47            1.0000        \u001b[32m0.0533\u001b[0m       0.3958            0.3958        1.9048  0.0004  0.1179\n",
      "     48            1.0000        \u001b[32m0.0529\u001b[0m       0.3958            0.3958        1.8688  0.0004  0.1202\n",
      "     49            1.0000        \u001b[32m0.0489\u001b[0m       0.3958            0.3958        1.8292  0.0003  0.1244\n",
      "     50            1.0000        0.0615       0.4062            0.4062        1.7842  0.0003  0.1315\n",
      "Fine tuning model for subject 8 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        0.8904       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0052\u001b[0m  0.0004  0.1180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6800        1.0580       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        1.0250  0.0005  0.1216\n",
      "     33            \u001b[36m0.8800\u001b[0m        0.8396       0.5938            0.5938        1.0714  0.0005  0.1233\n",
      "     34            \u001b[36m0.9200\u001b[0m        \u001b[32m0.6596\u001b[0m       0.5625            0.5625        1.1594  0.0006  0.1226\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3724\u001b[0m       0.5000            0.5000        1.2507  0.0006  0.1192\n",
      "     36            1.0000        0.4230       0.4896            0.4896        1.3427  0.0007  0.1253\n",
      "     37            1.0000        \u001b[32m0.1889\u001b[0m       0.4792            0.4792        1.4055  0.0007  0.1237\n",
      "     38            1.0000        0.1936       0.4792            0.4792        1.4301  0.0007  0.1249\n",
      "     39            1.0000        \u001b[32m0.1208\u001b[0m       0.4688            0.4688        1.4442  0.0007  0.1172\n",
      "     40            1.0000        \u001b[32m0.0744\u001b[0m       0.4688            0.4688        1.4424  0.0007  0.1172\n",
      "     41            1.0000        \u001b[32m0.0737\u001b[0m       0.4688            0.4688        1.4376  0.0007  0.1178\n",
      "     42            1.0000        0.0880       0.4792            0.4792        1.4280  0.0007  0.1230\n",
      "     43            1.0000        \u001b[32m0.0734\u001b[0m       0.4792            0.4792        1.4189  0.0006  0.1387\n",
      "     44            1.0000        0.0737       0.4688            0.4688        1.4142  0.0006  0.1731\n",
      "     45            1.0000        0.0751       0.4896            0.4896        1.4108  0.0005  0.1953\n",
      "     46            1.0000        \u001b[32m0.0379\u001b[0m       0.5000            0.5000        1.4096  0.0005  0.1555\n",
      "     47            1.0000        \u001b[32m0.0368\u001b[0m       0.5000            0.5000        1.4107  0.0004  0.1658\n",
      "     48            1.0000        0.0419       0.5000            0.5000        1.4119  0.0004  0.1562\n",
      "     49            1.0000        0.0500       0.4896            0.4896        1.4142  0.0003  0.1469\n",
      "     50            1.0000        0.0478       0.4792            0.4792        1.4155  0.0003  0.1575\n",
      "Fine tuning model for subject 8 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5200        1.3336       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0034\u001b[0m  0.0004  0.1508\n",
      "     32            0.6400        1.0769       0.5938            0.5938        1.0102  0.0005  0.1410\n",
      "     33            0.6800        0.8836       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.0252  0.0005  0.1382\n",
      "     34            \u001b[36m0.9600\u001b[0m        0.8054       0.5938            0.5938        1.0561  0.0006  0.1517\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4186\u001b[0m       0.5521            0.5521        1.1127  0.0006  0.1435\n",
      "     36            1.0000        \u001b[32m0.2558\u001b[0m       0.5104            0.5104        1.1988  0.0007  0.1454\n",
      "     37            1.0000        \u001b[32m0.1876\u001b[0m       0.4375            0.4375        1.2980  0.0007  0.1475\n",
      "     38            1.0000        \u001b[32m0.1325\u001b[0m       0.4167            0.4167        1.3877  0.0007  0.1362\n",
      "     39            1.0000        0.1376       0.3646            0.3646        1.4528  0.0007  0.1621\n",
      "     40            1.0000        \u001b[32m0.1135\u001b[0m       0.3646            0.3646        1.4909  0.0007  0.1437\n",
      "     41            1.0000        \u001b[32m0.0789\u001b[0m       0.3646            0.3646        1.5096  0.0007  0.1602\n",
      "     42            1.0000        0.1006       0.3750            0.3750        1.5193  0.0007  0.1815\n",
      "     43            1.0000        \u001b[32m0.0551\u001b[0m       0.3542            0.3542        1.5228  0.0006  0.1281\n",
      "     44            1.0000        0.0737       0.3542            0.3542        1.5187  0.0006  0.1172\n",
      "     45            1.0000        0.0594       0.3750            0.3750        1.5110  0.0005  0.1170\n",
      "     46            1.0000        \u001b[32m0.0450\u001b[0m       0.3958            0.3958        1.5029  0.0005  0.1188\n",
      "     47            1.0000        0.0480       0.4271            0.4271        1.4925  0.0004  0.1248\n",
      "     48            1.0000        0.0663       0.4271            0.4271        1.4822  0.0004  0.1208\n",
      "     49            1.0000        0.0559       0.4375            0.4375        1.4729  0.0003  0.1168\n",
      "     50            1.0000        0.0589       0.4375            0.4375        1.4658  0.0003  0.1224\n",
      "Fine tuning model for subject 8 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6400        1.2283       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m0.9913\u001b[0m  0.0004  0.1280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7600        1.1722       0.5938            0.5938        0.9965  0.0005  0.1258\n",
      "     33            \u001b[36m0.8400\u001b[0m        0.9384       0.5521            0.5521        1.0411  0.0005  0.1226\n",
      "     34            \u001b[36m1.0000\u001b[0m        0.7426       0.5521            0.5521        1.1042  0.0006  0.1248\n",
      "     35            1.0000        \u001b[32m0.3449\u001b[0m       0.4896            0.4896        1.1647  0.0006  0.1294\n",
      "     36            1.0000        \u001b[32m0.2401\u001b[0m       0.4896            0.4896        1.2275  0.0007  0.1276\n",
      "     37            1.0000        \u001b[32m0.2249\u001b[0m       0.4583            0.4583        1.2861  0.0007  0.1234\n",
      "     38            1.0000        \u001b[32m0.1715\u001b[0m       0.4167            0.4167        1.3417  0.0007  0.1207\n",
      "     39            1.0000        0.2024       0.4479            0.4479        1.3838  0.0007  0.1229\n",
      "     40            1.0000        \u001b[32m0.1083\u001b[0m       0.4583            0.4583        1.4160  0.0007  0.1160\n",
      "     41            1.0000        \u001b[32m0.0936\u001b[0m       0.4167            0.4167        1.4394  0.0007  0.1160\n",
      "     42            1.0000        0.1134       0.4062            0.4062        1.4615  0.0007  0.1232\n",
      "     43            1.0000        \u001b[32m0.0631\u001b[0m       0.4062            0.4062        1.4783  0.0006  0.1209\n",
      "     44            1.0000        \u001b[32m0.0569\u001b[0m       0.4062            0.4062        1.4909  0.0006  0.1225\n",
      "     45            1.0000        0.0785       0.4167            0.4167        1.4947  0.0005  0.1187\n",
      "     46            1.0000        \u001b[32m0.0435\u001b[0m       0.4167            0.4167        1.4950  0.0005  0.1256\n",
      "     47            1.0000        0.0498       0.4167            0.4167        1.4924  0.0004  0.1174\n",
      "     48            1.0000        \u001b[32m0.0312\u001b[0m       0.4062            0.4062        1.4887  0.0004  0.1240\n",
      "     49            1.0000        \u001b[32m0.0219\u001b[0m       0.4062            0.4062        1.4844  0.0003  0.1234\n",
      "     50            1.0000        0.0279       0.4062            0.4062        1.4798  0.0003  0.1173\n",
      "Fine tuning model for subject 8 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.0680       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0052\u001b[0m  0.0004  0.1243\n",
      "     32            0.6400        0.9848       0.5833            0.5833        1.0217  0.0005  0.1174\n",
      "     33            \u001b[36m0.8400\u001b[0m        0.7806       0.5833            0.5833        1.0522  0.0005  0.1231\n",
      "     34            \u001b[36m0.8800\u001b[0m        \u001b[32m0.6859\u001b[0m       0.5625            0.5625        1.0966  0.0006  0.1198\n",
      "     35            \u001b[36m0.9600\u001b[0m        \u001b[32m0.5003\u001b[0m       0.5312            0.5312        1.1616  0.0006  0.1173\n",
      "     36            0.9600        \u001b[32m0.4793\u001b[0m       0.5104            0.5104        1.2191  0.0007  0.1212\n",
      "     37            0.9600        \u001b[32m0.3150\u001b[0m       0.4792            0.4792        1.2546  0.0007  0.1361\n",
      "     38            0.9600        \u001b[32m0.2558\u001b[0m       0.4792            0.4792        1.2785  0.0007  0.1378\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1583\u001b[0m       0.4792            0.4792        1.3039  0.0007  0.1231\n",
      "     40            1.0000        \u001b[32m0.1302\u001b[0m       0.4792            0.4792        1.3312  0.0007  0.1193\n",
      "     41            1.0000        0.1556       0.5000            0.5000        1.3388  0.0007  0.1299\n",
      "     42            1.0000        \u001b[32m0.1253\u001b[0m       0.5000            0.5000        1.3372  0.0007  0.1518\n",
      "     43            1.0000        \u001b[32m0.0955\u001b[0m       0.5000            0.5000        1.3312  0.0006  0.1277\n",
      "     44            1.0000        \u001b[32m0.0699\u001b[0m       0.5104            0.5104        1.3215  0.0006  0.1233\n",
      "     45            1.0000        0.0807       0.5000            0.5000        1.3095  0.0005  0.1344\n",
      "     46            1.0000        \u001b[32m0.0638\u001b[0m       0.5000            0.5000        1.3001  0.0005  0.1196\n",
      "     47            1.0000        0.0684       0.5208            0.5208        1.2893  0.0004  0.1228\n",
      "     48            1.0000        \u001b[32m0.0486\u001b[0m       0.5312            0.5312        1.2786  0.0004  0.1170\n",
      "     49            1.0000        0.0492       0.5208            0.5208        1.2686  0.0003  0.1143\n",
      "     50            1.0000        0.0609       0.5312            0.5312        1.2598  0.0003  0.1246\n",
      "Fine tuning model for subject 8 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7200        0.7728       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m1.0000\u001b[0m  0.0004  0.1119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        0.8599       0.5833            0.5833        1.0054  0.0005  0.1238\n",
      "     33            \u001b[36m0.8400\u001b[0m        0.7289       0.6146            0.6146        1.0246  0.0005  0.1238\n",
      "     34            \u001b[36m0.9600\u001b[0m        \u001b[32m0.5349\u001b[0m       0.5625            0.5625        1.0733  0.0006  0.1190\n",
      "     35            0.9600        \u001b[32m0.3513\u001b[0m       0.5417            0.5417        1.1406  0.0006  0.1169\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3374\u001b[0m       0.5104            0.5104        1.2120  0.0007  0.1248\n",
      "     37            1.0000        \u001b[32m0.2236\u001b[0m       0.4792            0.4792        1.2738  0.0007  0.1214\n",
      "     38            1.0000        \u001b[32m0.1346\u001b[0m       0.4688            0.4688        1.3185  0.0007  0.1206\n",
      "     39            1.0000        \u001b[32m0.1250\u001b[0m       0.4688            0.4688        1.3495  0.0007  0.1198\n",
      "     40            1.0000        \u001b[32m0.0791\u001b[0m       0.4896            0.4896        1.3730  0.0007  0.1223\n",
      "     41            1.0000        \u001b[32m0.0652\u001b[0m       0.5000            0.5000        1.3929  0.0007  0.1167\n",
      "     42            1.0000        0.0797       0.4792            0.4792        1.4099  0.0007  0.1214\n",
      "     43            1.0000        0.0724       0.4792            0.4792        1.4253  0.0006  0.1232\n",
      "     44            1.0000        \u001b[32m0.0391\u001b[0m       0.4688            0.4688        1.4352  0.0006  0.1234\n",
      "     45            1.0000        0.0771       0.4688            0.4688        1.4464  0.0005  0.1241\n",
      "     46            1.0000        \u001b[32m0.0337\u001b[0m       0.4688            0.4688        1.4555  0.0005  0.1222\n",
      "     47            1.0000        \u001b[32m0.0260\u001b[0m       0.4583            0.4583        1.4614  0.0004  0.1212\n",
      "     48            1.0000        0.0335       0.4583            0.4583        1.4656  0.0004  0.1229\n",
      "     49            1.0000        0.0309       0.4583            0.4583        1.4675  0.0003  0.1172\n",
      "     50            1.0000        \u001b[32m0.0204\u001b[0m       0.4583            0.4583        1.4678  0.0003  0.1228\n",
      "Fine tuning model for subject 8 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        0.9465       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0029\u001b[0m  0.0004  0.1376\n",
      "     32            0.6400        1.0023       0.5833            0.5833        1.0140  0.0005  0.1273\n",
      "     33            \u001b[36m0.8400\u001b[0m        0.8120       0.5729            0.5729        1.0439  0.0005  0.1235\n",
      "     34            \u001b[36m0.8800\u001b[0m        0.7877       0.5521            0.5521        1.1014  0.0006  0.1202\n",
      "     35            0.8800        \u001b[32m0.5724\u001b[0m       0.4688            0.4688        1.1831  0.0006  0.1183\n",
      "     36            \u001b[36m0.9200\u001b[0m        \u001b[32m0.4041\u001b[0m       0.4688            0.4688        1.2650  0.0007  0.1163\n",
      "     37            0.9200        \u001b[32m0.2664\u001b[0m       0.4479            0.4479        1.3278  0.0007  0.1223\n",
      "     38            \u001b[36m0.9600\u001b[0m        \u001b[32m0.2239\u001b[0m       0.4167            0.4167        1.3627  0.0007  0.1184\n",
      "     39            0.9600        \u001b[32m0.1975\u001b[0m       0.3958            0.3958        1.3591  0.0007  0.1186\n",
      "     40            0.9600        \u001b[32m0.1345\u001b[0m       0.4062            0.4062        1.3443  0.0007  0.1266\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1085\u001b[0m       0.4062            0.4062        1.3298  0.0007  0.1168\n",
      "     42            1.0000        \u001b[32m0.0784\u001b[0m       0.4167            0.4167        1.3230  0.0007  0.1172\n",
      "     43            1.0000        \u001b[32m0.0710\u001b[0m       0.4375            0.4375        1.3209  0.0006  0.1239\n",
      "     44            1.0000        \u001b[32m0.0641\u001b[0m       0.4479            0.4479        1.3241  0.0006  0.1187\n",
      "     45            1.0000        0.0709       0.4479            0.4479        1.3313  0.0005  0.1193\n",
      "     46            1.0000        \u001b[32m0.0357\u001b[0m       0.4479            0.4479        1.3386  0.0005  0.1179\n",
      "     47            1.0000        0.0471       0.4479            0.4479        1.3457  0.0004  0.1256\n",
      "     48            1.0000        0.0431       0.4479            0.4479        1.3526  0.0004  0.1484\n",
      "     49            1.0000        \u001b[32m0.0354\u001b[0m       0.4583            0.4583        1.3583  0.0003  0.1365\n",
      "     50            1.0000        0.0396       0.4583            0.4583        1.3632  0.0003  0.1415\n",
      "Fine tuning model for subject 8 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7600        0.9441       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0026\u001b[0m  0.0004  0.1432\n",
      "     32            0.7600        0.9254       0.5521            0.5521        1.0074  0.0005  0.1416\n",
      "     33            \u001b[36m0.8400\u001b[0m        \u001b[32m0.6972\u001b[0m       0.5833            0.5833        1.0140  0.0005  0.1494\n",
      "     34            \u001b[36m0.9200\u001b[0m        \u001b[32m0.5106\u001b[0m       0.5833            0.5833        1.0235  0.0006  0.1450\n",
      "     35            \u001b[36m0.9600\u001b[0m        \u001b[32m0.3749\u001b[0m       0.5729            0.5729        1.0375  0.0006  0.1442\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2653\u001b[0m       0.5625            0.5625        1.0504  0.0007  0.1409\n",
      "     37            1.0000        0.2732       0.5521            0.5521        1.0728  0.0007  0.1596\n",
      "     38            1.0000        \u001b[32m0.1695\u001b[0m       0.5521            0.5521        1.1085  0.0007  0.1456\n",
      "     39            1.0000        \u001b[32m0.1317\u001b[0m       0.4896            0.4896        1.1559  0.0007  0.1460\n",
      "     40            1.0000        \u001b[32m0.0958\u001b[0m       0.4896            0.4896        1.2107  0.0007  0.1494\n",
      "     41            1.0000        \u001b[32m0.0853\u001b[0m       0.4896            0.4896        1.2634  0.0007  0.1319\n",
      "     42            1.0000        \u001b[32m0.0602\u001b[0m       0.4792            0.4792        1.3072  0.0007  0.1513\n",
      "     43            1.0000        0.0715       0.4792            0.4792        1.3392  0.0006  0.1416\n",
      "     44            1.0000        \u001b[32m0.0372\u001b[0m       0.4688            0.4688        1.3571  0.0006  0.1441\n",
      "     45            1.0000        0.0557       0.4583            0.4583        1.3628  0.0005  0.1598\n",
      "     46            1.0000        0.0808       0.4583            0.4583        1.3478  0.0005  0.1544\n",
      "     47            1.0000        \u001b[32m0.0366\u001b[0m       0.4688            0.4688        1.3291  0.0004  0.1243\n",
      "     48            1.0000        0.0730       0.4792            0.4792        1.3099  0.0004  0.1320\n",
      "     49            1.0000        0.0686       0.4792            0.4792        1.2850  0.0003  0.1421\n",
      "     50            1.0000        0.0432       0.4792            0.4792        1.2638  0.0003  0.1571\n",
      "Fine tuning model for subject 8 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6400        0.9772       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m1.0075\u001b[0m  0.0004  0.1179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7600        0.7410       0.5833            0.5833        1.0245  0.0005  0.1247\n",
      "     33            \u001b[36m0.8800\u001b[0m        0.7382       0.5729            0.5729        1.0488  0.0005  0.1252\n",
      "     34            \u001b[36m0.9200\u001b[0m        \u001b[32m0.4962\u001b[0m       0.5729            0.5729        1.0862  0.0006  0.1237\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4465\u001b[0m       0.5521            0.5521        1.1395  0.0006  0.1256\n",
      "     36            1.0000        \u001b[32m0.2101\u001b[0m       0.5312            0.5312        1.2037  0.0007  0.1188\n",
      "     37            1.0000        \u001b[32m0.1880\u001b[0m       0.5104            0.5104        1.2688  0.0007  0.1120\n",
      "     38            1.0000        \u001b[32m0.1224\u001b[0m       0.5208            0.5208        1.3258  0.0007  0.1255\n",
      "     39            1.0000        \u001b[32m0.1093\u001b[0m       0.5104            0.5104        1.3735  0.0007  0.1187\n",
      "     40            1.0000        \u001b[32m0.1065\u001b[0m       0.4896            0.4896        1.4082  0.0007  0.1245\n",
      "     41            1.0000        \u001b[32m0.1038\u001b[0m       0.4792            0.4792        1.4326  0.0007  0.1272\n",
      "     42            1.0000        \u001b[32m0.0717\u001b[0m       0.4688            0.4688        1.4525  0.0007  0.1236\n",
      "     43            1.0000        \u001b[32m0.0459\u001b[0m       0.4583            0.4583        1.4666  0.0006  0.1180\n",
      "     44            1.0000        0.0707       0.4583            0.4583        1.4792  0.0006  0.1223\n",
      "     45            1.0000        0.0816       0.4583            0.4583        1.4905  0.0005  0.1180\n",
      "     46            1.0000        0.0582       0.4583            0.4583        1.4981  0.0005  0.1253\n",
      "     47            1.0000        \u001b[32m0.0357\u001b[0m       0.4583            0.4583        1.5026  0.0004  0.1178\n",
      "     48            1.0000        0.0409       0.4479            0.4479        1.5047  0.0004  0.1173\n",
      "     49            1.0000        0.0394       0.4479            0.4479        1.5042  0.0003  0.1212\n",
      "     50            1.0000        \u001b[32m0.0350\u001b[0m       0.4583            0.4583        1.5016  0.0003  0.1179\n",
      "Fine tuning model for subject 8 with 30 = 30 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6333        0.8583       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9988\u001b[0m  0.0004  0.1252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7333        0.9674       0.6042            0.6042        \u001b[94m0.9960\u001b[0m  0.0005  0.1280\n",
      "     33            \u001b[36m0.8333\u001b[0m        0.8499       0.6146            0.6146        1.0029  0.0005  0.1285\n",
      "     34            \u001b[36m0.9667\u001b[0m        \u001b[32m0.6296\u001b[0m       0.5729            0.5729        1.0270  0.0006  0.1337\n",
      "     35            0.9667        \u001b[32m0.5870\u001b[0m       0.5729            0.5729        1.0683  0.0006  0.1275\n",
      "     36            0.9667        \u001b[32m0.5052\u001b[0m       0.5521            0.5521        1.1083  0.0007  0.1321\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3194\u001b[0m       0.5417            0.5417        1.1386  0.0007  0.1257\n",
      "     38            1.0000        \u001b[32m0.1808\u001b[0m       0.5417            0.5417        1.1670  0.0007  0.1295\n",
      "     39            1.0000        \u001b[32m0.1282\u001b[0m       0.5104            0.5104        1.1884  0.0007  0.1333\n",
      "     40            1.0000        0.1540       0.5000            0.5000        1.2008  0.0007  0.1268\n",
      "     41            1.0000        \u001b[32m0.1180\u001b[0m       0.5104            0.5104        1.2153  0.0007  0.1266\n",
      "     42            1.0000        \u001b[32m0.0855\u001b[0m       0.5208            0.5208        1.2312  0.0007  0.1303\n",
      "     43            1.0000        0.1442       0.5104            0.5104        1.2408  0.0006  0.1333\n",
      "     44            1.0000        0.1275       0.5000            0.5000        1.2487  0.0006  0.1303\n",
      "     45            1.0000        0.0898       0.5000            0.5000        1.2547  0.0005  0.1255\n",
      "     46            1.0000        \u001b[32m0.0648\u001b[0m       0.5104            0.5104        1.2588  0.0005  0.1307\n",
      "     47            1.0000        \u001b[32m0.0562\u001b[0m       0.5208            0.5208        1.2617  0.0004  0.1307\n",
      "     48            1.0000        0.0729       0.5208            0.5208        1.2644  0.0004  0.1394\n",
      "     49            1.0000        0.0626       0.5208            0.5208        1.2650  0.0003  0.1302\n",
      "     50            1.0000        \u001b[32m0.0422\u001b[0m       0.5208            0.5208        1.2648  0.0003  0.1266\n",
      "Fine tuning model for subject 8 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        1.0435       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0013\u001b[0m  0.0004  0.1276\n",
      "     32            0.7667        1.1128       0.6042            0.6042        1.0053  0.0005  0.1327\n",
      "     33            \u001b[36m0.8333\u001b[0m        0.7583       0.5625            0.5625        1.0206  0.0005  0.1333\n",
      "     34            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5746\u001b[0m       0.5417            0.5417        1.0458  0.0006  0.1352\n",
      "     35            \u001b[36m0.9667\u001b[0m        \u001b[32m0.4898\u001b[0m       0.5000            0.5000        1.0645  0.0006  0.1289\n",
      "     36            0.9667        \u001b[32m0.4147\u001b[0m       0.5104            0.5104        1.0819  0.0007  0.1256\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2728\u001b[0m       0.5312            0.5312        1.1009  0.0007  0.1319\n",
      "     38            1.0000        \u001b[32m0.1893\u001b[0m       0.5417            0.5417        1.1221  0.0007  0.1373\n",
      "     39            1.0000        \u001b[32m0.1528\u001b[0m       0.5208            0.5208        1.1468  0.0007  0.1299\n",
      "     40            1.0000        \u001b[32m0.1266\u001b[0m       0.5312            0.5312        1.1740  0.0007  0.1314\n",
      "     41            1.0000        0.1336       0.5208            0.5208        1.2025  0.0007  0.1230\n",
      "     42            1.0000        \u001b[32m0.0774\u001b[0m       0.5208            0.5208        1.2271  0.0007  0.1297\n",
      "     43            1.0000        \u001b[32m0.0729\u001b[0m       0.5208            0.5208        1.2458  0.0006  0.1289\n",
      "     44            1.0000        \u001b[32m0.0707\u001b[0m       0.5208            0.5208        1.2560  0.0006  0.1313\n",
      "     45            1.0000        0.0789       0.5208            0.5208        1.2551  0.0005  0.1266\n",
      "     46            1.0000        \u001b[32m0.0615\u001b[0m       0.5208            0.5208        1.2548  0.0005  0.1353\n",
      "     47            1.0000        \u001b[32m0.0432\u001b[0m       0.5208            0.5208        1.2514  0.0004  0.1272\n",
      "     48            1.0000        0.0486       0.5104            0.5104        1.2477  0.0004  0.1318\n",
      "     49            1.0000        0.0446       0.5104            0.5104        1.2430  0.0003  0.1373\n",
      "     50            1.0000        0.0738       0.5208            0.5208        1.2397  0.0003  0.1295\n",
      "Fine tuning model for subject 8 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        0.9891       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m1.0019\u001b[0m  0.0004  0.1320\n",
      "     32            0.8000        0.8939       0.5521            0.5521        1.0105  0.0005  0.1412\n",
      "     33            \u001b[36m0.8667\u001b[0m        \u001b[32m0.5765\u001b[0m       0.5833            0.5833        1.0273  0.0005  0.1310\n",
      "     34            \u001b[36m0.9667\u001b[0m        0.5938       0.5625            0.5625        1.0532  0.0006  0.1309\n",
      "     35            0.9667        0.6026       0.5417            0.5417        1.0800  0.0006  0.1305\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4817\u001b[0m       0.5625            0.5625        1.1101  0.0007  0.1306\n",
      "     37            1.0000        \u001b[32m0.2919\u001b[0m       0.5208            0.5208        1.1388  0.0007  0.1328\n",
      "     38            1.0000        \u001b[32m0.2689\u001b[0m       0.4896            0.4896        1.1551  0.0007  0.1290\n",
      "     39            1.0000        \u001b[32m0.2022\u001b[0m       0.4896            0.4896        1.1594  0.0007  0.1325\n",
      "     40            1.0000        0.2100       0.5208            0.5208        1.1511  0.0007  0.1280\n",
      "     41            1.0000        \u001b[32m0.1387\u001b[0m       0.5312            0.5312        1.1418  0.0007  0.1255\n",
      "     42            1.0000        \u001b[32m0.1231\u001b[0m       0.5312            0.5312        1.1314  0.0007  0.1275\n",
      "     43            1.0000        \u001b[32m0.0944\u001b[0m       0.5208            0.5208        1.1224  0.0006  0.1335\n",
      "     44            1.0000        \u001b[32m0.0868\u001b[0m       0.5104            0.5104        1.1174  0.0006  0.1284\n",
      "     45            1.0000        0.0922       0.5208            0.5208        1.1148  0.0005  0.1303\n",
      "     46            1.0000        0.0890       0.5417            0.5417        1.1155  0.0005  0.1309\n",
      "     47            1.0000        \u001b[32m0.0573\u001b[0m       0.5625            0.5625        1.1176  0.0004  0.1249\n",
      "     48            1.0000        0.0953       0.5729            0.5729        1.1204  0.0004  0.1557\n",
      "     49            1.0000        0.0627       0.5729            0.5729        1.1227  0.0003  0.1538\n",
      "     50            1.0000        0.0691       0.5729            0.5729        1.1244  0.0003  0.1535\n",
      "Fine tuning model for subject 8 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7333        0.9360       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m0.9999\u001b[0m  0.0004  0.1401\n",
      "     32            0.7667        0.9059       0.5833            0.5833        1.0059  0.0005  0.1693\n",
      "     33            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6729\u001b[0m       0.5833            0.5833        1.0199  0.0005  0.1536\n",
      "     34            \u001b[36m0.9333\u001b[0m        \u001b[32m0.6431\u001b[0m       0.5729            0.5729        1.0441  0.0006  0.1599\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4510\u001b[0m       0.5417            0.5417        1.0745  0.0006  0.1417\n",
      "     36            1.0000        \u001b[32m0.3059\u001b[0m       0.5417            0.5417        1.1058  0.0007  0.1694\n",
      "     37            1.0000        \u001b[32m0.1648\u001b[0m       0.5312            0.5312        1.1309  0.0007  0.1459\n",
      "     38            1.0000        0.1779       0.5000            0.5000        1.1520  0.0007  0.1531\n",
      "     39            1.0000        \u001b[32m0.1192\u001b[0m       0.4896            0.4896        1.1664  0.0007  0.1561\n",
      "     40            1.0000        0.1489       0.4896            0.4896        1.1754  0.0007  0.1569\n",
      "     41            1.0000        \u001b[32m0.0958\u001b[0m       0.4792            0.4792        1.1818  0.0007  0.1909\n",
      "     42            1.0000        \u001b[32m0.0837\u001b[0m       0.4896            0.4896        1.1864  0.0007  0.1582\n",
      "     43            1.0000        0.0867       0.4896            0.4896        1.1908  0.0006  0.1656\n",
      "     44            1.0000        \u001b[32m0.0629\u001b[0m       0.4792            0.4792        1.1942  0.0006  0.1867\n",
      "     45            1.0000        \u001b[32m0.0600\u001b[0m       0.4792            0.4792        1.1972  0.0005  0.1275\n",
      "     46            1.0000        \u001b[32m0.0398\u001b[0m       0.4792            0.4792        1.2005  0.0005  0.1315\n",
      "     47            1.0000        0.0433       0.5000            0.5000        1.2041  0.0004  0.1320\n",
      "     48            1.0000        \u001b[32m0.0374\u001b[0m       0.5000            0.5000        1.2100  0.0004  0.1433\n",
      "     49            1.0000        0.0420       0.4896            0.4896        1.2152  0.0003  0.1332\n",
      "     50            1.0000        \u001b[32m0.0314\u001b[0m       0.4792            0.4792        1.2203  0.0003  0.1301\n",
      "Fine tuning model for subject 8 with 30 = 30 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        1.2401       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m1.0021\u001b[0m  0.0004  0.1218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        1.1155       0.5833            0.5833        1.0057  0.0005  0.1335\n",
      "     33            0.7000        0.9325       0.5729            0.5729        1.0187  0.0005  0.1249\n",
      "     34            0.8000        0.7780       0.5833            0.5833        1.0490  0.0006  0.1280\n",
      "     35            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6307\u001b[0m       0.5729            0.5729        1.0990  0.0006  0.1322\n",
      "     36            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5652\u001b[0m       0.5521            0.5521        1.1616  0.0007  0.1351\n",
      "     37            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3583\u001b[0m       0.5521            0.5521        1.2215  0.0007  0.1328\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2758\u001b[0m       0.5312            0.5312        1.2637  0.0007  0.1414\n",
      "     39            1.0000        \u001b[32m0.2164\u001b[0m       0.5312            0.5312        1.2922  0.0007  0.1569\n",
      "     40            1.0000        \u001b[32m0.2134\u001b[0m       0.5417            0.5417        1.3069  0.0007  0.1310\n",
      "     41            1.0000        \u001b[32m0.1825\u001b[0m       0.5312            0.5312        1.3123  0.0007  0.1270\n",
      "     42            1.0000        \u001b[32m0.1151\u001b[0m       0.5417            0.5417        1.3189  0.0007  0.1287\n",
      "     43            1.0000        \u001b[32m0.0967\u001b[0m       0.5312            0.5312        1.3241  0.0006  0.1259\n",
      "     44            1.0000        \u001b[32m0.0773\u001b[0m       0.5312            0.5312        1.3274  0.0006  0.1289\n",
      "     45            1.0000        0.0874       0.5208            0.5208        1.3315  0.0005  0.1274\n",
      "     46            1.0000        0.0991       0.5312            0.5312        1.3325  0.0005  0.1279\n",
      "     47            1.0000        \u001b[32m0.0440\u001b[0m       0.5312            0.5312        1.3320  0.0004  0.1320\n",
      "     48            1.0000        0.0782       0.5312            0.5312        1.3296  0.0004  0.1297\n",
      "     49            1.0000        0.0616       0.5312            0.5312        1.3255  0.0003  0.1277\n",
      "     50            1.0000        0.0614       0.5417            0.5417        1.3217  0.0003  0.1303\n",
      "Fine tuning model for subject 8 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        1.1748       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0007\u001b[0m  0.0004  0.1438\n",
      "     32            0.7667        0.9787       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.0057  0.0005  0.1431\n",
      "     33            \u001b[36m0.8333\u001b[0m        0.9582       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        1.0120  0.0005  0.1500\n",
      "     34            \u001b[36m0.8667\u001b[0m        0.7415       0.5938            0.5938        1.0250  0.0006  0.1315\n",
      "     35            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5764\u001b[0m       0.6146            0.6146        1.0440  0.0006  0.1302\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3628\u001b[0m       0.6146            0.6146        1.0716  0.0007  0.1281\n",
      "     37            1.0000        \u001b[32m0.3200\u001b[0m       0.5938            0.5938        1.1060  0.0007  0.1296\n",
      "     38            1.0000        0.3773       0.5625            0.5625        1.1455  0.0007  0.1394\n",
      "     39            1.0000        \u001b[32m0.2463\u001b[0m       0.5417            0.5417        1.1819  0.0007  0.1298\n",
      "     40            1.0000        \u001b[32m0.1537\u001b[0m       0.5104            0.5104        1.2156  0.0007  0.1352\n",
      "     41            1.0000        \u001b[32m0.1477\u001b[0m       0.5104            0.5104        1.2369  0.0007  0.1211\n",
      "     42            1.0000        \u001b[32m0.1312\u001b[0m       0.5104            0.5104        1.2466  0.0007  0.1250\n",
      "     43            1.0000        \u001b[32m0.0937\u001b[0m       0.5000            0.5000        1.2526  0.0006  0.1376\n",
      "     44            1.0000        0.1500       0.5000            0.5000        1.2453  0.0006  0.1331\n",
      "     45            1.0000        \u001b[32m0.0818\u001b[0m       0.5000            0.5000        1.2357  0.0005  0.1346\n",
      "     46            1.0000        0.0839       0.4896            0.4896        1.2319  0.0005  0.1291\n",
      "     47            1.0000        \u001b[32m0.0777\u001b[0m       0.5000            0.5000        1.2242  0.0004  0.1247\n",
      "     48            1.0000        \u001b[32m0.0496\u001b[0m       0.5104            0.5104        1.2140  0.0004  0.1270\n",
      "     49            1.0000        0.0514       0.5104            0.5104        1.2005  0.0003  0.1421\n",
      "     50            1.0000        \u001b[32m0.0421\u001b[0m       0.5208            0.5208        1.1858  0.0003  0.1534\n",
      "Fine tuning model for subject 8 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5667        1.0850       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9983\u001b[0m  0.0004  0.1966\n",
      "     32            0.6667        0.8174       0.5833            0.5833        \u001b[94m0.9969\u001b[0m  0.0005  0.1598\n",
      "     33            0.7667        0.7527       0.5833            0.5833        1.0049  0.0005  0.1335\n",
      "     34            \u001b[36m0.8667\u001b[0m        \u001b[32m0.5248\u001b[0m       0.6146            0.6146        1.0273  0.0006  0.1388\n",
      "     35            \u001b[36m0.9667\u001b[0m        \u001b[32m0.5094\u001b[0m       0.5938            0.5938        1.0588  0.0006  0.1332\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3980\u001b[0m       0.5625            0.5625        1.0945  0.0007  0.1263\n",
      "     37            1.0000        \u001b[32m0.2404\u001b[0m       0.5521            0.5521        1.1261  0.0007  0.1469\n",
      "     38            1.0000        0.2583       0.5312            0.5312        1.1616  0.0007  0.1515\n",
      "     39            1.0000        \u001b[32m0.2149\u001b[0m       0.5208            0.5208        1.2060  0.0007  0.1330\n",
      "     40            1.0000        \u001b[32m0.1458\u001b[0m       0.5104            0.5104        1.2490  0.0007  0.1297\n",
      "     41            1.0000        \u001b[32m0.1274\u001b[0m       0.5000            0.5000        1.2904  0.0007  0.1330\n",
      "     42            1.0000        0.1307       0.5000            0.5000        1.3233  0.0007  0.1335\n",
      "     43            1.0000        \u001b[32m0.1157\u001b[0m       0.4792            0.4792        1.3479  0.0006  0.1317\n",
      "     44            1.0000        \u001b[32m0.0652\u001b[0m       0.4896            0.4896        1.3695  0.0006  0.1283\n",
      "     45            1.0000        0.0719       0.4896            0.4896        1.3871  0.0005  0.1294\n",
      "     46            1.0000        \u001b[32m0.0594\u001b[0m       0.4688            0.4688        1.3977  0.0005  0.1309\n",
      "     47            1.0000        0.0795       0.4688            0.4688        1.4056  0.0004  0.1336\n",
      "     48            1.0000        \u001b[32m0.0576\u001b[0m       0.4583            0.4583        1.4094  0.0004  0.1278\n",
      "     49            1.0000        0.0729       0.4688            0.4688        1.4102  0.0003  0.1307\n",
      "     50            1.0000        \u001b[32m0.0454\u001b[0m       0.4583            0.4583        1.4102  0.0003  0.1224\n",
      "Fine tuning model for subject 8 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        1.0325       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9989\u001b[0m  0.0004  0.1430\n",
      "     32            0.7000        0.9733       0.6042            0.6042        \u001b[94m0.9966\u001b[0m  0.0005  0.1298\n",
      "     33            0.7333        0.9098       0.5833            0.5833        0.9974  0.0005  0.1364\n",
      "     34            \u001b[36m0.8667\u001b[0m        \u001b[32m0.7178\u001b[0m       0.5833            0.5833        1.0032  0.0006  0.1284\n",
      "     35            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4991\u001b[0m       0.5729            0.5729        1.0194  0.0006  0.1318\n",
      "     36            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3704\u001b[0m       0.5729            0.5729        1.0490  0.0007  0.1337\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2669\u001b[0m       0.5625            0.5625        1.0867  0.0007  0.1280\n",
      "     38            1.0000        \u001b[32m0.2214\u001b[0m       0.5312            0.5312        1.1237  0.0007  0.1311\n",
      "     39            1.0000        \u001b[32m0.1777\u001b[0m       0.5312            0.5312        1.1696  0.0007  0.1249\n",
      "     40            1.0000        \u001b[32m0.1395\u001b[0m       0.5104            0.5104        1.2070  0.0007  0.1315\n",
      "     41            1.0000        \u001b[32m0.1272\u001b[0m       0.4792            0.4792        1.2403  0.0007  0.1356\n",
      "     42            1.0000        \u001b[32m0.1007\u001b[0m       0.4896            0.4896        1.2665  0.0007  0.1718\n",
      "     43            1.0000        0.1174       0.4688            0.4688        1.2879  0.0006  0.1627\n",
      "     44            1.0000        \u001b[32m0.0826\u001b[0m       0.4688            0.4688        1.3027  0.0006  0.1502\n",
      "     45            1.0000        \u001b[32m0.0704\u001b[0m       0.4688            0.4688        1.3067  0.0005  0.1663\n",
      "     46            1.0000        \u001b[32m0.0608\u001b[0m       0.4792            0.4792        1.3040  0.0005  0.1680\n",
      "     47            1.0000        0.0768       0.4688            0.4688        1.2989  0.0004  0.1566\n",
      "     48            1.0000        \u001b[32m0.0487\u001b[0m       0.4688            0.4688        1.2899  0.0004  0.1660\n",
      "     49            1.0000        0.0576       0.4792            0.4792        1.2800  0.0003  0.1476\n",
      "     50            1.0000        \u001b[32m0.0483\u001b[0m       0.4688            0.4688        1.2684  0.0003  0.1549\n",
      "Fine tuning model for subject 8 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7333        0.9036       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9965\u001b[0m  0.0004  0.1498\n",
      "     32            \u001b[36m0.8333\u001b[0m        \u001b[32m0.6687\u001b[0m       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        0.9973  0.0005  0.1439\n",
      "     33            \u001b[36m0.9333\u001b[0m        \u001b[32m0.6145\u001b[0m       0.6146            0.6146        1.0095  0.0005  0.1722\n",
      "     34            0.9333        \u001b[32m0.4499\u001b[0m       0.6250            0.6250        1.0359  0.0006  0.1611\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3424\u001b[0m       0.6042            0.6042        1.0742  0.0006  0.1699\n",
      "     36            1.0000        \u001b[32m0.3037\u001b[0m       0.6042            0.6042        1.1242  0.0007  0.1494\n",
      "     37            1.0000        \u001b[32m0.1814\u001b[0m       0.6042            0.6042        1.1769  0.0007  0.1389\n",
      "     38            1.0000        \u001b[32m0.1810\u001b[0m       0.6042            0.6042        1.2241  0.0007  0.1667\n",
      "     39            1.0000        \u001b[32m0.1162\u001b[0m       0.5833            0.5833        1.2585  0.0007  0.1852\n",
      "     40            1.0000        \u001b[32m0.1129\u001b[0m       0.5729            0.5729        1.2847  0.0007  0.1328\n",
      "     41            1.0000        \u001b[32m0.0913\u001b[0m       0.5625            0.5625        1.3033  0.0007  0.1391\n",
      "     42            1.0000        \u001b[32m0.0630\u001b[0m       0.5729            0.5729        1.3182  0.0007  0.1331\n",
      "     43            1.0000        0.0680       0.5833            0.5833        1.3306  0.0006  0.1512\n",
      "     44            1.0000        0.0893       0.5833            0.5833        1.3363  0.0006  0.1311\n",
      "     45            1.0000        \u001b[32m0.0383\u001b[0m       0.5625            0.5625        1.3443  0.0005  0.1260\n",
      "     46            1.0000        \u001b[32m0.0383\u001b[0m       0.5521            0.5521        1.3531  0.0005  0.1300\n",
      "     47            1.0000        \u001b[32m0.0319\u001b[0m       0.5417            0.5417        1.3608  0.0004  0.1281\n",
      "     48            1.0000        0.0429       0.5417            0.5417        1.3677  0.0004  0.1330\n",
      "     49            1.0000        0.0416       0.5312            0.5312        1.3739  0.0003  0.1300\n",
      "     50            1.0000        \u001b[32m0.0307\u001b[0m       0.5312            0.5312        1.3788  0.0003  0.1282\n",
      "Fine tuning model for subject 8 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.0928       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9992\u001b[0m  0.0004  0.1296\n",
      "     32            0.7333        1.0103       0.5833            0.5833        1.0019  0.0005  0.1278\n",
      "     33            \u001b[36m0.9000\u001b[0m        1.0170       0.6042            0.6042        1.0183  0.0005  0.1277\n",
      "     34            \u001b[36m0.9667\u001b[0m        0.7441       0.5938            0.5938        1.0559  0.0006  0.1272\n",
      "     35            0.9667        \u001b[32m0.4954\u001b[0m       0.5729            0.5729        1.1133  0.0006  0.1294\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3920\u001b[0m       0.5625            0.5625        1.1726  0.0007  0.1259\n",
      "     37            0.9667        \u001b[32m0.2992\u001b[0m       0.5104            0.5104        1.2143  0.0007  0.1270\n",
      "     38            0.9667        \u001b[32m0.2647\u001b[0m       0.4896            0.4896        1.2395  0.0007  0.1332\n",
      "     39            0.9667        \u001b[32m0.1771\u001b[0m       0.4792            0.4792        1.2570  0.0007  0.1283\n",
      "     40            0.9667        0.1811       0.4688            0.4688        1.2637  0.0007  0.1241\n",
      "     41            0.9667        \u001b[32m0.1728\u001b[0m       0.4896            0.4896        1.2690  0.0007  0.1262\n",
      "     42            0.9667        \u001b[32m0.1542\u001b[0m       0.4896            0.4896        1.2729  0.0007  0.1243\n",
      "     43            0.9667        \u001b[32m0.1091\u001b[0m       0.5208            0.5208        1.2753  0.0006  0.1374\n",
      "     44            0.9667        \u001b[32m0.1009\u001b[0m       0.5104            0.5104        1.2749  0.0006  0.1286\n",
      "     45            0.9667        0.1043       0.5208            0.5208        1.2696  0.0005  0.1281\n",
      "     46            1.0000        \u001b[32m0.0916\u001b[0m       0.5104            0.5104        1.2620  0.0005  0.1311\n",
      "     47            1.0000        \u001b[32m0.0788\u001b[0m       0.5208            0.5208        1.2535  0.0004  0.1272\n",
      "     48            1.0000        \u001b[32m0.0468\u001b[0m       0.5104            0.5104        1.2453  0.0004  0.1223\n",
      "     49            1.0000        0.0730       0.5104            0.5104        1.2383  0.0003  0.1202\n",
      "     50            1.0000        0.0642       0.5000            0.5000        1.2325  0.0003  0.1271\n",
      "Fine tuning model for subject 8 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6286        1.5362       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m0.9998\u001b[0m  0.0004  0.1332\n",
      "     32            0.6857        1.3786       0.5938            0.5938        1.0102  0.0005  0.1338\n",
      "     33            0.8000        1.1276       0.5729            0.5729        1.0529  0.0005  0.1299\n",
      "     34            \u001b[36m0.8857\u001b[0m        0.8668       0.5417            0.5417        1.1320  0.0006  0.1384\n",
      "     35            0.8571        0.7981       0.5000            0.5000        1.2270  0.0006  0.1400\n",
      "     36            0.8857        \u001b[32m0.5374\u001b[0m       0.4583            0.4583        1.3115  0.0007  0.1344\n",
      "     37            \u001b[36m0.9143\u001b[0m        \u001b[32m0.3767\u001b[0m       0.4062            0.4062        1.3756  0.0007  0.1414\n",
      "     38            \u001b[36m0.9429\u001b[0m        \u001b[32m0.3178\u001b[0m       0.4167            0.4167        1.4190  0.0007  0.1278\n",
      "     39            \u001b[36m0.9714\u001b[0m        \u001b[32m0.2688\u001b[0m       0.4271            0.4271        1.4413  0.0007  0.1376\n",
      "     40            0.9714        \u001b[32m0.2169\u001b[0m       0.4271            0.4271        1.4558  0.0007  0.1354\n",
      "     41            0.9714        \u001b[32m0.1876\u001b[0m       0.4167            0.4167        1.4561  0.0007  0.1251\n",
      "     42            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1819\u001b[0m       0.4271            0.4271        1.4569  0.0007  0.1312\n",
      "     43            1.0000        \u001b[32m0.1462\u001b[0m       0.4375            0.4375        1.4658  0.0006  0.1496\n",
      "     44            1.0000        \u001b[32m0.1428\u001b[0m       0.4271            0.4271        1.4729  0.0006  0.1329\n",
      "     45            1.0000        \u001b[32m0.1380\u001b[0m       0.4271            0.4271        1.4808  0.0005  0.1378\n",
      "     46            1.0000        \u001b[32m0.1134\u001b[0m       0.4375            0.4375        1.4907  0.0005  0.1324\n",
      "     47            1.0000        \u001b[32m0.1028\u001b[0m       0.4271            0.4271        1.5006  0.0004  0.1332\n",
      "     48            1.0000        \u001b[32m0.0819\u001b[0m       0.4271            0.4271        1.5090  0.0004  0.1384\n",
      "     49            1.0000        \u001b[32m0.0610\u001b[0m       0.4375            0.4375        1.5164  0.0003  0.1407\n",
      "     50            1.0000        0.0757       0.4375            0.4375        1.5224  0.0003  0.1493\n",
      "Fine tuning model for subject 8 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6286        1.1285       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m1.0050\u001b[0m  0.0004  0.1670\n",
      "     32            0.7143        1.1016       0.5625            0.5625        1.0157  0.0005  0.1306\n",
      "     33            0.7714        1.0698       0.5729            0.5729        1.0348  0.0005  0.1340\n",
      "     34            \u001b[36m0.8286\u001b[0m        0.7972       0.5521            0.5521        1.0680  0.0006  0.1386\n",
      "     35            \u001b[36m0.8571\u001b[0m        \u001b[32m0.7041\u001b[0m       0.5521            0.5521        1.1149  0.0006  0.1362\n",
      "     36            \u001b[36m0.8857\u001b[0m        \u001b[32m0.5882\u001b[0m       0.4792            0.4792        1.1867  0.0007  0.1406\n",
      "     37            \u001b[36m0.9143\u001b[0m        \u001b[32m0.4668\u001b[0m       0.4479            0.4479        1.2785  0.0007  0.1369\n",
      "     38            \u001b[36m0.9714\u001b[0m        \u001b[32m0.3221\u001b[0m       0.4167            0.4167        1.3832  0.0007  0.1487\n",
      "     39            \u001b[36m1.0000\u001b[0m        0.3534       0.4375            0.4375        1.4846  0.0007  0.1376\n",
      "     40            1.0000        \u001b[32m0.2458\u001b[0m       0.4271            0.4271        1.5611  0.0007  0.1449\n",
      "     41            1.0000        0.2771       0.4479            0.4479        1.6275  0.0007  0.1765\n",
      "     42            1.0000        \u001b[32m0.1588\u001b[0m       0.4479            0.4479        1.6576  0.0007  0.1440\n",
      "     43            1.0000        0.1790       0.4167            0.4167        1.6534  0.0006  0.1427\n",
      "     44            1.0000        \u001b[32m0.1384\u001b[0m       0.4167            0.4167        1.6297  0.0006  0.1360\n",
      "     45            1.0000        \u001b[32m0.1234\u001b[0m       0.4167            0.4167        1.5984  0.0005  0.1331\n",
      "     46            1.0000        \u001b[32m0.1024\u001b[0m       0.4271            0.4271        1.5606  0.0005  0.1371\n",
      "     47            1.0000        \u001b[32m0.0775\u001b[0m       0.4583            0.4583        1.5244  0.0004  0.1415\n",
      "     48            1.0000        0.1007       0.4896            0.4896        1.4906  0.0004  0.1388\n",
      "     49            1.0000        0.0835       0.5000            0.5000        1.4609  0.0003  0.1382\n",
      "     50            1.0000        0.0796       0.5000            0.5000        1.4333  0.0003  0.1336\n",
      "Fine tuning model for subject 8 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6286        0.9261       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9984\u001b[0m  0.0004  0.1356\n",
      "     32            0.6857        0.7780       0.5833            0.5833        \u001b[94m0.9976\u001b[0m  0.0005  0.1376\n",
      "     33            0.7429        \u001b[32m0.6999\u001b[0m       0.6042            0.6042        1.0050  0.0005  0.1373\n",
      "     34            \u001b[36m0.8571\u001b[0m        \u001b[32m0.6442\u001b[0m       0.5938            0.5938        1.0231  0.0006  0.1479\n",
      "     35            \u001b[36m0.9429\u001b[0m        \u001b[32m0.4525\u001b[0m       0.5417            0.5417        1.0485  0.0006  0.1509\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3864\u001b[0m       0.5208            0.5208        1.0751  0.0007  0.1546\n",
      "     37            1.0000        \u001b[32m0.2307\u001b[0m       0.5208            0.5208        1.1036  0.0007  0.1634\n",
      "     38            1.0000        \u001b[32m0.2134\u001b[0m       0.5208            0.5208        1.1233  0.0007  0.1671\n",
      "     39            1.0000        \u001b[32m0.1634\u001b[0m       0.5417            0.5417        1.1397  0.0007  0.1476\n",
      "     40            1.0000        \u001b[32m0.1135\u001b[0m       0.5312            0.5312        1.1481  0.0007  0.1946\n",
      "     41            1.0000        \u001b[32m0.1002\u001b[0m       0.5104            0.5104        1.1559  0.0007  0.1618\n",
      "     42            1.0000        0.1476       0.5000            0.5000        1.1629  0.0007  0.1603\n",
      "     43            1.0000        \u001b[32m0.0880\u001b[0m       0.4896            0.4896        1.1675  0.0006  0.1507\n",
      "     44            1.0000        0.0892       0.4896            0.4896        1.1719  0.0006  0.1614\n",
      "     45            1.0000        \u001b[32m0.0557\u001b[0m       0.5000            0.5000        1.1765  0.0005  0.1620\n",
      "     46            1.0000        0.0831       0.5000            0.5000        1.1785  0.0005  0.1542\n",
      "     47            1.0000        0.0661       0.5000            0.5000        1.1798  0.0004  0.1613\n",
      "     48            1.0000        0.0632       0.5104            0.5104        1.1795  0.0004  0.1656\n",
      "     49            1.0000        \u001b[32m0.0501\u001b[0m       0.5104            0.5104        1.1798  0.0003  0.1709\n",
      "     50            1.0000        \u001b[32m0.0468\u001b[0m       0.5104            0.5104        1.1805  0.0003  0.1543\n",
      "Fine tuning model for subject 8 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5714        1.1517       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9999\u001b[0m  0.0004  0.1811\n",
      "     32            0.6286        1.2277       0.6042            0.6042        1.0032  0.0005  0.1399\n",
      "     33            0.7714        0.7974       0.5938            0.5938        1.0184  0.0005  0.1491\n",
      "     34            \u001b[36m0.8571\u001b[0m        0.8211       0.5417            0.5417        1.0471  0.0006  0.1386\n",
      "     35            \u001b[36m0.9143\u001b[0m        \u001b[32m0.5486\u001b[0m       0.5208            0.5208        1.0800  0.0006  0.1405\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4079\u001b[0m       0.5208            0.5208        1.1057  0.0007  0.1337\n",
      "     37            1.0000        \u001b[32m0.2661\u001b[0m       0.5312            0.5312        1.1268  0.0007  0.1293\n",
      "     38            1.0000        \u001b[32m0.2235\u001b[0m       0.5312            0.5312        1.1455  0.0007  0.1343\n",
      "     39            1.0000        \u001b[32m0.2040\u001b[0m       0.4896            0.4896        1.1675  0.0007  0.1435\n",
      "     40            1.0000        \u001b[32m0.1505\u001b[0m       0.4896            0.4896        1.1844  0.0007  0.1442\n",
      "     41            1.0000        \u001b[32m0.1095\u001b[0m       0.5104            0.5104        1.2021  0.0007  0.1384\n",
      "     42            1.0000        0.1161       0.5000            0.5000        1.2168  0.0007  0.1422\n",
      "     43            1.0000        \u001b[32m0.0972\u001b[0m       0.5104            0.5104        1.2281  0.0006  0.1334\n",
      "     44            1.0000        0.1063       0.5104            0.5104        1.2386  0.0006  0.1412\n",
      "     45            1.0000        \u001b[32m0.0823\u001b[0m       0.5104            0.5104        1.2499  0.0005  0.1387\n",
      "     46            1.0000        \u001b[32m0.0651\u001b[0m       0.5104            0.5104        1.2597  0.0005  0.1386\n",
      "     47            1.0000        \u001b[32m0.0548\u001b[0m       0.5104            0.5104        1.2666  0.0004  0.1374\n",
      "     48            1.0000        \u001b[32m0.0532\u001b[0m       0.5000            0.5000        1.2723  0.0004  0.1361\n",
      "     49            1.0000        \u001b[32m0.0470\u001b[0m       0.4792            0.4792        1.2770  0.0003  0.1349\n",
      "     50            1.0000        0.0897       0.4792            0.4792        1.2809  0.0003  0.1322\n",
      "Fine tuning model for subject 8 with 35 = 35 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7714        \u001b[32m0.5496\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9997\u001b[0m  0.0004  0.1344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        0.6832       0.5833            0.5833        1.0035  0.0005  0.1363\n",
      "     33            \u001b[36m0.8286\u001b[0m        0.6043       0.6042            0.6042        1.0099  0.0005  0.1372\n",
      "     34            \u001b[36m0.8857\u001b[0m        \u001b[32m0.5260\u001b[0m       0.5938            0.5938        1.0203  0.0006  0.1335\n",
      "     35            \u001b[36m0.9143\u001b[0m        \u001b[32m0.5140\u001b[0m       0.5938            0.5938        1.0311  0.0006  0.1366\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3495\u001b[0m       0.6146            0.6146        1.0458  0.0007  0.1330\n",
      "     37            1.0000        \u001b[32m0.2889\u001b[0m       0.6042            0.6042        1.0578  0.0007  0.1331\n",
      "     38            1.0000        \u001b[32m0.2264\u001b[0m       0.5833            0.5833        1.0709  0.0007  0.1321\n",
      "     39            1.0000        0.2331       0.5729            0.5729        1.0823  0.0007  0.1366\n",
      "     40            1.0000        \u001b[32m0.1641\u001b[0m       0.5625            0.5625        1.0912  0.0007  0.1371\n",
      "     41            1.0000        \u001b[32m0.0820\u001b[0m       0.5417            0.5417        1.0993  0.0007  0.1347\n",
      "     42            1.0000        0.1010       0.5521            0.5521        1.1070  0.0007  0.1327\n",
      "     43            1.0000        0.0847       0.5625            0.5625        1.1143  0.0006  0.1270\n",
      "     44            1.0000        \u001b[32m0.0732\u001b[0m       0.5521            0.5521        1.1213  0.0006  0.1364\n",
      "     45            1.0000        \u001b[32m0.0633\u001b[0m       0.5417            0.5417        1.1275  0.0005  0.1375\n",
      "     46            1.0000        \u001b[32m0.0625\u001b[0m       0.5417            0.5417        1.1330  0.0005  0.1418\n",
      "     47            1.0000        \u001b[32m0.0452\u001b[0m       0.5417            0.5417        1.1376  0.0004  0.1266\n",
      "     48            1.0000        0.0536       0.5417            0.5417        1.1415  0.0004  0.1477\n",
      "     49            1.0000        0.0535       0.5312            0.5312        1.1448  0.0003  0.1334\n",
      "     50            1.0000        \u001b[32m0.0377\u001b[0m       0.5417            0.5417        1.1477  0.0003  0.1317\n",
      "Fine tuning model for subject 8 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6286        1.1107       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m1.0002\u001b[0m  0.0004  0.1367\n",
      "     32            0.6286        0.8015       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0001\u001b[0m  0.0005  0.1320\n",
      "     33            0.8000        0.8899       0.5729            0.5729        1.0121  0.0005  0.1353\n",
      "     34            \u001b[36m0.9143\u001b[0m        \u001b[32m0.5096\u001b[0m       0.5833            0.5833        1.0330  0.0006  0.1371\n",
      "     35            \u001b[36m0.9714\u001b[0m        \u001b[32m0.4458\u001b[0m       0.5208            0.5208        1.0574  0.0006  0.1355\n",
      "     36            0.9714        \u001b[32m0.3377\u001b[0m       0.5104            0.5104        1.0717  0.0007  0.1298\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2471\u001b[0m       0.5208            0.5208        1.0842  0.0007  0.1319\n",
      "     38            1.0000        \u001b[32m0.2419\u001b[0m       0.5312            0.5312        1.0990  0.0007  0.1268\n",
      "     39            1.0000        \u001b[32m0.1256\u001b[0m       0.5312            0.5312        1.1221  0.0007  0.1378\n",
      "     40            1.0000        0.1589       0.5625            0.5625        1.1513  0.0007  0.1406\n",
      "     41            1.0000        \u001b[32m0.1066\u001b[0m       0.5208            0.5208        1.1839  0.0007  0.1321\n",
      "     42            1.0000        \u001b[32m0.0986\u001b[0m       0.5104            0.5104        1.2176  0.0007  0.1339\n",
      "     43            1.0000        0.1167       0.5104            0.5104        1.2500  0.0006  0.1294\n",
      "     44            1.0000        \u001b[32m0.0956\u001b[0m       0.5208            0.5208        1.2792  0.0006  0.1322\n",
      "     45            1.0000        \u001b[32m0.0925\u001b[0m       0.5312            0.5312        1.3007  0.0005  0.1352\n",
      "     46            1.0000        \u001b[32m0.0788\u001b[0m       0.5312            0.5312        1.3117  0.0005  0.1380\n",
      "     47            1.0000        \u001b[32m0.0748\u001b[0m       0.5312            0.5312        1.3107  0.0004  0.1350\n",
      "     48            1.0000        \u001b[32m0.0515\u001b[0m       0.5417            0.5417        1.3032  0.0004  0.1339\n",
      "     49            1.0000        0.0596       0.5312            0.5312        1.2927  0.0003  0.1309\n",
      "     50            1.0000        \u001b[32m0.0409\u001b[0m       0.5417            0.5417        1.2816  0.0003  0.1419\n",
      "Fine tuning model for subject 8 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6857        \u001b[32m0.6055\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9989\u001b[0m  0.0004  0.1406\n",
      "     32            0.7714        0.9339       0.5833            0.5833        0.9999  0.0005  0.1412\n",
      "     33            0.8000        0.6461       0.6042            0.6042        1.0027  0.0005  0.1403\n",
      "     34            \u001b[36m0.9143\u001b[0m        0.6517       0.5833            0.5833        1.0050  0.0006  0.1332\n",
      "     35            \u001b[36m0.9714\u001b[0m        \u001b[32m0.5549\u001b[0m       0.5625            0.5625        1.0088  0.0006  0.1337\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3920\u001b[0m       0.5625            0.5625        1.0163  0.0007  0.1335\n",
      "     37            1.0000        \u001b[32m0.2912\u001b[0m       0.5625            0.5625        1.0242  0.0007  0.1354\n",
      "     38            1.0000        \u001b[32m0.2140\u001b[0m       0.5729            0.5729        1.0384  0.0007  0.1332\n",
      "     39            1.0000        \u001b[32m0.1529\u001b[0m       0.5729            0.5729        1.0570  0.0007  0.1339\n",
      "     40            1.0000        0.1562       0.5729            0.5729        1.0750  0.0007  0.1661\n",
      "     41            1.0000        \u001b[32m0.1349\u001b[0m       0.5521            0.5521        1.0938  0.0007  0.1307\n",
      "     42            1.0000        \u001b[32m0.1157\u001b[0m       0.5417            0.5417        1.1092  0.0007  0.1412\n",
      "     43            1.0000        0.1504       0.5312            0.5312        1.1183  0.0006  0.1448\n",
      "     44            1.0000        \u001b[32m0.0633\u001b[0m       0.5312            0.5312        1.1269  0.0006  0.1478\n",
      "     45            1.0000        0.1020       0.5208            0.5208        1.1356  0.0005  0.1448\n",
      "     46            1.0000        0.0691       0.5208            0.5208        1.1402  0.0005  0.1630\n",
      "     47            1.0000        \u001b[32m0.0605\u001b[0m       0.5104            0.5104        1.1415  0.0004  0.1647\n",
      "     48            1.0000        0.0707       0.5104            0.5104        1.1407  0.0004  0.1560\n",
      "     49            1.0000        0.0630       0.5104            0.5104        1.1377  0.0003  0.1550\n",
      "     50            1.0000        0.0695       0.5104            0.5104        1.1334  0.0003  0.1432\n",
      "Fine tuning model for subject 8 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7429        0.9327       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0005\u001b[0m  0.0004  0.1785\n",
      "     32            \u001b[36m0.8571\u001b[0m        0.7603       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        1.0212  0.0005  0.1667\n",
      "     33            \u001b[36m0.9143\u001b[0m        \u001b[32m0.6790\u001b[0m       0.5729            0.5729        1.0750  0.0005  0.1719\n",
      "     34            \u001b[36m0.9714\u001b[0m        \u001b[32m0.6167\u001b[0m       0.5625            0.5625        1.1821  0.0006  0.1657\n",
      "     35            0.9714        \u001b[32m0.3020\u001b[0m       0.5000            0.5000        1.3153  0.0006  0.1560\n",
      "     36            0.9714        0.3666       0.4896            0.4896        1.4308  0.0007  0.1545\n",
      "     37            0.9714        \u001b[32m0.2541\u001b[0m       0.5000            0.5000        1.5210  0.0007  0.1677\n",
      "     38            \u001b[36m1.0000\u001b[0m        0.2727       0.4896            0.4896        1.5615  0.0007  0.1536\n",
      "     39            1.0000        \u001b[32m0.2522\u001b[0m       0.4792            0.4792        1.5551  0.0007  0.1624\n",
      "     40            1.0000        \u001b[32m0.1618\u001b[0m       0.4688            0.4688        1.5312  0.0007  0.1485\n",
      "     41            1.0000        \u001b[32m0.1368\u001b[0m       0.4792            0.4792        1.5023  0.0007  0.1604\n",
      "     42            1.0000        \u001b[32m0.0989\u001b[0m       0.4792            0.4792        1.4777  0.0007  0.1764\n",
      "     43            1.0000        0.1235       0.4688            0.4688        1.4522  0.0006  0.1377\n",
      "     44            1.0000        \u001b[32m0.0879\u001b[0m       0.4792            0.4792        1.4305  0.0006  0.1371\n",
      "     45            1.0000        \u001b[32m0.0803\u001b[0m       0.4792            0.4792        1.4072  0.0005  0.1536\n",
      "     46            1.0000        \u001b[32m0.0566\u001b[0m       0.4792            0.4792        1.3881  0.0005  0.1292\n",
      "     47            1.0000        0.0704       0.4792            0.4792        1.3721  0.0004  0.1329\n",
      "     48            1.0000        0.0636       0.4896            0.4896        1.3601  0.0004  0.1340\n",
      "     49            1.0000        0.0767       0.4896            0.4896        1.3499  0.0003  0.1387\n",
      "     50            1.0000        \u001b[32m0.0430\u001b[0m       0.4896            0.4896        1.3437  0.0003  0.1356\n",
      "Fine tuning model for subject 8 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7143        0.7735       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9943\u001b[0m  0.0004  0.1317\n",
      "     32            0.7143        0.8799       0.5938            0.5938        \u001b[94m0.9860\u001b[0m  0.0005  0.1350\n",
      "     33            0.7429        0.8391       0.5938            0.5938        \u001b[94m0.9771\u001b[0m  0.0005  0.1330\n",
      "     34            \u001b[36m0.8857\u001b[0m        \u001b[32m0.6280\u001b[0m       0.6042            0.6042        \u001b[94m0.9761\u001b[0m  0.0006  0.1425\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4526\u001b[0m       0.6042            0.6042        0.9909  0.0006  0.1448\n",
      "     36            1.0000        \u001b[32m0.3198\u001b[0m       0.5625            0.5625        1.0241  0.0007  0.1381\n",
      "     37            1.0000        \u001b[32m0.2865\u001b[0m       0.5625            0.5625        1.0605  0.0007  0.1344\n",
      "     38            1.0000        \u001b[32m0.2097\u001b[0m       0.5208            0.5208        1.0997  0.0007  0.1385\n",
      "     39            1.0000        \u001b[32m0.1747\u001b[0m       0.4792            0.4792        1.1352  0.0007  0.1304\n",
      "     40            1.0000        \u001b[32m0.1350\u001b[0m       0.4688            0.4688        1.1591  0.0007  0.1348\n",
      "     41            1.0000        0.1426       0.4688            0.4688        1.1765  0.0007  0.1336\n",
      "     42            1.0000        \u001b[32m0.0778\u001b[0m       0.4896            0.4896        1.1907  0.0007  0.1338\n",
      "     43            1.0000        \u001b[32m0.0638\u001b[0m       0.4688            0.4688        1.2011  0.0006  0.1357\n",
      "     44            1.0000        0.0903       0.4792            0.4792        1.2081  0.0006  0.1329\n",
      "     45            1.0000        \u001b[32m0.0594\u001b[0m       0.4792            0.4792        1.2101  0.0005  0.1364\n",
      "     46            1.0000        0.0654       0.5000            0.5000        1.2092  0.0005  0.1332\n",
      "     47            1.0000        \u001b[32m0.0485\u001b[0m       0.5000            0.5000        1.2053  0.0004  0.1333\n",
      "     48            1.0000        0.0577       0.5208            0.5208        1.2007  0.0004  0.1355\n",
      "     49            1.0000        0.0572       0.5104            0.5104        1.1954  0.0003  0.1335\n",
      "     50            1.0000        0.0593       0.4896            0.4896        1.1907  0.0003  0.1321\n",
      "Fine tuning model for subject 8 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6286        1.1066       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m0.9982\u001b[0m  0.0004  0.1379\n",
      "     32            0.6857        0.9478       0.5625            0.5625        1.0083  0.0005  0.1369\n",
      "     33            0.7143        0.8506       0.5833            0.5833        1.0457  0.0005  0.1447\n",
      "     34            \u001b[36m0.8857\u001b[0m        0.7908       0.5417            0.5417        1.1164  0.0006  0.1354\n",
      "     35            0.8857        \u001b[32m0.5598\u001b[0m       0.5625            0.5625        1.2087  0.0006  0.1310\n",
      "     36            \u001b[36m0.9143\u001b[0m        \u001b[32m0.4399\u001b[0m       0.5312            0.5312        1.2878  0.0007  0.1318\n",
      "     37            \u001b[36m0.9714\u001b[0m        \u001b[32m0.3169\u001b[0m       0.5104            0.5104        1.3281  0.0007  0.1334\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2839\u001b[0m       0.5000            0.5000        1.3332  0.0007  0.1388\n",
      "     39            1.0000        \u001b[32m0.1433\u001b[0m       0.5000            0.5000        1.3219  0.0007  0.1408\n",
      "     40            1.0000        \u001b[32m0.1186\u001b[0m       0.5208            0.5208        1.3112  0.0007  0.1338\n",
      "     41            1.0000        \u001b[32m0.1029\u001b[0m       0.5208            0.5208        1.3054  0.0007  0.1390\n",
      "     42            1.0000        0.1106       0.5312            0.5312        1.3064  0.0007  0.1415\n",
      "     43            1.0000        0.1296       0.5208            0.5208        1.3105  0.0006  0.1334\n",
      "     44            1.0000        \u001b[32m0.0918\u001b[0m       0.5104            0.5104        1.3150  0.0006  0.1342\n",
      "     45            1.0000        \u001b[32m0.0880\u001b[0m       0.5104            0.5104        1.3177  0.0005  0.1294\n",
      "     46            1.0000        \u001b[32m0.0683\u001b[0m       0.5104            0.5104        1.3177  0.0005  0.1400\n",
      "     47            1.0000        0.0689       0.5208            0.5208        1.3168  0.0004  0.1354\n",
      "     48            1.0000        0.0856       0.5104            0.5104        1.3127  0.0004  0.1787\n",
      "     49            1.0000        0.0801       0.5104            0.5104        1.3061  0.0003  0.1735\n",
      "     50            1.0000        \u001b[32m0.0617\u001b[0m       0.5000            0.5000        1.2985  0.0003  0.1386\n",
      "Fine tuning model for subject 8 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.2354       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m1.0012\u001b[0m  0.0004  0.1492\n",
      "     32            0.6500        1.2034       0.6042            0.6042        1.0055  0.0005  0.1448\n",
      "     33            0.7250        1.1497       0.6042            0.6042        1.0166  0.0005  0.1450\n",
      "     34            0.7500        1.0040       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        1.0369  0.0006  0.1469\n",
      "     35            \u001b[36m0.8500\u001b[0m        0.8116       0.6146            0.6146        1.0658  0.0006  0.1462\n",
      "     36            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5450\u001b[0m       0.6042            0.6042        1.1080  0.0007  0.1432\n",
      "     37            \u001b[36m0.9750\u001b[0m        \u001b[32m0.4277\u001b[0m       0.5625            0.5625        1.1663  0.0007  0.1440\n",
      "     38            0.9750        \u001b[32m0.3697\u001b[0m       0.5521            0.5521        1.2381  0.0007  0.1464\n",
      "     39            0.9750        \u001b[32m0.3119\u001b[0m       0.5208            0.5208        1.3263  0.0007  0.1480\n",
      "     40            0.9750        \u001b[32m0.3102\u001b[0m       0.4792            0.4792        1.4210  0.0007  0.1429\n",
      "     41            0.9500        \u001b[32m0.2261\u001b[0m       0.4583            0.4583        1.5004  0.0007  0.1609\n",
      "     42            0.9500        \u001b[32m0.1614\u001b[0m       0.4583            0.4583        1.5561  0.0007  0.1418\n",
      "     43            0.9500        0.1716       0.4479            0.4479        1.5804  0.0006  0.1489\n",
      "     44            0.9500        \u001b[32m0.1229\u001b[0m       0.4479            0.4479        1.5897  0.0006  0.1562\n",
      "     45            0.9750        0.1285       0.4583            0.4583        1.5898  0.0005  0.1525\n",
      "     46            0.9750        \u001b[32m0.1116\u001b[0m       0.4688            0.4688        1.5779  0.0005  0.1461\n",
      "     47            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1037\u001b[0m       0.4688            0.4688        1.5584  0.0004  0.1401\n",
      "     48            1.0000        \u001b[32m0.0898\u001b[0m       0.4792            0.4792        1.5293  0.0004  0.1524\n",
      "     49            1.0000        0.1130       0.4896            0.4896        1.4959  0.0003  0.1468\n",
      "     50            1.0000        \u001b[32m0.0872\u001b[0m       0.5000            0.5000        1.4606  0.0003  0.1433\n",
      "Fine tuning model for subject 8 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7750        0.7405       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9973\u001b[0m  0.0004  0.1409\n",
      "     32            0.7750        0.8500       0.6146            0.6146        \u001b[94m0.9941\u001b[0m  0.0005  0.1516\n",
      "     33            0.8000        0.8795       0.6042            0.6042        \u001b[94m0.9937\u001b[0m  0.0005  0.1477\n",
      "     34            \u001b[36m0.8500\u001b[0m        0.7245       0.6042            0.6042        1.0035  0.0006  0.1602\n",
      "     35            \u001b[36m0.8750\u001b[0m        \u001b[32m0.5574\u001b[0m       0.5938            0.5938        1.0283  0.0006  0.1607\n",
      "     36            \u001b[36m0.9250\u001b[0m        \u001b[32m0.4063\u001b[0m       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        1.0561  0.0007  0.1785\n",
      "     37            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3494\u001b[0m       0.6250            0.6250        1.0744  0.0007  0.1680\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2945\u001b[0m       0.6042            0.6042        1.0847  0.0007  0.1681\n",
      "     39            1.0000        \u001b[32m0.2412\u001b[0m       0.5938            0.5938        1.0888  0.0007  0.1724\n",
      "     40            1.0000        \u001b[32m0.1874\u001b[0m       0.5833            0.5833        1.0986  0.0007  0.1810\n",
      "     41            1.0000        \u001b[32m0.1702\u001b[0m       0.5833            0.5833        1.1074  0.0007  0.1647\n",
      "     42            1.0000        \u001b[32m0.1627\u001b[0m       0.5625            0.5625        1.1176  0.0007  0.1784\n",
      "     43            1.0000        \u001b[32m0.1222\u001b[0m       0.5625            0.5625        1.1299  0.0006  0.1763\n",
      "     44            1.0000        0.1276       0.5521            0.5521        1.1372  0.0006  0.1617\n",
      "     45            1.0000        \u001b[32m0.1024\u001b[0m       0.5521            0.5521        1.1395  0.0005  0.1656\n",
      "     46            1.0000        \u001b[32m0.0834\u001b[0m       0.5417            0.5417        1.1342  0.0005  0.1733\n",
      "     47            1.0000        \u001b[32m0.0729\u001b[0m       0.5521            0.5521        1.1274  0.0004  0.1640\n",
      "     48            1.0000        \u001b[32m0.0682\u001b[0m       0.5521            0.5521        1.1188  0.0004  0.1899\n",
      "     49            1.0000        \u001b[32m0.0539\u001b[0m       0.5625            0.5625        1.1109  0.0003  0.1586\n",
      "     50            1.0000        \u001b[32m0.0442\u001b[0m       0.5625            0.5625        1.1041  0.0003  0.2131\n",
      "Fine tuning model for subject 8 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        0.9758       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0000\u001b[0m  0.0004  0.1589\n",
      "     32            0.6750        0.8774       0.5833            0.5833        1.0082  0.0005  0.1819\n",
      "     33            0.7500        \u001b[32m0.7151\u001b[0m       0.5729            0.5729        1.0343  0.0005  0.1507\n",
      "     34            0.8000        \u001b[32m0.6816\u001b[0m       0.5625            0.5625        1.0732  0.0006  0.1490\n",
      "     35            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6482\u001b[0m       0.5521            0.5521        1.0992  0.0006  0.1401\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5046\u001b[0m       0.5625            0.5625        1.1071  0.0007  0.1467\n",
      "     37            1.0000        \u001b[32m0.3574\u001b[0m       0.5312            0.5312        1.1146  0.0007  0.1536\n",
      "     38            1.0000        \u001b[32m0.2540\u001b[0m       0.5104            0.5104        1.1361  0.0007  0.1472\n",
      "     39            0.9750        \u001b[32m0.2133\u001b[0m       0.5000            0.5000        1.1755  0.0007  0.1470\n",
      "     40            1.0000        \u001b[32m0.1980\u001b[0m       0.5104            0.5104        1.2319  0.0007  0.1498\n",
      "     41            1.0000        \u001b[32m0.1613\u001b[0m       0.5208            0.5208        1.2901  0.0007  0.1436\n",
      "     42            0.9750        \u001b[32m0.1175\u001b[0m       0.5312            0.5312        1.3526  0.0007  0.1639\n",
      "     43            0.9750        0.1338       0.5104            0.5104        1.4004  0.0006  0.1492\n",
      "     44            0.9750        0.1267       0.5104            0.5104        1.4387  0.0006  0.1447\n",
      "     45            0.9750        0.1365       0.4792            0.4792        1.4663  0.0005  0.1465\n",
      "     46            0.9750        0.1296       0.4896            0.4896        1.4836  0.0005  0.1494\n",
      "     47            0.9750        \u001b[32m0.0968\u001b[0m       0.5000            0.5000        1.4865  0.0004  0.1406\n",
      "     48            0.9750        \u001b[32m0.0664\u001b[0m       0.5000            0.5000        1.4782  0.0004  0.1451\n",
      "     49            0.9750        0.0723       0.4896            0.4896        1.4647  0.0003  0.1443\n",
      "     50            1.0000        \u001b[32m0.0659\u001b[0m       0.5000            0.5000        1.4463  0.0003  0.1409\n",
      "Fine tuning model for subject 8 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5250        1.2455       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9995\u001b[0m  0.0004  0.1427\n",
      "     32            0.5750        1.2497       0.6042            0.6042        1.0059  0.0005  0.1479\n",
      "     33            0.6250        1.1339       0.6042            0.6042        1.0363  0.0005  0.1490\n",
      "     34            0.7250        0.9816       0.5521            0.5521        1.1070  0.0006  0.1466\n",
      "     35            0.8000        0.7897       0.5000            0.5000        1.1991  0.0006  0.1447\n",
      "     36            \u001b[36m0.8750\u001b[0m        \u001b[32m0.6275\u001b[0m       0.5000            0.5000        1.3095  0.0007  0.1459\n",
      "     37            \u001b[36m0.9250\u001b[0m        \u001b[32m0.4490\u001b[0m       0.4792            0.4792        1.4150  0.0007  0.1414\n",
      "     38            0.9000        \u001b[32m0.3983\u001b[0m       0.4792            0.4792        1.4903  0.0007  0.1412\n",
      "     39            0.9250        \u001b[32m0.3217\u001b[0m       0.4583            0.4583        1.5251  0.0007  0.1483\n",
      "     40            0.9250        0.3362       0.4479            0.4479        1.5308  0.0007  0.1424\n",
      "     41            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2252\u001b[0m       0.4479            0.4479        1.5083  0.0007  0.1374\n",
      "     42            0.9500        \u001b[32m0.1882\u001b[0m       0.4479            0.4479        1.4790  0.0007  0.1434\n",
      "     43            \u001b[36m0.9750\u001b[0m        0.1938       0.4271            0.4271        1.4537  0.0006  0.1527\n",
      "     44            0.9750        \u001b[32m0.1499\u001b[0m       0.3958            0.3958        1.4327  0.0006  0.1441\n",
      "     45            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1338\u001b[0m       0.4271            0.4271        1.4095  0.0005  0.1454\n",
      "     46            1.0000        \u001b[32m0.1290\u001b[0m       0.4375            0.4375        1.3899  0.0005  0.1430\n",
      "     47            1.0000        \u001b[32m0.1255\u001b[0m       0.4479            0.4479        1.3669  0.0004  0.1436\n",
      "     48            1.0000        \u001b[32m0.0948\u001b[0m       0.4271            0.4271        1.3434  0.0004  0.1434\n",
      "     49            1.0000        0.1203       0.4271            0.4271        1.3204  0.0003  0.1415\n",
      "     50            1.0000        \u001b[32m0.0839\u001b[0m       0.4167            0.4167        1.2987  0.0003  0.1526\n",
      "Fine tuning model for subject 8 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.4007       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0004\u001b[0m  0.0004  0.1386\n",
      "     32            0.5250        1.3662       0.5625            0.5625        1.0106  0.0005  0.1403\n",
      "     33            0.6500        0.9842       0.5833            0.5833        1.0389  0.0005  0.1449\n",
      "     34            0.7750        0.9409       0.5312            0.5312        1.1002  0.0006  0.1493\n",
      "     35            \u001b[36m0.8500\u001b[0m        0.7409       0.5417            0.5417        1.1761  0.0006  0.1525\n",
      "     36            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5093\u001b[0m       0.5312            0.5312        1.2468  0.0007  0.1468\n",
      "     37            \u001b[36m0.9250\u001b[0m        \u001b[32m0.4895\u001b[0m       0.5000            0.5000        1.3056  0.0007  0.1402\n",
      "     38            \u001b[36m0.9500\u001b[0m        \u001b[32m0.3872\u001b[0m       0.4583            0.4583        1.3443  0.0007  0.1431\n",
      "     39            0.9500        \u001b[32m0.3475\u001b[0m       0.4375            0.4375        1.3754  0.0007  0.1422\n",
      "     40            0.9250        \u001b[32m0.2032\u001b[0m       0.4271            0.4271        1.4124  0.0007  0.1452\n",
      "     41            0.9250        0.2074       0.4167            0.4167        1.4328  0.0007  0.1436\n",
      "     42            0.9500        \u001b[32m0.2025\u001b[0m       0.4167            0.4167        1.4484  0.0007  0.1408\n",
      "     43            0.9250        \u001b[32m0.1348\u001b[0m       0.4271            0.4271        1.4557  0.0006  0.1444\n",
      "     44            0.9250        0.1611       0.4375            0.4375        1.4492  0.0006  0.1437\n",
      "     45            \u001b[36m0.9750\u001b[0m        \u001b[32m0.1005\u001b[0m       0.4375            0.4375        1.4312  0.0005  0.1485\n",
      "     46            0.9750        0.1225       0.4375            0.4375        1.4042  0.0005  0.1467\n",
      "     47            0.9750        0.1407       0.4479            0.4479        1.3714  0.0004  0.1439\n",
      "     48            0.9750        0.1078       0.4375            0.4375        1.3369  0.0004  0.1438\n",
      "     49            0.9750        0.1206       0.4271            0.4271        1.3047  0.0003  0.1404\n",
      "     50            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0821\u001b[0m       0.4688            0.4688        1.2792  0.0003  0.1492\n",
      "Fine tuning model for subject 8 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6750        1.0730       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m1.0037\u001b[0m  0.0004  0.1437\n",
      "     32            0.7500        0.9331       0.5833            0.5833        1.0075  0.0005  0.1462\n",
      "     33            0.7500        0.8479       0.5729            0.5729        1.0141  0.0005  0.1413\n",
      "     34            0.7750        0.7437       0.6042            0.6042        1.0307  0.0006  0.1433\n",
      "     35            \u001b[36m0.8500\u001b[0m        \u001b[32m0.6707\u001b[0m       0.5729            0.5729        1.0519  0.0006  0.1461\n",
      "     36            0.8500        \u001b[32m0.5280\u001b[0m       0.5729            0.5729        1.0775  0.0007  0.1464\n",
      "     37            \u001b[36m0.9250\u001b[0m        \u001b[32m0.3942\u001b[0m       0.5938            0.5938        1.1127  0.0007  0.1454\n",
      "     38            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3141\u001b[0m       0.6042            0.6042        1.1471  0.0007  0.1435\n",
      "     39            0.9750        \u001b[32m0.2786\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.1890  0.0007  0.1875\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2783\u001b[0m       0.6042            0.6042        1.2310  0.0007  0.1660\n",
      "     41            1.0000        \u001b[32m0.2496\u001b[0m       0.5938            0.5938        1.2703  0.0007  0.1627\n",
      "     42            1.0000        \u001b[32m0.1968\u001b[0m       0.5833            0.5833        1.3027  0.0007  0.1633\n",
      "     43            1.0000        \u001b[32m0.1733\u001b[0m       0.5729            0.5729        1.3316  0.0006  0.1555\n",
      "     44            1.0000        \u001b[32m0.1131\u001b[0m       0.5729            0.5729        1.3484  0.0006  0.1982\n",
      "     45            1.0000        0.1336       0.5729            0.5729        1.3567  0.0005  0.1699\n",
      "     46            1.0000        0.1411       0.5833            0.5833        1.3570  0.0005  0.1632\n",
      "     47            1.0000        0.1416       0.5729            0.5729        1.3512  0.0004  0.1646\n",
      "     48            1.0000        \u001b[32m0.0863\u001b[0m       0.5729            0.5729        1.3389  0.0004  0.1746\n",
      "     49            1.0000        0.1094       0.5729            0.5729        1.3244  0.0003  0.1614\n",
      "     50            1.0000        \u001b[32m0.0693\u001b[0m       0.5833            0.5833        1.3081  0.0003  0.1653\n",
      "Fine tuning model for subject 8 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6500        0.9946       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0012\u001b[0m  0.0004  0.1591\n",
      "     32            0.6750        0.8712       0.5833            0.5833        1.0118  0.0005  0.1706\n",
      "     33            0.8000        1.0043       0.5938            0.5938        1.0441  0.0005  0.1628\n",
      "     34            \u001b[36m0.8750\u001b[0m        \u001b[32m0.6676\u001b[0m       0.5312            0.5312        1.1139  0.0006  0.1660\n",
      "     35            \u001b[36m0.9250\u001b[0m        \u001b[32m0.5841\u001b[0m       0.5104            0.5104        1.2026  0.0006  0.2209\n",
      "     36            0.9000        \u001b[32m0.4796\u001b[0m       0.4792            0.4792        1.2822  0.0007  0.1458\n",
      "     37            0.9250        \u001b[32m0.4345\u001b[0m       0.4792            0.4792        1.3335  0.0007  0.1633\n",
      "     38            \u001b[36m0.9500\u001b[0m        \u001b[32m0.3173\u001b[0m       0.4583            0.4583        1.3743  0.0007  0.1464\n",
      "     39            \u001b[36m0.9750\u001b[0m        0.3204       0.4583            0.4583        1.3696  0.0007  0.1457\n",
      "     40            0.9750        \u001b[32m0.2970\u001b[0m       0.4688            0.4688        1.3632  0.0007  0.1436\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2531\u001b[0m       0.4792            0.4792        1.3533  0.0007  0.1425\n",
      "     42            1.0000        \u001b[32m0.2414\u001b[0m       0.4792            0.4792        1.3432  0.0007  0.1450\n",
      "     43            1.0000        \u001b[32m0.2165\u001b[0m       0.5000            0.5000        1.3364  0.0006  0.1419\n",
      "     44            1.0000        \u001b[32m0.1414\u001b[0m       0.5000            0.5000        1.3296  0.0006  0.1451\n",
      "     45            1.0000        0.1478       0.4896            0.4896        1.3261  0.0005  0.1440\n",
      "     46            1.0000        \u001b[32m0.1024\u001b[0m       0.4896            0.4896        1.3217  0.0005  0.1443\n",
      "     47            1.0000        0.1191       0.5000            0.5000        1.3165  0.0004  0.1554\n",
      "     48            1.0000        0.1140       0.5000            0.5000        1.3092  0.0004  0.1488\n",
      "     49            1.0000        \u001b[32m0.0832\u001b[0m       0.5000            0.5000        1.3022  0.0003  0.1497\n",
      "     50            1.0000        0.1101       0.5000            0.5000        1.2954  0.0003  0.1432\n",
      "Fine tuning model for subject 8 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7500        0.9207       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        \u001b[94m0.9949\u001b[0m  0.0004  0.1448\n",
      "     32            0.7500        0.8760       0.5833            0.5833        \u001b[94m0.9935\u001b[0m  0.0005  0.1438\n",
      "     33            0.8000        0.8428       0.5833            0.5833        0.9956  0.0005  0.1433\n",
      "     34            \u001b[36m0.9000\u001b[0m        0.7382       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        0.9994  0.0006  0.1492\n",
      "     35            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5395\u001b[0m       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        1.0066  0.0006  0.1406\n",
      "     36            0.9500        \u001b[32m0.3339\u001b[0m       0.6042            0.6042        1.0170  0.0007  0.1522\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2814\u001b[0m       0.5833            0.5833        1.0222  0.0007  0.1573\n",
      "     38            1.0000        0.2946       0.5625            0.5625        1.0229  0.0007  0.1588\n",
      "     39            1.0000        0.2940       0.5729            0.5729        1.0197  0.0007  0.1681\n",
      "     40            1.0000        \u001b[32m0.1748\u001b[0m       0.5625            0.5625        1.0151  0.0007  0.1445\n",
      "     41            1.0000        0.1799       0.5521            0.5521        1.0112  0.0007  0.1435\n",
      "     42            1.0000        \u001b[32m0.1510\u001b[0m       0.5417            0.5417        1.0108  0.0007  0.1418\n",
      "     43            1.0000        \u001b[32m0.1347\u001b[0m       0.5521            0.5521        1.0121  0.0006  0.1452\n",
      "     44            1.0000        0.1413       0.5625            0.5625        1.0100  0.0006  0.1592\n",
      "     45            1.0000        \u001b[32m0.0996\u001b[0m       0.5417            0.5417        1.0056  0.0005  0.1495\n",
      "     46            1.0000        0.1241       0.5312            0.5312        0.9991  0.0005  0.1454\n",
      "     47            1.0000        0.1397       0.5417            0.5417        \u001b[94m0.9918\u001b[0m  0.0004  0.1437\n",
      "     48            1.0000        0.1018       0.5417            0.5417        \u001b[94m0.9849\u001b[0m  0.0004  0.1611\n",
      "     49            1.0000        \u001b[32m0.0606\u001b[0m       0.5417            0.5417        \u001b[94m0.9795\u001b[0m  0.0003  0.1445\n",
      "     50            1.0000        0.0710       0.5417            0.5417        \u001b[94m0.9753\u001b[0m  0.0003  0.1452\n",
      "Fine tuning model for subject 8 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5750        1.2540       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9978\u001b[0m  0.0004  0.1396\n",
      "     32            0.6500        1.1793       0.5938            0.5938        \u001b[94m0.9962\u001b[0m  0.0005  0.1437\n",
      "     33            0.7500        0.9499       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.0007  0.0005  0.1481\n",
      "     34            \u001b[36m0.8500\u001b[0m        0.8646       0.6146            0.6146        1.0092  0.0006  0.1482\n",
      "     35            \u001b[36m0.9500\u001b[0m        0.7716       0.6042            0.6042        1.0184  0.0006  0.1426\n",
      "     36            \u001b[36m0.9750\u001b[0m        \u001b[32m0.6426\u001b[0m       0.5312            0.5312        1.0316  0.0007  0.1408\n",
      "     37            0.9750        \u001b[32m0.4721\u001b[0m       0.5833            0.5833        1.0473  0.0007  0.1390\n",
      "     38            0.9750        \u001b[32m0.3851\u001b[0m       0.6042            0.6042        1.0750  0.0007  0.1429\n",
      "     39            0.9750        \u001b[32m0.2972\u001b[0m       0.5417            0.5417        1.1210  0.0007  0.1446\n",
      "     40            0.9500        \u001b[32m0.2670\u001b[0m       0.5417            0.5417        1.1674  0.0007  0.1436\n",
      "     41            0.9500        \u001b[32m0.2101\u001b[0m       0.5417            0.5417        1.1968  0.0007  0.1418\n",
      "     42            0.9750        \u001b[32m0.1710\u001b[0m       0.5104            0.5104        1.2102  0.0007  0.1470\n",
      "     43            0.9750        \u001b[32m0.1651\u001b[0m       0.5104            0.5104        1.2069  0.0006  0.1400\n",
      "     44            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1317\u001b[0m       0.5312            0.5312        1.1947  0.0006  0.1429\n",
      "     45            1.0000        0.1329       0.5312            0.5312        1.1780  0.0005  0.1449\n",
      "     46            1.0000        \u001b[32m0.1134\u001b[0m       0.5521            0.5521        1.1666  0.0005  0.1462\n",
      "     47            1.0000        \u001b[32m0.0913\u001b[0m       0.5417            0.5417        1.1563  0.0004  0.1508\n",
      "     48            1.0000        \u001b[32m0.0738\u001b[0m       0.5208            0.5208        1.1463  0.0004  0.1391\n",
      "     49            1.0000        \u001b[32m0.0718\u001b[0m       0.5312            0.5312        1.1371  0.0003  0.1408\n",
      "     50            1.0000        0.0957       0.5208            0.5208        1.1291  0.0003  0.1458\n",
      "Fine tuning model for subject 8 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6500        1.1870       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9976\u001b[0m  0.0004  0.1436\n",
      "     32            0.6750        0.9143       0.5938            0.5938        \u001b[94m0.9920\u001b[0m  0.0005  0.1379\n",
      "     33            0.7250        0.8314       0.6146            0.6146        \u001b[94m0.9891\u001b[0m  0.0005  0.1391\n",
      "     34            \u001b[36m0.8500\u001b[0m        0.7695       0.6042            0.6042        1.0020  0.0006  0.1492\n",
      "     35            0.8500        \u001b[32m0.6700\u001b[0m       0.5729            0.5729        1.0408  0.0006  0.1470\n",
      "     36            \u001b[36m0.8750\u001b[0m        \u001b[32m0.4378\u001b[0m       0.5521            0.5521        1.1018  0.0007  0.1479\n",
      "     37            0.8750        \u001b[32m0.3034\u001b[0m       0.5417            0.5417        1.1693  0.0007  0.1435\n",
      "     38            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2494\u001b[0m       0.5104            0.5104        1.2292  0.0007  0.1467\n",
      "     39            \u001b[36m0.9750\u001b[0m        0.2790       0.4896            0.4896        1.2795  0.0007  0.1484\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1589\u001b[0m       0.4583            0.4583        1.3096  0.0007  0.1413\n",
      "     41            1.0000        0.2043       0.4583            0.4583        1.3216  0.0007  0.1397\n",
      "     42            1.0000        \u001b[32m0.1523\u001b[0m       0.4375            0.4375        1.3181  0.0007  0.1427\n",
      "     43            1.0000        \u001b[32m0.0932\u001b[0m       0.4375            0.4375        1.3031  0.0006  0.1546\n",
      "     44            1.0000        \u001b[32m0.0896\u001b[0m       0.4583            0.4583        1.2866  0.0006  0.1638\n",
      "     45            1.0000        0.1241       0.4583            0.4583        1.2725  0.0005  0.1705\n",
      "     46            1.0000        \u001b[32m0.0754\u001b[0m       0.4688            0.4688        1.2587  0.0005  0.1704\n",
      "     47            1.0000        0.0811       0.4792            0.4792        1.2447  0.0004  0.1696\n",
      "     48            1.0000        0.0817       0.5000            0.5000        1.2304  0.0004  0.1806\n",
      "     49            1.0000        \u001b[32m0.0651\u001b[0m       0.5000            0.5000        1.2180  0.0003  0.1718\n",
      "     50            1.0000        \u001b[32m0.0443\u001b[0m       0.4896            0.4896        1.2069  0.0003  0.1689\n",
      "Fine tuning model for subject 8 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5556        1.2577       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9992\u001b[0m  0.0004  0.2026\n",
      "     32            0.6000        1.0011       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        1.0095  0.0005  0.2489\n",
      "     33            0.6889        0.9760       0.5938            0.5938        1.0478  0.0005  0.1674\n",
      "     34            \u001b[36m0.8222\u001b[0m        0.9796       0.5938            0.5938        1.1274  0.0006  0.1907\n",
      "     35            \u001b[36m0.8667\u001b[0m        0.8247       0.5417            0.5417        1.2257  0.0006  0.1854\n",
      "     36            \u001b[36m0.9111\u001b[0m        \u001b[32m0.6072\u001b[0m       0.4896            0.4896        1.3145  0.0007  0.1952\n",
      "     37            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3915\u001b[0m       0.4479            0.4479        1.3819  0.0007  0.1662\n",
      "     38            \u001b[36m0.9556\u001b[0m        \u001b[32m0.3880\u001b[0m       0.4479            0.4479        1.4307  0.0007  0.2186\n",
      "     39            0.9333        \u001b[32m0.2630\u001b[0m       0.4375            0.4375        1.4741  0.0007  0.1495\n",
      "     40            0.9556        0.2782       0.4271            0.4271        1.5054  0.0007  0.1600\n",
      "     41            0.9556        0.2899       0.4271            0.4271        1.5320  0.0007  0.1526\n",
      "     42            0.9556        0.2716       0.4271            0.4271        1.5483  0.0007  0.1477\n",
      "     43            0.9333        \u001b[32m0.1933\u001b[0m       0.4062            0.4062        1.5578  0.0006  0.1491\n",
      "     44            0.9333        \u001b[32m0.1639\u001b[0m       0.4271            0.4271        1.5644  0.0006  0.1546\n",
      "     45            0.9333        0.1689       0.4167            0.4167        1.5656  0.0005  0.1570\n",
      "     46            0.9333        \u001b[32m0.1416\u001b[0m       0.4167            0.4167        1.5604  0.0005  0.1493\n",
      "     47            0.9333        \u001b[32m0.1373\u001b[0m       0.4167            0.4167        1.5491  0.0004  0.1467\n",
      "     48            0.9333        0.1429       0.4167            0.4167        1.5332  0.0004  0.1528\n",
      "     49            \u001b[36m0.9778\u001b[0m        \u001b[32m0.1218\u001b[0m       0.4062            0.4062        1.5154  0.0003  0.1510\n",
      "     50            0.9778        0.1419       0.4062            0.4062        1.4961  0.0003  0.1650\n",
      "Fine tuning model for subject 8 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5778        1.0890       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9987\u001b[0m  0.0004  0.1488\n",
      "     32            0.6444        0.9171       0.5833            0.5833        0.9988  0.0005  0.1609\n",
      "     33            0.6889        0.9196       0.5833            0.5833        1.0053  0.0005  0.1492\n",
      "     34            0.8000        0.7460       0.5833            0.5833        1.0139  0.0006  0.1581\n",
      "     35            \u001b[36m0.9556\u001b[0m        \u001b[32m0.6706\u001b[0m       0.5833            0.5833        1.0200  0.0006  0.1510\n",
      "     36            \u001b[36m0.9778\u001b[0m        \u001b[32m0.4841\u001b[0m       0.5938            0.5938        1.0222  0.0007  0.1495\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4006\u001b[0m       0.6042            0.6042        1.0243  0.0007  0.1540\n",
      "     38            1.0000        \u001b[32m0.2344\u001b[0m       0.5625            0.5625        1.0293  0.0007  0.1543\n",
      "     39            1.0000        0.2657       0.5625            0.5625        1.0331  0.0007  0.1554\n",
      "     40            1.0000        \u001b[32m0.1963\u001b[0m       0.5729            0.5729        1.0410  0.0007  0.1515\n",
      "     41            1.0000        0.2517       0.5729            0.5729        1.0502  0.0007  0.1532\n",
      "     42            1.0000        \u001b[32m0.1778\u001b[0m       0.5521            0.5521        1.0603  0.0007  0.1468\n",
      "     43            1.0000        \u001b[32m0.1250\u001b[0m       0.5521            0.5521        1.0689  0.0006  0.1508\n",
      "     44            1.0000        0.1526       0.5417            0.5417        1.0754  0.0006  0.1490\n",
      "     45            1.0000        \u001b[32m0.1119\u001b[0m       0.5312            0.5312        1.0814  0.0005  0.1500\n",
      "     46            1.0000        0.1255       0.5208            0.5208        1.0879  0.0005  0.1544\n",
      "     47            1.0000        \u001b[32m0.0772\u001b[0m       0.5104            0.5104        1.0946  0.0004  0.1550\n",
      "     48            1.0000        0.0869       0.5104            0.5104        1.1009  0.0004  0.1564\n",
      "     49            1.0000        0.0847       0.5208            0.5208        1.1062  0.0003  0.1585\n",
      "     50            1.0000        0.0853       0.5312            0.5312        1.1099  0.0003  0.1534\n",
      "Fine tuning model for subject 8 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6222        1.0389       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0037\u001b[0m  0.0004  0.1645\n",
      "     32            0.6667        1.1864       0.5729            0.5729        1.0077  0.0005  0.1494\n",
      "     33            0.7111        0.9961       0.5625            0.5625        1.0210  0.0005  0.1497\n",
      "     34            0.7111        0.8061       0.5521            0.5521        1.0370  0.0006  0.1493\n",
      "     35            0.7556        \u001b[32m0.7054\u001b[0m       0.5625            0.5625        1.0569  0.0006  0.1563\n",
      "     36            \u001b[36m0.8889\u001b[0m        \u001b[32m0.5055\u001b[0m       0.5625            0.5625        1.0803  0.0007  0.1510\n",
      "     37            \u001b[36m0.9778\u001b[0m        \u001b[32m0.4870\u001b[0m       0.5729            0.5729        1.1041  0.0007  0.1569\n",
      "     38            0.9778        \u001b[32m0.4399\u001b[0m       0.5729            0.5729        1.1289  0.0007  0.1541\n",
      "     39            0.9778        \u001b[32m0.3270\u001b[0m       0.5729            0.5729        1.1490  0.0007  0.1561\n",
      "     40            0.9556        0.3422       0.5625            0.5625        1.1653  0.0007  0.1623\n",
      "     41            0.9778        \u001b[32m0.2751\u001b[0m       0.5312            0.5312        1.1786  0.0007  0.1674\n",
      "     42            0.9778        \u001b[32m0.2411\u001b[0m       0.5312            0.5312        1.1948  0.0007  0.1765\n",
      "     43            0.9778        \u001b[32m0.2124\u001b[0m       0.5312            0.5312        1.2133  0.0006  0.1534\n",
      "     44            0.9778        \u001b[32m0.2004\u001b[0m       0.5417            0.5417        1.2225  0.0006  0.1706\n",
      "     45            0.9778        \u001b[32m0.1360\u001b[0m       0.5417            0.5417        1.2207  0.0005  0.1548\n",
      "     46            0.9778        0.1427       0.5521            0.5521        1.2132  0.0005  0.1558\n",
      "     47            0.9778        \u001b[32m0.1108\u001b[0m       0.5625            0.5625        1.2004  0.0004  0.1596\n",
      "     48            0.9778        0.1333       0.5833            0.5833        1.1850  0.0004  0.1548\n",
      "     49            0.9778        0.1411       0.5833            0.5833        1.1685  0.0003  0.1516\n",
      "     50            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0910\u001b[0m       0.5833            0.5833        1.1540  0.0003  0.1550\n",
      "Fine tuning model for subject 8 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6222        1.1480       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        \u001b[94m0.9990\u001b[0m  0.0004  0.1553\n",
      "     32            0.6444        1.0396       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.0024  0.0005  0.1524\n",
      "     33            0.7333        0.9347       0.5938            0.5938        1.0240  0.0005  0.1568\n",
      "     34            \u001b[36m0.8444\u001b[0m        0.7731       0.5417            0.5417        1.0774  0.0006  0.1599\n",
      "     35            \u001b[36m0.9111\u001b[0m        \u001b[32m0.5977\u001b[0m       0.5000            0.5000        1.1602  0.0006  0.1622\n",
      "     36            \u001b[36m0.9778\u001b[0m        \u001b[32m0.5434\u001b[0m       0.4583            0.4583        1.2543  0.0007  0.1592\n",
      "     37            0.9778        \u001b[32m0.3643\u001b[0m       0.4479            0.4479        1.3445  0.0007  0.1496\n",
      "     38            0.9778        \u001b[32m0.3526\u001b[0m       0.4375            0.4375        1.4257  0.0007  0.1551\n",
      "     39            0.9556        \u001b[32m0.2437\u001b[0m       0.4271            0.4271        1.4957  0.0007  0.1449\n",
      "     40            0.9556        \u001b[32m0.2419\u001b[0m       0.4167            0.4167        1.5417  0.0007  0.1489\n",
      "     41            0.9556        \u001b[32m0.2192\u001b[0m       0.3854            0.3854        1.5760  0.0007  0.1535\n",
      "     42            0.9556        0.2671       0.3646            0.3646        1.6041  0.0007  0.1530\n",
      "     43            0.9778        \u001b[32m0.1764\u001b[0m       0.3542            0.3542        1.6178  0.0006  0.1722\n",
      "     44            0.9778        \u001b[32m0.1671\u001b[0m       0.3438            0.3438        1.6141  0.0006  0.1724\n",
      "     45            0.9778        \u001b[32m0.1444\u001b[0m       0.3438            0.3438        1.6090  0.0005  0.1668\n",
      "     46            0.9778        \u001b[32m0.1133\u001b[0m       0.3438            0.3438        1.5980  0.0005  0.1624\n",
      "     47            0.9778        0.1178       0.3438            0.3438        1.5780  0.0004  0.1747\n",
      "     48            \u001b[36m1.0000\u001b[0m        0.1151       0.3646            0.3646        1.5522  0.0004  0.1839\n",
      "     49            1.0000        \u001b[32m0.0843\u001b[0m       0.3854            0.3854        1.5243  0.0003  0.1773\n",
      "     50            1.0000        0.1114       0.3958            0.3958        1.4993  0.0003  0.1684\n",
      "Fine tuning model for subject 8 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        0.9421       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m1.0009\u001b[0m  0.0004  0.1692\n",
      "     32            0.7111        0.8850       0.5833            0.5833        1.0054  0.0005  0.1897\n",
      "     33            0.8000        0.7931       0.5625            0.5625        1.0158  0.0005  0.1620\n",
      "     34            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5774\u001b[0m       0.5521            0.5521        1.0385  0.0006  0.1656\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5317\u001b[0m       0.5417            0.5417        1.0746  0.0006  0.1681\n",
      "     36            1.0000        \u001b[32m0.3574\u001b[0m       0.5312            0.5312        1.1237  0.0007  0.1625\n",
      "     37            1.0000        \u001b[32m0.3018\u001b[0m       0.5208            0.5208        1.1754  0.0007  0.1936\n",
      "     38            1.0000        \u001b[32m0.2860\u001b[0m       0.5104            0.5104        1.2186  0.0007  0.1725\n",
      "     39            1.0000        \u001b[32m0.2086\u001b[0m       0.4896            0.4896        1.2493  0.0007  0.2127\n",
      "     40            1.0000        \u001b[32m0.1746\u001b[0m       0.4896            0.4896        1.2765  0.0007  0.2008\n",
      "     41            1.0000        0.1882       0.4792            0.4792        1.2935  0.0007  0.1542\n",
      "     42            1.0000        \u001b[32m0.1400\u001b[0m       0.4792            0.4792        1.3030  0.0007  0.1570\n",
      "     43            1.0000        \u001b[32m0.1137\u001b[0m       0.4688            0.4688        1.3069  0.0006  0.1524\n",
      "     44            1.0000        0.1158       0.4583            0.4583        1.3049  0.0006  0.1513\n",
      "     45            1.0000        0.1219       0.4792            0.4792        1.3019  0.0005  0.1525\n",
      "     46            1.0000        0.1236       0.4896            0.4896        1.2946  0.0005  0.1437\n",
      "     47            1.0000        \u001b[32m0.0898\u001b[0m       0.5104            0.5104        1.2873  0.0004  0.1487\n",
      "     48            1.0000        \u001b[32m0.0759\u001b[0m       0.5104            0.5104        1.2807  0.0004  0.1548\n",
      "     49            1.0000        0.1281       0.5000            0.5000        1.2746  0.0003  0.1513\n",
      "     50            1.0000        \u001b[32m0.0753\u001b[0m       0.5000            0.5000        1.2702  0.0003  0.1595\n",
      "Fine tuning model for subject 8 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5778        1.3316       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9957\u001b[0m  0.0004  0.1580\n",
      "     32            0.6444        1.4301       0.5729            0.5729        \u001b[94m0.9927\u001b[0m  0.0005  0.1533\n",
      "     33            0.7333        1.1398       0.5938            0.5938        1.0027  0.0005  0.1472\n",
      "     34            0.7556        1.0857       0.5521            0.5521        1.0289  0.0006  0.1549\n",
      "     35            0.7333        0.8955       0.5625            0.5625        1.0616  0.0006  0.1511\n",
      "     36            0.8000        \u001b[32m0.7017\u001b[0m       0.4688            0.4688        1.0928  0.0007  0.1582\n",
      "     37            \u001b[36m0.8667\u001b[0m        \u001b[32m0.4861\u001b[0m       0.4167            0.4167        1.1215  0.0007  0.1531\n",
      "     38            \u001b[36m0.9111\u001b[0m        0.5380       0.4375            0.4375        1.1312  0.0007  0.1506\n",
      "     39            \u001b[36m0.9778\u001b[0m        \u001b[32m0.4141\u001b[0m       0.4688            0.4688        1.1360  0.0007  0.1553\n",
      "     40            0.9778        0.4159       0.4479            0.4479        1.1485  0.0007  0.1550\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3353\u001b[0m       0.4583            0.4583        1.1597  0.0007  0.1500\n",
      "     42            1.0000        \u001b[32m0.2954\u001b[0m       0.4583            0.4583        1.1752  0.0007  0.1491\n",
      "     43            1.0000        \u001b[32m0.2532\u001b[0m       0.4896            0.4896        1.1938  0.0006  0.1466\n",
      "     44            1.0000        \u001b[32m0.2132\u001b[0m       0.4792            0.4792        1.2111  0.0006  0.1576\n",
      "     45            1.0000        0.2566       0.4583            0.4583        1.2241  0.0005  0.1492\n",
      "     46            1.0000        \u001b[32m0.1615\u001b[0m       0.4583            0.4583        1.2326  0.0005  0.1502\n",
      "     47            1.0000        0.1654       0.4792            0.4792        1.2407  0.0004  0.1530\n",
      "     48            1.0000        \u001b[32m0.1594\u001b[0m       0.4896            0.4896        1.2477  0.0004  0.1536\n",
      "     49            1.0000        \u001b[32m0.1562\u001b[0m       0.4792            0.4792        1.2527  0.0003  0.1559\n",
      "     50            1.0000        \u001b[32m0.0923\u001b[0m       0.4792            0.4792        1.2575  0.0003  0.1528\n",
      "Fine tuning model for subject 8 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6222        1.2473       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9992\u001b[0m  0.0004  0.1588\n",
      "     32            0.6444        1.2306       0.5938            0.5938        1.0035  0.0005  0.1498\n",
      "     33            0.6667        0.9838       0.6042            0.6042        1.0141  0.0005  0.1501\n",
      "     34            0.6889        0.9842       0.6042            0.6042        1.0335  0.0006  0.1540\n",
      "     35            0.7111        0.7930       0.6042            0.6042        1.0639  0.0006  0.1496\n",
      "     36            0.7778        \u001b[32m0.7032\u001b[0m       0.6042            0.6042        1.0982  0.0007  0.1514\n",
      "     37            \u001b[36m0.8444\u001b[0m        \u001b[32m0.6051\u001b[0m       0.5729            0.5729        1.1207  0.0007  0.1486\n",
      "     38            \u001b[36m0.8889\u001b[0m        \u001b[32m0.3970\u001b[0m       0.5625            0.5625        1.1360  0.0007  0.1475\n",
      "     39            \u001b[36m0.9556\u001b[0m        \u001b[32m0.3310\u001b[0m       0.5833            0.5833        1.1529  0.0007  0.1587\n",
      "     40            0.9556        \u001b[32m0.2778\u001b[0m       0.5625            0.5625        1.1772  0.0007  0.1499\n",
      "     41            \u001b[36m0.9778\u001b[0m        \u001b[32m0.2508\u001b[0m       0.5521            0.5521        1.1994  0.0007  0.1638\n",
      "     42            0.9778        0.2525       0.5625            0.5625        1.2281  0.0007  0.1496\n",
      "     43            0.9778        \u001b[32m0.1648\u001b[0m       0.5312            0.5312        1.2467  0.0006  0.1532\n",
      "     44            0.9556        \u001b[32m0.1586\u001b[0m       0.5312            0.5312        1.2580  0.0006  0.1542\n",
      "     45            0.9778        0.1604       0.5312            0.5312        1.2610  0.0005  0.1551\n",
      "     46            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1203\u001b[0m       0.5312            0.5312        1.2573  0.0005  0.1508\n",
      "     47            1.0000        0.1702       0.5104            0.5104        1.2506  0.0004  0.1489\n",
      "     48            1.0000        0.1357       0.5000            0.5000        1.2409  0.0004  0.1494\n",
      "     49            1.0000        \u001b[32m0.1104\u001b[0m       0.5104            0.5104        1.2294  0.0003  0.1520\n",
      "     50            1.0000        \u001b[32m0.1097\u001b[0m       0.5312            0.5312        1.2175  0.0003  0.1530\n",
      "Fine tuning model for subject 8 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.1042       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9975\u001b[0m  0.0004  0.1561\n",
      "     32            0.6000        1.0537       0.5833            0.5833        \u001b[94m0.9963\u001b[0m  0.0005  0.1494\n",
      "     33            0.7111        1.1341       0.5833            0.5833        1.0079  0.0005  0.1526\n",
      "     34            0.7556        0.9661       0.5729            0.5729        1.0403  0.0006  0.1483\n",
      "     35            \u001b[36m0.8222\u001b[0m        \u001b[32m0.7005\u001b[0m       0.5625            0.5625        1.0979  0.0006  0.1541\n",
      "     36            \u001b[36m0.8667\u001b[0m        \u001b[32m0.5719\u001b[0m       0.5104            0.5104        1.1633  0.0007  0.1494\n",
      "     37            0.8667        \u001b[32m0.3651\u001b[0m       0.4792            0.4792        1.2022  0.0007  0.1534\n",
      "     38            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3347\u001b[0m       0.4896            0.4896        1.2105  0.0007  0.1542\n",
      "     39            \u001b[36m0.9556\u001b[0m        \u001b[32m0.3321\u001b[0m       0.4896            0.4896        1.2014  0.0007  0.1504\n",
      "     40            \u001b[36m0.9778\u001b[0m        \u001b[32m0.2955\u001b[0m       0.5000            0.5000        1.1956  0.0007  0.1608\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2335\u001b[0m       0.5312            0.5312        1.2023  0.0007  0.1645\n",
      "     42            1.0000        \u001b[32m0.1774\u001b[0m       0.5104            0.5104        1.2180  0.0007  0.1761\n",
      "     43            1.0000        0.2036       0.5208            0.5208        1.2381  0.0006  0.2158\n",
      "     44            1.0000        \u001b[32m0.1478\u001b[0m       0.5312            0.5312        1.2534  0.0006  0.1770\n",
      "     45            1.0000        \u001b[32m0.1434\u001b[0m       0.5312            0.5312        1.2611  0.0005  0.1868\n",
      "     46            1.0000        \u001b[32m0.1195\u001b[0m       0.5208            0.5208        1.2638  0.0005  0.1792\n",
      "     47            1.0000        \u001b[32m0.1181\u001b[0m       0.5208            0.5208        1.2620  0.0004  0.1709\n",
      "     48            1.0000        \u001b[32m0.1000\u001b[0m       0.5104            0.5104        1.2595  0.0004  0.1757\n",
      "     49            1.0000        \u001b[32m0.0802\u001b[0m       0.5312            0.5312        1.2558  0.0003  0.1757\n",
      "     50            1.0000        0.0958       0.5312            0.5312        1.2512  0.0003  0.1838\n",
      "Fine tuning model for subject 8 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        0.9410       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        \u001b[94m1.0018\u001b[0m  0.0004  0.1836\n",
      "     32            0.6889        0.9439       0.5938            0.5938        1.0129  0.0005  0.1739\n",
      "     33            0.8000        0.8694       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.0433  0.0005  0.1921\n",
      "     34            \u001b[36m0.8222\u001b[0m        \u001b[32m0.6497\u001b[0m       0.5833            0.5833        1.0860  0.0006  0.1761\n",
      "     35            \u001b[36m0.8889\u001b[0m        0.6697       0.5417            0.5417        1.1371  0.0006  0.2122\n",
      "     36            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5442\u001b[0m       0.5417            0.5417        1.1863  0.0007  0.1764\n",
      "     37            0.9333        \u001b[32m0.4950\u001b[0m       0.5312            0.5312        1.2333  0.0007  0.2289\n",
      "     38            0.9333        \u001b[32m0.4021\u001b[0m       0.5104            0.5104        1.2660  0.0007  0.1864\n",
      "     39            \u001b[36m0.9556\u001b[0m        \u001b[32m0.3297\u001b[0m       0.5000            0.5000        1.2856  0.0007  0.1939\n",
      "     40            \u001b[36m0.9778\u001b[0m        \u001b[32m0.2108\u001b[0m       0.4896            0.4896        1.2957  0.0007  0.2427\n",
      "     41            0.9778        0.2611       0.4792            0.4792        1.3047  0.0007  0.1608\n",
      "     42            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1604\u001b[0m       0.4792            0.4792        1.3136  0.0007  0.1842\n",
      "     43            1.0000        0.1997       0.4688            0.4688        1.3252  0.0006  0.1815\n",
      "     44            1.0000        \u001b[32m0.1389\u001b[0m       0.4688            0.4688        1.3337  0.0006  0.1628\n",
      "     45            1.0000        0.1973       0.4896            0.4896        1.3402  0.0005  0.1540\n",
      "     46            1.0000        \u001b[32m0.1228\u001b[0m       0.4896            0.4896        1.3446  0.0005  0.1546\n",
      "     47            1.0000        0.1382       0.4688            0.4688        1.3478  0.0004  0.1493\n",
      "     48            1.0000        0.1235       0.4688            0.4688        1.3464  0.0004  0.1651\n",
      "     49            1.0000        0.1366       0.4688            0.4688        1.3410  0.0003  0.1514\n",
      "     50            1.0000        \u001b[32m0.1054\u001b[0m       0.4792            0.4792        1.3349  0.0003  0.1548\n",
      "Fine tuning model for subject 8 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        0.9093       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        \u001b[94m0.9968\u001b[0m  0.0004  0.1507\n",
      "     32            0.7111        0.9110       0.5938            0.5938        \u001b[94m0.9896\u001b[0m  0.0005  0.1591\n",
      "     33            0.7556        0.8057       0.6042            0.6042        \u001b[94m0.9821\u001b[0m  0.0005  0.1576\n",
      "     34            \u001b[36m0.8222\u001b[0m        0.7409       0.5833            0.5833        \u001b[94m0.9776\u001b[0m  0.0006  0.1609\n",
      "     35            \u001b[36m0.9556\u001b[0m        \u001b[32m0.6194\u001b[0m       0.6042            0.6042        \u001b[94m0.9743\u001b[0m  0.0006  0.2054\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4440\u001b[0m       0.5833            0.5833        0.9769  0.0007  0.1515\n",
      "     37            1.0000        \u001b[32m0.3056\u001b[0m       0.5625            0.5625        0.9850  0.0007  0.1543\n",
      "     38            1.0000        \u001b[32m0.2580\u001b[0m       0.5625            0.5625        0.9958  0.0007  0.1554\n",
      "     39            1.0000        \u001b[32m0.1816\u001b[0m       0.5625            0.5625        1.0107  0.0007  0.1526\n",
      "     40            1.0000        0.2223       0.5938            0.5938        1.0324  0.0007  0.1524\n",
      "     41            1.0000        0.1909       0.5729            0.5729        1.0587  0.0007  0.1543\n",
      "     42            1.0000        \u001b[32m0.1654\u001b[0m       0.5729            0.5729        1.0926  0.0007  0.1545\n",
      "     43            1.0000        \u001b[32m0.1408\u001b[0m       0.5833            0.5833        1.1235  0.0006  0.1540\n",
      "     44            1.0000        \u001b[32m0.1071\u001b[0m       0.5625            0.5625        1.1484  0.0006  0.1519\n",
      "     45            1.0000        \u001b[32m0.0994\u001b[0m       0.5625            0.5625        1.1626  0.0005  0.1539\n",
      "     46            1.0000        \u001b[32m0.0954\u001b[0m       0.5625            0.5625        1.1691  0.0005  0.1748\n",
      "     47            1.0000        \u001b[32m0.0937\u001b[0m       0.5625            0.5625        1.1677  0.0004  0.1704\n",
      "     48            1.0000        0.1068       0.5729            0.5729        1.1581  0.0004  0.1533\n",
      "     49            1.0000        \u001b[32m0.0676\u001b[0m       0.5833            0.5833        1.1457  0.0003  0.1600\n",
      "     50            1.0000        0.0743       0.5833            0.5833        1.1333  0.0003  0.1557\n",
      "Hold out data from subject 9\n",
      "Pre-training model with data from all subjects but subject 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3573\u001b[0m        \u001b[32m1.5924\u001b[0m       \u001b[35m0.3151\u001b[0m            \u001b[31m0.3151\u001b[0m        \u001b[94m1.4354\u001b[0m  0.0007  7.7253\n",
      "      2            \u001b[36m0.4487\u001b[0m        \u001b[32m1.4425\u001b[0m       \u001b[35m0.3646\u001b[0m            \u001b[31m0.3646\u001b[0m        \u001b[94m1.3329\u001b[0m  0.0007  7.5694\n",
      "      3            \u001b[36m0.5133\u001b[0m        \u001b[32m1.3251\u001b[0m       \u001b[35m0.3893\u001b[0m            \u001b[31m0.3893\u001b[0m        \u001b[94m1.2873\u001b[0m  0.0007  8.0129\n",
      "      4            \u001b[36m0.5328\u001b[0m        \u001b[32m1.2588\u001b[0m       \u001b[35m0.4323\u001b[0m            \u001b[31m0.4323\u001b[0m        \u001b[94m1.2628\u001b[0m  0.0007  7.4791\n",
      "      5            \u001b[36m0.5716\u001b[0m        \u001b[32m1.2198\u001b[0m       \u001b[35m0.4349\u001b[0m            \u001b[31m0.4349\u001b[0m        \u001b[94m1.2578\u001b[0m  0.0007  7.3159\n",
      "      6            0.5604        \u001b[32m1.1800\u001b[0m       0.4141            0.4141        1.3017  0.0006  7.9562\n",
      "      7            \u001b[36m0.5927\u001b[0m        \u001b[32m1.1287\u001b[0m       \u001b[35m0.4492\u001b[0m            \u001b[31m0.4492\u001b[0m        \u001b[94m1.2554\u001b[0m  0.0006  7.3857\n",
      "      8            \u001b[36m0.6297\u001b[0m        \u001b[32m1.0910\u001b[0m       \u001b[35m0.4779\u001b[0m            \u001b[31m0.4779\u001b[0m        \u001b[94m1.1675\u001b[0m  0.0006  7.4227\n",
      "      9            \u001b[36m0.6310\u001b[0m        \u001b[32m1.0568\u001b[0m       \u001b[35m0.4792\u001b[0m            \u001b[31m0.4792\u001b[0m        1.2078  0.0006  8.0214\n",
      "     10            \u001b[36m0.6419\u001b[0m        \u001b[32m1.0278\u001b[0m       \u001b[35m0.4922\u001b[0m            \u001b[31m0.4922\u001b[0m        1.1910  0.0005  7.3151\n",
      "     11            0.6326        \u001b[32m1.0107\u001b[0m       0.4766            0.4766        1.2123  0.0005  7.6573\n",
      "     12            \u001b[36m0.6763\u001b[0m        \u001b[32m0.9858\u001b[0m       0.4909            0.4909        1.1783  0.0005  8.2583\n",
      "     13            \u001b[36m0.7031\u001b[0m        \u001b[32m0.9644\u001b[0m       \u001b[35m0.4974\u001b[0m            \u001b[31m0.4974\u001b[0m        \u001b[94m1.1364\u001b[0m  0.0004  7.4364\n",
      "     14            0.6573        \u001b[32m0.9423\u001b[0m       0.4883            0.4883        1.2137  0.0004  7.6056\n",
      "     15            \u001b[36m0.7169\u001b[0m        \u001b[32m0.9133\u001b[0m       \u001b[35m0.5013\u001b[0m            \u001b[31m0.5013\u001b[0m        \u001b[94m1.1224\u001b[0m  0.0004  7.9751\n",
      "     16            0.7109        \u001b[32m0.8959\u001b[0m       0.4961            0.4961        1.1556  0.0003  7.3002\n",
      "     17            \u001b[36m0.7284\u001b[0m        \u001b[32m0.8742\u001b[0m       \u001b[35m0.5312\u001b[0m            \u001b[31m0.5312\u001b[0m        \u001b[94m1.1179\u001b[0m  0.0003  7.8998\n",
      "     18            \u001b[36m0.7357\u001b[0m        \u001b[32m0.8460\u001b[0m       0.5299            0.5299        \u001b[94m1.1110\u001b[0m  0.0003  7.7510\n",
      "     19            0.7318        \u001b[32m0.8453\u001b[0m       0.5104            0.5104        1.1513  0.0002  7.4433\n",
      "     20            \u001b[36m0.7487\u001b[0m        \u001b[32m0.8240\u001b[0m       \u001b[35m0.5404\u001b[0m            \u001b[31m0.5404\u001b[0m        \u001b[94m1.1077\u001b[0m  0.0002  7.9305\n",
      "     21            \u001b[36m0.7688\u001b[0m        \u001b[32m0.7945\u001b[0m       0.5273            0.5273        \u001b[94m1.0834\u001b[0m  0.0002  7.7097\n",
      "     22            0.7651        0.7956       \u001b[35m0.5508\u001b[0m            \u001b[31m0.5508\u001b[0m        1.0856  0.0001  7.3335\n",
      "     23            \u001b[36m0.7695\u001b[0m        \u001b[32m0.7889\u001b[0m       0.5443            0.5443        \u001b[94m1.0787\u001b[0m  0.0001  8.3469\n",
      "     24            \u001b[36m0.7714\u001b[0m        \u001b[32m0.7791\u001b[0m       0.5247            0.5247        1.0836  0.0001  7.7174\n",
      "     25            \u001b[36m0.7823\u001b[0m        \u001b[32m0.7656\u001b[0m       0.5443            0.5443        \u001b[94m1.0710\u001b[0m  0.0001  7.4630\n",
      "     26            0.7766        \u001b[32m0.7586\u001b[0m       0.5247            0.5247        1.0799  0.0000  8.0016\n",
      "     27            \u001b[36m0.7844\u001b[0m        0.7620       0.5365            0.5365        1.0740  0.0000  7.4343\n",
      "     28            0.7831        \u001b[32m0.7544\u001b[0m       0.5352            0.5352        1.0712  0.0000  7.3686\n",
      "     29            0.7828        \u001b[32m0.7530\u001b[0m       0.5378            0.5378        1.0716  0.0000  8.0450\n",
      "     30            0.7844        \u001b[32m0.7442\u001b[0m       0.5299            0.5299        1.0710  0.0000  7.4294\n",
      "Before finetuning for subject 9, the baseline accuracy is 0.46875\n",
      "Fine tuning model for subject 9 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        1.2268       0.4479            0.4479        1.3071  0.0004  0.1301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        0.9677       0.4375            0.4375        1.2809  0.0005  0.1390\n",
      "     33            1.0000        \u001b[32m0.1012\u001b[0m       0.4479            0.4479        1.2685  0.0005  0.1466\n",
      "     34            1.0000        \u001b[32m0.0280\u001b[0m       0.5208            0.5208        1.2894  0.0006  0.1360\n",
      "     35            1.0000        0.0302       0.5208            0.5208        1.3336  0.0006  0.1307\n",
      "     36            1.0000        \u001b[32m0.0261\u001b[0m       0.5000            0.5000        1.3859  0.0007  0.1230\n",
      "     37            1.0000        \u001b[32m0.0070\u001b[0m       0.5000            0.5000        1.4456  0.0007  0.1284\n",
      "     38            1.0000        0.0265       0.4375            0.4375        1.5103  0.0007  0.1298\n",
      "     39            1.0000        \u001b[32m0.0036\u001b[0m       0.4271            0.4271        1.5791  0.0007  0.1330\n",
      "     40            1.0000        0.0054       0.3958            0.3958        1.6485  0.0007  0.1250\n",
      "     41            1.0000        0.0084       0.3854            0.3854        1.7151  0.0007  0.1250\n",
      "     42            1.0000        0.0088       0.3958            0.3958        1.7754  0.0007  0.1335\n",
      "     43            1.0000        0.0260       0.3854            0.3854        1.8288  0.0006  0.1253\n",
      "     44            1.0000        0.0117       0.3854            0.3854        1.8713  0.0006  0.1363\n",
      "     45            1.0000        0.0038       0.3854            0.3854        1.9046  0.0005  0.1269\n",
      "     46            1.0000        0.0319       0.3958            0.3958        1.9299  0.0005  0.1280\n",
      "     47            1.0000        \u001b[32m0.0028\u001b[0m       0.3646            0.3646        1.9494  0.0004  0.1244\n",
      "     48            1.0000        0.0032       0.3646            0.3646        1.9640  0.0004  0.1278\n",
      "     49            1.0000        0.0048       0.3750            0.3750        1.9746  0.0003  0.1287\n",
      "     50            1.0000        0.0029       0.3958            0.3958        1.9824  0.0003  0.1278\n",
      "Fine tuning model for subject 9 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        0.7586       0.4688            0.4688        1.3198  0.0004  0.1242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        0.8854       0.4792            0.4792        1.3186  0.0005  0.1298\n",
      "     33            1.0000        \u001b[32m0.2163\u001b[0m       0.4896            0.4896        1.3221  0.0005  0.1285\n",
      "     34            1.0000        \u001b[32m0.0865\u001b[0m       0.4896            0.4896        1.3440  0.0006  0.1277\n",
      "     35            1.0000        \u001b[32m0.0433\u001b[0m       0.5000            0.5000        1.3820  0.0006  0.1258\n",
      "     36            1.0000        \u001b[32m0.0216\u001b[0m       0.5208            0.5208        1.4254  0.0007  0.1255\n",
      "     37            1.0000        \u001b[32m0.0094\u001b[0m       0.5208            0.5208        1.4673  0.0007  0.1331\n",
      "     38            1.0000        \u001b[32m0.0057\u001b[0m       0.5208            0.5208        1.5024  0.0007  0.1270\n",
      "     39            1.0000        \u001b[32m0.0051\u001b[0m       0.5208            0.5208        1.5276  0.0007  0.1268\n",
      "     40            1.0000        \u001b[32m0.0047\u001b[0m       0.5208            0.5208        1.5429  0.0007  0.1271\n",
      "     41            1.0000        0.0070       0.5104            0.5104        1.5521  0.0007  0.1293\n",
      "     42            1.0000        \u001b[32m0.0039\u001b[0m       0.5104            0.5104        1.5552  0.0007  0.1263\n",
      "     43            1.0000        \u001b[32m0.0008\u001b[0m       0.5104            0.5104        1.5523  0.0006  0.1282\n",
      "     44            1.0000        0.0030       0.5104            0.5104        1.5449  0.0006  0.1244\n",
      "     45            1.0000        0.0011       0.5104            0.5104        1.5346  0.0005  0.1544\n",
      "     46            1.0000        \u001b[32m0.0006\u001b[0m       0.5104            0.5104        1.5225  0.0005  0.1360\n",
      "     47            1.0000        \u001b[32m0.0004\u001b[0m       0.5104            0.5104        1.5095  0.0004  0.1419\n",
      "     48            1.0000        \u001b[32m0.0004\u001b[0m       0.5000            0.5000        1.4963  0.0004  0.1293\n",
      "     49            1.0000        0.0010       0.5104            0.5104        1.4836  0.0003  0.1262\n",
      "     50            1.0000        0.0008       0.5104            0.5104        1.4716  0.0003  0.1352\n",
      "Fine tuning model for subject 9 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        \u001b[32m0.5371\u001b[0m       0.4792            0.4792        1.3342  0.0004  0.1277\n",
      "     32            \u001b[36m1.0000\u001b[0m        0.6956       0.4688            0.4688        1.3580  0.0005  0.1356\n",
      "     33            1.0000        \u001b[32m0.0966\u001b[0m       0.4479            0.4479        1.3811  0.0005  0.1271\n",
      "     34            1.0000        \u001b[32m0.0832\u001b[0m       0.4375            0.4375        1.4008  0.0006  0.1497\n",
      "     35            1.0000        \u001b[32m0.0222\u001b[0m       0.4271            0.4271        1.4152  0.0006  0.1279\n",
      "     36            1.0000        \u001b[32m0.0110\u001b[0m       0.4271            0.4271        1.4268  0.0007  0.1364\n",
      "     37            1.0000        \u001b[32m0.0042\u001b[0m       0.4167            0.4167        1.4383  0.0007  0.1282\n",
      "     38            1.0000        \u001b[32m0.0031\u001b[0m       0.4062            0.4062        1.4514  0.0007  0.1265\n",
      "     39            1.0000        \u001b[32m0.0025\u001b[0m       0.4062            0.4062        1.4664  0.0007  0.1363\n",
      "     40            1.0000        0.0093       0.3854            0.3854        1.4833  0.0007  0.1430\n",
      "     41            1.0000        \u001b[32m0.0024\u001b[0m       0.3750            0.3750        1.5000  0.0007  0.1335\n",
      "     42            1.0000        0.0047       0.3750            0.3750        1.5161  0.0007  0.1283\n",
      "     43            1.0000        0.0026       0.3750            0.3750        1.5303  0.0006  0.1272\n",
      "     44            1.0000        0.0063       0.3646            0.3646        1.5434  0.0006  0.1322\n",
      "     45            1.0000        \u001b[32m0.0023\u001b[0m       0.3542            0.3542        1.5550  0.0005  0.1249\n",
      "     46            1.0000        0.0028       0.3542            0.3542        1.5659  0.0005  0.1290\n",
      "     47            1.0000        \u001b[32m0.0020\u001b[0m       0.3542            0.3542        1.5764  0.0004  0.1257\n",
      "     48            1.0000        0.0021       0.3438            0.3438        1.5869  0.0004  0.1279\n",
      "     49            1.0000        0.0056       0.3333            0.3333        1.5984  0.0003  0.1288\n",
      "     50            1.0000        0.0033       0.3438            0.3438        1.6102  0.0003  0.1296\n",
      "Fine tuning model for subject 9 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        1.0399       0.4792            0.4792        1.3128  0.0004  0.1233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4697\u001b[0m       0.4792            0.4792        1.3079  0.0005  0.1329\n",
      "     33            1.0000        \u001b[32m0.2601\u001b[0m       0.4688            0.4688        1.2946  0.0005  0.1249\n",
      "     34            1.0000        \u001b[32m0.1477\u001b[0m       0.4792            0.4792        1.2882  0.0006  0.1270\n",
      "     35            1.0000        \u001b[32m0.0350\u001b[0m       0.5000            0.5000        1.2924  0.0006  0.1295\n",
      "     36            1.0000        \u001b[32m0.0063\u001b[0m       0.5104            0.5104        1.3027  0.0007  0.1285\n",
      "     37            1.0000        0.0097       0.5208            0.5208        1.3161  0.0007  0.1280\n",
      "     38            1.0000        \u001b[32m0.0025\u001b[0m       0.5208            0.5208        1.3304  0.0007  0.1282\n",
      "     39            1.0000        0.0248       0.5417            0.5417        1.3452  0.0007  0.1263\n",
      "     40            1.0000        \u001b[32m0.0010\u001b[0m       0.5208            0.5208        1.3591  0.0007  0.1339\n",
      "     41            1.0000        0.0090       0.5104            0.5104        1.3722  0.0007  0.1305\n",
      "     42            1.0000        \u001b[32m0.0008\u001b[0m       0.5104            0.5104        1.3838  0.0007  0.1286\n",
      "     43            1.0000        0.0027       0.5000            0.5000        1.3940  0.0006  0.1306\n",
      "     44            1.0000        0.0034       0.4896            0.4896        1.4027  0.0006  0.1273\n",
      "     45            1.0000        0.0016       0.4896            0.4896        1.4101  0.0005  0.1320\n",
      "     46            1.0000        \u001b[32m0.0006\u001b[0m       0.5000            0.5000        1.4163  0.0005  0.1283\n",
      "     47            1.0000        \u001b[32m0.0005\u001b[0m       0.4896            0.4896        1.4215  0.0004  0.1261\n",
      "     48            1.0000        0.0006       0.4896            0.4896        1.4260  0.0004  0.1265\n",
      "     49            1.0000        0.0022       0.4896            0.4896        1.4298  0.0003  0.1282\n",
      "     50            1.0000        0.0062       0.4896            0.4896        1.4332  0.0003  0.1261\n",
      "Fine tuning model for subject 9 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4788\u001b[0m       0.4792            0.4792        1.2916  0.0004  0.1284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            1.0000        0.5549       0.5000            0.5000        1.2449  0.0005  0.1308\n",
      "     33            1.0000        \u001b[32m0.3437\u001b[0m       0.5312            0.5312        1.2295  0.0005  0.1245\n",
      "     34            1.0000        \u001b[32m0.0117\u001b[0m       0.5208            0.5208        1.2381  0.0006  0.1292\n",
      "     35            1.0000        0.0290       0.5104            0.5104        1.2563  0.0006  0.1800\n",
      "     36            1.0000        0.0148       0.5208            0.5208        1.2746  0.0007  0.1538\n",
      "     37            1.0000        \u001b[32m0.0056\u001b[0m       0.5208            0.5208        1.2909  0.0007  0.1618\n",
      "     38            1.0000        0.0106       0.5104            0.5104        1.3065  0.0007  0.1585\n",
      "     39            1.0000        0.0573       0.5312            0.5312        1.3117  0.0007  0.1570\n",
      "     40            1.0000        0.0343       0.5417            0.5417        1.3214  0.0007  0.1545\n",
      "     41            1.0000        0.0062       0.5208            0.5208        1.3377  0.0007  0.1681\n",
      "     42            1.0000        \u001b[32m0.0047\u001b[0m       0.5000            0.5000        1.3585  0.0007  0.1581\n",
      "     43            1.0000        \u001b[32m0.0009\u001b[0m       0.5104            0.5104        1.3816  0.0006  0.1637\n",
      "     44            1.0000        0.0009       0.4896            0.4896        1.4056  0.0006  0.1664\n",
      "     45            1.0000        0.0020       0.5000            0.5000        1.4296  0.0005  0.1673\n",
      "     46            1.0000        0.0010       0.5000            0.5000        1.4531  0.0005  0.1731\n",
      "     47            1.0000        \u001b[32m0.0006\u001b[0m       0.5000            0.5000        1.4760  0.0004  0.1545\n",
      "     48            1.0000        0.0007       0.5000            0.5000        1.4985  0.0004  0.1704\n",
      "     49            1.0000        \u001b[32m0.0005\u001b[0m       0.5104            0.5104        1.5205  0.0003  0.1644\n",
      "     50            1.0000        \u001b[32m0.0003\u001b[0m       0.5208            0.5208        1.5422  0.0003  0.1690\n",
      "Fine tuning model for subject 9 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        1.7706       0.4896            0.4896        1.2457  0.0004  0.1502\n",
      "     32            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4993\u001b[0m       0.5208            0.5208        1.1955  0.0005  0.1367\n",
      "     33            1.0000        \u001b[32m0.2230\u001b[0m       0.5312            0.5312        1.1647  0.0005  0.1341\n",
      "     34            1.0000        \u001b[32m0.0665\u001b[0m       0.5312            0.5312        1.1296  0.0006  0.1282\n",
      "     35            1.0000        \u001b[32m0.0111\u001b[0m       0.5312            0.5312        1.0875  0.0006  0.1281\n",
      "     36            1.0000        \u001b[32m0.0057\u001b[0m       0.5417            0.5417        \u001b[94m1.0526\u001b[0m  0.0007  0.1340\n",
      "     37            1.0000        0.0254       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        \u001b[94m1.0443\u001b[0m  0.0007  0.1511\n",
      "     38            1.0000        \u001b[32m0.0045\u001b[0m       0.5312            0.5312        1.0690  0.0007  0.1543\n",
      "     39            1.0000        \u001b[32m0.0023\u001b[0m       0.5312            0.5312        1.1238  0.0007  0.1617\n",
      "     40            1.0000        \u001b[32m0.0010\u001b[0m       0.5312            0.5312        1.1973  0.0007  0.1288\n",
      "     41            1.0000        \u001b[32m0.0008\u001b[0m       0.5000            0.5000        1.2753  0.0007  0.1407\n",
      "     42            1.0000        \u001b[32m0.0006\u001b[0m       0.4375            0.4375        1.3464  0.0007  0.1262\n",
      "     43            1.0000        0.0034       0.4062            0.4062        1.4026  0.0006  0.1256\n",
      "     44            1.0000        0.0019       0.4167            0.4167        1.4416  0.0006  0.1350\n",
      "     45            1.0000        0.0012       0.4167            0.4167        1.4639  0.0005  0.1296\n",
      "     46            1.0000        0.0065       0.4167            0.4167        1.4713  0.0005  0.1354\n",
      "     47            1.0000        0.0008       0.4167            0.4167        1.4682  0.0004  0.1271\n",
      "     48            1.0000        0.0017       0.4062            0.4062        1.4579  0.0004  0.1328\n",
      "     49            1.0000        \u001b[32m0.0004\u001b[0m       0.4375            0.4375        1.4442  0.0003  0.1228\n",
      "     50            1.0000        0.0005       0.4583            0.4583        1.4297  0.0003  0.1280\n",
      "Fine tuning model for subject 9 with 5 = 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.9078       0.4688            0.4688        1.3430  0.0004  0.1335\n",
      "     32            0.6000        1.6827       0.4688            0.4688        1.4051  0.0005  0.1281\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.7009\u001b[0m       0.4583            0.4583        1.5124  0.0005  0.1289\n",
      "     34            1.0000        \u001b[32m0.2630\u001b[0m       0.4583            0.4583        1.6434  0.0006  0.1299\n",
      "     35            1.0000        \u001b[32m0.0692\u001b[0m       0.4896            0.4896        1.7540  0.0006  0.1290\n",
      "     36            1.0000        \u001b[32m0.0652\u001b[0m       0.4583            0.4583        1.8328  0.0007  0.1300\n",
      "     37            1.0000        \u001b[32m0.0046\u001b[0m       0.4583            0.4583        1.8905  0.0007  0.1311\n",
      "     38            1.0000        0.0174       0.4688            0.4688        1.9361  0.0007  0.1262\n",
      "     39            1.0000        0.0171       0.4375            0.4375        1.9781  0.0007  0.1286\n",
      "     40            1.0000        \u001b[32m0.0018\u001b[0m       0.4479            0.4479        2.0205  0.0007  0.1284\n",
      "     41            1.0000        0.0025       0.4271            0.4271        2.0606  0.0007  0.1258\n",
      "     42            1.0000        0.0038       0.4375            0.4375        2.0938  0.0007  0.1278\n",
      "     43            1.0000        0.0042       0.4375            0.4375        2.1176  0.0006  0.1231\n",
      "     44            1.0000        0.0032       0.4271            0.4271        2.1313  0.0006  0.1327\n",
      "     45            1.0000        \u001b[32m0.0005\u001b[0m       0.4062            0.4062        2.1357  0.0005  0.1240\n",
      "     46            1.0000        0.0013       0.3958            0.3958        2.1328  0.0005  0.1257\n",
      "     47            1.0000        0.0013       0.4062            0.4062        2.1247  0.0004  0.1281\n",
      "     48            1.0000        0.0007       0.3958            0.3958        2.1141  0.0004  0.1282\n",
      "     49            1.0000        0.0016       0.3958            0.3958        2.1029  0.0003  0.1271\n",
      "     50            1.0000        0.0007       0.3854            0.3854        2.0928  0.0003  0.1283\n",
      "Fine tuning model for subject 9 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        1.3331       0.4688            0.4688        1.3099  0.0004  0.1325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5522\u001b[0m       0.4896            0.4896        1.2995  0.0005  0.1302\n",
      "     33            1.0000        \u001b[32m0.3763\u001b[0m       0.5312            0.5312        1.2935  0.0005  0.1235\n",
      "     34            1.0000        \u001b[32m0.0625\u001b[0m       0.5000            0.5000        1.2911  0.0006  0.1278\n",
      "     35            1.0000        \u001b[32m0.0168\u001b[0m       0.5417            0.5417        1.2922  0.0006  0.1260\n",
      "     36            1.0000        \u001b[32m0.0060\u001b[0m       0.5104            0.5104        1.2935  0.0007  0.1268\n",
      "     37            1.0000        0.0135       0.5000            0.5000        1.2954  0.0007  0.1264\n",
      "     38            1.0000        0.0092       0.4792            0.4792        1.2968  0.0007  0.1278\n",
      "     39            1.0000        \u001b[32m0.0019\u001b[0m       0.4479            0.4479        1.3002  0.0007  0.1264\n",
      "     40            1.0000        \u001b[32m0.0016\u001b[0m       0.4479            0.4479        1.3082  0.0007  0.1247\n",
      "     41            1.0000        \u001b[32m0.0013\u001b[0m       0.4167            0.4167        1.3218  0.0007  0.1229\n",
      "     42            1.0000        0.0031       0.4375            0.4375        1.3406  0.0007  0.1262\n",
      "     43            1.0000        0.0017       0.4479            0.4479        1.3633  0.0006  0.1224\n",
      "     44            1.0000        0.0037       0.4688            0.4688        1.3880  0.0006  0.1256\n",
      "     45            1.0000        \u001b[32m0.0009\u001b[0m       0.4479            0.4479        1.4128  0.0005  0.1321\n",
      "     46            1.0000        0.0020       0.4479            0.4479        1.4363  0.0005  0.1287\n",
      "     47            1.0000        0.0036       0.4479            0.4479        1.4576  0.0004  0.1364\n",
      "     48            1.0000        \u001b[32m0.0009\u001b[0m       0.4375            0.4375        1.4763  0.0004  0.1208\n",
      "     49            1.0000        \u001b[32m0.0007\u001b[0m       0.4271            0.4271        1.4924  0.0003  0.1301\n",
      "     50            1.0000        0.0016       0.4271            0.4271        1.5059  0.0003  0.1268\n",
      "Fine tuning model for subject 9 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        \u001b[32m0.4773\u001b[0m       0.4583            0.4583        1.3329  0.0004  0.1333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4610\u001b[0m       0.4688            0.4688        1.3795  0.0005  0.1301\n",
      "     33            1.0000        \u001b[32m0.0887\u001b[0m       0.4792            0.4792        1.4344  0.0005  0.1277\n",
      "     34            1.0000        \u001b[32m0.0372\u001b[0m       0.4792            0.4792        1.4871  0.0006  0.1223\n",
      "     35            1.0000        \u001b[32m0.0238\u001b[0m       0.4583            0.4583        1.5306  0.0006  0.1259\n",
      "     36            1.0000        \u001b[32m0.0035\u001b[0m       0.4688            0.4688        1.5595  0.0007  0.1301\n",
      "     37            1.0000        \u001b[32m0.0018\u001b[0m       0.4583            0.4583        1.5747  0.0007  0.1286\n",
      "     38            1.0000        0.0050       0.4583            0.4583        1.5787  0.0007  0.1223\n",
      "     39            1.0000        0.0056       0.4375            0.4375        1.5744  0.0007  0.1331\n",
      "     40            1.0000        0.0035       0.4271            0.4271        1.5677  0.0007  0.1280\n",
      "     41            1.0000        0.0031       0.4167            0.4167        1.5603  0.0007  0.1262\n",
      "     42            1.0000        \u001b[32m0.0010\u001b[0m       0.4062            0.4062        1.5533  0.0007  0.1308\n",
      "     43            1.0000        0.0128       0.4167            0.4167        1.5456  0.0006  0.1173\n",
      "     44            1.0000        0.0029       0.4167            0.4167        1.5389  0.0006  0.1201\n",
      "     45            1.0000        0.0018       0.4167            0.4167        1.5331  0.0005  0.1232\n",
      "     46            1.0000        \u001b[32m0.0009\u001b[0m       0.4271            0.4271        1.5277  0.0005  0.1196\n",
      "     47            1.0000        0.0016       0.4167            0.4167        1.5226  0.0004  0.1334\n",
      "     48            1.0000        0.0022       0.4271            0.4271        1.5179  0.0004  0.1354\n",
      "     49            1.0000        0.0014       0.4167            0.4167        1.5136  0.0003  0.1261\n",
      "     50            1.0000        0.0078       0.4062            0.4062        1.5101  0.0003  0.1325\n",
      "Fine tuning model for subject 9 with 5 = 5 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        0.9790       0.4688            0.4688        1.2913  0.0004  0.1221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.8000        \u001b[32m0.5112\u001b[0m       0.4792            0.4792        1.2731  0.0005  0.1285\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1387\u001b[0m       0.5104            0.5104        1.2738  0.0005  0.1285\n",
      "     34            1.0000        \u001b[32m0.0830\u001b[0m       0.5417            0.5417        1.2856  0.0006  0.1309\n",
      "     35            1.0000        \u001b[32m0.0551\u001b[0m       0.5312            0.5312        1.3000  0.0006  0.1270\n",
      "     36            1.0000        \u001b[32m0.0294\u001b[0m       0.5417            0.5417        1.3112  0.0007  0.1309\n",
      "     37            1.0000        0.0445       0.5417            0.5417        1.3216  0.0007  0.1266\n",
      "     38            1.0000        \u001b[32m0.0122\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.3271  0.0007  0.1262\n",
      "     39            1.0000        \u001b[32m0.0077\u001b[0m       0.5312            0.5312        1.3295  0.0007  0.1330\n",
      "     40            1.0000        \u001b[32m0.0029\u001b[0m       0.5312            0.5312        1.3302  0.0007  0.1244\n",
      "     41            1.0000        0.0066       0.5208            0.5208        1.3303  0.0007  0.1248\n",
      "     42            1.0000        0.0066       0.5417            0.5417        1.3315  0.0007  0.1265\n",
      "     43            1.0000        0.0089       0.5521            0.5521        1.3348  0.0006  0.1240\n",
      "     44            1.0000        0.0102       0.5417            0.5417        1.3385  0.0006  0.1279\n",
      "     45            1.0000        \u001b[32m0.0020\u001b[0m       0.5417            0.5417        1.3422  0.0005  0.1432\n",
      "     46            1.0000        0.0035       0.5521            0.5521        1.3462  0.0005  0.1609\n",
      "     47            1.0000        0.0023       0.5417            0.5417        1.3502  0.0004  0.1619\n",
      "     48            1.0000        0.0041       0.5521            0.5521        1.3542  0.0004  0.1647\n",
      "     49            1.0000        0.0039       0.5521            0.5521        1.3581  0.0003  0.1563\n",
      "     50            1.0000        \u001b[32m0.0018\u001b[0m       0.5417            0.5417        1.3615  0.0003  0.1574\n",
      "Fine tuning model for subject 9 with 10 = 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.9000\u001b[0m        0.8943       0.4792            0.4792        1.3138  0.0004  0.1346\n",
      "     32            0.9000        \u001b[32m0.3280\u001b[0m       0.4896            0.4896        1.3061  0.0005  0.1316\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.4155       0.5104            0.5104        1.2854  0.0005  0.1652\n",
      "     34            1.0000        \u001b[32m0.1847\u001b[0m       0.5417            0.5417        1.2639  0.0006  0.1427\n",
      "     35            1.0000        \u001b[32m0.0456\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.2433  0.0006  0.1586\n",
      "     36            1.0000        0.0928       0.5729            0.5729        1.2224  0.0007  0.1622\n",
      "     37            1.0000        0.0565       0.5521            0.5521        1.2072  0.0007  0.1385\n",
      "     38            1.0000        0.0730       0.5521            0.5521        1.1983  0.0007  0.1409\n",
      "     39            1.0000        \u001b[32m0.0108\u001b[0m       0.5729            0.5729        1.1973  0.0007  0.1377\n",
      "     40            1.0000        \u001b[32m0.0057\u001b[0m       0.5625            0.5625        1.2047  0.0007  0.1706\n",
      "     41            1.0000        \u001b[32m0.0057\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.2183  0.0007  0.1329\n",
      "     42            1.0000        0.0076       0.5625            0.5625        1.2347  0.0007  0.1258\n",
      "     43            1.0000        0.0098       0.5521            0.5521        1.2506  0.0006  0.1120\n",
      "     44            1.0000        0.0127       0.5521            0.5521        1.2633  0.0006  0.1117\n",
      "     45            1.0000        0.0123       0.5521            0.5521        1.2738  0.0005  0.1106\n",
      "     46            1.0000        \u001b[32m0.0028\u001b[0m       0.5521            0.5521        1.2804  0.0005  0.1092\n",
      "     47            1.0000        0.0083       0.5521            0.5521        1.2838  0.0004  0.1106\n",
      "     48            1.0000        0.0138       0.5521            0.5521        1.2837  0.0004  0.1131\n",
      "     49            1.0000        \u001b[32m0.0021\u001b[0m       0.5521            0.5521        1.2820  0.0003  0.1067\n",
      "     50            1.0000        0.0024       0.5521            0.5521        1.2791  0.0003  0.1084\n",
      "Fine tuning model for subject 9 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.2918       0.4688            0.4688        1.2897  0.0004  0.1060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.9000\u001b[0m        0.9175       0.5104            0.5104        1.2777  0.0005  0.1124\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6774\u001b[0m       0.5417            0.5417        1.2834  0.0005  0.1069\n",
      "     34            1.0000        \u001b[32m0.3055\u001b[0m       0.5417            0.5417        1.3040  0.0006  0.1112\n",
      "     35            1.0000        \u001b[32m0.1297\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.3304  0.0006  0.1164\n",
      "     36            1.0000        \u001b[32m0.0484\u001b[0m       0.5729            0.5729        1.3517  0.0007  0.1126\n",
      "     37            1.0000        \u001b[32m0.0171\u001b[0m       0.5521            0.5521        1.3683  0.0007  0.1129\n",
      "     38            1.0000        0.0250       0.5521            0.5521        1.3842  0.0007  0.1091\n",
      "     39            1.0000        0.0222       0.5312            0.5312        1.4015  0.0007  0.1076\n",
      "     40            1.0000        \u001b[32m0.0065\u001b[0m       0.5104            0.5104        1.4179  0.0007  0.1117\n",
      "     41            1.0000        \u001b[32m0.0062\u001b[0m       0.5000            0.5000        1.4325  0.0007  0.1129\n",
      "     42            1.0000        0.0154       0.5000            0.5000        1.4445  0.0007  0.1090\n",
      "     43            1.0000        0.0111       0.5000            0.5000        1.4537  0.0006  0.1123\n",
      "     44            1.0000        0.0205       0.4896            0.4896        1.4596  0.0006  0.1275\n",
      "     45            1.0000        0.0131       0.4896            0.4896        1.4631  0.0005  0.1116\n",
      "     46            1.0000        0.0066       0.4896            0.4896        1.4646  0.0005  0.1118\n",
      "     47            1.0000        0.0114       0.4896            0.4896        1.4654  0.0004  0.1039\n",
      "     48            1.0000        \u001b[32m0.0044\u001b[0m       0.5000            0.5000        1.4658  0.0004  0.1114\n",
      "     49            1.0000        0.0189       0.5104            0.5104        1.4667  0.0003  0.1173\n",
      "     50            1.0000        0.0089       0.5000            0.5000        1.4681  0.0003  0.1085\n",
      "Fine tuning model for subject 9 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        0.8108       0.4583            0.4583        1.3239  0.0004  0.1069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        \u001b[32m0.6847\u001b[0m       0.4167            0.4167        1.3476  0.0005  0.1126\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4331\u001b[0m       0.4479            0.4479        1.3803  0.0005  0.1042\n",
      "     34            1.0000        \u001b[32m0.1309\u001b[0m       0.4479            0.4479        1.4145  0.0006  0.1117\n",
      "     35            1.0000        \u001b[32m0.0963\u001b[0m       0.4479            0.4479        1.4547  0.0006  0.1095\n",
      "     36            1.0000        \u001b[32m0.0692\u001b[0m       0.4688            0.4688        1.4940  0.0007  0.1082\n",
      "     37            1.0000        \u001b[32m0.0630\u001b[0m       0.4688            0.4688        1.5204  0.0007  0.1057\n",
      "     38            1.0000        0.0882       0.4688            0.4688        1.5261  0.0007  0.1097\n",
      "     39            1.0000        \u001b[32m0.0321\u001b[0m       0.4896            0.4896        1.5230  0.0007  0.1235\n",
      "     40            1.0000        0.0660       0.4688            0.4688        1.5203  0.0007  0.1281\n",
      "     41            1.0000        0.0481       0.4792            0.4792        1.5156  0.0007  0.1271\n",
      "     42            1.0000        \u001b[32m0.0148\u001b[0m       0.4688            0.4688        1.5066  0.0007  0.1358\n",
      "     43            1.0000        0.0180       0.4688            0.4688        1.4956  0.0006  0.1164\n",
      "     44            1.0000        \u001b[32m0.0065\u001b[0m       0.4792            0.4792        1.4832  0.0006  0.1107\n",
      "     45            1.0000        \u001b[32m0.0065\u001b[0m       0.5000            0.5000        1.4707  0.0005  0.1072\n",
      "     46            1.0000        0.0097       0.5104            0.5104        1.4587  0.0005  0.1127\n",
      "     47            1.0000        \u001b[32m0.0046\u001b[0m       0.5104            0.5104        1.4470  0.0004  0.1071\n",
      "     48            1.0000        \u001b[32m0.0037\u001b[0m       0.5104            0.5104        1.4358  0.0004  0.1080\n",
      "     49            1.0000        0.0068       0.5104            0.5104        1.4254  0.0003  0.1154\n",
      "     50            1.0000        \u001b[32m0.0026\u001b[0m       0.5000            0.5000        1.4157  0.0003  0.1102\n",
      "Fine tuning model for subject 9 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        1.2021       0.4688            0.4688        1.3352  0.0004  0.1081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6951\u001b[0m       0.4271            0.4271        1.3948  0.0005  0.1085\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4255\u001b[0m       0.4479            0.4479        1.4995  0.0005  0.1071\n",
      "     34            1.0000        \u001b[32m0.3969\u001b[0m       0.3854            0.3854        1.6513  0.0006  0.1171\n",
      "     35            1.0000        \u001b[32m0.1810\u001b[0m       0.3750            0.3750        1.8355  0.0006  0.1104\n",
      "     36            1.0000        \u001b[32m0.0363\u001b[0m       0.3750            0.3750        2.0084  0.0007  0.1117\n",
      "     37            1.0000        \u001b[32m0.0274\u001b[0m       0.3542            0.3542        2.1500  0.0007  0.1104\n",
      "     38            1.0000        0.0328       0.3542            0.3542        2.2445  0.0007  0.1085\n",
      "     39            1.0000        0.0356       0.3542            0.3542        2.2988  0.0007  0.1097\n",
      "     40            1.0000        \u001b[32m0.0215\u001b[0m       0.3854            0.3854        2.3234  0.0007  0.1118\n",
      "     41            1.0000        0.0234       0.3750            0.3750        2.3202  0.0007  0.1085\n",
      "     42            1.0000        0.0331       0.3750            0.3750        2.2930  0.0007  0.1056\n",
      "     43            1.0000        \u001b[32m0.0108\u001b[0m       0.3646            0.3646        2.2526  0.0006  0.1098\n",
      "     44            1.0000        0.0111       0.3542            0.3542        2.2041  0.0006  0.1037\n",
      "     45            1.0000        0.0135       0.3646            0.3646        2.1545  0.0005  0.1089\n",
      "     46            1.0000        \u001b[32m0.0097\u001b[0m       0.3750            0.3750        2.1051  0.0005  0.1117\n",
      "     47            1.0000        0.0219       0.3646            0.3646        2.0561  0.0004  0.1111\n",
      "     48            1.0000        0.0172       0.3542            0.3542        2.0112  0.0004  0.1091\n",
      "     49            1.0000        0.0106       0.3646            0.3646        1.9719  0.0003  0.1097\n",
      "     50            1.0000        \u001b[32m0.0041\u001b[0m       0.3958            0.3958        1.9378  0.0003  0.1121\n",
      "Fine tuning model for subject 9 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        \u001b[32m0.6722\u001b[0m       0.4688            0.4688        1.3301  0.0004  0.1078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.9000\u001b[0m        1.0563       0.4688            0.4688        1.3182  0.0005  0.1117\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5163\u001b[0m       0.4583            0.4583        1.3289  0.0005  0.1113\n",
      "     34            1.0000        \u001b[32m0.3065\u001b[0m       0.4583            0.4583        1.3427  0.0006  0.1112\n",
      "     35            1.0000        \u001b[32m0.1537\u001b[0m       0.4271            0.4271        1.3719  0.0006  0.1115\n",
      "     36            1.0000        \u001b[32m0.0331\u001b[0m       0.4062            0.4062        1.4079  0.0007  0.1122\n",
      "     37            1.0000        0.0446       0.3958            0.3958        1.4490  0.0007  0.1086\n",
      "     38            1.0000        \u001b[32m0.0250\u001b[0m       0.4167            0.4167        1.4809  0.0007  0.1131\n",
      "     39            1.0000        0.0354       0.4167            0.4167        1.5080  0.0007  0.1071\n",
      "     40            1.0000        \u001b[32m0.0133\u001b[0m       0.4167            0.4167        1.5292  0.0007  0.1139\n",
      "     41            1.0000        0.0168       0.4375            0.4375        1.5447  0.0007  0.1107\n",
      "     42            1.0000        0.0263       0.4479            0.4479        1.5568  0.0007  0.1098\n",
      "     43            1.0000        0.0169       0.4479            0.4479        1.5633  0.0006  0.1084\n",
      "     44            1.0000        0.0304       0.4479            0.4479        1.5653  0.0006  0.1083\n",
      "     45            1.0000        \u001b[32m0.0082\u001b[0m       0.4479            0.4479        1.5651  0.0005  0.1081\n",
      "     46            1.0000        0.0218       0.4375            0.4375        1.5625  0.0005  0.1027\n",
      "     47            1.0000        0.0214       0.4375            0.4375        1.5572  0.0004  0.1071\n",
      "     48            1.0000        0.0123       0.4375            0.4375        1.5519  0.0004  0.1064\n",
      "     49            1.0000        0.0117       0.4375            0.4375        1.5468  0.0003  0.1113\n",
      "     50            1.0000        0.0100       0.4271            0.4271        1.5428  0.0003  0.1083\n",
      "Fine tuning model for subject 9 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5000        1.6304       0.4792            0.4792        1.3074  0.0004  0.1077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6000        1.4265       0.4792            0.4792        1.2835  0.0005  0.1066\n",
      "     33            \u001b[36m1.0000\u001b[0m        0.9097       0.5104            0.5104        1.2698  0.0005  0.1166\n",
      "     34            1.0000        \u001b[32m0.4139\u001b[0m       0.4896            0.4896        1.2707  0.0006  0.1294\n",
      "     35            1.0000        \u001b[32m0.0961\u001b[0m       0.5208            0.5208        1.2856  0.0006  0.1581\n",
      "     36            1.0000        \u001b[32m0.0648\u001b[0m       0.5208            0.5208        1.3124  0.0007  0.1789\n",
      "     37            1.0000        \u001b[32m0.0362\u001b[0m       0.5104            0.5104        1.3481  0.0007  0.1248\n",
      "     38            1.0000        \u001b[32m0.0154\u001b[0m       0.5104            0.5104        1.3874  0.0007  0.1133\n",
      "     39            1.0000        \u001b[32m0.0091\u001b[0m       0.5104            0.5104        1.4258  0.0007  0.1142\n",
      "     40            1.0000        \u001b[32m0.0061\u001b[0m       0.5208            0.5208        1.4585  0.0007  0.1131\n",
      "     41            1.0000        0.0114       0.5208            0.5208        1.4830  0.0007  0.1222\n",
      "     42            1.0000        0.0137       0.5208            0.5208        1.4984  0.0007  0.1170\n",
      "     43            1.0000        0.0218       0.5208            0.5208        1.5050  0.0006  0.1091\n",
      "     44            1.0000        \u001b[32m0.0060\u001b[0m       0.5104            0.5104        1.5037  0.0006  0.1329\n",
      "     45            1.0000        0.0122       0.5000            0.5000        1.4959  0.0005  0.1374\n",
      "     46            1.0000        0.0139       0.4896            0.4896        1.4843  0.0005  0.1318\n",
      "     47            1.0000        0.0241       0.4896            0.4896        1.4702  0.0004  0.1598\n",
      "     48            1.0000        0.0159       0.4792            0.4792        1.4552  0.0004  0.1313\n",
      "     49            1.0000        0.0101       0.4792            0.4792        1.4407  0.0003  0.1408\n",
      "     50            1.0000        0.0074       0.4792            0.4792        1.4279  0.0003  0.1622\n",
      "Fine tuning model for subject 9 with 10 = 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        1.0864       0.4792            0.4792        1.3273  0.0004  0.1394\n",
      "     32            \u001b[36m0.9000\u001b[0m        0.9658       0.4479            0.4479        1.3456  0.0005  0.1445\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3548\u001b[0m       0.4271            0.4271        1.3541  0.0005  0.1668\n",
      "     34            1.0000        \u001b[32m0.1883\u001b[0m       0.4583            0.4583        1.3493  0.0006  0.1251\n",
      "     35            1.0000        \u001b[32m0.0507\u001b[0m       0.4688            0.4688        1.3486  0.0006  0.1375\n",
      "     36            1.0000        0.1151       0.4792            0.4792        1.3435  0.0007  0.1298\n",
      "     37            1.0000        0.0899       0.5000            0.5000        1.3360  0.0007  0.1446\n",
      "     38            1.0000        \u001b[32m0.0359\u001b[0m       0.4896            0.4896        1.3302  0.0007  0.1601\n",
      "     39            1.0000        0.0576       0.4792            0.4792        1.3268  0.0007  0.1279\n",
      "     40            1.0000        \u001b[32m0.0355\u001b[0m       0.4792            0.4792        1.3265  0.0007  0.1759\n",
      "     41            1.0000        \u001b[32m0.0121\u001b[0m       0.4792            0.4792        1.3271  0.0007  0.1436\n",
      "     42            1.0000        0.0481       0.5208            0.5208        1.3297  0.0007  0.1120\n",
      "     43            1.0000        0.0624       0.5312            0.5312        1.3353  0.0006  0.1115\n",
      "     44            1.0000        \u001b[32m0.0076\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.3411  0.0006  0.1105\n",
      "     45            1.0000        0.0600       0.5625            0.5625        1.3485  0.0005  0.1069\n",
      "     46            1.0000        0.0107       0.5417            0.5417        1.3553  0.0005  0.1129\n",
      "     47            1.0000        \u001b[32m0.0049\u001b[0m       0.5312            0.5312        1.3607  0.0004  0.1099\n",
      "     48            1.0000        0.0117       0.5625            0.5625        1.3641  0.0004  0.1108\n",
      "     49            1.0000        0.0057       0.5521            0.5521        1.3667  0.0003  0.1086\n",
      "     50            1.0000        0.0075       0.5521            0.5521        1.3686  0.0003  0.1078\n",
      "Fine tuning model for subject 9 with 10 = 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        1.4977       0.4792            0.4792        1.3102  0.0004  0.1375\n",
      "     32            \u001b[36m1.0000\u001b[0m        0.9745       0.4792            0.4792        1.3085  0.0005  0.1318\n",
      "     33            1.0000        \u001b[32m0.6496\u001b[0m       0.4688            0.4688        1.2949  0.0005  0.1117\n",
      "     34            1.0000        \u001b[32m0.2359\u001b[0m       0.4896            0.4896        1.2841  0.0006  0.1160\n",
      "     35            1.0000        \u001b[32m0.0762\u001b[0m       0.5417            0.5417        1.2895  0.0006  0.1121\n",
      "     36            1.0000        \u001b[32m0.0707\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.3124  0.0007  0.1159\n",
      "     37            1.0000        0.0959       0.5417            0.5417        1.3480  0.0007  0.1135\n",
      "     38            1.0000        \u001b[32m0.0353\u001b[0m       0.5417            0.5417        1.3819  0.0007  0.1176\n",
      "     39            1.0000        0.0540       0.5208            0.5208        1.4061  0.0007  0.1176\n",
      "     40            1.0000        0.0511       0.5208            0.5208        1.4185  0.0007  0.1139\n",
      "     41            1.0000        \u001b[32m0.0234\u001b[0m       0.5208            0.5208        1.4199  0.0007  0.1149\n",
      "     42            1.0000        \u001b[32m0.0181\u001b[0m       0.5208            0.5208        1.4140  0.0007  0.1229\n",
      "     43            1.0000        \u001b[32m0.0075\u001b[0m       0.5312            0.5312        1.4025  0.0006  0.1214\n",
      "     44            1.0000        0.0126       0.5417            0.5417        1.3884  0.0006  0.1121\n",
      "     45            1.0000        0.0139       0.5417            0.5417        1.3743  0.0005  0.1094\n",
      "     46            1.0000        0.0094       0.5521            0.5521        1.3615  0.0005  0.1096\n",
      "     47            1.0000        0.0229       0.5625            0.5625        1.3513  0.0004  0.1086\n",
      "     48            1.0000        \u001b[32m0.0067\u001b[0m       0.5521            0.5521        1.3434  0.0004  0.1094\n",
      "     49            1.0000        \u001b[32m0.0067\u001b[0m       0.5625            0.5625        1.3381  0.0003  0.1171\n",
      "     50            1.0000        0.0116       0.5625            0.5625        1.3351  0.0003  0.1107\n",
      "Fine tuning model for subject 9 with 10 = 10 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        1.1726       0.4792            0.4792        1.3197  0.0004  0.1046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m1.0000\u001b[0m        0.8615       0.4688            0.4688        1.3329  0.0005  0.1105\n",
      "     33            1.0000        \u001b[32m0.4492\u001b[0m       0.4479            0.4479        1.3360  0.0005  0.1117\n",
      "     34            1.0000        \u001b[32m0.1488\u001b[0m       0.4792            0.4792        1.3254  0.0006  0.1113\n",
      "     35            1.0000        \u001b[32m0.0512\u001b[0m       0.5000            0.5000        1.3062  0.0006  0.1078\n",
      "     36            1.0000        \u001b[32m0.0460\u001b[0m       0.4792            0.4792        1.2861  0.0007  0.1149\n",
      "     37            1.0000        \u001b[32m0.0269\u001b[0m       0.5208            0.5208        1.2727  0.0007  0.1111\n",
      "     38            1.0000        \u001b[32m0.0123\u001b[0m       0.5104            0.5104        1.2703  0.0007  0.1111\n",
      "     39            1.0000        0.0163       0.4896            0.4896        1.2801  0.0007  0.1144\n",
      "     40            1.0000        \u001b[32m0.0065\u001b[0m       0.4896            0.4896        1.2998  0.0007  0.1120\n",
      "     41            1.0000        \u001b[32m0.0041\u001b[0m       0.5104            0.5104        1.3248  0.0007  0.1400\n",
      "     42            1.0000        0.0047       0.4792            0.4792        1.3510  0.0007  0.1104\n",
      "     43            1.0000        \u001b[32m0.0016\u001b[0m       0.4792            0.4792        1.3751  0.0006  0.1042\n",
      "     44            1.0000        \u001b[32m0.0012\u001b[0m       0.4792            0.4792        1.3950  0.0006  0.1102\n",
      "     45            1.0000        0.0055       0.4792            0.4792        1.4102  0.0005  0.1148\n",
      "     46            1.0000        0.0040       0.4688            0.4688        1.4203  0.0005  0.1116\n",
      "     47            1.0000        0.0022       0.4688            0.4688        1.4261  0.0004  0.1160\n",
      "     48            1.0000        \u001b[32m0.0009\u001b[0m       0.4583            0.4583        1.4282  0.0004  0.1062\n",
      "     49            1.0000        0.0014       0.4688            0.4688        1.4276  0.0003  0.1126\n",
      "     50            1.0000        0.0020       0.4792            0.4792        1.4251  0.0003  0.1080\n",
      "Fine tuning model for subject 9 with 10 = 10 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        1.8061       0.4583            0.4583        1.3145  0.0004  0.1122\n",
      "     32            0.7000        1.3112       0.4479            0.4479        1.3263  0.0005  0.1065\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6856\u001b[0m       0.4271            0.4271        1.3500  0.0005  0.1176\n",
      "     34            1.0000        \u001b[32m0.2716\u001b[0m       0.3958            0.3958        1.4020  0.0006  0.1117\n",
      "     35            1.0000        \u001b[32m0.1339\u001b[0m       0.4062            0.4062        1.4775  0.0006  0.1120\n",
      "     36            1.0000        \u001b[32m0.0571\u001b[0m       0.4271            0.4271        1.5621  0.0007  0.1083\n",
      "     37            1.0000        0.0596       0.3854            0.3854        1.6486  0.0007  0.1095\n",
      "     38            1.0000        \u001b[32m0.0395\u001b[0m       0.3646            0.3646        1.7374  0.0007  0.1090\n",
      "     39            1.0000        0.0851       0.3646            0.3646        1.8194  0.0007  0.1063\n",
      "     40            1.0000        \u001b[32m0.0059\u001b[0m       0.3333            0.3333        1.8966  0.0007  0.1120\n",
      "     41            1.0000        0.0142       0.3333            0.3333        1.9678  0.0007  0.1109\n",
      "     42            1.0000        \u001b[32m0.0039\u001b[0m       0.3438            0.3438        2.0329  0.0007  0.1124\n",
      "     43            1.0000        0.0049       0.3333            0.3333        2.0916  0.0006  0.1132\n",
      "     44            1.0000        \u001b[32m0.0037\u001b[0m       0.3333            0.3333        2.1435  0.0006  0.1125\n",
      "     45            1.0000        \u001b[32m0.0021\u001b[0m       0.3333            0.3333        2.1890  0.0005  0.1091\n",
      "     46            1.0000        0.0036       0.3229            0.3229        2.2286  0.0005  0.1114\n",
      "     47            1.0000        \u001b[32m0.0014\u001b[0m       0.3333            0.3333        2.2629  0.0004  0.1159\n",
      "     48            1.0000        0.0074       0.3333            0.3333        2.2924  0.0004  0.1223\n",
      "     49            1.0000        0.0074       0.3333            0.3333        2.3178  0.0003  0.1252\n",
      "     50            1.0000        0.0039       0.3333            0.3333        2.3396  0.0003  0.1238\n",
      "Fine tuning model for subject 9 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        1.1356       0.4792            0.4792        1.3159  0.0004  0.1385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8667\u001b[0m        1.1517       0.4792            0.4792        1.3196  0.0005  0.1176\n",
      "     33            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5651\u001b[0m       0.5000            0.5000        1.3142  0.0005  0.1188\n",
      "     34            0.9333        \u001b[32m0.3373\u001b[0m       0.5208            0.5208        1.3001  0.0006  0.0997\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2773\u001b[0m       0.4896            0.4896        1.2808  0.0006  0.1113\n",
      "     36            1.0000        \u001b[32m0.1782\u001b[0m       0.4896            0.4896        1.2738  0.0007  0.1035\n",
      "     37            1.0000        \u001b[32m0.0778\u001b[0m       0.4896            0.4896        1.2796  0.0007  0.1175\n",
      "     38            1.0000        0.0823       0.5104            0.5104        1.2962  0.0007  0.1288\n",
      "     39            1.0000        0.1656       0.5312            0.5312        1.3185  0.0007  0.1065\n",
      "     40            1.0000        0.1168       0.5208            0.5208        1.3387  0.0007  0.1125\n",
      "     41            1.0000        0.1291       0.5312            0.5312        1.3555  0.0007  0.1116\n",
      "     42            1.0000        \u001b[32m0.0703\u001b[0m       0.5208            0.5208        1.3698  0.0007  0.1195\n",
      "     43            1.0000        \u001b[32m0.0194\u001b[0m       0.5312            0.5312        1.3808  0.0006  0.1122\n",
      "     44            1.0000        \u001b[32m0.0148\u001b[0m       0.5208            0.5208        1.3880  0.0006  0.1196\n",
      "     45            1.0000        0.0190       0.5104            0.5104        1.3923  0.0005  0.1136\n",
      "     46            1.0000        0.0341       0.5104            0.5104        1.3925  0.0005  0.1121\n",
      "     47            1.0000        \u001b[32m0.0125\u001b[0m       0.5000            0.5000        1.3916  0.0004  0.1126\n",
      "     48            1.0000        \u001b[32m0.0118\u001b[0m       0.5000            0.5000        1.3897  0.0004  0.1079\n",
      "     49            1.0000        0.0558       0.4896            0.4896        1.3868  0.0003  0.1069\n",
      "     50            1.0000        \u001b[32m0.0041\u001b[0m       0.4583            0.4583        1.3836  0.0003  0.1102\n",
      "Fine tuning model for subject 9 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        1.1343       0.4792            0.4792        1.2907  0.0004  0.1112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7333        1.1154       0.4688            0.4688        1.2441  0.0005  0.1170\n",
      "     33            \u001b[36m0.8667\u001b[0m        0.9472       0.4896            0.4896        1.1904  0.0005  0.1047\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5312\u001b[0m       0.4583            0.4583        1.1650  0.0006  0.1100\n",
      "     35            1.0000        \u001b[32m0.3876\u001b[0m       0.4271            0.4271        1.1786  0.0006  0.1910\n",
      "     36            1.0000        \u001b[32m0.1440\u001b[0m       0.4271            0.4271        1.2175  0.0007  0.1156\n",
      "     37            1.0000        0.1804       0.4167            0.4167        1.2680  0.0007  0.1193\n",
      "     38            1.0000        \u001b[32m0.1047\u001b[0m       0.4583            0.4583        1.2716  0.0007  0.1128\n",
      "     39            1.0000        \u001b[32m0.0505\u001b[0m       0.4479            0.4479        1.2685  0.0007  0.1136\n",
      "     40            1.0000        0.0579       0.4688            0.4688        1.2601  0.0007  0.1295\n",
      "     41            1.0000        0.0579       0.4583            0.4583        1.2464  0.0007  0.1229\n",
      "     42            1.0000        \u001b[32m0.0162\u001b[0m       0.4792            0.4792        1.2372  0.0007  0.1536\n",
      "     43            1.0000        0.0290       0.4688            0.4688        1.2284  0.0006  0.1492\n",
      "     44            1.0000        \u001b[32m0.0152\u001b[0m       0.4792            0.4792        1.2194  0.0006  0.1456\n",
      "     45            1.0000        \u001b[32m0.0149\u001b[0m       0.4896            0.4896        1.2116  0.0005  0.1416\n",
      "     46            1.0000        0.0306       0.4792            0.4792        1.2011  0.0005  0.1381\n",
      "     47            1.0000        \u001b[32m0.0127\u001b[0m       0.4896            0.4896        1.1897  0.0004  0.1595\n",
      "     48            1.0000        0.0263       0.4792            0.4792        1.1775  0.0004  0.1374\n",
      "     49            1.0000        \u001b[32m0.0125\u001b[0m       0.4896            0.4896        1.1652  0.0003  0.1326\n",
      "     50            1.0000        0.0142       0.5000            0.5000        1.1533  0.0003  0.1380\n",
      "Fine tuning model for subject 9 with 15 = 15 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4667        1.5281       0.4688            0.4688        1.2945  0.0004  0.1375\n",
      "     32            0.6000        1.5882       0.4896            0.4896        1.2464  0.0005  0.1404\n",
      "     33            \u001b[36m0.8667\u001b[0m        1.3159       0.5208            0.5208        1.2092  0.0005  0.1429\n",
      "     34            \u001b[36m0.9333\u001b[0m        \u001b[32m0.6206\u001b[0m       0.5104            0.5104        1.1924  0.0006  0.1370\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3244\u001b[0m       0.5104            0.5104        1.1965  0.0006  0.1826\n",
      "     36            1.0000        \u001b[32m0.1879\u001b[0m       0.4688            0.4688        1.2262  0.0007  0.1279\n",
      "     37            1.0000        0.2073       0.4583            0.4583        1.2784  0.0007  0.1440\n",
      "     38            1.0000        \u001b[32m0.1345\u001b[0m       0.4375            0.4375        1.3388  0.0007  0.1427\n",
      "     39            1.0000        \u001b[32m0.0857\u001b[0m       0.4062            0.4062        1.4046  0.0007  0.1130\n",
      "     40            1.0000        \u001b[32m0.0636\u001b[0m       0.3854            0.3854        1.4680  0.0007  0.1066\n",
      "     41            1.0000        0.1099       0.4062            0.4062        1.5158  0.0007  0.1106\n",
      "     42            1.0000        0.1408       0.4062            0.4062        1.5490  0.0007  0.1094\n",
      "     43            1.0000        \u001b[32m0.0322\u001b[0m       0.4062            0.4062        1.5751  0.0006  0.1130\n",
      "     44            1.0000        \u001b[32m0.0289\u001b[0m       0.3958            0.3958        1.5931  0.0006  0.1105\n",
      "     45            1.0000        0.0297       0.4271            0.4271        1.6024  0.0005  0.1125\n",
      "     46            1.0000        0.0372       0.4479            0.4479        1.6060  0.0005  0.1087\n",
      "     47            1.0000        0.0358       0.4583            0.4583        1.6049  0.0004  0.1097\n",
      "     48            1.0000        \u001b[32m0.0248\u001b[0m       0.4583            0.4583        1.6014  0.0004  0.1138\n",
      "     49            1.0000        \u001b[32m0.0199\u001b[0m       0.4688            0.4688        1.5969  0.0003  0.1148\n",
      "     50            1.0000        \u001b[32m0.0162\u001b[0m       0.4479            0.4479        1.5927  0.0003  0.1137\n",
      "Fine tuning model for subject 9 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7333        0.7780       0.4792            0.4792        1.3108  0.0004  0.1090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        0.7986       0.4896            0.4896        1.3013  0.0005  0.1137\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4262\u001b[0m       0.5000            0.5000        1.3044  0.0005  0.1115\n",
      "     34            1.0000        \u001b[32m0.3181\u001b[0m       0.5208            0.5208        1.3147  0.0006  0.1105\n",
      "     35            1.0000        \u001b[32m0.1125\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.3370  0.0006  0.1028\n",
      "     36            1.0000        \u001b[32m0.0857\u001b[0m       0.5417            0.5417        1.3657  0.0007  0.1174\n",
      "     37            1.0000        0.1276       0.5208            0.5208        1.4056  0.0007  0.1118\n",
      "     38            1.0000        \u001b[32m0.0331\u001b[0m       0.5208            0.5208        1.4431  0.0007  0.1114\n",
      "     39            1.0000        \u001b[32m0.0143\u001b[0m       0.5208            0.5208        1.4759  0.0007  0.1174\n",
      "     40            1.0000        0.0469       0.5312            0.5312        1.5030  0.0007  0.1154\n",
      "     41            1.0000        0.0210       0.5208            0.5208        1.5230  0.0007  0.1142\n",
      "     42            1.0000        0.0330       0.5208            0.5208        1.5337  0.0007  0.1075\n",
      "     43            1.0000        \u001b[32m0.0110\u001b[0m       0.5208            0.5208        1.5381  0.0006  0.1136\n",
      "     44            1.0000        0.0562       0.5208            0.5208        1.5374  0.0006  0.1133\n",
      "     45            1.0000        0.0129       0.5312            0.5312        1.5319  0.0005  0.1103\n",
      "     46            1.0000        \u001b[32m0.0101\u001b[0m       0.5417            0.5417        1.5229  0.0005  0.1117\n",
      "     47            1.0000        0.0123       0.5417            0.5417        1.5112  0.0004  0.1130\n",
      "     48            1.0000        0.0164       0.5417            0.5417        1.4977  0.0004  0.1134\n",
      "     49            1.0000        0.0118       0.5417            0.5417        1.4835  0.0003  0.1052\n",
      "     50            1.0000        \u001b[32m0.0080\u001b[0m       0.5417            0.5417        1.4694  0.0003  0.1172\n",
      "Fine tuning model for subject 9 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        1.2903       0.4792            0.4792        1.3166  0.0004  0.1097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7333        0.9662       0.4688            0.4688        1.3198  0.0005  0.1128\n",
      "     33            \u001b[36m0.9333\u001b[0m        0.8138       0.4688            0.4688        1.3148  0.0005  0.1124\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3509\u001b[0m       0.4896            0.4896        1.3336  0.0006  0.1137\n",
      "     35            1.0000        \u001b[32m0.2500\u001b[0m       0.4896            0.4896        1.3645  0.0006  0.1070\n",
      "     36            1.0000        \u001b[32m0.0789\u001b[0m       0.4583            0.4583        1.3974  0.0007  0.1131\n",
      "     37            1.0000        \u001b[32m0.0302\u001b[0m       0.4271            0.4271        1.4230  0.0007  0.1114\n",
      "     38            1.0000        0.0789       0.4375            0.4375        1.4358  0.0007  0.1063\n",
      "     39            1.0000        0.0955       0.4688            0.4688        1.4383  0.0007  0.1087\n",
      "     40            1.0000        \u001b[32m0.0227\u001b[0m       0.4479            0.4479        1.4359  0.0007  0.1123\n",
      "     41            1.0000        0.0366       0.4375            0.4375        1.4322  0.0007  0.1076\n",
      "     42            1.0000        \u001b[32m0.0193\u001b[0m       0.4479            0.4479        1.4256  0.0007  0.1084\n",
      "     43            1.0000        \u001b[32m0.0079\u001b[0m       0.4583            0.4583        1.4186  0.0006  0.1112\n",
      "     44            1.0000        0.0135       0.4375            0.4375        1.4117  0.0006  0.1125\n",
      "     45            1.0000        0.0170       0.4479            0.4479        1.4054  0.0005  0.1112\n",
      "     46            1.0000        0.0099       0.4479            0.4479        1.3994  0.0005  0.1112\n",
      "     47            1.0000        0.0266       0.4479            0.4479        1.3939  0.0004  0.1064\n",
      "     48            1.0000        \u001b[32m0.0069\u001b[0m       0.4479            0.4479        1.3888  0.0004  0.1107\n",
      "     49            1.0000        \u001b[32m0.0049\u001b[0m       0.4479            0.4479        1.3839  0.0003  0.1089\n",
      "     50            1.0000        \u001b[32m0.0044\u001b[0m       0.4583            0.4583        1.3793  0.0003  0.1123\n",
      "Fine tuning model for subject 9 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.1903       0.4792            0.4792        1.3223  0.0004  0.1175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        0.8695       0.4479            0.4479        1.3260  0.0005  0.1140\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6897\u001b[0m       0.4375            0.4375        1.3295  0.0005  0.1119\n",
      "     34            1.0000        \u001b[32m0.4231\u001b[0m       0.4688            0.4688        1.3433  0.0006  0.1102\n",
      "     35            1.0000        \u001b[32m0.1304\u001b[0m       0.4583            0.4583        1.3589  0.0006  0.1121\n",
      "     36            1.0000        \u001b[32m0.0815\u001b[0m       0.4583            0.4583        1.3771  0.0007  0.1123\n",
      "     37            1.0000        \u001b[32m0.0492\u001b[0m       0.4271            0.4271        1.3972  0.0007  0.1144\n",
      "     38            1.0000        0.0720       0.4271            0.4271        1.4238  0.0007  0.1147\n",
      "     39            1.0000        \u001b[32m0.0447\u001b[0m       0.4167            0.4167        1.4537  0.0007  0.1120\n",
      "     40            1.0000        \u001b[32m0.0434\u001b[0m       0.4167            0.4167        1.4826  0.0007  0.1057\n",
      "     41            1.0000        \u001b[32m0.0276\u001b[0m       0.4375            0.4375        1.5083  0.0007  0.1110\n",
      "     42            1.0000        \u001b[32m0.0116\u001b[0m       0.4375            0.4375        1.5281  0.0007  0.1124\n",
      "     43            1.0000        0.0165       0.4375            0.4375        1.5415  0.0006  0.1116\n",
      "     44            1.0000        0.0235       0.4271            0.4271        1.5479  0.0006  0.1125\n",
      "     45            1.0000        \u001b[32m0.0107\u001b[0m       0.4271            0.4271        1.5484  0.0005  0.1122\n",
      "     46            1.0000        \u001b[32m0.0096\u001b[0m       0.4271            0.4271        1.5445  0.0005  0.1094\n",
      "     47            1.0000        0.0110       0.4375            0.4375        1.5373  0.0004  0.1068\n",
      "     48            1.0000        \u001b[32m0.0068\u001b[0m       0.4375            0.4375        1.5279  0.0004  0.1117\n",
      "     49            1.0000        0.0174       0.4271            0.4271        1.5175  0.0003  0.1174\n",
      "     50            1.0000        0.0116       0.4271            0.4271        1.5068  0.0003  0.1084\n",
      "Fine tuning model for subject 9 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        1.2869       0.4792            0.4792        1.3230  0.0004  0.1083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7333        0.8609       0.4583            0.4583        1.3159  0.0005  0.1127\n",
      "     33            \u001b[36m0.8000\u001b[0m        0.8710       0.4375            0.4375        1.3035  0.0005  0.1102\n",
      "     34            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3510\u001b[0m       0.4896            0.4896        1.2880  0.0006  0.1169\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2267\u001b[0m       0.4479            0.4479        1.2796  0.0006  0.1037\n",
      "     36            1.0000        \u001b[32m0.1672\u001b[0m       0.4583            0.4583        1.2893  0.0007  0.1174\n",
      "     37            1.0000        \u001b[32m0.0800\u001b[0m       0.4271            0.4271        1.3110  0.0007  0.1129\n",
      "     38            1.0000        0.1997       0.4479            0.4479        1.3242  0.0007  0.1098\n",
      "     39            1.0000        \u001b[32m0.0777\u001b[0m       0.4583            0.4583        1.3366  0.0007  0.1170\n",
      "     40            1.0000        0.1187       0.4479            0.4479        1.3466  0.0007  0.1117\n",
      "     41            1.0000        0.0832       0.4375            0.4375        1.3578  0.0007  0.1070\n",
      "     42            1.0000        \u001b[32m0.0754\u001b[0m       0.4375            0.4375        1.3698  0.0007  0.1113\n",
      "     43            1.0000        \u001b[32m0.0370\u001b[0m       0.4375            0.4375        1.3851  0.0006  0.1114\n",
      "     44            1.0000        0.0502       0.4479            0.4479        1.4025  0.0006  0.1064\n",
      "     45            1.0000        \u001b[32m0.0145\u001b[0m       0.4583            0.4583        1.4193  0.0005  0.1121\n",
      "     46            1.0000        0.0162       0.4688            0.4688        1.4336  0.0005  0.1120\n",
      "     47            1.0000        \u001b[32m0.0123\u001b[0m       0.4583            0.4583        1.4453  0.0004  0.1068\n",
      "     48            1.0000        0.0167       0.4688            0.4688        1.4543  0.0004  0.1087\n",
      "     49            1.0000        \u001b[32m0.0103\u001b[0m       0.4688            0.4688        1.4602  0.0003  0.1088\n",
      "     50            1.0000        0.0114       0.4688            0.4688        1.4637  0.0003  0.1125\n",
      "Fine tuning model for subject 9 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.9566       0.4688            0.4688        1.2870  0.0004  0.1081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        1.1466       0.4896            0.4896        1.2369  0.0005  0.1122\n",
      "     33            \u001b[36m0.8667\u001b[0m        0.8654       0.5312            0.5312        1.1993  0.0005  0.1066\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4341\u001b[0m       0.5312            0.5312        1.1881  0.0006  0.1161\n",
      "     35            1.0000        \u001b[32m0.2958\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.2022  0.0006  0.1215\n",
      "     36            1.0000        \u001b[32m0.2091\u001b[0m       0.5625            0.5625        1.2384  0.0007  0.1279\n",
      "     37            1.0000        \u001b[32m0.1675\u001b[0m       0.5521            0.5521        1.2834  0.0007  0.1214\n",
      "     38            1.0000        0.1723       0.5417            0.5417        1.3244  0.0007  0.1497\n",
      "     39            1.0000        \u001b[32m0.0487\u001b[0m       0.5104            0.5104        1.3632  0.0007  0.1311\n",
      "     40            1.0000        0.1029       0.4896            0.4896        1.3939  0.0007  0.1376\n",
      "     41            1.0000        \u001b[32m0.0446\u001b[0m       0.4896            0.4896        1.4215  0.0007  0.1303\n",
      "     42            1.0000        \u001b[32m0.0372\u001b[0m       0.5000            0.5000        1.4464  0.0007  0.1461\n",
      "     43            1.0000        0.0743       0.5104            0.5104        1.4723  0.0006  0.1491\n",
      "     44            1.0000        \u001b[32m0.0288\u001b[0m       0.5208            0.5208        1.4939  0.0006  0.1496\n",
      "     45            1.0000        \u001b[32m0.0159\u001b[0m       0.5104            0.5104        1.5145  0.0005  0.1367\n",
      "     46            1.0000        0.0313       0.5000            0.5000        1.5320  0.0005  0.1339\n",
      "     47            1.0000        \u001b[32m0.0153\u001b[0m       0.5208            0.5208        1.5488  0.0004  0.1240\n",
      "     48            1.0000        \u001b[32m0.0113\u001b[0m       0.5312            0.5312        1.5650  0.0004  0.1388\n",
      "     49            1.0000        0.0267       0.5208            0.5208        1.5804  0.0003  0.1327\n",
      "     50            1.0000        \u001b[32m0.0090\u001b[0m       0.5208            0.5208        1.5954  0.0003  0.1298\n",
      "Fine tuning model for subject 9 with 15 = 15 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.8613       0.4688            0.4688        1.2984  0.0004  0.1561\n",
      "     32            0.7333        1.5536       0.5104            0.5104        1.2800  0.0005  0.1443\n",
      "     33            \u001b[36m0.8000\u001b[0m        1.0273       0.5417            0.5417        1.2921  0.0005  0.1434\n",
      "     34            \u001b[36m0.9333\u001b[0m        0.7446       0.5104            0.5104        1.3139  0.0006  0.1263\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2016\u001b[0m       0.5208            0.5208        1.3406  0.0006  0.1632\n",
      "     36            1.0000        0.2919       0.5417            0.5417        1.3639  0.0007  0.1587\n",
      "     37            1.0000        \u001b[32m0.1797\u001b[0m       0.5208            0.5208        1.3899  0.0007  0.1089\n",
      "     38            1.0000        \u001b[32m0.1148\u001b[0m       0.5312            0.5312        1.4156  0.0007  0.1104\n",
      "     39            1.0000        \u001b[32m0.1109\u001b[0m       0.5208            0.5208        1.4397  0.0007  0.1090\n",
      "     40            1.0000        \u001b[32m0.0650\u001b[0m       0.5000            0.5000        1.4612  0.0007  0.1115\n",
      "     41            1.0000        \u001b[32m0.0558\u001b[0m       0.5000            0.5000        1.4701  0.0007  0.1120\n",
      "     42            1.0000        \u001b[32m0.0414\u001b[0m       0.4896            0.4896        1.4699  0.0007  0.1081\n",
      "     43            1.0000        0.0519       0.4896            0.4896        1.4644  0.0006  0.1195\n",
      "     44            1.0000        \u001b[32m0.0204\u001b[0m       0.5000            0.5000        1.4564  0.0006  0.1144\n",
      "     45            1.0000        \u001b[32m0.0202\u001b[0m       0.5000            0.5000        1.4476  0.0005  0.1058\n",
      "     46            1.0000        0.0285       0.5000            0.5000        1.4391  0.0005  0.1121\n",
      "     47            1.0000        0.0233       0.5000            0.5000        1.4320  0.0004  0.1109\n",
      "     48            1.0000        \u001b[32m0.0164\u001b[0m       0.5000            0.5000        1.4267  0.0004  0.1105\n",
      "     49            1.0000        0.0287       0.4896            0.4896        1.4238  0.0003  0.1122\n",
      "     50            1.0000        0.0237       0.5000            0.5000        1.4212  0.0003  0.1097\n",
      "Fine tuning model for subject 9 with 15 = 15 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6667        0.9944       0.4792            0.4792        1.3121  0.0004  0.1104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6377\u001b[0m       0.4583            0.4583        1.3048  0.0005  0.1127\n",
      "     33            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5080\u001b[0m       0.5000            0.5000        1.2943  0.0005  0.1097\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3467\u001b[0m       0.5208            0.5208        1.2950  0.0006  0.1136\n",
      "     35            1.0000        \u001b[32m0.1730\u001b[0m       0.5417            0.5417        1.3074  0.0006  0.1166\n",
      "     36            1.0000        \u001b[32m0.1104\u001b[0m       0.5417            0.5417        1.3339  0.0007  0.1180\n",
      "     37            0.9333        \u001b[32m0.0595\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.3712  0.0007  0.1116\n",
      "     38            0.9333        0.0686       0.5625            0.5625        1.4095  0.0007  0.1115\n",
      "     39            0.9333        \u001b[32m0.0563\u001b[0m       0.5625            0.5625        1.4445  0.0007  0.1094\n",
      "     40            0.9333        0.0565       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.4723  0.0007  0.1118\n",
      "     41            0.9333        0.0768       0.5729            0.5729        1.4895  0.0007  0.1110\n",
      "     42            0.9333        0.0608       0.5729            0.5729        1.4892  0.0007  0.1115\n",
      "     43            0.9333        \u001b[32m0.0259\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.4813  0.0006  0.1037\n",
      "     44            0.9333        0.0281       0.5833            0.5833        1.4646  0.0006  0.1094\n",
      "     45            1.0000        \u001b[32m0.0216\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.4435  0.0005  0.1108\n",
      "     46            1.0000        \u001b[32m0.0184\u001b[0m       0.5938            0.5938        1.4204  0.0005  0.1115\n",
      "     47            1.0000        0.0261       0.5938            0.5938        1.3981  0.0004  0.1059\n",
      "     48            1.0000        \u001b[32m0.0173\u001b[0m       0.5625            0.5625        1.3766  0.0004  0.1121\n",
      "     49            1.0000        \u001b[32m0.0135\u001b[0m       0.5625            0.5625        1.3569  0.0003  0.1038\n",
      "     50            1.0000        0.0156       0.5417            0.5417        1.3396  0.0003  0.1089\n",
      "Fine tuning model for subject 9 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7500        1.4573       0.4792            0.4792        1.3067  0.0004  0.1114\n",
      "     32            0.7500        1.0368       0.4896            0.4896        1.2933  0.0005  0.1198\n",
      "     33            \u001b[36m0.8500\u001b[0m        0.8017       0.4896            0.4896        1.2863  0.0005  0.1133\n",
      "     34            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4264\u001b[0m       0.5104            0.5104        1.2846  0.0006  0.1123\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3690\u001b[0m       0.5208            0.5208        1.2792  0.0006  0.1119\n",
      "     36            1.0000        \u001b[32m0.2936\u001b[0m       0.5312            0.5312        1.2677  0.0007  0.1174\n",
      "     37            1.0000        \u001b[32m0.1603\u001b[0m       0.5417            0.5417        1.2450  0.0007  0.1117\n",
      "     38            1.0000        \u001b[32m0.1555\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.2205  0.0007  0.1130\n",
      "     39            1.0000        \u001b[32m0.0683\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.1999  0.0007  0.1163\n",
      "     40            1.0000        0.0777       0.5312            0.5312        1.1934  0.0007  0.1172\n",
      "     41            1.0000        \u001b[32m0.0307\u001b[0m       0.5625            0.5625        1.1988  0.0007  0.1159\n",
      "     42            1.0000        0.0380       0.5729            0.5729        1.2142  0.0007  0.1132\n",
      "     43            1.0000        0.0403       0.5729            0.5729        1.2349  0.0006  0.1130\n",
      "     44            1.0000        0.0319       0.5729            0.5729        1.2556  0.0006  0.1174\n",
      "     45            1.0000        0.0376       0.5625            0.5625        1.2742  0.0005  0.1155\n",
      "     46            1.0000        \u001b[32m0.0210\u001b[0m       0.5625            0.5625        1.2886  0.0005  0.1112\n",
      "     47            1.0000        \u001b[32m0.0137\u001b[0m       0.5521            0.5521        1.2987  0.0004  0.1193\n",
      "     48            1.0000        0.0178       0.5625            0.5625        1.3046  0.0004  0.1107\n",
      "     49            1.0000        0.0291       0.5625            0.5625        1.3075  0.0003  0.1123\n",
      "     50            1.0000        0.0211       0.5729            0.5729        1.3084  0.0003  0.1171\n",
      "Fine tuning model for subject 9 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7500        1.3306       0.4792            0.4792        1.3233  0.0004  0.1109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7500        0.9097       0.4688            0.4688        1.3201  0.0005  0.1163\n",
      "     33            \u001b[36m0.9500\u001b[0m        \u001b[32m0.6647\u001b[0m       0.4792            0.4792        1.3004  0.0005  0.1091\n",
      "     34            0.9500        0.7029       0.4792            0.4792        1.2785  0.0006  0.1130\n",
      "     35            0.9500        \u001b[32m0.3973\u001b[0m       0.5208            0.5208        1.2753  0.0006  0.1107\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1943\u001b[0m       0.5104            0.5104        1.2875  0.0007  0.1114\n",
      "     37            1.0000        \u001b[32m0.1332\u001b[0m       0.5417            0.5417        1.3079  0.0007  0.1041\n",
      "     38            1.0000        0.1541       0.5208            0.5208        1.3311  0.0007  0.1102\n",
      "     39            1.0000        \u001b[32m0.0907\u001b[0m       0.5417            0.5417        1.3522  0.0007  0.1073\n",
      "     40            1.0000        \u001b[32m0.0689\u001b[0m       0.5417            0.5417        1.3642  0.0007  0.1115\n",
      "     41            1.0000        \u001b[32m0.0601\u001b[0m       0.5312            0.5312        1.3661  0.0007  0.1314\n",
      "     42            1.0000        \u001b[32m0.0423\u001b[0m       0.5312            0.5312        1.3639  0.0007  0.1264\n",
      "     43            1.0000        \u001b[32m0.0242\u001b[0m       0.5312            0.5312        1.3574  0.0006  0.1138\n",
      "     44            1.0000        0.0557       0.5208            0.5208        1.3498  0.0006  0.1130\n",
      "     45            1.0000        \u001b[32m0.0133\u001b[0m       0.5104            0.5104        1.3399  0.0005  0.1140\n",
      "     46            1.0000        0.0167       0.5312            0.5312        1.3283  0.0005  0.1084\n",
      "     47            1.0000        0.0261       0.5312            0.5312        1.3168  0.0004  0.1136\n",
      "     48            1.0000        0.0188       0.5312            0.5312        1.3050  0.0004  0.1172\n",
      "     49            1.0000        \u001b[32m0.0105\u001b[0m       0.5312            0.5312        1.2939  0.0003  0.1147\n",
      "     50            1.0000        0.0197       0.5312            0.5312        1.2840  0.0003  0.1144\n",
      "Fine tuning model for subject 9 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.9261       0.4688            0.4688        1.3266  0.0004  0.1180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.6500        1.6555       0.4271            0.4271        1.3326  0.0005  0.1159\n",
      "     33            0.7500        1.1544       0.4479            0.4479        1.3324  0.0005  0.1163\n",
      "     34            \u001b[36m0.9000\u001b[0m        0.8478       0.4583            0.4583        1.3464  0.0006  0.1156\n",
      "     35            0.9000        \u001b[32m0.5109\u001b[0m       0.4688            0.4688        1.3719  0.0006  0.1186\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4068\u001b[0m       0.4792            0.4792        1.4203  0.0007  0.1243\n",
      "     37            1.0000        \u001b[32m0.2294\u001b[0m       0.4583            0.4583        1.4955  0.0007  0.1116\n",
      "     38            0.9500        0.2337       0.4688            0.4688        1.5785  0.0007  0.1043\n",
      "     39            0.9500        \u001b[32m0.1165\u001b[0m       0.4583            0.4583        1.6622  0.0007  0.1140\n",
      "     40            0.9500        \u001b[32m0.0681\u001b[0m       0.4271            0.4271        1.7394  0.0007  0.1174\n",
      "     41            0.9500        0.1137       0.4375            0.4375        1.8015  0.0007  0.1168\n",
      "     42            1.0000        0.0973       0.4271            0.4271        1.8518  0.0007  0.1092\n",
      "     43            1.0000        \u001b[32m0.0479\u001b[0m       0.4271            0.4271        1.8928  0.0006  0.1173\n",
      "     44            1.0000        \u001b[32m0.0413\u001b[0m       0.4167            0.4167        1.9271  0.0006  0.1083\n",
      "     45            1.0000        \u001b[32m0.0258\u001b[0m       0.4167            0.4167        1.9549  0.0005  0.1126\n",
      "     46            1.0000        0.0381       0.4167            0.4167        1.9750  0.0005  0.1153\n",
      "     47            1.0000        0.0413       0.4479            0.4479        1.9908  0.0004  0.1137\n",
      "     48            1.0000        0.0317       0.4479            0.4479        2.0053  0.0004  0.1611\n",
      "     49            1.0000        \u001b[32m0.0200\u001b[0m       0.4167            0.4167        2.0198  0.0003  0.1176\n",
      "     50            1.0000        0.0368       0.4167            0.4167        2.0348  0.0003  0.1137\n",
      "Fine tuning model for subject 9 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.6314       0.4688            0.4688        1.3025  0.0004  0.1161\n",
      "     32            0.7500        1.2995       0.5104            0.5104        1.2823  0.0005  0.1116\n",
      "     33            0.7500        0.8610       0.5312            0.5312        1.2830  0.0005  0.1383\n",
      "     34            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6095\u001b[0m       0.5208            0.5208        1.3021  0.0006  0.1283\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4529\u001b[0m       0.5208            0.5208        1.3382  0.0006  0.1383\n",
      "     36            1.0000        \u001b[32m0.2368\u001b[0m       0.5312            0.5312        1.3761  0.0007  0.1491\n",
      "     37            1.0000        \u001b[32m0.1673\u001b[0m       0.5312            0.5312        1.4117  0.0007  0.1367\n",
      "     38            1.0000        \u001b[32m0.1086\u001b[0m       0.5312            0.5312        1.4372  0.0007  0.1703\n",
      "     39            1.0000        \u001b[32m0.0887\u001b[0m       0.5417            0.5417        1.4519  0.0007  0.1634\n",
      "     40            1.0000        0.1016       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.4483  0.0007  0.1271\n",
      "     41            1.0000        \u001b[32m0.0840\u001b[0m       0.5417            0.5417        1.4324  0.0007  0.1434\n",
      "     42            1.0000        \u001b[32m0.0781\u001b[0m       0.5417            0.5417        1.4082  0.0007  0.1430\n",
      "     43            1.0000        \u001b[32m0.0536\u001b[0m       0.5312            0.5312        1.3817  0.0006  0.1323\n",
      "     44            1.0000        \u001b[32m0.0255\u001b[0m       0.5417            0.5417        1.3573  0.0006  0.1312\n",
      "     45            1.0000        0.0287       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.3372  0.0005  0.1391\n",
      "     46            1.0000        0.0440       0.5729            0.5729        1.3212  0.0005  0.1315\n",
      "     47            1.0000        0.0365       0.5729            0.5729        1.3090  0.0004  0.1343\n",
      "     48            1.0000        0.0267       0.5729            0.5729        1.2998  0.0004  0.1493\n",
      "     49            1.0000        0.0457       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.2926  0.0003  0.1350\n",
      "     50            1.0000        \u001b[32m0.0208\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.2875  0.0003  0.1323\n",
      "Fine tuning model for subject 9 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6500        1.1561       0.4792            0.4792        1.2899  0.0004  0.1571\n",
      "     32            \u001b[36m0.8500\u001b[0m        1.1921       0.4896            0.4896        1.2384  0.0005  0.1153\n",
      "     33            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6766\u001b[0m       0.5104            0.5104        1.2040  0.0005  0.1222\n",
      "     34            0.9000        \u001b[32m0.5436\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.1866  0.0006  0.1263\n",
      "     35            \u001b[36m0.9500\u001b[0m        \u001b[32m0.2787\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.1782  0.0006  0.1224\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1139\u001b[0m       0.5729            0.5729        1.1651  0.0007  0.1345\n",
      "     37            1.0000        \u001b[32m0.0689\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.1560  0.0007  0.1445\n",
      "     38            1.0000        0.0788       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.1566  0.0007  0.1117\n",
      "     39            1.0000        \u001b[32m0.0436\u001b[0m       0.6146            0.6146        1.1656  0.0007  0.1322\n",
      "     40            1.0000        0.0557       0.6146            0.6146        1.1814  0.0007  0.1163\n",
      "     41            1.0000        0.0829       0.5833            0.5833        1.1969  0.0007  0.1047\n",
      "     42            1.0000        \u001b[32m0.0382\u001b[0m       0.5833            0.5833        1.2134  0.0007  0.1187\n",
      "     43            1.0000        0.0620       0.5833            0.5833        1.2312  0.0006  0.1189\n",
      "     44            1.0000        \u001b[32m0.0245\u001b[0m       0.5833            0.5833        1.2489  0.0006  0.1173\n",
      "     45            1.0000        0.0633       0.5625            0.5625        1.2641  0.0005  0.1191\n",
      "     46            1.0000        0.0304       0.5625            0.5625        1.2801  0.0005  0.1095\n",
      "     47            1.0000        \u001b[32m0.0164\u001b[0m       0.5625            0.5625        1.2971  0.0004  0.1174\n",
      "     48            1.0000        \u001b[32m0.0109\u001b[0m       0.5417            0.5417        1.3137  0.0004  0.1156\n",
      "     49            1.0000        \u001b[32m0.0056\u001b[0m       0.5417            0.5417        1.3299  0.0003  0.1230\n",
      "     50            1.0000        0.0150       0.5417            0.5417        1.3451  0.0003  0.1173\n",
      "Fine tuning model for subject 9 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7000        1.1186       0.4792            0.4792        1.3152  0.0004  0.1059\n",
      "     32            \u001b[36m0.8000\u001b[0m        \u001b[32m0.6357\u001b[0m       0.4688            0.4688        1.3140  0.0005  0.1095\n",
      "     33            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5756\u001b[0m       0.4688            0.4688        1.3182  0.0005  0.1074\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4803\u001b[0m       0.4688            0.4688        1.3196  0.0006  0.1100\n",
      "     35            1.0000        \u001b[32m0.3426\u001b[0m       0.5104            0.5104        1.3284  0.0006  0.1113\n",
      "     36            1.0000        \u001b[32m0.2068\u001b[0m       0.5208            0.5208        1.3426  0.0007  0.1121\n",
      "     37            1.0000        \u001b[32m0.0554\u001b[0m       0.5312            0.5312        1.3572  0.0007  0.1136\n",
      "     38            1.0000        \u001b[32m0.0453\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.3716  0.0007  0.1145\n",
      "     39            1.0000        0.0585       0.5521            0.5521        1.3875  0.0007  0.1143\n",
      "     40            1.0000        \u001b[32m0.0347\u001b[0m       0.5312            0.5312        1.4033  0.0007  0.1154\n",
      "     41            1.0000        0.0395       0.5417            0.5417        1.4195  0.0007  0.1077\n",
      "     42            1.0000        \u001b[32m0.0240\u001b[0m       0.5417            0.5417        1.4326  0.0007  0.1120\n",
      "     43            1.0000        \u001b[32m0.0191\u001b[0m       0.5521            0.5521        1.4411  0.0006  0.1118\n",
      "     44            1.0000        \u001b[32m0.0114\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.4449  0.0006  0.1106\n",
      "     45            1.0000        0.0223       0.5625            0.5625        1.4427  0.0005  0.1266\n",
      "     46            1.0000        0.0173       0.5625            0.5625        1.4364  0.0005  0.1141\n",
      "     47            1.0000        0.0172       0.5625            0.5625        1.4272  0.0004  0.1150\n",
      "     48            1.0000        0.0141       0.5625            0.5625        1.4157  0.0004  0.1192\n",
      "     49            1.0000        0.0170       0.5625            0.5625        1.4028  0.0003  0.1157\n",
      "     50            1.0000        0.0127       0.5625            0.5625        1.3891  0.0003  0.1183\n",
      "Fine tuning model for subject 9 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7500        0.9383       0.4688            0.4688        1.3101  0.0004  0.1146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        1.2769       0.4792            0.4792        1.2959  0.0005  0.1192\n",
      "     33            \u001b[36m0.9000\u001b[0m        0.8771       0.4896            0.4896        1.2758  0.0005  0.1189\n",
      "     34            0.9000        \u001b[32m0.4693\u001b[0m       0.5312            0.5312        1.2610  0.0006  0.1149\n",
      "     35            \u001b[36m1.0000\u001b[0m        0.5104       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.2525  0.0006  0.1154\n",
      "     36            1.0000        \u001b[32m0.2752\u001b[0m       0.5625            0.5625        1.2602  0.0007  0.1149\n",
      "     37            1.0000        \u001b[32m0.1203\u001b[0m       0.5208            0.5208        1.2786  0.0007  0.1143\n",
      "     38            1.0000        \u001b[32m0.0855\u001b[0m       0.5104            0.5104        1.3047  0.0007  0.1173\n",
      "     39            1.0000        0.1232       0.5000            0.5000        1.3350  0.0007  0.1117\n",
      "     40            1.0000        0.1115       0.5000            0.5000        1.3622  0.0007  0.1111\n",
      "     41            1.0000        0.0864       0.4896            0.4896        1.3846  0.0007  0.1163\n",
      "     42            1.0000        \u001b[32m0.0381\u001b[0m       0.4792            0.4792        1.4035  0.0007  0.1141\n",
      "     43            1.0000        \u001b[32m0.0325\u001b[0m       0.4792            0.4792        1.4175  0.0006  0.1143\n",
      "     44            1.0000        0.0461       0.4896            0.4896        1.4249  0.0006  0.1129\n",
      "     45            1.0000        0.0394       0.4896            0.4896        1.4290  0.0005  0.1193\n",
      "     46            1.0000        \u001b[32m0.0122\u001b[0m       0.5000            0.5000        1.4298  0.0005  0.1195\n",
      "     47            1.0000        0.0284       0.5000            0.5000        1.4273  0.0004  0.1136\n",
      "     48            1.0000        0.0294       0.5000            0.5000        1.4224  0.0004  0.1136\n",
      "     49            1.0000        \u001b[32m0.0112\u001b[0m       0.5000            0.5000        1.4162  0.0003  0.1108\n",
      "     50            1.0000        \u001b[32m0.0109\u001b[0m       0.5000            0.5000        1.4095  0.0003  0.1148\n",
      "Fine tuning model for subject 9 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4500        1.6433       0.4688            0.4688        1.2999  0.0004  0.1123\n",
      "     32            0.5500        1.8769       0.4688            0.4688        1.2671  0.0005  0.1180\n",
      "     33            0.7000        1.1117       0.5104            0.5104        1.2373  0.0005  0.1145\n",
      "     34            \u001b[36m0.9500\u001b[0m        1.0210       0.4479            0.4479        1.2220  0.0006  0.1162\n",
      "     35            0.9000        \u001b[32m0.5903\u001b[0m       0.4583            0.4583        1.2461  0.0006  0.1099\n",
      "     36            0.9500        \u001b[32m0.2157\u001b[0m       0.4583            0.4583        1.2845  0.0007  0.1107\n",
      "     37            0.9500        0.2900       0.4688            0.4688        1.3201  0.0007  0.1165\n",
      "     38            \u001b[36m1.0000\u001b[0m        0.2497       0.4792            0.4792        1.3439  0.0007  0.1113\n",
      "     39            1.0000        \u001b[32m0.1571\u001b[0m       0.4688            0.4688        1.3535  0.0007  0.1108\n",
      "     40            1.0000        \u001b[32m0.0919\u001b[0m       0.4583            0.4583        1.3605  0.0007  0.1124\n",
      "     41            1.0000        \u001b[32m0.0869\u001b[0m       0.4479            0.4479        1.3662  0.0007  0.1191\n",
      "     42            1.0000        \u001b[32m0.0452\u001b[0m       0.4583            0.4583        1.3704  0.0007  0.1134\n",
      "     43            1.0000        \u001b[32m0.0306\u001b[0m       0.4583            0.4583        1.3696  0.0006  0.1159\n",
      "     44            1.0000        0.0429       0.4583            0.4583        1.3618  0.0006  0.1213\n",
      "     45            1.0000        0.0388       0.4479            0.4479        1.3503  0.0005  0.1087\n",
      "     46            1.0000        0.0593       0.4688            0.4688        1.3324  0.0005  0.1099\n",
      "     47            1.0000        \u001b[32m0.0184\u001b[0m       0.4688            0.4688        1.3136  0.0004  0.1193\n",
      "     48            1.0000        \u001b[32m0.0156\u001b[0m       0.4688            0.4688        1.2944  0.0004  0.1104\n",
      "     49            1.0000        0.0167       0.4896            0.4896        1.2756  0.0003  0.1094\n",
      "     50            1.0000        0.0228       0.4896            0.4896        1.2580  0.0003  0.1210\n",
      "Fine tuning model for subject 9 with 20 = 20 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5500        0.7446       0.4792            0.4792        1.3001  0.0004  0.1136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            0.7000        0.8012       0.4688            0.4688        1.2579  0.0005  0.1114\n",
      "     33            \u001b[36m0.8000\u001b[0m        \u001b[32m0.3916\u001b[0m       0.5104            0.5104        1.2134  0.0005  0.1118\n",
      "     34            \u001b[36m0.9500\u001b[0m        0.4575       0.5104            0.5104        1.1743  0.0006  0.1111\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3159\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.1479  0.0006  0.1141\n",
      "     36            1.0000        \u001b[32m0.1620\u001b[0m       0.5417            0.5417        1.1340  0.0007  0.1172\n",
      "     37            1.0000        \u001b[32m0.0994\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.1313  0.0007  0.1151\n",
      "     38            1.0000        0.1054       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.1365  0.0007  0.1145\n",
      "     39            1.0000        \u001b[32m0.0762\u001b[0m       0.5625            0.5625        1.1468  0.0007  0.1127\n",
      "     40            1.0000        \u001b[32m0.0445\u001b[0m       0.5625            0.5625        1.1588  0.0007  0.1107\n",
      "     41            1.0000        \u001b[32m0.0384\u001b[0m       0.5833            0.5833        1.1695  0.0007  0.1103\n",
      "     42            1.0000        \u001b[32m0.0359\u001b[0m       0.5833            0.5833        1.1790  0.0007  0.1084\n",
      "     43            1.0000        0.0579       0.5729            0.5729        1.1862  0.0006  0.1172\n",
      "     44            1.0000        0.0479       0.5729            0.5729        1.1914  0.0006  0.1078\n",
      "     45            1.0000        \u001b[32m0.0192\u001b[0m       0.5729            0.5729        1.1953  0.0005  0.1368\n",
      "     46            1.0000        0.0384       0.5938            0.5938        1.1982  0.0005  0.1346\n",
      "     47            1.0000        0.0243       0.5833            0.5833        1.2010  0.0004  0.1368\n",
      "     48            1.0000        \u001b[32m0.0117\u001b[0m       0.5833            0.5833        1.2038  0.0004  0.1614\n",
      "     49            1.0000        0.0222       0.5833            0.5833        1.2071  0.0003  0.1402\n",
      "     50            1.0000        0.0185       0.5625            0.5625        1.2106  0.0003  0.1388\n",
      "Fine tuning model for subject 9 with 20 = 20 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7500        1.0352       0.4688            0.4688        1.3147  0.0004  0.1383\n",
      "     32            0.7500        1.0005       0.4479            0.4479        1.3011  0.0005  0.1455\n",
      "     33            \u001b[36m0.8000\u001b[0m        0.7731       0.4792            0.4792        1.2772  0.0005  0.1338\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5104\u001b[0m       0.5000            0.5000        1.2758  0.0006  0.1460\n",
      "     35            1.0000        \u001b[32m0.3008\u001b[0m       0.5000            0.5000        1.3036  0.0006  0.1339\n",
      "     36            1.0000        \u001b[32m0.2400\u001b[0m       0.4896            0.4896        1.3373  0.0007  0.1332\n",
      "     37            1.0000        \u001b[32m0.1725\u001b[0m       0.4792            0.4792        1.3657  0.0007  0.1350\n",
      "     38            1.0000        \u001b[32m0.1581\u001b[0m       0.4896            0.4896        1.3805  0.0007  0.1356\n",
      "     39            1.0000        \u001b[32m0.0772\u001b[0m       0.5104            0.5104        1.3897  0.0007  0.1578\n",
      "     40            1.0000        \u001b[32m0.0537\u001b[0m       0.5104            0.5104        1.3942  0.0007  0.1340\n",
      "     41            1.0000        0.0784       0.5104            0.5104        1.4009  0.0007  0.1304\n",
      "     42            1.0000        \u001b[32m0.0448\u001b[0m       0.5208            0.5208        1.4029  0.0007  0.1534\n",
      "     43            1.0000        0.0989       0.5208            0.5208        1.4046  0.0006  0.1540\n",
      "     44            1.0000        0.0682       0.5208            0.5208        1.4104  0.0006  0.1172\n",
      "     45            1.0000        \u001b[32m0.0373\u001b[0m       0.5104            0.5104        1.4113  0.0005  0.1144\n",
      "     46            1.0000        0.0413       0.5104            0.5104        1.4112  0.0005  0.1156\n",
      "     47            1.0000        \u001b[32m0.0215\u001b[0m       0.5000            0.5000        1.4096  0.0004  0.1067\n",
      "     48            1.0000        0.0407       0.5208            0.5208        1.4070  0.0004  0.1277\n",
      "     49            1.0000        \u001b[32m0.0145\u001b[0m       0.4792            0.4792        1.4041  0.0003  0.1139\n",
      "     50            1.0000        0.0308       0.4792            0.4792        1.4014  0.0003  0.1136\n",
      "Fine tuning model for subject 9 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5600        1.6166       0.4792            0.4792        1.2760  0.0004  0.1133\n",
      "     32            \u001b[36m0.8000\u001b[0m        1.0295       0.5000            0.5000        1.2368  0.0005  0.1225\n",
      "     33            \u001b[36m0.9600\u001b[0m        0.8861       0.5000            0.5000        1.2318  0.0005  0.1249\n",
      "     34            0.9200        \u001b[32m0.3806\u001b[0m       0.5417            0.5417        1.2645  0.0006  0.1252\n",
      "     35            0.9200        \u001b[32m0.2269\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.3138  0.0006  0.1213\n",
      "     36            0.9600        0.2320       0.5521            0.5521        1.3619  0.0007  0.1275\n",
      "     37            0.9600        \u001b[32m0.1721\u001b[0m       0.5521            0.5521        1.3938  0.0007  0.1246\n",
      "     38            0.9600        \u001b[32m0.1212\u001b[0m       0.5729            0.5729        1.4065  0.0007  0.1227\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0541\u001b[0m       0.5729            0.5729        1.4117  0.0007  0.1217\n",
      "     40            1.0000        0.0790       0.5625            0.5625        1.4063  0.0007  0.1220\n",
      "     41            1.0000        0.0716       0.5625            0.5625        1.3958  0.0007  0.1194\n",
      "     42            1.0000        0.0797       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.3845  0.0007  0.1164\n",
      "     43            1.0000        \u001b[32m0.0510\u001b[0m       0.5729            0.5729        1.3769  0.0006  0.1214\n",
      "     44            1.0000        \u001b[32m0.0208\u001b[0m       0.5625            0.5625        1.3714  0.0006  0.1215\n",
      "     45            1.0000        0.0444       0.5833            0.5833        1.3679  0.0005  0.1229\n",
      "     46            1.0000        0.0366       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.3663  0.0005  0.1251\n",
      "     47            1.0000        0.0435       0.5833            0.5833        1.3670  0.0004  0.1187\n",
      "     48            1.0000        0.0347       0.5729            0.5729        1.3695  0.0004  0.1283\n",
      "     49            1.0000        \u001b[32m0.0149\u001b[0m       0.5729            0.5729        1.3725  0.0003  0.1136\n",
      "     50            1.0000        0.0184       0.5729            0.5729        1.3760  0.0003  0.1231\n",
      "Fine tuning model for subject 9 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.4761       0.4792            0.4792        1.3040  0.0004  0.1193\n",
      "     32            0.6000        1.2567       0.4688            0.4688        1.2871  0.0005  0.1231\n",
      "     33            0.6800        1.2057       0.4792            0.4792        1.2566  0.0005  0.1347\n",
      "     34            0.7600        0.8133       0.5104            0.5104        1.2399  0.0006  0.1335\n",
      "     35            \u001b[36m0.8400\u001b[0m        \u001b[32m0.7397\u001b[0m       0.5208            0.5208        1.2439  0.0006  0.1578\n",
      "     36            \u001b[36m0.9200\u001b[0m        \u001b[32m0.5514\u001b[0m       0.5208            0.5208        1.2550  0.0007  0.1282\n",
      "     37            \u001b[36m0.9600\u001b[0m        \u001b[32m0.3162\u001b[0m       0.5312            0.5312        1.2695  0.0007  0.1216\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2198\u001b[0m       0.5312            0.5312        1.2813  0.0007  0.1233\n",
      "     39            1.0000        \u001b[32m0.1493\u001b[0m       0.5208            0.5208        1.2881  0.0007  0.1212\n",
      "     40            1.0000        0.2037       0.5312            0.5312        1.2884  0.0007  0.1223\n",
      "     41            1.0000        \u001b[32m0.1049\u001b[0m       0.5104            0.5104        1.2868  0.0007  0.1233\n",
      "     42            1.0000        0.1312       0.5208            0.5208        1.2814  0.0007  0.1271\n",
      "     43            1.0000        \u001b[32m0.0888\u001b[0m       0.5312            0.5312        1.2752  0.0006  0.1220\n",
      "     44            1.0000        0.1417       0.5312            0.5312        1.2698  0.0006  0.1286\n",
      "     45            1.0000        0.0893       0.5312            0.5312        1.2662  0.0005  0.1238\n",
      "     46            1.0000        \u001b[32m0.0586\u001b[0m       0.5312            0.5312        1.2643  0.0005  0.1340\n",
      "     47            1.0000        0.0980       0.5312            0.5312        1.2653  0.0004  0.1331\n",
      "     48            1.0000        0.0702       0.5312            0.5312        1.2650  0.0004  0.1234\n",
      "     49            1.0000        \u001b[32m0.0498\u001b[0m       0.5312            0.5312        1.2650  0.0003  0.1247\n",
      "     50            1.0000        \u001b[32m0.0357\u001b[0m       0.5208            0.5208        1.2638  0.0003  0.1208\n",
      "Fine tuning model for subject 9 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7600        1.3692       0.4792            0.4792        1.2799  0.0004  0.1171\n",
      "     32            \u001b[36m0.8000\u001b[0m        1.1504       0.5000            0.5000        1.2177  0.0005  0.1243\n",
      "     33            \u001b[36m0.8800\u001b[0m        \u001b[32m0.6914\u001b[0m       0.5417            0.5417        1.1672  0.0005  0.1184\n",
      "     34            \u001b[36m0.9600\u001b[0m        \u001b[32m0.6115\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.1296  0.0006  0.1170\n",
      "     35            0.9600        \u001b[32m0.2196\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.0988  0.0006  0.1232\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2138\u001b[0m       0.5729            0.5729        1.0837  0.0007  0.1215\n",
      "     37            0.9600        \u001b[32m0.1283\u001b[0m       0.5729            0.5729        1.0904  0.0007  0.1238\n",
      "     38            0.9600        0.1908       0.5521            0.5521        1.1122  0.0007  0.1180\n",
      "     39            0.9200        \u001b[32m0.0882\u001b[0m       0.5729            0.5729        1.1359  0.0007  0.1186\n",
      "     40            0.9200        \u001b[32m0.0863\u001b[0m       0.5625            0.5625        1.1567  0.0007  0.1223\n",
      "     41            0.9200        0.1161       0.5625            0.5625        1.1746  0.0007  0.1282\n",
      "     42            0.9200        0.1034       0.5625            0.5625        1.1918  0.0007  0.1195\n",
      "     43            0.9200        \u001b[32m0.0467\u001b[0m       0.5729            0.5729        1.2064  0.0006  0.1223\n",
      "     44            1.0000        0.0993       0.5938            0.5938        1.2227  0.0006  0.1210\n",
      "     45            1.0000        \u001b[32m0.0348\u001b[0m       0.6042            0.6042        1.2372  0.0005  0.1174\n",
      "     46            1.0000        0.0370       0.6042            0.6042        1.2515  0.0005  0.1172\n",
      "     47            1.0000        0.0394       0.5938            0.5938        1.2638  0.0004  0.1159\n",
      "     48            1.0000        \u001b[32m0.0234\u001b[0m       0.6042            0.6042        1.2761  0.0004  0.1196\n",
      "     49            1.0000        0.0244       0.6042            0.6042        1.2888  0.0003  0.1177\n",
      "     50            1.0000        \u001b[32m0.0162\u001b[0m       0.5833            0.5833        1.3020  0.0003  0.1176\n",
      "Fine tuning model for subject 9 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        0.9500       0.4688            0.4688        1.2966  0.0004  0.1252\n",
      "     32            \u001b[36m0.8400\u001b[0m        1.1283       0.5104            0.5104        1.2687  0.0005  0.1172\n",
      "     33            \u001b[36m0.8800\u001b[0m        0.9617       0.5417            0.5417        1.2786  0.0005  0.1472\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4042\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.3239  0.0006  0.1191\n",
      "     35            1.0000        \u001b[32m0.2540\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.3677  0.0006  0.1222\n",
      "     36            0.9600        \u001b[32m0.2053\u001b[0m       0.5729            0.5729        1.3989  0.0007  0.1190\n",
      "     37            1.0000        \u001b[32m0.1586\u001b[0m       0.5521            0.5521        1.4116  0.0007  0.1175\n",
      "     38            1.0000        0.1656       0.5312            0.5312        1.4095  0.0007  0.1165\n",
      "     39            1.0000        \u001b[32m0.0969\u001b[0m       0.5104            0.5104        1.3975  0.0007  0.1220\n",
      "     40            1.0000        0.1274       0.5312            0.5312        1.3732  0.0007  0.1218\n",
      "     41            1.0000        \u001b[32m0.0891\u001b[0m       0.5521            0.5521        1.3493  0.0007  0.1174\n",
      "     42            1.0000        \u001b[32m0.0668\u001b[0m       0.5625            0.5625        1.3280  0.0007  0.1169\n",
      "     43            1.0000        \u001b[32m0.0317\u001b[0m       0.5625            0.5625        1.3100  0.0006  0.1167\n",
      "     44            1.0000        0.0401       0.5625            0.5625        1.2945  0.0006  0.1225\n",
      "     45            1.0000        \u001b[32m0.0278\u001b[0m       0.5729            0.5729        1.2796  0.0005  0.1231\n",
      "     46            1.0000        0.0300       0.5729            0.5729        1.2666  0.0005  0.1176\n",
      "     47            1.0000        \u001b[32m0.0206\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.2559  0.0004  0.1252\n",
      "     48            1.0000        \u001b[32m0.0176\u001b[0m       0.5833            0.5833        1.2477  0.0004  0.1131\n",
      "     49            1.0000        0.0373       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.2431  0.0003  0.1194\n",
      "     50            1.0000        0.0299       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.2411  0.0003  0.1404\n",
      "Fine tuning model for subject 9 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        0.8356       0.4688            0.4688        1.3094  0.0004  0.1494\n",
      "     32            \u001b[36m0.8400\u001b[0m        0.7866       0.4688            0.4688        1.2948  0.0005  0.1552\n",
      "     33            \u001b[36m0.9600\u001b[0m        \u001b[32m0.5514\u001b[0m       0.4896            0.4896        1.2607  0.0005  0.1357\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4980\u001b[0m       0.4792            0.4792        1.2298  0.0006  0.1516\n",
      "     35            1.0000        \u001b[32m0.2708\u001b[0m       0.5104            0.5104        1.2100  0.0006  0.1426\n",
      "     36            1.0000        \u001b[32m0.1740\u001b[0m       0.5104            0.5104        1.1992  0.0007  0.1334\n",
      "     37            1.0000        \u001b[32m0.1248\u001b[0m       0.5417            0.5417        1.1958  0.0007  0.1397\n",
      "     38            1.0000        0.1653       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.1927  0.0007  0.1385\n",
      "     39            1.0000        \u001b[32m0.0701\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.1940  0.0007  0.1446\n",
      "     40            1.0000        \u001b[32m0.0339\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.1975  0.0007  0.1332\n",
      "     41            1.0000        0.0803       0.5729            0.5729        1.2011  0.0007  0.1377\n",
      "     42            1.0000        0.0423       0.5833            0.5833        1.2052  0.0007  0.1488\n",
      "     43            1.0000        0.0655       0.5833            0.5833        1.2088  0.0006  0.1295\n",
      "     44            1.0000        0.0551       0.5729            0.5729        1.2122  0.0006  0.1512\n",
      "     45            1.0000        0.0380       0.5833            0.5833        1.2148  0.0005  0.1734\n",
      "     46            1.0000        0.0406       0.5729            0.5729        1.2173  0.0005  0.1574\n",
      "     47            1.0000        0.0653       0.5729            0.5729        1.2184  0.0004  0.1703\n",
      "     48            1.0000        \u001b[32m0.0193\u001b[0m       0.5729            0.5729        1.2197  0.0004  0.1258\n",
      "     49            1.0000        0.0312       0.5729            0.5729        1.2211  0.0003  0.1204\n",
      "     50            1.0000        0.0369       0.5625            0.5625        1.2224  0.0003  0.1176\n",
      "Fine tuning model for subject 9 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7200        1.0561       0.4688            0.4688        1.2858  0.0004  0.1408\n",
      "     32            \u001b[36m0.9600\u001b[0m        \u001b[32m0.6893\u001b[0m       0.5000            0.5000        1.2484  0.0005  0.1173\n",
      "     33            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4480\u001b[0m       0.5312            0.5312        1.2200  0.0005  0.1152\n",
      "     34            1.0000        \u001b[32m0.4185\u001b[0m       0.5417            0.5417        1.2024  0.0006  0.1181\n",
      "     35            1.0000        \u001b[32m0.2517\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.1930  0.0006  0.1218\n",
      "     36            1.0000        \u001b[32m0.1607\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.1910  0.0007  0.1611\n",
      "     37            1.0000        0.1705       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.1964  0.0007  0.1299\n",
      "     38            1.0000        \u001b[32m0.0610\u001b[0m       0.5625            0.5625        1.2082  0.0007  0.1205\n",
      "     39            1.0000        0.0686       0.5625            0.5625        1.2214  0.0007  0.1164\n",
      "     40            1.0000        0.0747       0.5625            0.5625        1.2347  0.0007  0.1120\n",
      "     41            1.0000        0.0738       0.5625            0.5625        1.2451  0.0007  0.1224\n",
      "     42            1.0000        \u001b[32m0.0567\u001b[0m       0.5625            0.5625        1.2495  0.0007  0.1224\n",
      "     43            1.0000        \u001b[32m0.0229\u001b[0m       0.5729            0.5729        1.2482  0.0006  0.1329\n",
      "     44            1.0000        0.0346       0.5729            0.5729        1.2428  0.0006  0.1298\n",
      "     45            1.0000        \u001b[32m0.0178\u001b[0m       0.5729            0.5729        1.2320  0.0005  0.1268\n",
      "     46            1.0000        0.0245       0.5833            0.5833        1.2174  0.0005  0.1209\n",
      "     47            1.0000        0.0250       0.5833            0.5833        1.2018  0.0004  0.1237\n",
      "     48            1.0000        0.0253       0.5938            0.5938        1.1863  0.0004  0.1178\n",
      "     49            1.0000        \u001b[32m0.0135\u001b[0m       0.5938            0.5938        1.1721  0.0003  0.1254\n",
      "     50            1.0000        0.0337       0.5833            0.5833        1.1599  0.0003  0.1187\n",
      "Fine tuning model for subject 9 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        0.9621       0.4688            0.4688        1.3032  0.0004  0.1139\n",
      "     32            0.6800        0.9865       0.4688            0.4688        1.2755  0.0005  0.1213\n",
      "     33            \u001b[36m0.8800\u001b[0m        0.7632       0.5000            0.5000        1.2590  0.0005  0.1183\n",
      "     34            \u001b[36m0.9200\u001b[0m        \u001b[32m0.4854\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.2559  0.0006  0.1253\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2730\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.2674  0.0006  0.1145\n",
      "     36            1.0000        \u001b[32m0.1530\u001b[0m       0.5729            0.5729        1.2842  0.0007  0.1175\n",
      "     37            1.0000        \u001b[32m0.1396\u001b[0m       0.5729            0.5729        1.3029  0.0007  0.1244\n",
      "     38            1.0000        \u001b[32m0.1337\u001b[0m       0.5938            0.5938        1.3148  0.0007  0.1251\n",
      "     39            1.0000        0.1419       0.5938            0.5938        1.3256  0.0007  0.1197\n",
      "     40            1.0000        \u001b[32m0.0609\u001b[0m       0.5729            0.5729        1.3350  0.0007  0.1215\n",
      "     41            1.0000        0.0649       0.5833            0.5833        1.3375  0.0007  0.1173\n",
      "     42            1.0000        \u001b[32m0.0483\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.3363  0.0007  0.1165\n",
      "     43            1.0000        \u001b[32m0.0321\u001b[0m       0.6042            0.6042        1.3352  0.0006  0.1221\n",
      "     44            1.0000        0.0356       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.3337  0.0006  0.1300\n",
      "     45            1.0000        \u001b[32m0.0208\u001b[0m       0.6042            0.6042        1.3316  0.0005  0.1231\n",
      "     46            1.0000        0.0358       0.5938            0.5938        1.3316  0.0005  0.1236\n",
      "     47            1.0000        0.0342       0.5938            0.5938        1.3288  0.0004  0.1209\n",
      "     48            1.0000        \u001b[32m0.0194\u001b[0m       0.6042            0.6042        1.3261  0.0004  0.1144\n",
      "     49            1.0000        \u001b[32m0.0172\u001b[0m       0.6042            0.6042        1.3228  0.0003  0.1130\n",
      "     50            1.0000        0.0335       0.6042            0.6042        1.3197  0.0003  0.1422\n",
      "Fine tuning model for subject 9 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.2814       0.4792            0.4792        1.3026  0.0004  0.1093\n",
      "     32            0.6800        1.0990       0.4896            0.4896        1.2623  0.0005  0.1169\n",
      "     33            \u001b[36m0.8400\u001b[0m        \u001b[32m0.7057\u001b[0m       0.5104            0.5104        1.2168  0.0005  0.1218\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.6145\u001b[0m       0.5104            0.5104        1.1838  0.0006  0.1160\n",
      "     35            1.0000        \u001b[32m0.3762\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.1618  0.0006  0.1209\n",
      "     36            1.0000        0.3986       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.1536  0.0007  0.1173\n",
      "     37            1.0000        \u001b[32m0.1887\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.1571  0.0007  0.1227\n",
      "     38            1.0000        \u001b[32m0.1082\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.1700  0.0007  0.1181\n",
      "     39            1.0000        0.1138       0.5938            0.5938        1.1890  0.0007  0.1173\n",
      "     40            1.0000        \u001b[32m0.0827\u001b[0m       0.5833            0.5833        1.2105  0.0007  0.1222\n",
      "     41            1.0000        \u001b[32m0.0743\u001b[0m       0.5833            0.5833        1.2309  0.0007  0.1243\n",
      "     42            1.0000        0.1001       0.5938            0.5938        1.2492  0.0007  0.1341\n",
      "     43            1.0000        \u001b[32m0.0416\u001b[0m       0.5833            0.5833        1.2628  0.0006  0.1379\n",
      "     44            1.0000        0.0558       0.5938            0.5938        1.2727  0.0006  0.1357\n",
      "     45            1.0000        \u001b[32m0.0326\u001b[0m       0.5938            0.5938        1.2792  0.0005  0.1298\n",
      "     46            1.0000        \u001b[32m0.0300\u001b[0m       0.5938            0.5938        1.2823  0.0005  0.1217\n",
      "     47            1.0000        0.0351       0.5938            0.5938        1.2835  0.0004  0.1236\n",
      "     48            1.0000        0.0363       0.6042            0.6042        1.2829  0.0004  0.1181\n",
      "     49            1.0000        \u001b[32m0.0268\u001b[0m       0.6146            0.6146        1.2814  0.0003  0.1182\n",
      "     50            1.0000        \u001b[32m0.0250\u001b[0m       0.6146            0.6146        1.2788  0.0003  0.1171\n",
      "Fine tuning model for subject 9 with 25 = 25 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5200        1.4626       0.4792            0.4792        1.3166  0.0004  0.1220\n",
      "     32            0.6800        1.4757       0.4479            0.4479        1.3067  0.0005  0.1287\n",
      "     33            0.6800        0.9384       0.4583            0.4583        1.2923  0.0005  0.1174\n",
      "     34            \u001b[36m0.9600\u001b[0m        \u001b[32m0.5018\u001b[0m       0.4896            0.4896        1.2873  0.0006  0.1227\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3050\u001b[0m       0.4688            0.4688        1.3000  0.0006  0.1226\n",
      "     36            1.0000        \u001b[32m0.2829\u001b[0m       0.4688            0.4688        1.3379  0.0007  0.1268\n",
      "     37            1.0000        \u001b[32m0.2259\u001b[0m       0.4896            0.4896        1.4017  0.0007  0.1231\n",
      "     38            0.9600        \u001b[32m0.1025\u001b[0m       0.4479            0.4479        1.4845  0.0007  0.1239\n",
      "     39            0.9600        \u001b[32m0.1014\u001b[0m       0.4271            0.4271        1.5714  0.0007  0.1200\n",
      "     40            0.9600        0.1287       0.3958            0.3958        1.6487  0.0007  0.1177\n",
      "     41            0.9600        \u001b[32m0.0840\u001b[0m       0.3958            0.3958        1.7161  0.0007  0.1215\n",
      "     42            1.0000        0.1105       0.3958            0.3958        1.7652  0.0007  0.1230\n",
      "     43            1.0000        \u001b[32m0.0698\u001b[0m       0.3958            0.3958        1.8008  0.0006  0.1149\n",
      "     44            1.0000        \u001b[32m0.0357\u001b[0m       0.3958            0.3958        1.8239  0.0006  0.1154\n",
      "     45            1.0000        \u001b[32m0.0356\u001b[0m       0.3854            0.3854        1.8352  0.0005  0.1133\n",
      "     46            1.0000        0.0438       0.3958            0.3958        1.8366  0.0005  0.1210\n",
      "     47            1.0000        \u001b[32m0.0265\u001b[0m       0.3854            0.3854        1.8322  0.0004  0.1214\n",
      "     48            1.0000        0.0400       0.3854            0.3854        1.8207  0.0004  0.1174\n",
      "     49            1.0000        0.0282       0.3750            0.3750        1.8072  0.0003  0.1239\n",
      "     50            1.0000        0.0306       0.3854            0.3854        1.7925  0.0003  0.1279\n",
      "Fine tuning model for subject 9 with 25 = 25 trials\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7200        0.8676       0.4583            0.4583        1.2976  0.0004  0.1175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     32            \u001b[36m0.8000\u001b[0m        0.9092       0.4688            0.4688        1.2702  0.0005  0.1515\n",
      "     33            \u001b[36m0.8800\u001b[0m        \u001b[32m0.5879\u001b[0m       0.4688            0.4688        1.2485  0.0005  0.1385\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.5193\u001b[0m       0.5000            0.5000        1.2458  0.0006  0.1465\n",
      "     35            1.0000        \u001b[32m0.2400\u001b[0m       0.5208            0.5208        1.2587  0.0006  0.1422\n",
      "     36            1.0000        \u001b[32m0.1458\u001b[0m       0.5208            0.5208        1.2794  0.0007  0.1396\n",
      "     37            1.0000        0.1595       0.5312            0.5312        1.2962  0.0007  0.1457\n",
      "     38            1.0000        \u001b[32m0.0903\u001b[0m       0.5312            0.5312        1.3094  0.0007  0.1656\n",
      "     39            1.0000        \u001b[32m0.0650\u001b[0m       0.5417            0.5417        1.3203  0.0007  0.1326\n",
      "     40            1.0000        0.0996       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.3371  0.0007  0.1444\n",
      "     41            1.0000        \u001b[32m0.0398\u001b[0m       0.5521            0.5521        1.3580  0.0007  0.1453\n",
      "     42            1.0000        0.0678       0.5521            0.5521        1.3790  0.0007  0.1749\n",
      "     43            1.0000        \u001b[32m0.0264\u001b[0m       0.5625            0.5625        1.3990  0.0006  0.1592\n",
      "     44            1.0000        0.0430       0.5625            0.5625        1.4184  0.0006  0.1500\n",
      "     45            1.0000        \u001b[32m0.0264\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.4379  0.0005  0.1495\n",
      "     46            1.0000        \u001b[32m0.0254\u001b[0m       0.5625            0.5625        1.4552  0.0005  0.1491\n",
      "     47            1.0000        \u001b[32m0.0107\u001b[0m       0.5625            0.5625        1.4693  0.0004  0.1708\n",
      "     48            1.0000        0.0150       0.5625            0.5625        1.4804  0.0004  0.1594\n",
      "     49            1.0000        0.0157       0.5625            0.5625        1.4893  0.0003  0.1768\n",
      "     50            1.0000        0.0172       0.5625            0.5625        1.4964  0.0003  0.1425\n",
      "Fine tuning model for subject 9 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6333        1.3075       0.4792            0.4792        1.3066  0.0004  0.1346\n",
      "     32            0.7000        1.2422       0.4896            0.4896        1.2831  0.0005  0.1223\n",
      "     33            0.7667        1.0056       0.5104            0.5104        1.2865  0.0005  0.1428\n",
      "     34            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6957\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.3385  0.0006  0.1352\n",
      "     35            0.8667        \u001b[32m0.4625\u001b[0m       0.5417            0.5417        1.4045  0.0006  0.1277\n",
      "     36            0.8667        0.4733       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.4575  0.0007  0.1334\n",
      "     37            0.8667        \u001b[32m0.2630\u001b[0m       0.5521            0.5521        1.4842  0.0007  0.1307\n",
      "     38            \u001b[36m0.9667\u001b[0m        \u001b[32m0.1613\u001b[0m       0.5521            0.5521        1.4899  0.0007  0.1238\n",
      "     39            0.9667        \u001b[32m0.1303\u001b[0m       0.5208            0.5208        1.4800  0.0007  0.1304\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0865\u001b[0m       0.5312            0.5312        1.4627  0.0007  0.1365\n",
      "     41            1.0000        0.0950       0.5417            0.5417        1.4431  0.0007  0.1274\n",
      "     42            1.0000        \u001b[32m0.0797\u001b[0m       0.5521            0.5521        1.4277  0.0007  0.1359\n",
      "     43            1.0000        \u001b[32m0.0759\u001b[0m       0.5417            0.5417        1.4172  0.0006  0.1281\n",
      "     44            1.0000        \u001b[32m0.0527\u001b[0m       0.5312            0.5312        1.4118  0.0006  0.1366\n",
      "     45            1.0000        0.0605       0.5208            0.5208        1.4100  0.0005  0.1308\n",
      "     46            1.0000        0.0647       0.5417            0.5417        1.4110  0.0005  0.1318\n",
      "     47            1.0000        0.0554       0.5521            0.5521        1.4126  0.0004  0.1297\n",
      "     48            1.0000        0.0618       0.5625            0.5625        1.4157  0.0004  0.1270\n",
      "     49            1.0000        0.0882       0.5625            0.5625        1.4178  0.0003  0.1312\n",
      "     50            1.0000        \u001b[32m0.0418\u001b[0m       0.5625            0.5625        1.4214  0.0003  0.1252\n",
      "Fine tuning model for subject 9 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        0.7690       0.4688            0.4688        1.3295  0.0004  0.1301\n",
      "     32            \u001b[36m0.8333\u001b[0m        \u001b[32m0.6731\u001b[0m       0.4583            0.4583        1.3437  0.0005  0.1280\n",
      "     33            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5361\u001b[0m       0.4375            0.4375        1.3566  0.0005  0.1328\n",
      "     34            \u001b[36m1.0000\u001b[0m        0.5818       0.4688            0.4688        1.3526  0.0006  0.1279\n",
      "     35            1.0000        \u001b[32m0.3364\u001b[0m       0.4792            0.4792        1.3423  0.0006  0.1326\n",
      "     36            1.0000        \u001b[32m0.1844\u001b[0m       0.4792            0.4792        1.3363  0.0007  0.1322\n",
      "     37            1.0000        \u001b[32m0.1628\u001b[0m       0.4896            0.4896        1.3368  0.0007  0.1227\n",
      "     38            1.0000        \u001b[32m0.1380\u001b[0m       0.5000            0.5000        1.3425  0.0007  0.1281\n",
      "     39            1.0000        \u001b[32m0.1093\u001b[0m       0.4792            0.4792        1.3458  0.0007  0.1279\n",
      "     40            1.0000        \u001b[32m0.0767\u001b[0m       0.4896            0.4896        1.3500  0.0007  0.1272\n",
      "     41            1.0000        0.0785       0.5000            0.5000        1.3540  0.0007  0.1295\n",
      "     42            1.0000        \u001b[32m0.0533\u001b[0m       0.5208            0.5208        1.3566  0.0007  0.1255\n",
      "     43            1.0000        0.0554       0.5312            0.5312        1.3579  0.0006  0.1283\n",
      "     44            1.0000        \u001b[32m0.0422\u001b[0m       0.5208            0.5208        1.3578  0.0006  0.1280\n",
      "     45            1.0000        0.0603       0.5208            0.5208        1.3556  0.0005  0.1335\n",
      "     46            1.0000        0.0435       0.5208            0.5208        1.3527  0.0005  0.1320\n",
      "     47            1.0000        \u001b[32m0.0293\u001b[0m       0.5104            0.5104        1.3502  0.0004  0.1330\n",
      "     48            1.0000        0.0303       0.5000            0.5000        1.3474  0.0004  0.1321\n",
      "     49            1.0000        0.0361       0.5000            0.5000        1.3446  0.0003  0.1248\n",
      "     50            1.0000        \u001b[32m0.0252\u001b[0m       0.5000            0.5000        1.3418  0.0003  0.1389\n",
      "Fine tuning model for subject 9 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7667        0.9413       0.4792            0.4792        1.3158  0.0004  0.1346\n",
      "     32            \u001b[36m0.8000\u001b[0m        0.8151       0.4792            0.4792        1.3064  0.0005  0.1280\n",
      "     33            0.8000        \u001b[32m0.6664\u001b[0m       0.4688            0.4688        1.2927  0.0005  0.1310\n",
      "     34            \u001b[36m0.9333\u001b[0m        \u001b[32m0.6335\u001b[0m       0.4792            0.4792        1.2664  0.0006  0.1291\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3922\u001b[0m       0.4792            0.4792        1.2438  0.0006  0.1251\n",
      "     36            1.0000        \u001b[32m0.3518\u001b[0m       0.5208            0.5208        1.2295  0.0007  0.1321\n",
      "     37            1.0000        \u001b[32m0.1401\u001b[0m       0.5000            0.5000        1.2320  0.0007  0.1366\n",
      "     38            1.0000        0.1681       0.5000            0.5000        1.2479  0.0007  0.1282\n",
      "     39            1.0000        \u001b[32m0.0653\u001b[0m       0.5104            0.5104        1.2731  0.0007  0.1350\n",
      "     40            1.0000        0.0710       0.4896            0.4896        1.3034  0.0007  0.1291\n",
      "     41            1.0000        0.0937       0.5000            0.5000        1.3328  0.0007  0.1273\n",
      "     42            1.0000        \u001b[32m0.0562\u001b[0m       0.5104            0.5104        1.3593  0.0007  0.1337\n",
      "     43            1.0000        0.0680       0.5104            0.5104        1.3767  0.0006  0.1257\n",
      "     44            1.0000        \u001b[32m0.0373\u001b[0m       0.4896            0.4896        1.3875  0.0006  0.1389\n",
      "     45            1.0000        0.0645       0.5000            0.5000        1.3878  0.0005  0.1276\n",
      "     46            1.0000        \u001b[32m0.0292\u001b[0m       0.5000            0.5000        1.3844  0.0005  0.1327\n",
      "     47            1.0000        \u001b[32m0.0251\u001b[0m       0.5000            0.5000        1.3780  0.0004  0.1279\n",
      "     48            1.0000        0.0528       0.5104            0.5104        1.3705  0.0004  0.1324\n",
      "     49            1.0000        \u001b[32m0.0238\u001b[0m       0.5000            0.5000        1.3618  0.0003  0.1300\n",
      "     50            1.0000        \u001b[32m0.0225\u001b[0m       0.5000            0.5000        1.3532  0.0003  0.1315\n",
      "Fine tuning model for subject 9 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7333        1.0150       0.4792            0.4792        1.3168  0.0004  0.1279\n",
      "     32            0.7667        1.3499       0.4583            0.4583        1.3002  0.0005  0.1339\n",
      "     33            \u001b[36m0.8333\u001b[0m        0.9107       0.5000            0.5000        1.2792  0.0005  0.1263\n",
      "     34            0.8333        \u001b[32m0.6980\u001b[0m       0.4896            0.4896        1.2591  0.0006  0.1342\n",
      "     35            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5931\u001b[0m       0.4896            0.4896        1.2492  0.0006  0.1290\n",
      "     36            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3528\u001b[0m       0.5208            0.5208        1.2533  0.0007  0.1345\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2576\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.2621  0.0007  0.1280\n",
      "     38            1.0000        \u001b[32m0.1764\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.2719  0.0007  0.1207\n",
      "     39            1.0000        \u001b[32m0.1209\u001b[0m       0.5625            0.5625        1.2836  0.0007  0.1291\n",
      "     40            1.0000        \u001b[32m0.1200\u001b[0m       0.5625            0.5625        1.2982  0.0007  0.1377\n",
      "     41            1.0000        \u001b[32m0.1168\u001b[0m       0.5625            0.5625        1.3132  0.0007  0.1342\n",
      "     42            1.0000        \u001b[32m0.0909\u001b[0m       0.5625            0.5625        1.3295  0.0007  0.1260\n",
      "     43            1.0000        \u001b[32m0.0721\u001b[0m       0.5625            0.5625        1.3430  0.0006  0.1314\n",
      "     44            1.0000        \u001b[32m0.0475\u001b[0m       0.5625            0.5625        1.3539  0.0006  0.1336\n",
      "     45            1.0000        0.0556       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.3636  0.0005  0.1291\n",
      "     46            1.0000        \u001b[32m0.0385\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.3686  0.0005  0.1286\n",
      "     47            1.0000        \u001b[32m0.0239\u001b[0m       0.5833            0.5833        1.3696  0.0004  0.1442\n",
      "     48            1.0000        0.0365       0.5833            0.5833        1.3673  0.0004  0.1457\n",
      "     49            1.0000        \u001b[32m0.0150\u001b[0m       0.5625            0.5625        1.3629  0.0003  0.1736\n",
      "     50            1.0000        0.0276       0.5729            0.5729        1.3574  0.0003  0.1440\n",
      "Fine tuning model for subject 9 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5667        1.6782       0.4792            0.4792        1.3149  0.0004  0.1704\n",
      "     32            0.6000        1.2345       0.4688            0.4688        1.3064  0.0005  0.1578\n",
      "     33            0.6667        1.2726       0.5000            0.5000        1.2944  0.0005  0.1513\n",
      "     34            \u001b[36m0.8000\u001b[0m        1.0302       0.5312            0.5312        1.3031  0.0006  0.1945\n",
      "     35            \u001b[36m0.9000\u001b[0m        0.7464       0.5312            0.5312        1.3272  0.0006  0.1597\n",
      "     36            \u001b[36m0.9333\u001b[0m        \u001b[32m0.6332\u001b[0m       0.5417            0.5417        1.3580  0.0007  0.1931\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2889\u001b[0m       0.5312            0.5312        1.3939  0.0007  0.1707\n",
      "     38            1.0000        0.3132       0.4792            0.4792        1.4374  0.0007  0.1557\n",
      "     39            1.0000        \u001b[32m0.1903\u001b[0m       0.4375            0.4375        1.4765  0.0007  0.1428\n",
      "     40            1.0000        \u001b[32m0.1463\u001b[0m       0.4479            0.4479        1.5018  0.0007  0.1721\n",
      "     41            1.0000        0.1545       0.4271            0.4271        1.5101  0.0007  0.1496\n",
      "     42            1.0000        \u001b[32m0.1171\u001b[0m       0.4375            0.4375        1.5100  0.0007  0.1935\n",
      "     43            1.0000        \u001b[32m0.0718\u001b[0m       0.4375            0.4375        1.5011  0.0006  0.1598\n",
      "     44            1.0000        0.1111       0.4375            0.4375        1.4846  0.0006  0.2149\n",
      "     45            1.0000        \u001b[32m0.0494\u001b[0m       0.4583            0.4583        1.4632  0.0005  0.1454\n",
      "     46            1.0000        0.0554       0.4896            0.4896        1.4393  0.0005  0.1337\n",
      "     47            1.0000        0.0581       0.5104            0.5104        1.4148  0.0004  0.1280\n",
      "     48            1.0000        0.0565       0.5000            0.5000        1.3927  0.0004  0.1374\n",
      "     49            1.0000        0.0545       0.4896            0.4896        1.3735  0.0003  0.1411\n",
      "     50            1.0000        0.0507       0.5208            0.5208        1.3565  0.0003  0.1365\n",
      "Fine tuning model for subject 9 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.3667        2.0143       0.4792            0.4792        1.3027  0.0004  0.1313\n",
      "     32            0.4667        1.7807       0.4896            0.4896        1.2750  0.0005  0.1348\n",
      "     33            0.6000        1.8277       0.5312            0.5312        1.2607  0.0005  0.1283\n",
      "     34            0.7000        1.1903       0.5417            0.5417        1.2803  0.0006  0.1306\n",
      "     35            \u001b[36m0.8000\u001b[0m        \u001b[32m0.7196\u001b[0m       0.5417            0.5417        1.3174  0.0006  0.1262\n",
      "     36            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5814\u001b[0m       0.5104            0.5104        1.3600  0.0007  0.1299\n",
      "     37            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3569\u001b[0m       0.5208            0.5208        1.4023  0.0007  0.1270\n",
      "     38            0.9667        \u001b[32m0.2953\u001b[0m       0.5000            0.5000        1.4351  0.0007  0.1254\n",
      "     39            0.9667        \u001b[32m0.2589\u001b[0m       0.4896            0.4896        1.4545  0.0007  0.1427\n",
      "     40            0.9667        \u001b[32m0.2209\u001b[0m       0.5104            0.5104        1.4578  0.0007  0.1289\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1719\u001b[0m       0.5208            0.5208        1.4496  0.0007  0.1242\n",
      "     42            1.0000        \u001b[32m0.1341\u001b[0m       0.5000            0.5000        1.4337  0.0007  0.1185\n",
      "     43            1.0000        \u001b[32m0.1240\u001b[0m       0.5000            0.5000        1.4123  0.0006  0.1256\n",
      "     44            1.0000        \u001b[32m0.0807\u001b[0m       0.5000            0.5000        1.3895  0.0006  0.1350\n",
      "     45            1.0000        0.1476       0.4896            0.4896        1.3644  0.0005  0.1282\n",
      "     46            1.0000        0.0998       0.4896            0.4896        1.3442  0.0005  0.1273\n",
      "     47            1.0000        0.1188       0.5000            0.5000        1.3236  0.0004  0.1315\n",
      "     48            1.0000        0.0966       0.5000            0.5000        1.3052  0.0004  0.1357\n",
      "     49            1.0000        \u001b[32m0.0792\u001b[0m       0.5208            0.5208        1.2886  0.0003  0.1275\n",
      "     50            1.0000        0.0823       0.5208            0.5208        1.2736  0.0003  0.1315\n",
      "Fine tuning model for subject 9 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6333        0.9611       0.4792            0.4792        1.3103  0.0004  0.1572\n",
      "     32            0.7000        0.9740       0.4792            0.4792        1.2925  0.0005  0.2203\n",
      "     33            \u001b[36m0.8333\u001b[0m        0.9115       0.5104            0.5104        1.2664  0.0005  0.1900\n",
      "     34            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4337\u001b[0m       0.5104            0.5104        1.2507  0.0006  0.1327\n",
      "     35            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2670\u001b[0m       0.5104            0.5104        1.2518  0.0006  0.1355\n",
      "     36            1.0000        \u001b[32m0.2062\u001b[0m       0.5417            0.5417        1.2565  0.0007  0.1621\n",
      "     37            1.0000        \u001b[32m0.0974\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.2586  0.0007  0.1931\n",
      "     38            1.0000        \u001b[32m0.0847\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.2636  0.0007  0.1265\n",
      "     39            1.0000        0.1043       0.5521            0.5521        1.2717  0.0007  0.1318\n",
      "     40            1.0000        \u001b[32m0.0827\u001b[0m       0.5625            0.5625        1.2825  0.0007  0.1298\n",
      "     41            1.0000        0.0849       0.5521            0.5521        1.2928  0.0007  0.1316\n",
      "     42            1.0000        \u001b[32m0.0684\u001b[0m       0.5312            0.5312        1.3039  0.0007  0.1427\n",
      "     43            1.0000        0.0829       0.5312            0.5312        1.3134  0.0006  0.1340\n",
      "     44            1.0000        0.0695       0.5208            0.5208        1.3197  0.0006  0.1335\n",
      "     45            1.0000        \u001b[32m0.0503\u001b[0m       0.5000            0.5000        1.3238  0.0005  0.1276\n",
      "     46            1.0000        0.0644       0.5000            0.5000        1.3258  0.0005  0.1242\n",
      "     47            1.0000        \u001b[32m0.0330\u001b[0m       0.5104            0.5104        1.3273  0.0004  0.1327\n",
      "     48            1.0000        0.0372       0.5104            0.5104        1.3282  0.0004  0.1272\n",
      "     49            1.0000        \u001b[32m0.0328\u001b[0m       0.5000            0.5000        1.3285  0.0003  0.1324\n",
      "     50            1.0000        0.0453       0.5000            0.5000        1.3296  0.0003  0.1252\n",
      "Fine tuning model for subject 9 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5667        1.2495       0.4792            0.4792        1.3102  0.0004  0.1281\n",
      "     32            0.6333        1.1816       0.4896            0.4896        1.2841  0.0005  0.1282\n",
      "     33            0.7333        0.9610       0.5312            0.5312        1.2615  0.0005  0.1291\n",
      "     34            \u001b[36m0.8667\u001b[0m        0.8826       0.5208            0.5208        1.2693  0.0006  0.1313\n",
      "     35            \u001b[36m0.9333\u001b[0m        \u001b[32m0.6593\u001b[0m       0.5417            0.5417        1.2922  0.0006  0.1277\n",
      "     36            0.9333        \u001b[32m0.3796\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.3027  0.0007  0.1333\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2316\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.3038  0.0007  0.1280\n",
      "     38            1.0000        0.2750       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.2923  0.0007  0.1325\n",
      "     39            1.0000        \u001b[32m0.1728\u001b[0m       0.5729            0.5729        1.2664  0.0007  0.1282\n",
      "     40            1.0000        \u001b[32m0.1618\u001b[0m       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        1.2336  0.0007  0.1321\n",
      "     41            1.0000        \u001b[32m0.0750\u001b[0m       0.6250            0.6250        1.2011  0.0007  0.1590\n",
      "     42            1.0000        0.0993       0.6250            0.6250        1.1675  0.0007  0.1385\n",
      "     43            1.0000        \u001b[32m0.0664\u001b[0m       0.6250            0.6250        1.1378  0.0006  0.1302\n",
      "     44            1.0000        \u001b[32m0.0513\u001b[0m       0.6250            0.6250        1.1121  0.0006  0.1763\n",
      "     45            1.0000        \u001b[32m0.0295\u001b[0m       0.6146            0.6146        1.0889  0.0005  0.1288\n",
      "     46            1.0000        0.0569       0.6146            0.6146        \u001b[94m1.0700\u001b[0m  0.0005  0.1386\n",
      "     47            1.0000        0.0422       0.6146            0.6146        \u001b[94m1.0537\u001b[0m  0.0004  0.1316\n",
      "     48            1.0000        0.0740       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        \u001b[94m1.0397\u001b[0m  0.0004  0.1381\n",
      "     49            1.0000        \u001b[32m0.0265\u001b[0m       \u001b[35m0.6562\u001b[0m            \u001b[31m0.6562\u001b[0m        \u001b[94m1.0281\u001b[0m  0.0003  0.1320\n",
      "     50            1.0000        0.0339       0.6562            0.6562        \u001b[94m1.0185\u001b[0m  0.0003  0.1378\n",
      "Fine tuning model for subject 9 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6333        1.2286       0.4688            0.4688        1.2922  0.0004  0.1282\n",
      "     32            0.7667        0.9052       0.4896            0.4896        1.2525  0.0005  0.1337\n",
      "     33            \u001b[36m0.8333\u001b[0m        \u001b[32m0.7271\u001b[0m       0.4896            0.4896        1.2277  0.0005  0.1339\n",
      "     34            \u001b[36m0.9000\u001b[0m        \u001b[32m0.4343\u001b[0m       0.5312            0.5312        1.2234  0.0006  0.1301\n",
      "     35            \u001b[36m0.9333\u001b[0m        0.4481       0.5417            0.5417        1.2281  0.0006  0.1325\n",
      "     36            \u001b[36m0.9667\u001b[0m        \u001b[32m0.3804\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.2374  0.0007  0.1329\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2880\u001b[0m       0.5729            0.5729        1.2447  0.0007  0.1315\n",
      "     38            1.0000        \u001b[32m0.1348\u001b[0m       0.5729            0.5729        1.2455  0.0007  0.1337\n",
      "     39            1.0000        0.2035       0.5729            0.5729        1.2405  0.0007  0.1270\n",
      "     40            1.0000        \u001b[32m0.0969\u001b[0m       0.5729            0.5729        1.2331  0.0007  0.1300\n",
      "     41            1.0000        0.1335       0.5729            0.5729        1.2292  0.0007  0.1549\n",
      "     42            1.0000        \u001b[32m0.0729\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.2278  0.0007  0.1419\n",
      "     43            1.0000        0.0782       0.5833            0.5833        1.2269  0.0006  0.1600\n",
      "     44            1.0000        \u001b[32m0.0387\u001b[0m       0.5833            0.5833        1.2251  0.0006  0.1741\n",
      "     45            1.0000        \u001b[32m0.0360\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.2221  0.0005  0.1687\n",
      "     46            1.0000        0.0530       0.5938            0.5938        1.2185  0.0005  0.1660\n",
      "     47            1.0000        \u001b[32m0.0314\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.2140  0.0004  0.1706\n",
      "     48            1.0000        \u001b[32m0.0279\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.2087  0.0004  0.1646\n",
      "     49            1.0000        \u001b[32m0.0248\u001b[0m       0.6146            0.6146        1.2033  0.0003  0.2250\n",
      "     50            1.0000        \u001b[32m0.0198\u001b[0m       0.6042            0.6042        1.1979  0.0003  0.1731\n",
      "Fine tuning model for subject 9 with 30 = 30 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8333\u001b[0m        0.7536       0.4792            0.4792        1.3154  0.0004  0.1709\n",
      "     32            0.8000        \u001b[32m0.6300\u001b[0m       0.4688            0.4688        1.3155  0.0005  0.1537\n",
      "     33            \u001b[36m0.8667\u001b[0m        0.7550       0.4792            0.4792        1.3201  0.0005  0.1553\n",
      "     34            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2403\u001b[0m       0.4688            0.4688        1.3266  0.0006  0.1611\n",
      "     35            1.0000        0.3411       0.4583            0.4583        1.3281  0.0006  0.1575\n",
      "     36            1.0000        0.2410       0.5000            0.5000        1.3262  0.0007  0.1721\n",
      "     37            1.0000        \u001b[32m0.1477\u001b[0m       0.5000            0.5000        1.3224  0.0007  0.1889\n",
      "     38            1.0000        \u001b[32m0.1374\u001b[0m       0.4896            0.4896        1.3182  0.0007  0.1417\n",
      "     39            1.0000        \u001b[32m0.0880\u001b[0m       0.4896            0.4896        1.3143  0.0007  0.1479\n",
      "     40            1.0000        \u001b[32m0.0738\u001b[0m       0.5208            0.5208        1.3127  0.0007  0.1382\n",
      "     41            1.0000        \u001b[32m0.0681\u001b[0m       0.5312            0.5312        1.3120  0.0007  0.1787\n",
      "     42            1.0000        \u001b[32m0.0317\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.3109  0.0007  0.1355\n",
      "     43            1.0000        0.0511       0.5417            0.5417        1.3095  0.0006  0.1265\n",
      "     44            1.0000        0.0423       0.5417            0.5417        1.3083  0.0006  0.1315\n",
      "     45            1.0000        0.0543       0.5417            0.5417        1.3064  0.0005  0.1335\n",
      "     46            1.0000        \u001b[32m0.0217\u001b[0m       0.5417            0.5417        1.3035  0.0005  0.1237\n",
      "     47            1.0000        0.0249       0.5521            0.5521        1.3007  0.0004  0.1434\n",
      "     48            1.0000        0.0224       0.5417            0.5417        1.2976  0.0004  0.1343\n",
      "     49            1.0000        \u001b[32m0.0159\u001b[0m       0.5521            0.5521        1.2941  0.0003  0.1345\n",
      "     50            1.0000        0.0268       0.5521            0.5521        1.2906  0.0003  0.1286\n",
      "Fine tuning model for subject 9 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6857        1.0172       0.4688            0.4688        1.2962  0.0004  0.1370\n",
      "     32            0.7429        0.7755       0.4792            0.4792        1.2731  0.0005  0.1367\n",
      "     33            \u001b[36m0.8000\u001b[0m        \u001b[32m0.6789\u001b[0m       0.4896            0.4896        1.2587  0.0005  0.1387\n",
      "     34            \u001b[36m0.9143\u001b[0m        \u001b[32m0.4491\u001b[0m       0.5208            0.5208        1.2632  0.0006  0.1337\n",
      "     35            \u001b[36m0.9714\u001b[0m        \u001b[32m0.4360\u001b[0m       0.5208            0.5208        1.2753  0.0006  0.1447\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3539\u001b[0m       0.5312            0.5312        1.2883  0.0007  0.1391\n",
      "     37            1.0000        \u001b[32m0.2526\u001b[0m       0.5417            0.5417        1.2977  0.0007  0.1295\n",
      "     38            1.0000        \u001b[32m0.1479\u001b[0m       0.5312            0.5312        1.3032  0.0007  0.1484\n",
      "     39            1.0000        \u001b[32m0.1006\u001b[0m       0.5312            0.5312        1.3072  0.0007  0.1323\n",
      "     40            1.0000        0.1076       0.5312            0.5312        1.3129  0.0007  0.1312\n",
      "     41            1.0000        \u001b[32m0.0896\u001b[0m       0.5312            0.5312        1.3221  0.0007  0.1339\n",
      "     42            1.0000        \u001b[32m0.0662\u001b[0m       0.5312            0.5312        1.3331  0.0007  0.1331\n",
      "     43            1.0000        0.0787       0.5312            0.5312        1.3454  0.0006  0.1318\n",
      "     44            1.0000        \u001b[32m0.0501\u001b[0m       0.5312            0.5312        1.3595  0.0006  0.1367\n",
      "     45            1.0000        \u001b[32m0.0378\u001b[0m       0.5417            0.5417        1.3727  0.0005  0.1370\n",
      "     46            1.0000        0.0524       0.5417            0.5417        1.3832  0.0005  0.1320\n",
      "     47            1.0000        \u001b[32m0.0368\u001b[0m       0.5417            0.5417        1.3920  0.0004  0.1400\n",
      "     48            1.0000        \u001b[32m0.0305\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.3991  0.0004  0.1300\n",
      "     49            1.0000        \u001b[32m0.0223\u001b[0m       0.5625            0.5625        1.4052  0.0003  0.1350\n",
      "     50            1.0000        0.0523       0.5625            0.5625        1.4105  0.0003  0.1368\n",
      "Fine tuning model for subject 9 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.2400       0.4792            0.4792        1.3040  0.0004  0.1426\n",
      "     32            0.6000        1.0621       0.4688            0.4688        1.2804  0.0005  0.1335\n",
      "     33            0.6571        0.9426       0.5417            0.5417        1.2461  0.0005  0.1411\n",
      "     34            0.7714        \u001b[32m0.6868\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.2147  0.0006  0.1377\n",
      "     35            \u001b[36m0.9429\u001b[0m        \u001b[32m0.5000\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.1926  0.0006  0.1397\n",
      "     36            0.9429        \u001b[32m0.3734\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.1884  0.0007  0.1409\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2869\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.2186  0.0007  0.1339\n",
      "     38            0.9714        \u001b[32m0.2132\u001b[0m       0.5729            0.5729        1.2690  0.0007  0.1410\n",
      "     39            0.9714        \u001b[32m0.1612\u001b[0m       0.5521            0.5521        1.3238  0.0007  0.1342\n",
      "     40            0.9714        \u001b[32m0.1424\u001b[0m       0.5417            0.5417        1.3687  0.0007  0.1310\n",
      "     41            0.9714        0.1979       0.5312            0.5312        1.3895  0.0007  0.1385\n",
      "     42            1.0000        \u001b[32m0.1092\u001b[0m       0.5521            0.5521        1.3879  0.0007  0.1367\n",
      "     43            1.0000        \u001b[32m0.1014\u001b[0m       0.5729            0.5729        1.3727  0.0006  0.1407\n",
      "     44            1.0000        \u001b[32m0.0851\u001b[0m       0.5729            0.5729        1.3473  0.0006  0.1362\n",
      "     45            1.0000        \u001b[32m0.0711\u001b[0m       0.5833            0.5833        1.3203  0.0005  0.1363\n",
      "     46            1.0000        \u001b[32m0.0309\u001b[0m       0.6042            0.6042        1.2958  0.0005  0.1352\n",
      "     47            1.0000        0.0421       0.6146            0.6146        1.2728  0.0004  0.1365\n",
      "     48            1.0000        0.0583       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        1.2524  0.0004  0.1347\n",
      "     49            1.0000        0.0318       0.6250            0.6250        1.2350  0.0003  0.1752\n",
      "     50            1.0000        0.0357       0.6146            0.6146        1.2205  0.0003  0.1344\n",
      "Fine tuning model for subject 9 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            \u001b[36m0.8000\u001b[0m        \u001b[32m0.7060\u001b[0m       0.4792            0.4792        1.3047  0.0004  0.1401\n",
      "     32            \u001b[36m0.8286\u001b[0m        \u001b[32m0.6742\u001b[0m       0.4896            0.4896        1.2864  0.0005  0.1341\n",
      "     33            \u001b[36m0.8857\u001b[0m        \u001b[32m0.6423\u001b[0m       0.5104            0.5104        1.2768  0.0005  0.1386\n",
      "     34            \u001b[36m0.9143\u001b[0m        \u001b[32m0.4211\u001b[0m       0.5104            0.5104        1.2854  0.0006  0.1396\n",
      "     35            \u001b[36m0.9714\u001b[0m        \u001b[32m0.1529\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.3009  0.0006  0.1350\n",
      "     36            0.9714        \u001b[32m0.1435\u001b[0m       0.5417            0.5417        1.3150  0.0007  0.1369\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1084\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.3274  0.0007  0.1378\n",
      "     38            1.0000        0.1197       0.5938            0.5938        1.3335  0.0007  0.1308\n",
      "     39            1.0000        \u001b[32m0.0768\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.3374  0.0007  0.1361\n",
      "     40            1.0000        \u001b[32m0.0576\u001b[0m       0.6042            0.6042        1.3400  0.0007  0.1374\n",
      "     41            1.0000        \u001b[32m0.0492\u001b[0m       0.5938            0.5938        1.3415  0.0007  0.1352\n",
      "     42            1.0000        0.0665       0.5938            0.5938        1.3394  0.0007  0.1364\n",
      "     43            1.0000        \u001b[32m0.0238\u001b[0m       0.5938            0.5938        1.3356  0.0006  0.1390\n",
      "     44            1.0000        0.0340       0.5833            0.5833        1.3309  0.0006  0.1314\n",
      "     45            1.0000        0.0286       0.6042            0.6042        1.3253  0.0005  0.1343\n",
      "     46            1.0000        0.0258       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.3199  0.0005  0.1393\n",
      "     47            1.0000        0.0329       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        1.3157  0.0004  0.1312\n",
      "     48            1.0000        \u001b[32m0.0213\u001b[0m       0.6250            0.6250        1.3108  0.0004  0.1367\n",
      "     49            1.0000        0.0470       0.6146            0.6146        1.3048  0.0003  0.1377\n",
      "     50            1.0000        \u001b[32m0.0198\u001b[0m       0.6146            0.6146        1.2988  0.0003  0.1345\n",
      "Fine tuning model for subject 9 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5714        1.1374       0.4688            0.4688        1.3022  0.0004  0.1309\n",
      "     32            0.7143        1.2576       0.4896            0.4896        1.2810  0.0005  0.1698\n",
      "     33            0.7714        1.0785       0.5208            0.5208        1.2554  0.0005  0.1570\n",
      "     34            \u001b[36m0.8571\u001b[0m        0.8022       0.5417            0.5417        1.2351  0.0006  0.1647\n",
      "     35            \u001b[36m0.9143\u001b[0m        \u001b[32m0.5525\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.2287  0.0006  0.1763\n",
      "     36            \u001b[36m0.9429\u001b[0m        \u001b[32m0.4571\u001b[0m       0.5521            0.5521        1.2328  0.0007  0.1998\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2756\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.2480  0.0007  0.1857\n",
      "     38            1.0000        \u001b[32m0.2550\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.2720  0.0007  0.1685\n",
      "     39            1.0000        \u001b[32m0.2528\u001b[0m       0.5833            0.5833        1.3068  0.0007  0.1544\n",
      "     40            1.0000        \u001b[32m0.1710\u001b[0m       0.5729            0.5729        1.3412  0.0007  0.1602\n",
      "     41            1.0000        \u001b[32m0.1643\u001b[0m       0.5729            0.5729        1.3764  0.0007  0.1658\n",
      "     42            1.0000        \u001b[32m0.1356\u001b[0m       0.5729            0.5729        1.4083  0.0007  0.1689\n",
      "     43            1.0000        \u001b[32m0.0996\u001b[0m       0.5625            0.5625        1.4379  0.0006  0.1603\n",
      "     44            1.0000        \u001b[32m0.0919\u001b[0m       0.5625            0.5625        1.4617  0.0006  0.1590\n",
      "     45            1.0000        \u001b[32m0.0528\u001b[0m       0.5625            0.5625        1.4797  0.0005  0.1697\n",
      "     46            1.0000        \u001b[32m0.0456\u001b[0m       0.5521            0.5521        1.4942  0.0005  0.1562\n",
      "     47            1.0000        0.0489       0.5521            0.5521        1.5057  0.0004  0.1780\n",
      "     48            1.0000        0.0548       0.5521            0.5521        1.5135  0.0004  0.1923\n",
      "     49            1.0000        0.0499       0.5417            0.5417        1.5184  0.0003  0.1438\n",
      "     50            1.0000        \u001b[32m0.0233\u001b[0m       0.5521            0.5521        1.5219  0.0003  0.1290\n",
      "Fine tuning model for subject 9 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7714        1.0988       0.4688            0.4688        1.2978  0.0004  0.1387\n",
      "     32            \u001b[36m0.8000\u001b[0m        0.8672       0.4792            0.4792        1.2620  0.0005  0.1402\n",
      "     33            \u001b[36m0.8286\u001b[0m        0.8778       0.5000            0.5000        1.2245  0.0005  0.1342\n",
      "     34            \u001b[36m0.9429\u001b[0m        \u001b[32m0.5355\u001b[0m       0.5312            0.5312        1.2015  0.0006  0.1385\n",
      "     35            0.9429        \u001b[32m0.3719\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.1985  0.0006  0.1330\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3161\u001b[0m       0.5625            0.5625        1.2125  0.0007  0.1323\n",
      "     37            1.0000        \u001b[32m0.2209\u001b[0m       0.5625            0.5625        1.2341  0.0007  0.1369\n",
      "     38            1.0000        \u001b[32m0.1004\u001b[0m       0.5625            0.5625        1.2569  0.0007  0.1368\n",
      "     39            1.0000        \u001b[32m0.0935\u001b[0m       0.5625            0.5625        1.2802  0.0007  0.1356\n",
      "     40            1.0000        0.1104       0.5521            0.5521        1.3027  0.0007  0.1386\n",
      "     41            1.0000        \u001b[32m0.0550\u001b[0m       0.5417            0.5417        1.3245  0.0007  0.1464\n",
      "     42            1.0000        0.0786       0.5417            0.5417        1.3458  0.0007  0.1320\n",
      "     43            1.0000        0.0618       0.5625            0.5625        1.3590  0.0006  0.1384\n",
      "     44            1.0000        \u001b[32m0.0540\u001b[0m       0.5625            0.5625        1.3660  0.0006  0.1303\n",
      "     45            1.0000        0.0813       0.5625            0.5625        1.3705  0.0005  0.1360\n",
      "     46            1.0000        \u001b[32m0.0356\u001b[0m       0.5625            0.5625        1.3688  0.0005  0.1364\n",
      "     47            1.0000        0.0477       0.5625            0.5625        1.3615  0.0004  0.1366\n",
      "     48            1.0000        0.0704       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.3530  0.0004  0.1371\n",
      "     49            1.0000        \u001b[32m0.0236\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.3435  0.0003  0.1303\n",
      "     50            1.0000        0.0300       0.5625            0.5625        1.3330  0.0003  0.1643\n",
      "Fine tuning model for subject 9 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6286        1.2406       0.4792            0.4792        1.3140  0.0004  0.1385\n",
      "     32            0.7143        1.2563       0.4583            0.4583        1.3038  0.0005  0.1559\n",
      "     33            0.7714        0.8807       0.4271            0.4271        1.2875  0.0005  0.1515\n",
      "     34            \u001b[36m0.8571\u001b[0m        0.7910       0.4583            0.4583        1.2653  0.0006  0.1693\n",
      "     35            \u001b[36m0.9429\u001b[0m        \u001b[32m0.6869\u001b[0m       0.5104            0.5104        1.2516  0.0006  0.1442\n",
      "     36            \u001b[36m0.9714\u001b[0m        \u001b[32m0.4476\u001b[0m       0.5000            0.5000        1.2492  0.0007  0.1342\n",
      "     37            0.9714        \u001b[32m0.2892\u001b[0m       0.5208            0.5208        1.2566  0.0007  0.1327\n",
      "     38            0.9714        0.3022       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.2731  0.0007  0.1311\n",
      "     39            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2067\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.2957  0.0007  0.1338\n",
      "     40            1.0000        \u001b[32m0.1490\u001b[0m       0.5625            0.5625        1.3224  0.0007  0.1534\n",
      "     41            1.0000        0.2206       0.5729            0.5729        1.3485  0.0007  0.1346\n",
      "     42            1.0000        \u001b[32m0.1305\u001b[0m       0.5625            0.5625        1.3760  0.0007  0.1411\n",
      "     43            1.0000        \u001b[32m0.0708\u001b[0m       0.5625            0.5625        1.3987  0.0006  0.1420\n",
      "     44            1.0000        0.0770       0.5521            0.5521        1.4137  0.0006  0.1409\n",
      "     45            1.0000        0.0899       0.5521            0.5521        1.4213  0.0005  0.1289\n",
      "     46            1.0000        0.0766       0.5417            0.5417        1.4258  0.0005  0.1365\n",
      "     47            1.0000        \u001b[32m0.0502\u001b[0m       0.5521            0.5521        1.4256  0.0004  0.1362\n",
      "     48            1.0000        0.1069       0.5521            0.5521        1.4223  0.0004  0.1361\n",
      "     49            1.0000        0.0817       0.5521            0.5521        1.4183  0.0003  0.1414\n",
      "     50            1.0000        0.0866       0.5521            0.5521        1.4149  0.0003  0.1385\n",
      "Fine tuning model for subject 9 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6571        0.8978       0.4792            0.4792        1.3195  0.0004  0.1284\n",
      "     32            0.7714        0.8275       0.4583            0.4583        1.3169  0.0005  0.1336\n",
      "     33            \u001b[36m0.8286\u001b[0m        0.8199       0.4479            0.4479        1.3085  0.0005  0.1386\n",
      "     34            \u001b[36m0.8571\u001b[0m        \u001b[32m0.5217\u001b[0m       0.4583            0.4583        1.2881  0.0006  0.1364\n",
      "     35            \u001b[36m0.9429\u001b[0m        \u001b[32m0.4796\u001b[0m       0.4688            0.4688        1.2594  0.0006  0.1387\n",
      "     36            0.9429        \u001b[32m0.2721\u001b[0m       0.4688            0.4688        1.2320  0.0007  0.1552\n",
      "     37            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2107\u001b[0m       0.4688            0.4688        1.2152  0.0007  0.1341\n",
      "     38            1.0000        \u001b[32m0.1969\u001b[0m       0.4688            0.4688        1.2061  0.0007  0.1354\n",
      "     39            1.0000        \u001b[32m0.1100\u001b[0m       0.4688            0.4688        1.2020  0.0007  0.1386\n",
      "     40            1.0000        \u001b[32m0.0968\u001b[0m       0.4792            0.4792        1.2008  0.0007  0.1348\n",
      "     41            1.0000        \u001b[32m0.0753\u001b[0m       0.5000            0.5000        1.2029  0.0007  0.1413\n",
      "     42            1.0000        \u001b[32m0.0682\u001b[0m       0.5000            0.5000        1.2090  0.0007  0.1392\n",
      "     43            1.0000        0.0724       0.5104            0.5104        1.2176  0.0006  0.1416\n",
      "     44            1.0000        \u001b[32m0.0474\u001b[0m       0.5208            0.5208        1.2265  0.0006  0.1392\n",
      "     45            1.0000        0.0702       0.5208            0.5208        1.2336  0.0005  0.1406\n",
      "     46            1.0000        \u001b[32m0.0251\u001b[0m       0.5208            0.5208        1.2398  0.0005  0.1375\n",
      "     47            1.0000        0.0363       0.5417            0.5417        1.2451  0.0004  0.1407\n",
      "     48            1.0000        0.0335       0.5417            0.5417        1.2495  0.0004  0.1390\n",
      "     49            1.0000        0.0437       0.5417            0.5417        1.2528  0.0003  0.1334\n",
      "     50            1.0000        0.0431       0.5417            0.5417        1.2546  0.0003  0.1322\n",
      "Fine tuning model for subject 9 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6571        1.0236       0.4792            0.4792        1.3138  0.0004  0.1456\n",
      "     32            0.7429        0.8743       0.4792            0.4792        1.3134  0.0005  0.1288\n",
      "     33            0.7429        \u001b[32m0.6934\u001b[0m       0.4792            0.4792        1.3217  0.0005  0.1353\n",
      "     34            \u001b[36m0.8571\u001b[0m        \u001b[32m0.3924\u001b[0m       0.4688            0.4688        1.3316  0.0006  0.1385\n",
      "     35            \u001b[36m0.9714\u001b[0m        \u001b[32m0.3703\u001b[0m       0.4792            0.4792        1.3390  0.0006  0.1345\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3430\u001b[0m       0.4792            0.4792        1.3472  0.0007  0.1324\n",
      "     37            1.0000        \u001b[32m0.2361\u001b[0m       0.4896            0.4896        1.3513  0.0007  0.1367\n",
      "     38            1.0000        \u001b[32m0.1659\u001b[0m       0.4896            0.4896        1.3569  0.0007  0.1324\n",
      "     39            1.0000        \u001b[32m0.1430\u001b[0m       0.4896            0.4896        1.3533  0.0007  0.1330\n",
      "     40            1.0000        \u001b[32m0.1275\u001b[0m       0.4896            0.4896        1.3387  0.0007  0.1384\n",
      "     41            1.0000        0.1371       0.5000            0.5000        1.3223  0.0007  0.1495\n",
      "     42            1.0000        \u001b[32m0.1073\u001b[0m       0.5104            0.5104        1.3109  0.0007  0.1579\n",
      "     43            1.0000        \u001b[32m0.0619\u001b[0m       0.5000            0.5000        1.3045  0.0006  0.1579\n",
      "     44            1.0000        \u001b[32m0.0455\u001b[0m       0.5312            0.5312        1.3040  0.0006  0.1560\n",
      "     45            1.0000        0.0539       0.5208            0.5208        1.3072  0.0005  0.1686\n",
      "     46            1.0000        \u001b[32m0.0414\u001b[0m       0.5208            0.5208        1.3119  0.0005  0.1787\n",
      "     47            1.0000        0.0476       0.5208            0.5208        1.3162  0.0004  0.1611\n",
      "     48            1.0000        \u001b[32m0.0317\u001b[0m       0.5208            0.5208        1.3194  0.0004  0.1699\n",
      "     49            1.0000        \u001b[32m0.0263\u001b[0m       0.5312            0.5312        1.3217  0.0003  0.1619\n",
      "     50            1.0000        \u001b[32m0.0218\u001b[0m       0.5312            0.5312        1.3228  0.0003  0.1553\n",
      "Fine tuning model for subject 9 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.0368       0.4792            0.4792        1.3090  0.0004  0.1603\n",
      "     32            0.6286        1.0408       0.4688            0.4688        1.2938  0.0005  0.1593\n",
      "     33            0.7429        \u001b[32m0.7076\u001b[0m       0.5000            0.5000        1.2698  0.0005  0.1811\n",
      "     34            \u001b[36m0.9143\u001b[0m        \u001b[32m0.6955\u001b[0m       0.5104            0.5104        1.2486  0.0006  0.1663\n",
      "     35            \u001b[36m0.9429\u001b[0m        \u001b[32m0.5384\u001b[0m       0.5417            0.5417        1.2464  0.0006  0.1923\n",
      "     36            0.9429        \u001b[32m0.4084\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.2553  0.0007  0.1707\n",
      "     37            \u001b[36m0.9714\u001b[0m        \u001b[32m0.3004\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.2696  0.0007  0.1945\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2611\u001b[0m       0.5625            0.5625        1.2850  0.0007  0.1512\n",
      "     39            1.0000        \u001b[32m0.1921\u001b[0m       0.5625            0.5625        1.3080  0.0007  0.1370\n",
      "     40            1.0000        \u001b[32m0.1678\u001b[0m       0.5625            0.5625        1.3316  0.0007  0.1560\n",
      "     41            1.0000        \u001b[32m0.1221\u001b[0m       0.5729            0.5729        1.3559  0.0007  0.1339\n",
      "     42            1.0000        \u001b[32m0.0707\u001b[0m       0.5729            0.5729        1.3757  0.0007  0.1388\n",
      "     43            1.0000        0.0787       0.5625            0.5625        1.3933  0.0006  0.1356\n",
      "     44            1.0000        0.0769       0.5521            0.5521        1.4060  0.0006  0.1377\n",
      "     45            1.0000        0.0877       0.5312            0.5312        1.4094  0.0005  0.1345\n",
      "     46            1.0000        0.1080       0.5417            0.5417        1.4051  0.0005  0.1355\n",
      "     47            1.0000        0.0805       0.5417            0.5417        1.3954  0.0004  0.1325\n",
      "     48            1.0000        \u001b[32m0.0574\u001b[0m       0.5312            0.5312        1.3818  0.0004  0.1340\n",
      "     49            1.0000        \u001b[32m0.0440\u001b[0m       0.5417            0.5417        1.3657  0.0003  0.1396\n",
      "     50            1.0000        0.0503       0.5417            0.5417        1.3493  0.0003  0.1434\n",
      "Fine tuning model for subject 9 with 35 = 35 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7429        0.8525       0.4688            0.4688        1.3190  0.0004  0.1421\n",
      "     32            \u001b[36m0.8000\u001b[0m        0.9273       0.4688            0.4688        1.3154  0.0005  0.1354\n",
      "     33            0.8000        1.1266       0.4479            0.4479        1.3039  0.0005  0.1373\n",
      "     34            \u001b[36m0.9429\u001b[0m        0.8471       0.4583            0.4583        1.2897  0.0006  0.1325\n",
      "     35            \u001b[36m0.9714\u001b[0m        \u001b[32m0.4651\u001b[0m       0.4896            0.4896        1.2792  0.0006  0.1310\n",
      "     36            0.9714        \u001b[32m0.4212\u001b[0m       0.4792            0.4792        1.2860  0.0007  0.1329\n",
      "     37            0.9714        \u001b[32m0.3694\u001b[0m       0.4896            0.4896        1.3056  0.0007  0.1339\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2348\u001b[0m       0.4896            0.4896        1.3334  0.0007  0.1334\n",
      "     39            1.0000        0.2569       0.4896            0.4896        1.3619  0.0007  0.1331\n",
      "     40            1.0000        0.2922       0.4792            0.4792        1.3831  0.0007  0.1345\n",
      "     41            1.0000        \u001b[32m0.1839\u001b[0m       0.4896            0.4896        1.4000  0.0007  0.1380\n",
      "     42            1.0000        \u001b[32m0.1273\u001b[0m       0.4896            0.4896        1.4074  0.0007  0.1334\n",
      "     43            1.0000        \u001b[32m0.0938\u001b[0m       0.5000            0.5000        1.4145  0.0006  0.1412\n",
      "     44            1.0000        0.1038       0.5000            0.5000        1.4218  0.0006  0.1337\n",
      "     45            1.0000        0.1044       0.5104            0.5104        1.4172  0.0005  0.1349\n",
      "     46            1.0000        \u001b[32m0.0734\u001b[0m       0.5104            0.5104        1.4073  0.0005  0.1512\n",
      "     47            1.0000        \u001b[32m0.0604\u001b[0m       0.5312            0.5312        1.3908  0.0004  0.1643\n",
      "     48            1.0000        0.0658       0.5417            0.5417        1.3700  0.0004  0.1540\n",
      "     49            1.0000        \u001b[32m0.0388\u001b[0m       0.5417            0.5417        1.3480  0.0003  0.1474\n",
      "     50            1.0000        0.1412       0.5417            0.5417        1.3261  0.0003  0.1451\n",
      "Fine tuning model for subject 9 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.5007       0.4792            0.4792        1.2967  0.0004  0.1522\n",
      "     32            0.6750        1.4645       0.4583            0.4583        1.2499  0.0005  0.1510\n",
      "     33            0.7250        1.4552       0.4792            0.4792        1.1873  0.0005  0.1492\n",
      "     34            0.7750        0.9513       0.5104            0.5104        1.1331  0.0006  0.1426\n",
      "     35            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6770\u001b[0m       0.5208            0.5208        1.1069  0.0006  0.1429\n",
      "     36            \u001b[36m0.9750\u001b[0m        \u001b[32m0.5347\u001b[0m       0.5312            0.5312        1.1088  0.0007  0.1495\n",
      "     37            0.9500        \u001b[32m0.3841\u001b[0m       0.5312            0.5312        1.1264  0.0007  0.1440\n",
      "     38            0.9750        \u001b[32m0.3272\u001b[0m       0.5417            0.5417        1.1407  0.0007  0.1460\n",
      "     39            \u001b[36m1.0000\u001b[0m        0.4196       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.1490  0.0007  0.1425\n",
      "     40            1.0000        \u001b[32m0.2119\u001b[0m       0.5521            0.5521        1.1552  0.0007  0.1419\n",
      "     41            1.0000        \u001b[32m0.1947\u001b[0m       0.5521            0.5521        1.1711  0.0007  0.1403\n",
      "     42            1.0000        \u001b[32m0.1072\u001b[0m       0.5417            0.5417        1.1881  0.0007  0.1426\n",
      "     43            1.0000        0.1417       0.5312            0.5312        1.2045  0.0006  0.1418\n",
      "     44            1.0000        0.1306       0.5312            0.5312        1.2151  0.0006  0.1490\n",
      "     45            1.0000        \u001b[32m0.0911\u001b[0m       0.5312            0.5312        1.2197  0.0005  0.1415\n",
      "     46            1.0000        0.1039       0.5208            0.5208        1.2199  0.0005  0.1493\n",
      "     47            1.0000        \u001b[32m0.0887\u001b[0m       0.5208            0.5208        1.2144  0.0004  0.1449\n",
      "     48            1.0000        0.1143       0.5312            0.5312        1.2064  0.0004  0.1500\n",
      "     49            1.0000        0.0907       0.5312            0.5312        1.1976  0.0003  0.1442\n",
      "     50            1.0000        0.1113       0.5417            0.5417        1.1894  0.0003  0.1418\n",
      "Fine tuning model for subject 9 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6000        1.5010       0.4792            0.4792        1.2990  0.0004  0.1404\n",
      "     32            0.6250        1.4223       0.4896            0.4896        1.2585  0.0005  0.1443\n",
      "     33            0.6750        1.0714       0.5208            0.5208        1.2121  0.0005  0.1348\n",
      "     34            0.7500        0.9920       0.5208            0.5208        1.1757  0.0006  0.1423\n",
      "     35            \u001b[36m0.8500\u001b[0m        0.8474       0.5000            0.5000        1.1620  0.0006  0.1546\n",
      "     36            \u001b[36m0.9000\u001b[0m        \u001b[32m0.6165\u001b[0m       0.5000            0.5000        1.1672  0.0007  0.1436\n",
      "     37            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4881\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.1755  0.0007  0.1429\n",
      "     38            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3571\u001b[0m       0.5417            0.5417        1.1821  0.0007  0.1440\n",
      "     39            0.9750        \u001b[32m0.2630\u001b[0m       0.5312            0.5312        1.1882  0.0007  0.1337\n",
      "     40            \u001b[36m1.0000\u001b[0m        0.3146       0.5521            0.5521        1.1992  0.0007  0.1436\n",
      "     41            1.0000        \u001b[32m0.1726\u001b[0m       0.5417            0.5417        1.2101  0.0007  0.1393\n",
      "     42            1.0000        0.1743       0.5417            0.5417        1.2190  0.0007  0.1430\n",
      "     43            1.0000        \u001b[32m0.1350\u001b[0m       0.5208            0.5208        1.2274  0.0006  0.1421\n",
      "     44            1.0000        0.1385       0.5208            0.5208        1.2316  0.0006  0.1452\n",
      "     45            1.0000        \u001b[32m0.1046\u001b[0m       0.5208            0.5208        1.2339  0.0005  0.1462\n",
      "     46            1.0000        0.1066       0.5208            0.5208        1.2344  0.0005  0.1429\n",
      "     47            1.0000        \u001b[32m0.0932\u001b[0m       0.5104            0.5104        1.2339  0.0004  0.1451\n",
      "     48            1.0000        0.1094       0.5104            0.5104        1.2330  0.0004  0.1525\n",
      "     49            1.0000        0.0933       0.5000            0.5000        1.2312  0.0003  0.1758\n",
      "     50            1.0000        \u001b[32m0.0736\u001b[0m       0.5208            0.5208        1.2287  0.0003  0.1708\n",
      "Fine tuning model for subject 9 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7500        0.8347       0.4792            0.4792        1.3223  0.0004  0.1894\n",
      "     32            0.7500        0.8138       0.4792            0.4792        1.3271  0.0005  0.1643\n",
      "     33            \u001b[36m0.9000\u001b[0m        0.7509       0.4688            0.4688        1.3185  0.0005  0.1986\n",
      "     34            0.9000        \u001b[32m0.6589\u001b[0m       0.4792            0.4792        1.2940  0.0006  0.1684\n",
      "     35            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4579\u001b[0m       0.4792            0.4792        1.2681  0.0006  0.1684\n",
      "     36            0.9500        \u001b[32m0.3795\u001b[0m       0.5000            0.5000        1.2445  0.0007  0.1814\n",
      "     37            \u001b[36m0.9750\u001b[0m        \u001b[32m0.2791\u001b[0m       0.5104            0.5104        1.2272  0.0007  0.1774\n",
      "     38            0.9750        \u001b[32m0.2304\u001b[0m       0.5208            0.5208        1.2191  0.0007  0.1547\n",
      "     39            0.9750        \u001b[32m0.1790\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.2143  0.0007  0.1546\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1035\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.2133  0.0007  0.1543\n",
      "     41            1.0000        \u001b[32m0.1018\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.2162  0.0007  0.1717\n",
      "     42            1.0000        \u001b[32m0.0822\u001b[0m       0.5729            0.5729        1.2205  0.0007  0.1639\n",
      "     43            1.0000        0.0979       0.5833            0.5833        1.2264  0.0006  0.1632\n",
      "     44            1.0000        \u001b[32m0.0694\u001b[0m       0.5833            0.5833        1.2332  0.0006  0.1898\n",
      "     45            1.0000        \u001b[32m0.0592\u001b[0m       0.5833            0.5833        1.2384  0.0005  0.1498\n",
      "     46            1.0000        0.0681       0.5833            0.5833        1.2419  0.0005  0.1262\n",
      "     47            1.0000        \u001b[32m0.0509\u001b[0m       0.5729            0.5729        1.2444  0.0004  0.1530\n",
      "     48            1.0000        \u001b[32m0.0384\u001b[0m       0.5729            0.5729        1.2458  0.0004  0.1429\n",
      "     49            1.0000        0.0677       0.5729            0.5729        1.2453  0.0003  0.1539\n",
      "     50            1.0000        \u001b[32m0.0360\u001b[0m       0.5938            0.5938        1.2437  0.0003  0.1342\n",
      "Fine tuning model for subject 9 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5750        1.2964       0.4688            0.4688        1.2965  0.0004  0.1317\n",
      "     32            0.6750        0.9099       0.5104            0.5104        1.2590  0.0005  0.1504\n",
      "     33            0.7750        0.9868       0.5208            0.5208        1.2307  0.0005  0.1533\n",
      "     34            \u001b[36m0.8250\u001b[0m        \u001b[32m0.6347\u001b[0m       0.5208            0.5208        1.2188  0.0006  0.1386\n",
      "     35            \u001b[36m0.9000\u001b[0m        \u001b[32m0.5756\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.2112  0.0006  0.1313\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.4050\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.2040  0.0007  0.1429\n",
      "     37            0.9750        \u001b[32m0.2352\u001b[0m       0.5833            0.5833        1.2017  0.0007  0.1421\n",
      "     38            0.9750        \u001b[32m0.1795\u001b[0m       0.5833            0.5833        1.1916  0.0007  0.1521\n",
      "     39            0.9750        \u001b[32m0.1636\u001b[0m       0.6042            0.6042        1.1718  0.0007  0.1334\n",
      "     40            0.9750        0.2016       0.5625            0.5625        1.1544  0.0007  0.1316\n",
      "     41            0.9750        \u001b[32m0.0915\u001b[0m       0.5521            0.5521        1.1366  0.0007  0.1280\n",
      "     42            0.9750        \u001b[32m0.0896\u001b[0m       0.5833            0.5833        1.1203  0.0007  0.1386\n",
      "     43            1.0000        \u001b[32m0.0800\u001b[0m       0.5833            0.5833        1.1086  0.0006  0.1333\n",
      "     44            1.0000        \u001b[32m0.0631\u001b[0m       0.5833            0.5833        1.0992  0.0006  0.1386\n",
      "     45            1.0000        0.0966       0.5729            0.5729        1.0925  0.0005  0.1390\n",
      "     46            1.0000        \u001b[32m0.0589\u001b[0m       0.5729            0.5729        1.0873  0.0005  0.1415\n",
      "     47            1.0000        0.0677       0.5938            0.5938        1.0844  0.0004  0.1417\n",
      "     48            1.0000        \u001b[32m0.0573\u001b[0m       0.5833            0.5833        1.0831  0.0004  0.1347\n",
      "     49            1.0000        0.0578       0.5938            0.5938        1.0830  0.0003  0.1402\n",
      "     50            1.0000        \u001b[32m0.0311\u001b[0m       0.5938            0.5938        1.0839  0.0003  0.1449\n",
      "Fine tuning model for subject 9 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4750        1.7725       0.4792            0.4792        1.2924  0.0004  0.1473\n",
      "     32            0.5500        1.3448       0.5000            0.5000        1.2491  0.0005  0.1500\n",
      "     33            0.6000        1.2302       0.5104            0.5104        1.2033  0.0005  0.1928\n",
      "     34            0.7250        0.9931       0.5208            0.5208        1.1691  0.0006  0.1584\n",
      "     35            0.7750        \u001b[32m0.6688\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.1442  0.0006  0.1528\n",
      "     36            \u001b[36m0.8250\u001b[0m        \u001b[32m0.4782\u001b[0m       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        1.1298  0.0007  0.1476\n",
      "     37            \u001b[36m0.9250\u001b[0m        0.5864       0.6250            0.6250        1.1317  0.0007  0.1470\n",
      "     38            \u001b[36m1.0000\u001b[0m        0.5066       0.6146            0.6146        1.1429  0.0007  0.1646\n",
      "     39            1.0000        \u001b[32m0.3011\u001b[0m       0.6250            0.6250        1.1540  0.0007  0.1451\n",
      "     40            1.0000        \u001b[32m0.2946\u001b[0m       0.6042            0.6042        1.1665  0.0007  0.1500\n",
      "     41            1.0000        \u001b[32m0.2472\u001b[0m       0.6042            0.6042        1.1799  0.0007  0.1476\n",
      "     42            1.0000        \u001b[32m0.1713\u001b[0m       0.6146            0.6146        1.1920  0.0007  0.1448\n",
      "     43            1.0000        \u001b[32m0.1225\u001b[0m       0.6250            0.6250        1.2065  0.0006  0.1438\n",
      "     44            1.0000        \u001b[32m0.0829\u001b[0m       0.6250            0.6250        1.2197  0.0006  0.1496\n",
      "     45            1.0000        0.0921       0.6146            0.6146        1.2297  0.0005  0.1475\n",
      "     46            1.0000        0.0843       0.6146            0.6146        1.2378  0.0005  0.1404\n",
      "     47            1.0000        \u001b[32m0.0672\u001b[0m       0.5938            0.5938        1.2425  0.0004  0.1448\n",
      "     48            1.0000        \u001b[32m0.0490\u001b[0m       0.5938            0.5938        1.2453  0.0004  0.1527\n",
      "     49            1.0000        0.0970       0.5938            0.5938        1.2458  0.0003  0.1564\n",
      "     50            1.0000        0.0711       0.5833            0.5833        1.2449  0.0003  0.1490\n",
      "Fine tuning model for subject 9 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.4500        1.6304       0.4792            0.4792        1.2845  0.0004  0.1434\n",
      "     32            0.4750        1.6071       0.5104            0.5104        1.2230  0.0005  0.1493\n",
      "     33            0.6250        1.1813       0.5104            0.5104        1.1670  0.0005  0.1427\n",
      "     34            \u001b[36m0.8250\u001b[0m        0.9933       0.5312            0.5312        1.1434  0.0006  0.1456\n",
      "     35            0.8250        0.8316       0.5417            0.5417        1.1552  0.0006  0.1531\n",
      "     36            0.8250        \u001b[32m0.6710\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.1875  0.0007  0.1540\n",
      "     37            0.8250        \u001b[32m0.3853\u001b[0m       0.5521            0.5521        1.2262  0.0007  0.1467\n",
      "     38            \u001b[36m0.8750\u001b[0m        \u001b[32m0.2853\u001b[0m       0.5208            0.5208        1.2745  0.0007  0.1456\n",
      "     39            0.8750        \u001b[32m0.2331\u001b[0m       0.5312            0.5312        1.3154  0.0007  0.1485\n",
      "     40            \u001b[36m0.9000\u001b[0m        \u001b[32m0.1817\u001b[0m       0.5417            0.5417        1.3520  0.0007  0.1708\n",
      "     41            \u001b[36m0.9250\u001b[0m        0.1939       0.5521            0.5521        1.3753  0.0007  0.1494\n",
      "     42            \u001b[36m0.9500\u001b[0m        0.1860       0.5521            0.5521        1.3950  0.0007  0.1444\n",
      "     43            \u001b[36m0.9750\u001b[0m        \u001b[32m0.1174\u001b[0m       0.5417            0.5417        1.4059  0.0006  0.1428\n",
      "     44            0.9750        \u001b[32m0.1091\u001b[0m       0.5521            0.5521        1.4092  0.0006  0.1441\n",
      "     45            0.9750        \u001b[32m0.0659\u001b[0m       0.5521            0.5521        1.4030  0.0005  0.1454\n",
      "     46            0.9750        0.0887       0.5625            0.5625        1.3892  0.0005  0.1492\n",
      "     47            \u001b[36m1.0000\u001b[0m        0.0703       0.5625            0.5625        1.3713  0.0004  0.1495\n",
      "     48            1.0000        0.0671       0.5521            0.5521        1.3475  0.0004  0.1475\n",
      "     49            1.0000        0.0695       0.5625            0.5625        1.3194  0.0003  0.1499\n",
      "     50            1.0000        \u001b[32m0.0583\u001b[0m       0.5729            0.5729        1.2900  0.0003  0.1452\n",
      "Fine tuning model for subject 9 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6250        1.1685       0.4792            0.4792        1.3070  0.0004  0.1461\n",
      "     32            0.6500        1.1654       0.4688            0.4688        1.2827  0.0005  0.1512\n",
      "     33            0.7000        0.7782       0.5000            0.5000        1.2600  0.0005  0.1628\n",
      "     34            \u001b[36m0.8500\u001b[0m        \u001b[32m0.7339\u001b[0m       0.5104            0.5104        1.2389  0.0006  0.1851\n",
      "     35            \u001b[36m0.9750\u001b[0m        \u001b[32m0.5635\u001b[0m       0.5104            0.5104        1.2329  0.0006  0.1653\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3393\u001b[0m       0.5208            0.5208        1.2359  0.0007  0.1835\n",
      "     37            1.0000        \u001b[32m0.3028\u001b[0m       0.5312            0.5312        1.2451  0.0007  0.1835\n",
      "     38            1.0000        \u001b[32m0.2130\u001b[0m       0.5417            0.5417        1.2479  0.0007  0.1738\n",
      "     39            1.0000        \u001b[32m0.1633\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.2469  0.0007  0.1674\n",
      "     40            1.0000        \u001b[32m0.1274\u001b[0m       0.5312            0.5312        1.2471  0.0007  0.1814\n",
      "     41            1.0000        0.1377       0.5104            0.5104        1.2491  0.0007  0.1812\n",
      "     42            1.0000        \u001b[32m0.1161\u001b[0m       0.5104            0.5104        1.2513  0.0007  0.1699\n",
      "     43            1.0000        \u001b[32m0.1154\u001b[0m       0.5104            0.5104        1.2532  0.0006  0.1625\n",
      "     44            1.0000        \u001b[32m0.0906\u001b[0m       0.5000            0.5000        1.2563  0.0006  0.1680\n",
      "     45            1.0000        \u001b[32m0.0903\u001b[0m       0.4896            0.4896        1.2612  0.0005  0.1594\n",
      "     46            1.0000        \u001b[32m0.0894\u001b[0m       0.4896            0.4896        1.2655  0.0005  0.2146\n",
      "     47            1.0000        0.0956       0.4792            0.4792        1.2686  0.0004  0.1642\n",
      "     48            1.0000        \u001b[32m0.0764\u001b[0m       0.4896            0.4896        1.2706  0.0004  0.2334\n",
      "     49            1.0000        \u001b[32m0.0590\u001b[0m       0.4896            0.4896        1.2720  0.0003  0.1489\n",
      "     50            1.0000        \u001b[32m0.0427\u001b[0m       0.5000            0.5000        1.2733  0.0003  0.1543\n",
      "Fine tuning model for subject 9 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6250        1.4112       0.4688            0.4688        1.2888  0.0004  0.1501\n",
      "     32            0.6750        1.3487       0.5000            0.5000        1.2499  0.0005  0.1430\n",
      "     33            \u001b[36m0.8250\u001b[0m        0.8453       0.5312            0.5312        1.2357  0.0005  0.1456\n",
      "     34            \u001b[36m0.9250\u001b[0m        \u001b[32m0.5014\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.2642  0.0006  0.1443\n",
      "     35            \u001b[36m0.9500\u001b[0m        \u001b[32m0.4212\u001b[0m       0.5312            0.5312        1.3089  0.0006  0.1462\n",
      "     36            0.9500        \u001b[32m0.2460\u001b[0m       0.5208            0.5208        1.3469  0.0007  0.1453\n",
      "     37            0.9500        \u001b[32m0.2153\u001b[0m       0.5312            0.5312        1.3802  0.0007  0.1430\n",
      "     38            \u001b[36m0.9750\u001b[0m        \u001b[32m0.1638\u001b[0m       0.5000            0.5000        1.3954  0.0007  0.1486\n",
      "     39            0.9750        \u001b[32m0.1553\u001b[0m       0.5521            0.5521        1.3907  0.0007  0.1460\n",
      "     40            0.9750        \u001b[32m0.1043\u001b[0m       0.5208            0.5208        1.3741  0.0007  0.1480\n",
      "     41            \u001b[36m1.0000\u001b[0m        0.1190       0.5521            0.5521        1.3527  0.0007  0.1424\n",
      "     42            1.0000        \u001b[32m0.1019\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.3347  0.0007  0.1492\n",
      "     43            1.0000        0.1116       0.5521            0.5521        1.3222  0.0006  0.1467\n",
      "     44            1.0000        \u001b[32m0.0902\u001b[0m       0.5521            0.5521        1.3113  0.0006  0.1423\n",
      "     45            1.0000        \u001b[32m0.0686\u001b[0m       0.5521            0.5521        1.3058  0.0005  0.1377\n",
      "     46            1.0000        \u001b[32m0.0530\u001b[0m       0.5521            0.5521        1.3042  0.0005  0.1436\n",
      "     47            1.0000        0.0767       0.5521            0.5521        1.3043  0.0004  0.1463\n",
      "     48            1.0000        0.0560       0.5312            0.5312        1.3048  0.0004  0.1492\n",
      "     49            1.0000        \u001b[32m0.0382\u001b[0m       0.5312            0.5312        1.3057  0.0003  0.1477\n",
      "     50            1.0000        \u001b[32m0.0298\u001b[0m       0.5312            0.5312        1.3068  0.0003  0.1466\n",
      "Fine tuning model for subject 9 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6500        1.0522       0.4792            0.4792        1.3107  0.0004  0.1462\n",
      "     32            0.7000        1.3923       0.4583            0.4583        1.2958  0.0005  0.1436\n",
      "     33            \u001b[36m0.8000\u001b[0m        0.9297       0.4792            0.4792        1.2830  0.0005  0.1384\n",
      "     34            \u001b[36m0.8500\u001b[0m        0.8393       0.4896            0.4896        1.2812  0.0006  0.1492\n",
      "     35            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5146\u001b[0m       0.4792            0.4792        1.2890  0.0006  0.1438\n",
      "     36            \u001b[36m0.9750\u001b[0m        \u001b[32m0.4952\u001b[0m       0.5000            0.5000        1.2997  0.0007  0.1386\n",
      "     37            0.9750        \u001b[32m0.3905\u001b[0m       0.5104            0.5104        1.3060  0.0007  0.1436\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2307\u001b[0m       0.5312            0.5312        1.2954  0.0007  0.1441\n",
      "     39            1.0000        \u001b[32m0.1596\u001b[0m       0.5312            0.5312        1.2779  0.0007  0.1491\n",
      "     40            1.0000        0.1703       0.5312            0.5312        1.2550  0.0007  0.1489\n",
      "     41            1.0000        0.1706       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.2299  0.0007  0.1519\n",
      "     42            1.0000        \u001b[32m0.1014\u001b[0m       0.5417            0.5417        1.2068  0.0007  0.1490\n",
      "     43            1.0000        \u001b[32m0.0768\u001b[0m       0.5521            0.5521        1.1902  0.0006  0.1426\n",
      "     44            1.0000        \u001b[32m0.0719\u001b[0m       0.5521            0.5521        1.1766  0.0006  0.1446\n",
      "     45            1.0000        \u001b[32m0.0719\u001b[0m       0.5521            0.5521        1.1657  0.0005  0.1465\n",
      "     46            1.0000        0.0807       0.5521            0.5521        1.1575  0.0005  0.1495\n",
      "     47            1.0000        \u001b[32m0.0719\u001b[0m       0.5417            0.5417        1.1511  0.0004  0.1451\n",
      "     48            1.0000        \u001b[32m0.0674\u001b[0m       0.5417            0.5417        1.1478  0.0004  0.1441\n",
      "     49            1.0000        \u001b[32m0.0465\u001b[0m       0.5417            0.5417        1.1456  0.0003  0.1443\n",
      "     50            1.0000        \u001b[32m0.0318\u001b[0m       0.5312            0.5312        1.1453  0.0003  0.1452\n",
      "Fine tuning model for subject 9 with 40 = 40 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6500        1.2616       0.4792            0.4792        1.3057  0.0004  0.1496\n",
      "     32            0.7000        1.1147       0.4583            0.4583        1.2952  0.0005  0.1410\n",
      "     33            0.7250        0.9961       0.4792            0.4792        1.2818  0.0005  0.1449\n",
      "     34            \u001b[36m0.8250\u001b[0m        \u001b[32m0.6948\u001b[0m       0.5000            0.5000        1.2749  0.0006  0.1437\n",
      "     35            \u001b[36m0.9500\u001b[0m        \u001b[32m0.5437\u001b[0m       0.5312            0.5312        1.2763  0.0006  0.1512\n",
      "     36            \u001b[36m0.9750\u001b[0m        \u001b[32m0.3527\u001b[0m       0.5417            0.5417        1.2868  0.0007  0.1406\n",
      "     37            0.9250        \u001b[32m0.2816\u001b[0m       0.5417            0.5417        1.3093  0.0007  0.1431\n",
      "     38            0.9250        \u001b[32m0.2059\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.3426  0.0007  0.1533\n",
      "     39            0.9500        \u001b[32m0.1986\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.3775  0.0007  0.1488\n",
      "     40            0.9250        \u001b[32m0.1362\u001b[0m       0.5521            0.5521        1.4095  0.0007  0.1454\n",
      "     41            0.9250        \u001b[32m0.1340\u001b[0m       0.5521            0.5521        1.4311  0.0007  0.1442\n",
      "     42            0.9250        0.1405       0.5521            0.5521        1.4384  0.0007  0.1409\n",
      "     43            0.9500        \u001b[32m0.0993\u001b[0m       0.5521            0.5521        1.4420  0.0006  0.1555\n",
      "     44            0.9750        \u001b[32m0.0615\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.4402  0.0006  0.1492\n",
      "     45            \u001b[36m1.0000\u001b[0m        0.0680       0.5729            0.5729        1.4362  0.0005  0.1502\n",
      "     46            1.0000        \u001b[32m0.0599\u001b[0m       0.5625            0.5625        1.4319  0.0005  0.1928\n",
      "     47            1.0000        0.0709       0.5521            0.5521        1.4277  0.0004  0.1464\n",
      "     48            1.0000        \u001b[32m0.0595\u001b[0m       0.5521            0.5521        1.4223  0.0004  0.1479\n",
      "     49            1.0000        \u001b[32m0.0563\u001b[0m       0.5417            0.5417        1.4166  0.0003  0.1428\n",
      "     50            1.0000        \u001b[32m0.0238\u001b[0m       0.5417            0.5417        1.4127  0.0003  0.1444\n",
      "Fine tuning model for subject 9 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6889        1.1468       0.4792            0.4792        1.3134  0.0004  0.1636\n",
      "     32            0.6889        1.1513       0.4479            0.4479        1.2937  0.0005  0.1634\n",
      "     33            0.6889        0.9656       0.4271            0.4271        1.2617  0.0005  0.1565\n",
      "     34            0.7778        \u001b[32m0.7315\u001b[0m       0.4375            0.4375        1.2279  0.0006  0.1551\n",
      "     35            \u001b[36m0.8444\u001b[0m        \u001b[32m0.5073\u001b[0m       0.4479            0.4479        1.2150  0.0006  0.1694\n",
      "     36            \u001b[36m0.9556\u001b[0m        \u001b[32m0.4700\u001b[0m       0.4688            0.4688        1.2450  0.0007  0.1600\n",
      "     37            0.9333        \u001b[32m0.3590\u001b[0m       0.4896            0.4896        1.2852  0.0007  0.1897\n",
      "     38            0.9556        \u001b[32m0.3569\u001b[0m       0.4896            0.4896        1.3237  0.0007  0.1763\n",
      "     39            \u001b[36m0.9778\u001b[0m        0.4343       0.5104            0.5104        1.3381  0.0007  0.1834\n",
      "     40            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2328\u001b[0m       0.5208            0.5208        1.3391  0.0007  0.1766\n",
      "     41            1.0000        \u001b[32m0.1698\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.3358  0.0007  0.1912\n",
      "     42            1.0000        0.1725       0.5625            0.5625        1.3289  0.0007  0.1793\n",
      "     43            1.0000        \u001b[32m0.1086\u001b[0m       0.5625            0.5625        1.3241  0.0006  0.1762\n",
      "     44            1.0000        0.1275       0.5521            0.5521        1.3185  0.0006  0.2291\n",
      "     45            1.0000        \u001b[32m0.1022\u001b[0m       0.5521            0.5521        1.3140  0.0005  0.1988\n",
      "     46            1.0000        0.1027       0.5521            0.5521        1.3107  0.0005  0.2071\n",
      "     47            1.0000        \u001b[32m0.0728\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.3055  0.0004  0.2349\n",
      "     48            1.0000        0.1334       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.2997  0.0004  0.1960\n",
      "     49            1.0000        \u001b[32m0.0698\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.2941  0.0003  0.1867\n",
      "     50            1.0000        \u001b[32m0.0589\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.2879  0.0003  0.2277\n",
      "Fine tuning model for subject 9 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7556        0.9968       0.4792            0.4792        1.3291  0.0004  0.1707\n",
      "     32            \u001b[36m0.8000\u001b[0m        0.9786       0.4375            0.4375        1.3427  0.0005  0.1571\n",
      "     33            \u001b[36m0.8222\u001b[0m        0.9244       0.4271            0.4271        1.3463  0.0005  0.1604\n",
      "     34            \u001b[36m0.8444\u001b[0m        0.7660       0.4271            0.4271        1.3334  0.0006  0.1626\n",
      "     35            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6000\u001b[0m       0.4896            0.4896        1.3112  0.0006  0.1611\n",
      "     36            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5705\u001b[0m       0.4792            0.4792        1.2904  0.0007  0.1663\n",
      "     37            \u001b[36m0.9556\u001b[0m        \u001b[32m0.2891\u001b[0m       0.5208            0.5208        1.2799  0.0007  0.1656\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2698\u001b[0m       0.5104            0.5104        1.2769  0.0007  0.1702\n",
      "     39            1.0000        0.2885       0.5208            0.5208        1.2713  0.0007  0.1556\n",
      "     40            1.0000        \u001b[32m0.2070\u001b[0m       0.5208            0.5208        1.2636  0.0007  0.1706\n",
      "     41            1.0000        \u001b[32m0.1563\u001b[0m       0.5417            0.5417        1.2533  0.0007  0.1599\n",
      "     42            1.0000        \u001b[32m0.1231\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.2413  0.0007  0.1575\n",
      "     43            1.0000        0.1271       0.5625            0.5625        1.2288  0.0006  0.1544\n",
      "     44            1.0000        \u001b[32m0.0835\u001b[0m       0.5521            0.5521        1.2176  0.0006  0.1585\n",
      "     45            1.0000        \u001b[32m0.0727\u001b[0m       0.5521            0.5521        1.2095  0.0005  0.1576\n",
      "     46            1.0000        0.0762       0.5521            0.5521        1.2042  0.0005  0.1526\n",
      "     47            1.0000        \u001b[32m0.0708\u001b[0m       0.5521            0.5521        1.2013  0.0004  0.1507\n",
      "     48            1.0000        \u001b[32m0.0502\u001b[0m       0.5417            0.5417        1.2000  0.0004  0.1550\n",
      "     49            1.0000        \u001b[32m0.0469\u001b[0m       0.5729            0.5729        1.2005  0.0003  0.1576\n",
      "     50            1.0000        0.0614       0.5729            0.5729        1.2025  0.0003  0.1583\n",
      "Fine tuning model for subject 9 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6889        1.1663       0.4792            0.4792        1.3133  0.0004  0.1590\n",
      "     32            0.7111        1.1705       0.4688            0.4688        1.3012  0.0005  0.1559\n",
      "     33            0.7556        0.8614       0.5000            0.5000        1.2854  0.0005  0.1533\n",
      "     34            \u001b[36m0.8000\u001b[0m        0.8264       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.2695  0.0006  0.1576\n",
      "     35            \u001b[36m0.8667\u001b[0m        \u001b[32m0.5382\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.2614  0.0006  0.1571\n",
      "     36            \u001b[36m0.9111\u001b[0m        0.5471       0.5521            0.5521        1.2700  0.0007  0.1584\n",
      "     37            \u001b[36m0.9333\u001b[0m        \u001b[32m0.2395\u001b[0m       0.5625            0.5625        1.2911  0.0007  0.1551\n",
      "     38            \u001b[36m0.9556\u001b[0m        0.3337       0.5521            0.5521        1.3171  0.0007  0.1542\n",
      "     39            0.9556        0.2957       0.5521            0.5521        1.3507  0.0007  0.1522\n",
      "     40            \u001b[36m0.9778\u001b[0m        \u001b[32m0.1478\u001b[0m       0.5417            0.5417        1.3828  0.0007  0.1575\n",
      "     41            \u001b[36m1.0000\u001b[0m        0.1801       0.5417            0.5417        1.4119  0.0007  0.1462\n",
      "     42            1.0000        \u001b[32m0.1231\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.4309  0.0007  0.1492\n",
      "     43            1.0000        \u001b[32m0.1157\u001b[0m       0.5625            0.5625        1.4444  0.0006  0.1494\n",
      "     44            1.0000        \u001b[32m0.0924\u001b[0m       0.5625            0.5625        1.4504  0.0006  0.1558\n",
      "     45            1.0000        \u001b[32m0.0884\u001b[0m       0.5625            0.5625        1.4493  0.0005  0.1535\n",
      "     46            1.0000        \u001b[32m0.0684\u001b[0m       0.5625            0.5625        1.4443  0.0005  0.1613\n",
      "     47            1.0000        \u001b[32m0.0659\u001b[0m       0.5625            0.5625        1.4369  0.0004  0.1519\n",
      "     48            1.0000        \u001b[32m0.0633\u001b[0m       0.5625            0.5625        1.4278  0.0004  0.1598\n",
      "     49            1.0000        0.0804       0.5729            0.5729        1.4184  0.0003  0.1529\n",
      "     50            1.0000        0.0639       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.4093  0.0003  0.1521\n",
      "Fine tuning model for subject 9 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6444        0.9802       0.4792            0.4792        1.3037  0.0004  0.1554\n",
      "     32            0.6889        0.9966       0.4792            0.4792        1.2761  0.0005  0.1494\n",
      "     33            0.7778        0.8375       0.4896            0.4896        1.2498  0.0005  0.1544\n",
      "     34            \u001b[36m0.8000\u001b[0m        \u001b[32m0.5904\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.2368  0.0006  0.1505\n",
      "     35            \u001b[36m0.8889\u001b[0m        \u001b[32m0.5333\u001b[0m       0.5417            0.5417        1.2306  0.0006  0.1580\n",
      "     36            \u001b[36m0.9111\u001b[0m        \u001b[32m0.4451\u001b[0m       0.5521            0.5521        1.2184  0.0007  0.1575\n",
      "     37            \u001b[36m0.9333\u001b[0m        \u001b[32m0.3164\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.1942  0.0007  0.1593\n",
      "     38            \u001b[36m0.9556\u001b[0m        0.3536       0.5625            0.5625        1.1652  0.0007  0.1569\n",
      "     39            0.9556        \u001b[32m0.2441\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.1389  0.0007  0.1538\n",
      "     40            \u001b[36m0.9778\u001b[0m        0.2469       0.5625            0.5625        1.1181  0.0007  0.1563\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2295\u001b[0m       0.5729            0.5729        1.1053  0.0007  0.1603\n",
      "     42            1.0000        \u001b[32m0.2008\u001b[0m       0.5833            0.5833        1.1007  0.0007  0.1578\n",
      "     43            1.0000        \u001b[32m0.1840\u001b[0m       0.5833            0.5833        1.1020  0.0006  0.1550\n",
      "     44            1.0000        \u001b[32m0.1656\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.1112  0.0006  0.1514\n",
      "     45            1.0000        \u001b[32m0.1453\u001b[0m       0.6146            0.6146        1.1205  0.0005  0.1532\n",
      "     46            1.0000        \u001b[32m0.0737\u001b[0m       0.6146            0.6146        1.1276  0.0005  0.1530\n",
      "     47            1.0000        0.1087       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        1.1347  0.0004  0.1481\n",
      "     48            1.0000        0.0957       0.6146            0.6146        1.1404  0.0004  0.1580\n",
      "     49            1.0000        0.0746       0.6146            0.6146        1.1452  0.0003  0.1546\n",
      "     50            1.0000        0.0769       0.6250            0.6250        1.1500  0.0003  0.1535\n",
      "Fine tuning model for subject 9 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6222        1.3209       0.4792            0.4792        1.3035  0.0004  0.1613\n",
      "     32            0.7556        0.9512       0.4792            0.4792        1.2820  0.0005  0.1598\n",
      "     33            0.7556        1.0784       0.5000            0.5000        1.2515  0.0005  0.1828\n",
      "     34            \u001b[36m0.8000\u001b[0m        0.8278       0.5104            0.5104        1.2271  0.0006  0.1752\n",
      "     35            \u001b[36m0.8889\u001b[0m        \u001b[32m0.5382\u001b[0m       0.5312            0.5312        1.2183  0.0006  0.2499\n",
      "     36            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4340\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.2270  0.0007  0.1881\n",
      "     37            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3650\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.2515  0.0007  0.2255\n",
      "     38            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3164\u001b[0m       0.5625            0.5625        1.2862  0.0007  0.1904\n",
      "     39            1.0000        \u001b[32m0.1997\u001b[0m       0.5521            0.5521        1.3260  0.0007  0.2051\n",
      "     40            0.9778        0.2634       0.5521            0.5521        1.3628  0.0007  0.1806\n",
      "     41            0.9778        0.2284       0.5417            0.5417        1.3919  0.0007  0.2244\n",
      "     42            0.9778        \u001b[32m0.1596\u001b[0m       0.5417            0.5417        1.4148  0.0007  0.2494\n",
      "     43            1.0000        \u001b[32m0.1514\u001b[0m       0.5521            0.5521        1.4300  0.0006  0.1823\n",
      "     44            1.0000        \u001b[32m0.1486\u001b[0m       0.5312            0.5312        1.4447  0.0006  0.2163\n",
      "     45            1.0000        \u001b[32m0.0928\u001b[0m       0.5312            0.5312        1.4536  0.0005  0.2001\n",
      "     46            1.0000        0.0954       0.5312            0.5312        1.4576  0.0005  0.2095\n",
      "     47            1.0000        0.1066       0.5417            0.5417        1.4578  0.0004  0.1681\n",
      "     48            1.0000        \u001b[32m0.0731\u001b[0m       0.5521            0.5521        1.4555  0.0004  0.1711\n",
      "     49            1.0000        0.0961       0.5625            0.5625        1.4504  0.0003  0.1707\n",
      "     50            1.0000        0.0803       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.4440  0.0003  0.1596\n",
      "Fine tuning model for subject 9 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5778        1.5486       0.4792            0.4792        1.3140  0.0004  0.1612\n",
      "     32            0.6000        1.5881       0.4479            0.4479        1.3029  0.0005  0.1620\n",
      "     33            0.6444        1.1394       0.4479            0.4479        1.2882  0.0005  0.1583\n",
      "     34            0.7111        1.0876       0.4479            0.4479        1.2597  0.0006  0.1512\n",
      "     35            \u001b[36m0.8000\u001b[0m        1.1379       0.5000            0.5000        1.2173  0.0006  0.1544\n",
      "     36            \u001b[36m0.8667\u001b[0m        0.8125       0.5104            0.5104        1.1741  0.0007  0.1571\n",
      "     37            \u001b[36m0.8889\u001b[0m        \u001b[32m0.5733\u001b[0m       0.5417            0.5417        1.1365  0.0007  0.1564\n",
      "     38            \u001b[36m0.9333\u001b[0m        \u001b[32m0.5407\u001b[0m       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.1121  0.0007  0.1618\n",
      "     39            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3600\u001b[0m       0.5417            0.5417        1.1030  0.0007  0.1535\n",
      "     40            0.9778        \u001b[32m0.3038\u001b[0m       0.5417            0.5417        1.1121  0.0007  0.1496\n",
      "     41            0.9778        \u001b[32m0.2828\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.1291  0.0007  0.1597\n",
      "     42            \u001b[36m1.0000\u001b[0m        \u001b[32m0.2020\u001b[0m       0.5729            0.5729        1.1503  0.0007  0.1556\n",
      "     43            1.0000        0.2105       0.5625            0.5625        1.1730  0.0006  0.1536\n",
      "     44            1.0000        \u001b[32m0.1508\u001b[0m       0.5312            0.5312        1.1977  0.0006  0.1500\n",
      "     45            1.0000        \u001b[32m0.1310\u001b[0m       0.5312            0.5312        1.2192  0.0005  0.1477\n",
      "     46            1.0000        \u001b[32m0.1173\u001b[0m       0.5208            0.5208        1.2378  0.0005  0.1503\n",
      "     47            1.0000        0.1267       0.5000            0.5000        1.2512  0.0004  0.1580\n",
      "     48            1.0000        0.1522       0.5000            0.5000        1.2569  0.0004  0.1522\n",
      "     49            1.0000        0.1734       0.5000            0.5000        1.2631  0.0003  0.1588\n",
      "     50            1.0000        0.1176       0.5000            0.5000        1.2664  0.0003  0.1559\n",
      "Fine tuning model for subject 9 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5111        1.4532       0.4792            0.4792        1.3075  0.0004  0.1602\n",
      "     32            0.5556        1.3016       0.4792            0.4792        1.2813  0.0005  0.1504\n",
      "     33            0.6222        1.0747       0.4688            0.4688        1.2401  0.0005  0.1503\n",
      "     34            0.7333        0.9818       0.5000            0.5000        1.1973  0.0006  0.1583\n",
      "     35            \u001b[36m0.8889\u001b[0m        0.7561       \u001b[35m0.5625\u001b[0m            \u001b[31m0.5625\u001b[0m        1.1674  0.0006  0.1532\n",
      "     36            \u001b[36m0.9333\u001b[0m        \u001b[32m0.4599\u001b[0m       0.5625            0.5625        1.1555  0.0007  0.1555\n",
      "     37            \u001b[36m0.9556\u001b[0m        \u001b[32m0.4356\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.1627  0.0007  0.1787\n",
      "     38            \u001b[36m0.9778\u001b[0m        \u001b[32m0.3377\u001b[0m       0.5729            0.5729        1.1806  0.0007  0.1517\n",
      "     39            0.9778        \u001b[32m0.1792\u001b[0m       \u001b[35m0.5833\u001b[0m            \u001b[31m0.5833\u001b[0m        1.2035  0.0007  0.1505\n",
      "     40            \u001b[36m1.0000\u001b[0m        0.2415       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.2254  0.0007  0.1544\n",
      "     41            1.0000        \u001b[32m0.1674\u001b[0m       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.2431  0.0007  0.1521\n",
      "     42            1.0000        0.2455       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        1.2491  0.0007  0.1606\n",
      "     43            1.0000        \u001b[32m0.1204\u001b[0m       0.6250            0.6250        1.2520  0.0006  0.1591\n",
      "     44            1.0000        \u001b[32m0.1025\u001b[0m       \u001b[35m0.6354\u001b[0m            \u001b[31m0.6354\u001b[0m        1.2510  0.0006  0.1581\n",
      "     45            1.0000        \u001b[32m0.0845\u001b[0m       \u001b[35m0.6667\u001b[0m            \u001b[31m0.6667\u001b[0m        1.2444  0.0005  0.1549\n",
      "     46            1.0000        0.1053       \u001b[35m0.6771\u001b[0m            \u001b[31m0.6771\u001b[0m        1.2341  0.0005  0.1535\n",
      "     47            1.0000        0.0861       0.6771            0.6771        1.2213  0.0004  0.1564\n",
      "     48            1.0000        \u001b[32m0.0808\u001b[0m       0.6771            0.6771        1.2084  0.0004  0.1547\n",
      "     49            1.0000        \u001b[32m0.0587\u001b[0m       \u001b[35m0.6875\u001b[0m            \u001b[31m0.6875\u001b[0m        1.1959  0.0003  0.1497\n",
      "     50            1.0000        \u001b[32m0.0536\u001b[0m       \u001b[35m0.6979\u001b[0m            \u001b[31m0.6979\u001b[0m        1.1837  0.0003  0.1495\n",
      "Fine tuning model for subject 9 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.6889        1.0398       0.4792            0.4792        1.3186  0.0004  0.1555\n",
      "     32            0.7111        1.3104       0.4583            0.4583        1.3104  0.0005  0.1496\n",
      "     33            \u001b[36m0.8222\u001b[0m        1.1563       0.4479            0.4479        1.2880  0.0005  0.1600\n",
      "     34            \u001b[36m0.8889\u001b[0m        0.8075       0.4479            0.4479        1.2557  0.0006  0.1570\n",
      "     35            \u001b[36m0.9111\u001b[0m        \u001b[32m0.5407\u001b[0m       0.4896            0.4896        1.2363  0.0006  0.1548\n",
      "     36            0.9111        \u001b[32m0.4269\u001b[0m       0.4896            0.4896        1.2305  0.0007  0.1600\n",
      "     37            0.9111        \u001b[32m0.3650\u001b[0m       0.4896            0.4896        1.2363  0.0007  0.1663\n",
      "     38            \u001b[36m0.9333\u001b[0m        0.3721       0.4688            0.4688        1.2311  0.0007  0.1565\n",
      "     39            \u001b[36m0.9556\u001b[0m        \u001b[32m0.1957\u001b[0m       0.5000            0.5000        1.2205  0.0007  0.1547\n",
      "     40            \u001b[36m0.9778\u001b[0m        0.1988       0.5208            0.5208        1.2058  0.0007  0.1578\n",
      "     41            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1292\u001b[0m       0.5208            0.5208        1.1920  0.0007  0.1514\n",
      "     42            1.0000        0.1428       0.5208            0.5208        1.1750  0.0007  0.1556\n",
      "     43            1.0000        0.1322       0.5104            0.5104        1.1584  0.0006  0.1494\n",
      "     44            1.0000        \u001b[32m0.1128\u001b[0m       0.5417            0.5417        1.1401  0.0006  0.1471\n",
      "     45            1.0000        \u001b[32m0.0833\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.1218  0.0005  0.1554\n",
      "     46            1.0000        \u001b[32m0.0740\u001b[0m       \u001b[35m0.6042\u001b[0m            \u001b[31m0.6042\u001b[0m        1.1048  0.0005  0.1543\n",
      "     47            1.0000        0.0775       0.6042            0.6042        1.0881  0.0004  0.1617\n",
      "     48            1.0000        0.0876       0.5938            0.5938        1.0718  0.0004  0.1604\n",
      "     49            1.0000        0.0787       0.6042            0.6042        \u001b[94m1.0586\u001b[0m  0.0003  0.1628\n",
      "     50            1.0000        0.0881       0.6042            0.6042        \u001b[94m1.0478\u001b[0m  0.0003  0.1943\n",
      "Fine tuning model for subject 9 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.5556        1.5023       0.4688            0.4688        1.2864  0.0004  0.2088\n",
      "     32            0.6000        0.9746       0.5104            0.5104        1.2494  0.0005  0.2665\n",
      "     33            0.6889        0.8437       0.5104            0.5104        1.2249  0.0005  0.2269\n",
      "     34            \u001b[36m0.8667\u001b[0m        \u001b[32m0.6769\u001b[0m       0.5104            0.5104        1.2168  0.0006  0.2125\n",
      "     35            \u001b[36m0.9778\u001b[0m        \u001b[32m0.5179\u001b[0m       0.5208            0.5208        1.2116  0.0006  0.1918\n",
      "     36            0.9778        \u001b[32m0.4240\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.1953  0.0007  0.2118\n",
      "     37            0.9778        \u001b[32m0.2733\u001b[0m       0.5521            0.5521        1.1668  0.0007  0.2345\n",
      "     38            0.9778        \u001b[32m0.1542\u001b[0m       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.1340  0.0007  0.1873\n",
      "     39            0.9778        \u001b[32m0.1201\u001b[0m       \u001b[35m0.5938\u001b[0m            \u001b[31m0.5938\u001b[0m        1.1030  0.0007  0.2381\n",
      "     40            \u001b[36m1.0000\u001b[0m        0.1482       \u001b[35m0.6146\u001b[0m            \u001b[31m0.6146\u001b[0m        1.0806  0.0007  0.1720\n",
      "     41            1.0000        0.1529       \u001b[35m0.6250\u001b[0m            \u001b[31m0.6250\u001b[0m        \u001b[94m1.0668\u001b[0m  0.0007  0.2378\n",
      "     42            1.0000        0.1343       \u001b[35m0.6458\u001b[0m            \u001b[31m0.6458\u001b[0m        \u001b[94m1.0616\u001b[0m  0.0007  0.1519\n",
      "     43            1.0000        \u001b[32m0.1008\u001b[0m       0.6458            0.6458        \u001b[94m1.0614\u001b[0m  0.0006  0.1497\n",
      "     44            1.0000        \u001b[32m0.0884\u001b[0m       0.6250            0.6250        1.0671  0.0006  0.1589\n",
      "     45            1.0000        0.0934       0.6250            0.6250        1.0742  0.0005  0.1539\n",
      "     46            1.0000        0.1096       0.6354            0.6354        1.0814  0.0005  0.1603\n",
      "     47            1.0000        \u001b[32m0.0524\u001b[0m       0.6354            0.6354        1.0884  0.0004  0.1537\n",
      "     48            1.0000        \u001b[32m0.0475\u001b[0m       0.6250            0.6250        1.0947  0.0004  0.1648\n",
      "     49            1.0000        \u001b[32m0.0425\u001b[0m       0.6354            0.6354        1.1000  0.0003  0.1575\n",
      "     50            1.0000        0.0668       0.6354            0.6354        1.1046  0.0003  0.1508\n",
      "Fine tuning model for subject 9 with 45 = 45 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------\n",
      "     31            0.7556        0.7510       0.4792            0.4792        1.3138  0.0004  0.1544\n",
      "     32            0.7778        0.9521       0.4792            0.4792        1.2962  0.0005  0.1507\n",
      "     33            \u001b[36m0.8444\u001b[0m        \u001b[32m0.6614\u001b[0m       0.5000            0.5000        1.2743  0.0005  0.1567\n",
      "     34            \u001b[36m0.9778\u001b[0m        \u001b[32m0.5316\u001b[0m       0.4896            0.4896        1.2438  0.0006  0.1488\n",
      "     35            0.9778        \u001b[32m0.4215\u001b[0m       \u001b[35m0.5521\u001b[0m            \u001b[31m0.5521\u001b[0m        1.2148  0.0006  0.1542\n",
      "     36            \u001b[36m1.0000\u001b[0m        \u001b[32m0.3176\u001b[0m       0.5312            0.5312        1.1901  0.0007  0.1544\n",
      "     37            1.0000        \u001b[32m0.2368\u001b[0m       0.5417            0.5417        1.1738  0.0007  0.1981\n",
      "     38            1.0000        \u001b[32m0.1856\u001b[0m       0.5521            0.5521        1.1649  0.0007  0.1620\n",
      "     39            1.0000        \u001b[32m0.1320\u001b[0m       0.5312            0.5312        1.1628  0.0007  0.1544\n",
      "     40            1.0000        0.1461       0.5417            0.5417        1.1710  0.0007  0.1617\n",
      "     41            1.0000        \u001b[32m0.1007\u001b[0m       0.5417            0.5417        1.1772  0.0007  0.1642\n",
      "     42            1.0000        \u001b[32m0.0948\u001b[0m       0.5312            0.5312        1.1814  0.0007  0.1582\n",
      "     43            1.0000        \u001b[32m0.0625\u001b[0m       0.5417            0.5417        1.1839  0.0006  0.1591\n",
      "     44            1.0000        \u001b[32m0.0624\u001b[0m       0.5417            0.5417        1.1871  0.0006  0.1575\n",
      "     45            1.0000        \u001b[32m0.0458\u001b[0m       0.5417            0.5417        1.1896  0.0005  0.1581\n",
      "     46            1.0000        0.0730       0.5521            0.5521        1.1922  0.0005  0.1522\n",
      "     47            1.0000        0.0673       0.5521            0.5521        1.1923  0.0004  0.1593\n",
      "     48            1.0000        0.0645       0.5521            0.5521        1.1917  0.0004  0.1584\n",
      "     49            1.0000        0.0583       \u001b[35m0.5729\u001b[0m            \u001b[31m0.5729\u001b[0m        1.1861  0.0003  0.1619\n",
      "     50            1.0000        \u001b[32m0.0283\u001b[0m       0.5729            0.5729        1.1815  0.0003  0.1571\n"
     ]
    }
   ],
   "source": [
    "results_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'results'))\n",
    "exp_name = f'baseline_2_3_pretrain'\n",
    "\n",
    "### ---------- Pre-training parameters ----------\n",
    "lr = 0.07 * 0.01\n",
    "weight_decay = 0\n",
    "batch_size = 64\n",
    "n_epochs = 30\n",
    "\n",
    "splitted_by_subj = windows_dataset.split('subject')\n",
    "\n",
    "data_amount_step = 5 # trials\n",
    "finetune_trials_num = 45 # trials\n",
    "repetition = 10\n",
    "results_columns = ['valid_accuracy',]\n",
    "dict_results = {}\n",
    "\n",
    "for holdout_subj_id in range(1, 10):\n",
    "    \n",
    "    print(f'Hold out data from subject {holdout_subj_id}')\n",
    "    \n",
    "    ### ---------- Split dataset into pre-train set and fine-tune (holdout) set ----------\n",
    "    pre_train_set = BaseConcatDataset([splitted_by_subj.get(f'{i}') for i in range(1, 10) if i != holdout_subj_id])\n",
    "    fine_tune_set = BaseConcatDataset([splitted_by_subj.get(f'{holdout_subj_id}'),])\n",
    "\n",
    "    ### ---------- Split pre-train set into pre-train-train set and pre-train-test set ----------\n",
    "    pre_train_train_set_lst = []\n",
    "    pre_train_test_set_lst = []\n",
    "    pre_train_test_set_size = 1 # runs\n",
    "    for key, val in pre_train_set.split('subject').items():\n",
    "        subj_splitted_lst_by_run = list(val.split('run').values())\n",
    "        pre_train_train_set_lst.extend(subj_splitted_lst_by_run[:-pre_train_test_set_size])\n",
    "        pre_train_test_set_lst.extend(subj_splitted_lst_by_run[-pre_train_test_set_size:])\n",
    "    \n",
    "    pre_train_train_set = BaseConcatDataset(pre_train_train_set_lst)\n",
    "    pre_train_test_set = BaseConcatDataset(pre_train_test_set_lst)\n",
    "\n",
    "    ### ---------- Pre-training ----------\n",
    "    cuda = torch.cuda.is_available() \n",
    "    device = 'cuda' if cuda else 'cpu'\n",
    "    if cuda:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    seed = 20200220\n",
    "    set_random_seeds(seed=seed, cuda=cuda)\n",
    "    \n",
    "    n_classes = 4\n",
    "    classes = list(range(n_classes))\n",
    "    # Extract number of chans and time steps from dataset\n",
    "    n_chans = windows_dataset[0][0].shape[0]\n",
    "    input_window_samples = windows_dataset[0][0].shape[1]\n",
    "    \n",
    "    cur_model = ShallowFBCSPNet(\n",
    "        n_chans,\n",
    "        n_classes,\n",
    "        input_window_samples=input_window_samples,\n",
    "        final_conv_length='auto',\n",
    "    )\n",
    "    \n",
    "    cur_clf = EEGClassifier(\n",
    "        cur_model,\n",
    "        criterion=torch.nn.NLLLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        train_split=predefined_split(pre_train_test_set),  # using valid_set for validation\n",
    "        optimizer__lr=lr,\n",
    "        optimizer__weight_decay=weight_decay,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[\n",
    "            \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
    "        ],\n",
    "        device=device,\n",
    "        classes=classes,\n",
    "        warm_start=False\n",
    "    )\n",
    "\n",
    "    print(f'Pre-training model with data from all subjects but subject {holdout_subj_id}')\n",
    "    _ = cur_clf.fit(pre_train_train_set, y=None, epochs=n_epochs)\n",
    "\n",
    "    cur_clf.save_params(f_params=os.path.join(results_dir, f'{exp_name}_without_subj_{holdout_subj_id}_model.pkl'), \n",
    "                        f_optimizer=os.path.join(results_dir, f'{exp_name}_without_subj_{holdout_subj_id}_opt.pkl'), \n",
    "                        f_history=os.path.join(results_dir, f'{exp_name}_without_subj_{holdout_subj_id}_history.json'))\n",
    "\n",
    "    ### ---------- Split fine tune set into fine tune-train set and fine tune-valid set ----------\n",
    "    finetune_splitted_lst_by_run = list(fine_tune_set.split('run').values())\n",
    "    finetune_subj_train_set = BaseConcatDataset(finetune_splitted_lst_by_run[:-1])\n",
    "    finetune_subj_valid_set = BaseConcatDataset(finetune_splitted_lst_by_run[-1:])\n",
    "    \n",
    "    ### Baseline accuracy on the finetune_valid set\n",
    "    finetune_valid_predicted = cur_clf.predict(finetune_subj_valid_set)\n",
    "    finetune_valid_true = np.array(finetune_subj_valid_set.get_metadata().target)\n",
    "    finetune_baseline_correct = np.equal(finetune_valid_predicted, finetune_valid_true)\n",
    "    finetune_baseline_acc = np.sum(finetune_baseline_correct) / len(finetune_baseline_correct)\n",
    "    print(f'Before finetuning for subject {holdout_subj_id}, the baseline accuracy is {finetune_baseline_acc}')\n",
    "\n",
    "    ### ---------- Fine tuning ----------\n",
    "    dict_subj_results = {0: [finetune_baseline_acc,]}\n",
    "\n",
    "    ### Finetune with different amount of new data\n",
    "    for finetune_training_data_amount in np.arange(1, (finetune_trials_num // data_amount_step) + 1) * data_amount_step:\n",
    "\n",
    "        final_accuracy = []\n",
    "        \n",
    "        ### Since we're sampling randomly, repeat for 'repetition' times\n",
    "        for i in range(repetition):\n",
    "\n",
    "            ## Get current finetune samples\n",
    "            cur_finetune_subj_train_subset = get_subset(finetune_subj_train_set, int(finetune_training_data_amount), random_sample=True)\n",
    "    \n",
    "            finetune_model = ShallowFBCSPNet(\n",
    "                n_chans,\n",
    "                n_classes,\n",
    "                input_window_samples=input_window_samples,\n",
    "                final_conv_length='auto',\n",
    "            )\n",
    "    \n",
    "            ### ---------- Fine tune parameters ----------\n",
    "            finetune_lr = 0.07 * 0.01\n",
    "            finetune_weight_decay = 0\n",
    "            finetune_batch_size = int(min(finetune_training_data_amount, 64))\n",
    "            finetune_n_epochs = 20\n",
    "            \n",
    "            new_clf = EEGClassifier(\n",
    "                finetune_model,\n",
    "                criterion=torch.nn.NLLLoss,\n",
    "                optimizer=torch.optim.AdamW,\n",
    "                train_split=predefined_split(finetune_subj_valid_set), \n",
    "                optimizer__lr=finetune_lr,\n",
    "                optimizer__weight_decay=finetune_weight_decay,\n",
    "                batch_size=finetune_batch_size,\n",
    "                callbacks=[\n",
    "                    \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=finetune_n_epochs - 1)),\n",
    "                ],\n",
    "                device=device,\n",
    "                classes=classes,\n",
    "            )\n",
    "            new_clf.initialize()\n",
    "            \n",
    "            ## Load pretrained model\n",
    "            new_clf.load_params(f_params=os.path.join(results_dir, f'{exp_name}_without_subj_{holdout_subj_id}_model.pkl'), \n",
    "                                f_optimizer=os.path.join(results_dir, f'{exp_name}_without_subj_{holdout_subj_id}_opt.pkl'), \n",
    "                                f_history=os.path.join(results_dir, f'{exp_name}_without_subj_{holdout_subj_id}_history.json'))\n",
    "    \n",
    "            ## Continue training / finetuning\n",
    "            print(f'Fine tuning model for subject {holdout_subj_id} with {finetune_training_data_amount} = {len(cur_finetune_subj_train_subset)} trials')\n",
    "            _ = new_clf.partial_fit(cur_finetune_subj_train_subset, y=None, epochs=finetune_n_epochs)\n",
    "    \n",
    "            ## Get results after fine tuning\n",
    "            df = pd.DataFrame(new_clf.history[:, results_columns], columns=results_columns,\n",
    "                              # index=new_clf.history[:, 'epoch'],\n",
    "                             )\n",
    "    \n",
    "            cur_final_acc = np.mean(df.tail(5).valid_accuracy)\n",
    "            final_accuracy.append(cur_final_acc)\n",
    "        \n",
    "        dict_subj_results.update({finetune_training_data_amount: final_accuracy})\n",
    "\n",
    "    dict_results.update({holdout_subj_id: dict_subj_results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'ShallowFBCSPNet_BNCI2014_001_finetuning_3'\n",
    "file_path = os.path.join(results_dir, f'{file_name}.pkl')\n",
    "\n",
    "with open(f'{results_dir}\\\\{file_name}.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
    "#     with open(file_path, 'rb') as f:\n",
    "#         baseline_2_3 = pickle.load(f)\n",
    "#     print(\"Dictionary loaded successfully.\")\n",
    "# else:\n",
    "#     print(f\"Error: File '{file_path}' does not exist or is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.6041666666666666]</td>\n",
       "      <td>[0.3541666666666667]</td>\n",
       "      <td>[0.6458333333333334]</td>\n",
       "      <td>[0.5104166666666666]</td>\n",
       "      <td>[0.3125]</td>\n",
       "      <td>[0.34375]</td>\n",
       "      <td>[0.28125]</td>\n",
       "      <td>[0.6041666666666666]</td>\n",
       "      <td>[0.46875]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.4395833333333333, 0.425, 0.5375, 0.51041666...</td>\n",
       "      <td>[0.29375, 0.24375, 0.27291666666666664, 0.2270...</td>\n",
       "      <td>[0.38750000000000007, 0.33125000000000004, 0.4...</td>\n",
       "      <td>[0.33541666666666664, 0.28958333333333336, 0.3...</td>\n",
       "      <td>[0.24583333333333335, 0.23958333333333334, 0.2...</td>\n",
       "      <td>[0.33125, 0.35625, 0.2604166666666667, 0.37083...</td>\n",
       "      <td>[0.3229166666666667, 0.37916666666666665, 0.29...</td>\n",
       "      <td>[0.4208333333333333, 0.41875, 0.36249999999999...</td>\n",
       "      <td>[0.37916666666666665, 0.5083333333333333, 0.34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.3583333333333333, 0.3520833333333333, 0.504...</td>\n",
       "      <td>[0.18541666666666667, 0.26458333333333334, 0.2...</td>\n",
       "      <td>[0.5041666666666667, 0.4416666666666666, 0.397...</td>\n",
       "      <td>[0.38333333333333336, 0.4458333333333333, 0.31...</td>\n",
       "      <td>[0.2604166666666667, 0.2604166666666667, 0.285...</td>\n",
       "      <td>[0.3416666666666667, 0.3229166666666667, 0.268...</td>\n",
       "      <td>[0.3145833333333333, 0.31666666666666665, 0.33...</td>\n",
       "      <td>[0.4833333333333333, 0.3895833333333333, 0.439...</td>\n",
       "      <td>[0.5520833333333334, 0.4979166666666666, 0.508...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.5395833333333334, 0.5562500000000001, 0.531...</td>\n",
       "      <td>[0.25833333333333336, 0.2375, 0.29166666666666...</td>\n",
       "      <td>[0.3458333333333333, 0.5208333333333334, 0.560...</td>\n",
       "      <td>[0.39375, 0.32916666666666666, 0.39375, 0.3375...</td>\n",
       "      <td>[0.3020833333333333, 0.29791666666666666, 0.26...</td>\n",
       "      <td>[0.3354166666666666, 0.36041666666666666, 0.40...</td>\n",
       "      <td>[0.3458333333333333, 0.28958333333333336, 0.28...</td>\n",
       "      <td>[0.4458333333333333, 0.4270833333333333, 0.395...</td>\n",
       "      <td>[0.4916666666666666, 0.4875, 0.45625, 0.541666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.5479166666666667, 0.5270833333333333, 0.545...</td>\n",
       "      <td>[0.2958333333333333, 0.2541666666666667, 0.208...</td>\n",
       "      <td>[0.5083333333333333, 0.5458333333333333, 0.527...</td>\n",
       "      <td>[0.40625, 0.4604166666666666, 0.39999999999999...</td>\n",
       "      <td>[0.24583333333333335, 0.2729166666666667, 0.33...</td>\n",
       "      <td>[0.3333333333333333, 0.4333333333333333, 0.35,...</td>\n",
       "      <td>[0.2791666666666667, 0.3791666666666667, 0.352...</td>\n",
       "      <td>[0.5208333333333333, 0.5020833333333333, 0.547...</td>\n",
       "      <td>[0.5625, 0.53125, 0.4291666666666667, 0.579166...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.5645833333333333, 0.4875, 0.622916666666666...</td>\n",
       "      <td>[0.30416666666666664, 0.2708333333333333, 0.31...</td>\n",
       "      <td>[0.58125, 0.58125, 0.49375, 0.5354166666666667...</td>\n",
       "      <td>[0.3979166666666666, 0.41875, 0.39791666666666...</td>\n",
       "      <td>[0.31250000000000006, 0.2625, 0.29375, 0.32916...</td>\n",
       "      <td>[0.3416666666666667, 0.28125, 0.37083333333333...</td>\n",
       "      <td>[0.2791666666666667, 0.375, 0.3604166666666667...</td>\n",
       "      <td>[0.4708333333333333, 0.3958333333333333, 0.493...</td>\n",
       "      <td>[0.5791666666666666, 0.5291666666666667, 0.597...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.5125, 0.4875, 0.5895833333333333, 0.49375, ...</td>\n",
       "      <td>[0.25833333333333336, 0.18333333333333335, 0.2...</td>\n",
       "      <td>[0.4770833333333333, 0.48125, 0.54791666666666...</td>\n",
       "      <td>[0.3375, 0.40625, 0.3333333333333333, 0.389583...</td>\n",
       "      <td>[0.24791666666666665, 0.3395833333333333, 0.32...</td>\n",
       "      <td>[0.35624999999999996, 0.3625, 0.36041666666666...</td>\n",
       "      <td>[0.37083333333333335, 0.2729166666666667, 0.28...</td>\n",
       "      <td>[0.51875, 0.5166666666666667, 0.56458333333333...</td>\n",
       "      <td>[0.55625, 0.50625, 0.5020833333333333, 0.57708...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.4833333333333334, 0.6229166666666667, 0.658...</td>\n",
       "      <td>[0.22083333333333335, 0.32083333333333336, 0.3...</td>\n",
       "      <td>[0.4354166666666666, 0.5791666666666666, 0.456...</td>\n",
       "      <td>[0.3770833333333333, 0.4395833333333333, 0.443...</td>\n",
       "      <td>[0.28750000000000003, 0.3208333333333333, 0.28...</td>\n",
       "      <td>[0.3395833333333333, 0.36875, 0.34791666666666...</td>\n",
       "      <td>[0.2916666666666667, 0.4625, 0.320833333333333...</td>\n",
       "      <td>[0.4333333333333333, 0.475, 0.5062499999999999...</td>\n",
       "      <td>[0.5541666666666666, 0.6166666666666667, 0.618...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[0.4979166666666667, 0.5979166666666667, 0.535...</td>\n",
       "      <td>[0.30416666666666664, 0.225, 0.274999999999999...</td>\n",
       "      <td>[0.5625, 0.4979166666666667, 0.629166666666666...</td>\n",
       "      <td>[0.4416666666666667, 0.4520833333333333, 0.387...</td>\n",
       "      <td>[0.24791666666666665, 0.3083333333333333, 0.18...</td>\n",
       "      <td>[0.3541666666666667, 0.3104166666666667, 0.331...</td>\n",
       "      <td>[0.3333333333333333, 0.35625, 0.32291666666666...</td>\n",
       "      <td>[0.48125, 0.5541666666666667, 0.49583333333333...</td>\n",
       "      <td>[0.5291666666666667, 0.5125, 0.579166666666666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[0.4979166666666666, 0.5729166666666667, 0.654...</td>\n",
       "      <td>[0.2833333333333333, 0.25416666666666665, 0.22...</td>\n",
       "      <td>[0.5666666666666667, 0.5395833333333332, 0.627...</td>\n",
       "      <td>[0.425, 0.4833333333333333, 0.4541666666666667...</td>\n",
       "      <td>[0.33333333333333337, 0.325, 0.297916666666666...</td>\n",
       "      <td>[0.32916666666666666, 0.27291666666666664, 0.4...</td>\n",
       "      <td>[0.41875, 0.3625, 0.4, 0.35208333333333336, 0....</td>\n",
       "      <td>[0.4125, 0.51875, 0.5729166666666667, 0.366666...</td>\n",
       "      <td>[0.58125, 0.5583333333333333, 0.57083333333333...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    1  \\\n",
       "0                                [0.6041666666666666]   \n",
       "5   [0.4395833333333333, 0.425, 0.5375, 0.51041666...   \n",
       "10  [0.3583333333333333, 0.3520833333333333, 0.504...   \n",
       "15  [0.5395833333333334, 0.5562500000000001, 0.531...   \n",
       "20  [0.5479166666666667, 0.5270833333333333, 0.545...   \n",
       "25  [0.5645833333333333, 0.4875, 0.622916666666666...   \n",
       "30  [0.5125, 0.4875, 0.5895833333333333, 0.49375, ...   \n",
       "35  [0.4833333333333334, 0.6229166666666667, 0.658...   \n",
       "40  [0.4979166666666667, 0.5979166666666667, 0.535...   \n",
       "45  [0.4979166666666666, 0.5729166666666667, 0.654...   \n",
       "\n",
       "                                                    2  \\\n",
       "0                                [0.3541666666666667]   \n",
       "5   [0.29375, 0.24375, 0.27291666666666664, 0.2270...   \n",
       "10  [0.18541666666666667, 0.26458333333333334, 0.2...   \n",
       "15  [0.25833333333333336, 0.2375, 0.29166666666666...   \n",
       "20  [0.2958333333333333, 0.2541666666666667, 0.208...   \n",
       "25  [0.30416666666666664, 0.2708333333333333, 0.31...   \n",
       "30  [0.25833333333333336, 0.18333333333333335, 0.2...   \n",
       "35  [0.22083333333333335, 0.32083333333333336, 0.3...   \n",
       "40  [0.30416666666666664, 0.225, 0.274999999999999...   \n",
       "45  [0.2833333333333333, 0.25416666666666665, 0.22...   \n",
       "\n",
       "                                                    3  \\\n",
       "0                                [0.6458333333333334]   \n",
       "5   [0.38750000000000007, 0.33125000000000004, 0.4...   \n",
       "10  [0.5041666666666667, 0.4416666666666666, 0.397...   \n",
       "15  [0.3458333333333333, 0.5208333333333334, 0.560...   \n",
       "20  [0.5083333333333333, 0.5458333333333333, 0.527...   \n",
       "25  [0.58125, 0.58125, 0.49375, 0.5354166666666667...   \n",
       "30  [0.4770833333333333, 0.48125, 0.54791666666666...   \n",
       "35  [0.4354166666666666, 0.5791666666666666, 0.456...   \n",
       "40  [0.5625, 0.4979166666666667, 0.629166666666666...   \n",
       "45  [0.5666666666666667, 0.5395833333333332, 0.627...   \n",
       "\n",
       "                                                    4  \\\n",
       "0                                [0.5104166666666666]   \n",
       "5   [0.33541666666666664, 0.28958333333333336, 0.3...   \n",
       "10  [0.38333333333333336, 0.4458333333333333, 0.31...   \n",
       "15  [0.39375, 0.32916666666666666, 0.39375, 0.3375...   \n",
       "20  [0.40625, 0.4604166666666666, 0.39999999999999...   \n",
       "25  [0.3979166666666666, 0.41875, 0.39791666666666...   \n",
       "30  [0.3375, 0.40625, 0.3333333333333333, 0.389583...   \n",
       "35  [0.3770833333333333, 0.4395833333333333, 0.443...   \n",
       "40  [0.4416666666666667, 0.4520833333333333, 0.387...   \n",
       "45  [0.425, 0.4833333333333333, 0.4541666666666667...   \n",
       "\n",
       "                                                    5  \\\n",
       "0                                            [0.3125]   \n",
       "5   [0.24583333333333335, 0.23958333333333334, 0.2...   \n",
       "10  [0.2604166666666667, 0.2604166666666667, 0.285...   \n",
       "15  [0.3020833333333333, 0.29791666666666666, 0.26...   \n",
       "20  [0.24583333333333335, 0.2729166666666667, 0.33...   \n",
       "25  [0.31250000000000006, 0.2625, 0.29375, 0.32916...   \n",
       "30  [0.24791666666666665, 0.3395833333333333, 0.32...   \n",
       "35  [0.28750000000000003, 0.3208333333333333, 0.28...   \n",
       "40  [0.24791666666666665, 0.3083333333333333, 0.18...   \n",
       "45  [0.33333333333333337, 0.325, 0.297916666666666...   \n",
       "\n",
       "                                                    6  \\\n",
       "0                                           [0.34375]   \n",
       "5   [0.33125, 0.35625, 0.2604166666666667, 0.37083...   \n",
       "10  [0.3416666666666667, 0.3229166666666667, 0.268...   \n",
       "15  [0.3354166666666666, 0.36041666666666666, 0.40...   \n",
       "20  [0.3333333333333333, 0.4333333333333333, 0.35,...   \n",
       "25  [0.3416666666666667, 0.28125, 0.37083333333333...   \n",
       "30  [0.35624999999999996, 0.3625, 0.36041666666666...   \n",
       "35  [0.3395833333333333, 0.36875, 0.34791666666666...   \n",
       "40  [0.3541666666666667, 0.3104166666666667, 0.331...   \n",
       "45  [0.32916666666666666, 0.27291666666666664, 0.4...   \n",
       "\n",
       "                                                    7  \\\n",
       "0                                           [0.28125]   \n",
       "5   [0.3229166666666667, 0.37916666666666665, 0.29...   \n",
       "10  [0.3145833333333333, 0.31666666666666665, 0.33...   \n",
       "15  [0.3458333333333333, 0.28958333333333336, 0.28...   \n",
       "20  [0.2791666666666667, 0.3791666666666667, 0.352...   \n",
       "25  [0.2791666666666667, 0.375, 0.3604166666666667...   \n",
       "30  [0.37083333333333335, 0.2729166666666667, 0.28...   \n",
       "35  [0.2916666666666667, 0.4625, 0.320833333333333...   \n",
       "40  [0.3333333333333333, 0.35625, 0.32291666666666...   \n",
       "45  [0.41875, 0.3625, 0.4, 0.35208333333333336, 0....   \n",
       "\n",
       "                                                    8  \\\n",
       "0                                [0.6041666666666666]   \n",
       "5   [0.4208333333333333, 0.41875, 0.36249999999999...   \n",
       "10  [0.4833333333333333, 0.3895833333333333, 0.439...   \n",
       "15  [0.4458333333333333, 0.4270833333333333, 0.395...   \n",
       "20  [0.5208333333333333, 0.5020833333333333, 0.547...   \n",
       "25  [0.4708333333333333, 0.3958333333333333, 0.493...   \n",
       "30  [0.51875, 0.5166666666666667, 0.56458333333333...   \n",
       "35  [0.4333333333333333, 0.475, 0.5062499999999999...   \n",
       "40  [0.48125, 0.5541666666666667, 0.49583333333333...   \n",
       "45  [0.4125, 0.51875, 0.5729166666666667, 0.366666...   \n",
       "\n",
       "                                                    9  \n",
       "0                                           [0.46875]  \n",
       "5   [0.37916666666666665, 0.5083333333333333, 0.34...  \n",
       "10  [0.5520833333333334, 0.4979166666666666, 0.508...  \n",
       "15  [0.4916666666666666, 0.4875, 0.45625, 0.541666...  \n",
       "20  [0.5625, 0.53125, 0.4291666666666667, 0.579166...  \n",
       "25  [0.5791666666666666, 0.5291666666666667, 0.597...  \n",
       "30  [0.55625, 0.50625, 0.5020833333333333, 0.57708...  \n",
       "35  [0.5541666666666666, 0.6166666666666667, 0.618...  \n",
       "40  [0.5291666666666667, 0.5125, 0.579166666666666...  \n",
       "45  [0.58125, 0.5583333333333333, 0.57083333333333...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(dict_results)\n",
    "display(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'ShallowFBCSPNet on BNCI2014_001 Dataset \\n Fine-tuning (using each subject as holdout), 10 reps each point')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for col in df_results.columns:\n",
    "    y_values = [np.mean(lst) for lst in df_results[col]]\n",
    "    y_errors = [np.std(lst) for lst in df_results[col]]\n",
    "    ax1.errorbar(df_results.index * 4 / 60, y_values, yerr=y_errors, label=f'Subject {col}')\n",
    "    ax2.plot(df_results.index * 4 / 60, y_values, label=f'Subject {col}')\n",
    "\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('Finetune data amount (min)')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "\n",
    "ax2.legend()\n",
    "ax2.set_xlabel('Finetune data amount (min)')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "\n",
    "df_results_rep_avg = df_results.applymap(lambda x: np.mean(x))\n",
    "subject_averaged_df = df_results_rep_avg.mean(axis=1)\n",
    "std_err_df = df_results_rep_avg.sem(axis=1)\n",
    "conf_interval_df = stats.t.interval(0.95, len(df_results_rep_avg.columns) - 1, loc=subject_averaged_df, scale=std_err_df)\n",
    "\n",
    "ax3.plot(subject_averaged_df.index * 4 / 60, subject_averaged_df, label='Subject averaged')\n",
    "ax3.fill_between(subject_averaged_df.index * 4 / 60, conf_interval_df[0], conf_interval_df[1], color='b', alpha=0.3, label='95% CI')\n",
    "\n",
    "ax3.legend()\n",
    "ax3.set_xlabel('Finetune data amount (min)')\n",
    "ax3.set_ylabel('Accuracy')\n",
    "\n",
    "plt.suptitle('ShallowFBCSPNet on BNCI2014_001 Dataset \\n Fine-tuning (using each subject as holdout), 10 reps each point')\n",
    "\n",
    "plt.savefig(os.path.join(results_dir, f'{file_name}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
