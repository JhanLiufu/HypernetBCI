{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Cropped Decoding on BCIC IV 2a Dataset\n",
    "   :depth: 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building on the [Trialwise decoding](./plot_bcic_iv_2a_moabb_trial.html)_,\n",
    "we now do more data-efficient cropped decoding!\n",
    "\n",
    "In Braindecode, there are two supported configurations created for\n",
    "training models: trialwise decoding and cropped decoding. We will\n",
    "explain this visually by comparing trialwise to cropped decoding.\n",
    "\n",
    "<img src=\"https://braindecode.org/stable/_static/trialwise_explanation.png\" alt=\"Trialwise decoding\">\n",
    "<img src=\"https://braindecode.org/stable/_static/cropped_explanation.png\" alt=\"Cropped decoding\">\n",
    "On the left, you see trialwise decoding:\n",
    "\n",
    "1. A complete trial is pushed through the network.\n",
    "2. The network produces a prediction.\n",
    "3. The prediction is compared to the target (label) for that trial to\n",
    "   compute the loss.\n",
    "\n",
    "On the right, you see cropped decoding:\n",
    "\n",
    "1. Instead of a complete trial, crops are pushed through the network.\n",
    "2. For computational efficiency, multiple neighbouring crops are pushed\n",
    "   through the network simultaneously (these neighbouring crops are\n",
    "   called compute windows)\n",
    "3. Therefore, the network produces multiple predictions (one per crop in\n",
    "   the window)\n",
    "4. The individual crop predictions are *AVERAGED* before computing the\n",
    "   loss function\n",
    "\n",
    "This averaging of predictions of small sub-windows is the key difference\n",
    "between trialwise and cropped decoding. It was introduced in [1]_ and it impact\n",
    "on the parameters of the network.\n",
    "\n",
    "It is important to note that the averaging of predictions is only done\n",
    "during training. During testing, the network is still applied to crops\n",
    "and the predictions are averaged afterwards.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>-  The network architecture implicitly defines the crop size (it is the\n",
    "       receptive field size, i.e., the number of timesteps the network uses\n",
    "       to make a single prediction)\n",
    "    -  The window size is a user-defined hyperparameter, called\n",
    "       ``input_window_samples`` in Braindecode. It mostly affects runtime\n",
    "       (larger window sizes should be faster). As a rule of thumb, you can\n",
    "       set it to two times the crop size.\n",
    "    -  Crop size and window size together define how many predictions the\n",
    "       network makes per window: ``#window − #crop + 1 = #predictions``</p></div>\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>For cropped decoding, the above training setup is mathematically\n",
    "    similar to sampling crops in your dataset, pushing them through the\n",
    "    network and training directly on the individual crops. However, the\n",
    "    if their position would be randomly selected, the crops would be less\n",
    "    correlated in contrast to the neighbourhood crops selected from a window.\n",
    "    At the same time, the above training setup is much faster as it avoids\n",
    "    redundant computations by using dilated convolutions, see [2]_.\n",
    "    However, the two setups are only mathematically related in case (1)\n",
    "    your network does not use any padding or only left padding and\n",
    "    (2) your loss function leads\n",
    "    to the same gradients when using the averaged output. The first is true\n",
    "    for our shallow and deep ConvNet models and the second is true for the\n",
    "    log-softmax outputs and negative log likelihood loss that is typically\n",
    "    used for classification in PyTorch.</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing the dataset\n",
    "\n",
    "Loading and preprocessing stays the same as in the [Trialwise decoding\n",
    "tutorial](./plot_bcic_iv_2a_moabb_trial.html)_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.pick_types is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.pick_channels_regexp is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.channel_type is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\moabb\\pipelines\\__init__.py:26: ModuleNotFoundError: Tensorflow is not installed. You won't be able to use these MOABB pipelines if you attempt to do so.\n",
      "  warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\preprocessing\\preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
      "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<braindecode.datasets.moabb.MOABBDataset at 0x2ec7b5d6390>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from braindecode.datasets import MOABBDataset\n",
    "\n",
    "subject_id = 3\n",
    "dataset = MOABBDataset(dataset_name=\"BNCI2014_001\", subject_ids=[subject_id])\n",
    "\n",
    "from numpy import multiply\n",
    "\n",
    "from braindecode.preprocessing import (\n",
    "    Preprocessor,\n",
    "    exponential_moving_standardize,\n",
    "    preprocess,\n",
    ")\n",
    "\n",
    "low_cut_hz = 4.  # low cut frequency for filtering\n",
    "high_cut_hz = 38.  # high cut frequency for filtering\n",
    "# Parameters for exponential moving standardization\n",
    "factor_new = 1e-3\n",
    "init_block_size = 1000\n",
    "# Factor to convert from V to uV\n",
    "factor = 1e6\n",
    "\n",
    "preprocessors = [\n",
    "    Preprocessor('pick_types', eeg=True, meg=False, stim=False),\n",
    "    # Keep EEG sensors\n",
    "    Preprocessor(lambda data: multiply(data, factor)),  # Convert from V to uV\n",
    "    Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz),\n",
    "    # Bandpass filter\n",
    "    Preprocessor(exponential_moving_standardize,\n",
    "                 # Exponential moving standardization\n",
    "                 factor_new=factor_new, init_block_size=init_block_size)\n",
    "]\n",
    "\n",
    "# Transform the data\n",
    "preprocess(dataset, preprocessors, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model and compute windowing parameters\n",
    "In contrast to trialwise decoding, we first have to create the model\n",
    "before we can cut the dataset into windows. This is because we need to\n",
    "know the neural network parameters to know how large the sub-window\n",
    "stride should be.\n",
    "\n",
    "We first choose the compute/input window size that will be fed to the\n",
    "network during training. This has to be larger than the networks\n",
    "the number of timesteps size and can otherwise be chosen for computational\n",
    "efficiency (see explanations in the beginning of this tutorial). Here we\n",
    "choose 1000 samples, which are 4 seconds for the 250 Hz sampling rate.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "input_window_samples = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the model. To enable it to be used in cropped decoding\n",
    "efficiently, we manually set the length of the final convolution layer\n",
    "to some length that makes the number of timesteps of the ConvNet smaller\n",
    "than ``input_window_samples`` (see ``final_conv_length=30`` in the model\n",
    "definition).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================================================\n",
      "Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #                   Kernel Shape\n",
      "============================================================================================================================================\n",
      "ShallowFBCSPNet (ShallowFBCSPNet)        [1, 22, 1000]             [1, 4, 32]                --                        --\n",
      "├─Ensure4d (ensuredims): 1-1             [1, 22, 1000]             [1, 22, 1000, 1]          --                        --\n",
      "├─Rearrange (dimshuffle): 1-2            [1, 22, 1000, 1]          [1, 1, 1000, 22]          --                        --\n",
      "├─CombinedConv (conv_time_spat): 1-3     [1, 1, 1000, 22]          [1, 40, 976, 1]           36,240                    --\n",
      "├─BatchNorm2d (bnorm): 1-4               [1, 40, 976, 1]           [1, 40, 976, 1]           80                        --\n",
      "├─Expression (conv_nonlin_exp): 1-5      [1, 40, 976, 1]           [1, 40, 976, 1]           --                        --\n",
      "├─AvgPool2d (pool): 1-6                  [1, 40, 976, 1]           [1, 40, 61, 1]            --                        [75, 1]\n",
      "├─Expression (pool_nonlin_exp): 1-7      [1, 40, 61, 1]            [1, 40, 61, 1]            --                        --\n",
      "├─Dropout (drop): 1-8                    [1, 40, 61, 1]            [1, 40, 61, 1]            --                        --\n",
      "├─Sequential (final_layer): 1-9          [1, 40, 61, 1]            [1, 4, 32]                --                        --\n",
      "│    └─Conv2d (conv_classifier): 2-1     [1, 40, 61, 1]            [1, 4, 32, 1]             4,804                     [30, 1]\n",
      "│    └─LogSoftmax (logsoftmax): 2-2      [1, 4, 32, 1]             [1, 4, 32, 1]             --                        --\n",
      "│    └─Expression (squeeze): 2-3         [1, 4, 32, 1]             [1, 4, 32]                --                        --\n",
      "============================================================================================================================================\n",
      "Total params: 41,124\n",
      "Trainable params: 41,124\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0.15\n",
      "============================================================================================================================================\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 0.31\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.42\n",
      "============================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mengz\\anaconda3\\envs\\hyperBCI\\Lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from braindecode.models import ShallowFBCSPNet\n",
    "from braindecode.util import set_random_seeds\n",
    "\n",
    "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "if cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "# Set random seed to be able to roughly reproduce results\n",
    "# Note that with cudnn benchmark set to True, GPU indeterminism\n",
    "# may still make results substantially different between runs.\n",
    "# To obtain more consistent results at the cost of increased computation time,\n",
    "# you can set `cudnn_benchmark=False` in `set_random_seeds`\n",
    "# or remove `torch.backends.cudnn.benchmark = True`\n",
    "seed = 20200220\n",
    "set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "n_classes = 4\n",
    "classes = list(range(n_classes))\n",
    "# Extract number of chans from dataset\n",
    "n_chans = dataset[0][0].shape[0]\n",
    "\n",
    "model = ShallowFBCSPNet(\n",
    "    n_chans,\n",
    "    n_classes,\n",
    "    input_window_samples=input_window_samples,\n",
    "    final_conv_length=30,\n",
    ")\n",
    "\n",
    "# Display torchinfo table describing the model\n",
    "print(model)\n",
    "\n",
    "# Send model to GPU\n",
    "if cuda:\n",
    "    _ = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we transform model with strides to a model that outputs dense\n",
    "prediction, so we can use it to obtain predictions for all\n",
    "crops.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model.to_dense_prediction_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know the models’ output shape without the last layer, we calculate the\n",
    "shape of model output for a dummy input.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_preds_per_input = model.get_output_shape()[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut the data into windows\n",
    "In contrast to trialwise decoding, we have to supply an explicit\n",
    "window size and window stride to the ``create_windows_from_events``\n",
    "function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n"
     ]
    }
   ],
   "source": [
    "from braindecode.preprocessing import create_windows_from_events\n",
    "\n",
    "trial_start_offset_seconds = -0.5\n",
    "# Extract sampling frequency, check that they are same in all datasets\n",
    "sfreq = dataset.datasets[0].raw.info['sfreq']\n",
    "assert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])\n",
    "\n",
    "# Calculate the trial start offset in samples.\n",
    "trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
    "\n",
    "# Create windows using braindecode function for this. It needs parameters to define how\n",
    "# trials should be used.\n",
    "windows_dataset = create_windows_from_events(\n",
    "    dataset,\n",
    "    trial_start_offset_samples=trial_start_offset_samples,\n",
    "    trial_stop_offset_samples=0,\n",
    "    window_size_samples=input_window_samples,\n",
    "    window_stride_samples=n_preds_per_input,\n",
    "    drop_last_window=False,\n",
    "    preload=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset\n",
    "\n",
    "This code is the same as in trialwise decoding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "splitted = windows_dataset.split('session')\n",
    "train_set = splitted['0train']  # Session train\n",
    "valid_set = splitted['1test']  # Session evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "In difference to trialwise decoding, we now should supply\n",
    "``cropped=True`` to the EEGClassifier, and ``CroppedLoss`` as the\n",
    "criterion, as well as ``criterion__loss_function`` as the loss function\n",
    "applied to the meaned predictions.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>In this tutorial, we use some default parameters that we\n",
    "    have found to work well for motor decoding, however we strongly\n",
    "    encourage you to perform your own hyperparameter optimization using\n",
    "    cross validation on your training data.</p></div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.2778\u001b[0m        \u001b[32m1.5658\u001b[0m            \u001b[35m0.2535\u001b[0m        \u001b[31m1.5302\u001b[0m  0.0006  2.5641\n",
      "      2            \u001b[36m0.2812\u001b[0m        \u001b[32m1.4328\u001b[0m            \u001b[35m0.2604\u001b[0m        1.6947  0.0006  2.7230\n",
      "      3            \u001b[36m0.3958\u001b[0m        \u001b[32m1.3827\u001b[0m            \u001b[35m0.2882\u001b[0m        \u001b[31m1.4728\u001b[0m  0.0006  2.7150\n",
      "      4            \u001b[36m0.5764\u001b[0m        \u001b[32m1.3317\u001b[0m            \u001b[35m0.3958\u001b[0m        \u001b[31m1.3813\u001b[0m  0.0006  2.6347\n",
      "      5            0.5243        \u001b[32m1.2771\u001b[0m            0.3438        1.4023  0.0006  2.6013\n",
      "      6            \u001b[36m0.6285\u001b[0m        \u001b[32m1.2335\u001b[0m            0.3819        \u001b[31m1.3541\u001b[0m  0.0006  2.7207\n",
      "      7            \u001b[36m0.6910\u001b[0m        \u001b[32m1.1925\u001b[0m            \u001b[35m0.4514\u001b[0m        \u001b[31m1.3245\u001b[0m  0.0006  2.9948\n",
      "      8            \u001b[36m0.7118\u001b[0m        \u001b[32m1.1654\u001b[0m            0.4306        1.3435  0.0006  2.5780\n",
      "      9            \u001b[36m0.7222\u001b[0m        \u001b[32m1.1275\u001b[0m            0.3993        1.3424  0.0006  2.5611\n",
      "     10            \u001b[36m0.7847\u001b[0m        \u001b[32m1.0908\u001b[0m            0.4514        1.3496  0.0005  2.5568\n",
      "     11            0.7292        \u001b[32m1.0603\u001b[0m            0.4479        1.3676  0.0005  2.8247\n",
      "     12            \u001b[36m0.8090\u001b[0m        \u001b[32m1.0280\u001b[0m            0.4410        1.3410  0.0005  2.8833\n",
      "     13            0.7431        \u001b[32m1.0108\u001b[0m            0.4514        1.3943  0.0005  2.5568\n",
      "     14            0.6910        \u001b[32m0.9736\u001b[0m            0.4514        1.4157  0.0005  2.5652\n",
      "     15            0.7222        \u001b[32m0.9522\u001b[0m            0.4201        1.4282  0.0004  2.6039\n",
      "     16            \u001b[36m0.8299\u001b[0m        \u001b[32m0.9203\u001b[0m            0.4306        1.3764  0.0004  2.8411\n",
      "     17            \u001b[36m0.8993\u001b[0m        \u001b[32m0.8918\u001b[0m            0.4479        1.3702  0.0004  2.8181\n",
      "     18            0.8854        \u001b[32m0.8680\u001b[0m            0.4271        1.4221  0.0004  2.5574\n",
      "     19            0.8854        \u001b[32m0.8355\u001b[0m            0.4167        1.4006  0.0004  2.5728\n",
      "     20            \u001b[36m0.9340\u001b[0m        \u001b[32m0.8124\u001b[0m            0.4271        1.3893  0.0003  2.6077\n",
      "     21            \u001b[36m0.9375\u001b[0m        \u001b[32m0.7962\u001b[0m            0.4375        1.4016  0.0003  2.8671\n",
      "     22            0.9340        \u001b[32m0.7758\u001b[0m            0.4201        1.4057  0.0003  2.5771\n",
      "     23            0.9167        \u001b[32m0.7515\u001b[0m            0.4271        1.4085  0.0002  2.5782\n",
      "     24            \u001b[36m0.9514\u001b[0m        \u001b[32m0.7381\u001b[0m            0.4410        1.4168  0.0002  2.5431\n",
      "     25            0.9444        \u001b[32m0.7239\u001b[0m            0.4375        1.4076  0.0002  2.7232\n",
      "     26            0.9514        \u001b[32m0.7067\u001b[0m            0.4201        1.4062  0.0002  2.9206\n",
      "     27            \u001b[36m0.9653\u001b[0m        \u001b[32m0.6971\u001b[0m            0.4167        1.4291  0.0002  2.5770\n",
      "     28            0.9583        \u001b[32m0.6883\u001b[0m            0.4375        1.4120  0.0001  2.5547\n",
      "     29            0.9618        \u001b[32m0.6705\u001b[0m            0.4306        1.4408  0.0001  2.6098\n",
      "     30            \u001b[36m0.9757\u001b[0m        \u001b[32m0.6653\u001b[0m            0.4167        1.4314  0.0001  2.7834\n",
      "     31            0.9757        \u001b[32m0.6629\u001b[0m            0.4236        1.4204  0.0001  2.8431\n",
      "     32            \u001b[36m0.9826\u001b[0m        \u001b[32m0.6596\u001b[0m            0.4201        1.4349  0.0001  2.5696\n",
      "     33            0.9757        \u001b[32m0.6430\u001b[0m            0.4132        1.4347  0.0000  2.5641\n",
      "     34            0.9792        0.6472            0.4167        1.4287  0.0000  2.5644\n",
      "     35            0.9826        \u001b[32m0.6360\u001b[0m            0.4375        1.4315  0.0000  2.9442\n",
      "     36            \u001b[36m0.9861\u001b[0m        \u001b[32m0.6353\u001b[0m            0.4340        1.4326  0.0000  2.7630\n",
      "     37            0.9861        \u001b[32m0.6347\u001b[0m            0.4340        1.4321  0.0000  2.5963\n",
      "     38            0.9861        \u001b[32m0.6340\u001b[0m            0.4410        1.4320  0.0000  2.5965\n",
      "     39            0.9861        0.6342            0.4340        1.4304  0.0000  2.5925\n",
      "     40            0.9861        \u001b[32m0.6292\u001b[0m            0.4375        1.4303  0.0000  2.8499\n"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "\n",
    "from braindecode import EEGClassifier\n",
    "from braindecode.training import CroppedLoss\n",
    "\n",
    "# These values we found good for shallow network:\n",
    "lr = 0.0625 * 0.01\n",
    "weight_decay = 0\n",
    "\n",
    "# For deep4 they should be:\n",
    "# lr = 1 * 0.01\n",
    "# weight_decay = 0.5 * 0.001\n",
    "\n",
    "batch_size = 64\n",
    "n_epochs = 40\n",
    "\n",
    "clf = EEGClassifier(\n",
    "    model,\n",
    "    cropped=True,\n",
    "    criterion=CroppedLoss,\n",
    "    criterion__loss_function=torch.nn.functional.nll_loss,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    train_split=predefined_split(valid_set),\n",
    "    optimizer__lr=lr,\n",
    "    optimizer__weight_decay=weight_decay,\n",
    "    iterator_train__shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[\n",
    "        \"accuracy\",\n",
    "        (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
    "    ],\n",
    "    device=device,\n",
    "    classes=classes,\n",
    ")\n",
    "# Model training for a specified number of epochs. `y` is None as it is already supplied\n",
    "# in the dataset.\n",
    "_ = clf.fit(train_set, y=None, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Note that we drop further in the classification error and\n",
    "    loss as in the trialwise decoding tutorial.</p></div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Extract loss and accuracy values for plotting from history object\n",
    "results_columns = ['train_loss', 'valid_loss', 'train_accuracy',\n",
    "                   'valid_accuracy']\n",
    "df = pd.DataFrame(clf.history[:, results_columns], columns=results_columns,\n",
    "                  index=clf.history[:, 'epoch'])\n",
    "\n",
    "# get percent of misclass for better visual comparison to loss\n",
    "df = df.assign(train_misclass=100 - 100 * df.train_accuracy,\n",
    "               valid_misclass=100 - 100 * df.valid_accuracy)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 3))\n",
    "df.loc[:, ['train_loss', 'valid_loss']].plot(\n",
    "    ax=ax1, style=['-', ':'], marker='o', color='tab:blue', legend=False,\n",
    "    fontsize=14)\n",
    "\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue', labelsize=14)\n",
    "ax1.set_ylabel(\"Loss\", color='tab:blue', fontsize=14)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "df.loc[:, ['train_misclass', 'valid_misclass']].plot(\n",
    "    ax=ax2, style=['-', ':'], marker='o', color='tab:red', legend=False)\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red', labelsize=14)\n",
    "ax2.set_ylabel(\"Misclassification Rate [%]\", color='tab:red', fontsize=14)\n",
    "ax2.set_ylim(ax2.get_ylim()[0], 85)  # make some room for legend\n",
    "ax1.set_xlabel(\"Epoch\", fontsize=14)\n",
    "\n",
    "# where some data has already been plotted to ax\n",
    "handles = []\n",
    "handles.append(\n",
    "    Line2D([0], [0], color='black', linewidth=1, linestyle='-', label='Train'))\n",
    "handles.append(\n",
    "    Line2D([0], [0], color='black', linewidth=1, linestyle=':', label='Valid'))\n",
    "plt.legend(handles, [h.get_label() for h in handles], fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Confusion Matrix\n",
    "\n",
    "Generate a confusion matrix as in [2]_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from braindecode.visualization import plot_confusion_matrix\n",
    "\n",
    "# generate confusion matrices\n",
    "# get the targets\n",
    "y_true = valid_set.get_metadata().target\n",
    "y_pred = clf.predict(valid_set)\n",
    "\n",
    "# generating confusion matrix\n",
    "confusion_mat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# add class labels\n",
    "# label_dict is class_name : str -> i_class : int\n",
    "label_dict = valid_set.datasets[0].window_kwargs[0][1]['mapping']\n",
    "# sort the labels by values (values are integer class labels)\n",
    "labels = [k for k, v in sorted(label_dict.items(), key=lambda kv: kv[1])]\n",
    "\n",
    "# plot the basic conf. matrix\n",
    "plot_confusion_matrix(confusion_mat, class_names=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    ".. [1] Tangermann, M., Müller, K.R., Aertsen, A., Birbaumer, N., Braun, C.,\n",
    "       Brunner, C., Leeb, R., Mehring, C., Miller, K.J., Mueller-Putz, G.\n",
    "       and Nolte, G., 2012. Review of the BCI competition IV.\n",
    "       Frontiers in neuroscience, 6, p.55.\n",
    "\n",
    ".. [2] Schirrmeister, R.T., Springenberg, J.T., Fiederer, L.D.J., Glasstetter, M.,\n",
    "       Eggensperger, K., Tangermann, M., Hutter, F., Burgard, W. and Ball, T. (2017),\n",
    "       Deep learning with convolutional neural networks for EEG decoding and visualization.\n",
    "       Hum. Brain Mapping, 38: 5391-5420. https://doi.org/10.1002/hbm.23730.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
